---
title: "Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis"
author:
- name: "Jochen Büttner"
  affiliation: "Max Planck Institute of Geoanthropology"
  email: "buettner@gea.mpg.de"
date: '2025'
bibliography: bibliography.bib
---
## Overview {.unnumbered}

This report details a novel approach for imbuing large language models (*LLMs*) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, *LLMs* derive their understanding of time implicitly from statistical patterns within training texts [@Brown2020; @Devlin2019]. However, this implicit method proves insufficient for tasks demanding precise temporal context, often leading to "recency bias" or an inability to reconcile temporally contradictory information.

To overcome this, researchers propose the "*Time Transformer*", an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities arising from temporally contradictory information within training data.

To validate this concept, engineers developed a modest *Transformer* model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the *Time Transformer*'s efficacy. Experiments involved injecting synthetic temporal drifts—both synonymic succession (e.g., replacing "rain" with "liquid sunshine") and co-occurrence changes (e.g., `rain` becoming `rain and snow`)—into the training data. The *Time Transformer* consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.

Beyond this proof of concept, the *Time Transformer* holds significant implications for historical analysis, offering a robust foundation for downstream tasks on historical data and enabling instruction-tuned models to "talk to a specific time." Whilst this architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking against explicit time-token approaches and investigating the utility of a modest, targeted encoder model. This work represents a crucial step towards developing more temporally intelligent and historically accurate language models.

## Addressing Implicit Temporal Understanding

![Slide 01](images/ai-nepi_017_slide_01.jpg)

Current large language models (*LLMs*) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora [@Brown2020]. Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.

A critical challenge arises when training data contains temporally contradictory information. Consider, for instance, two sentences: "The primary architectures for processing text through NNs are *LSTMs*" (true in 2017) and "The primary architectures for processing text through NNs are *Transformers*" (true in 2025) [@Vaswani2017; @Hochreiter1997]. Without explicit temporal context, an *LLM* treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a "recency bias," favouring more recent information in next-token prediction. Current workarounds, such as prompt engineering—inserting explicit temporal cues like "In 2017"—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge [@Liu2023]. A more robust solution necessitates an architecture that enables *LLMs* to explicitly learn and reproduce these changing patterns as a direct function of time.

## Formalising Time-Dependent Probabilities: The Time Transformer

![Slide 05](images/ai-nepi_017_slide_05.jpg)

Formalising the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, `x_n`, given a sequence of preceding tokens, `x_1, ..., x_{n-1}` [@Radford2018]. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as `p(x_n | x_1, ..., x_{n-1}, t)`. Consequently, the probability for an entire sequence of tokens uttered at a specific time `t` is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing *LLMs* largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.

To overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution `p(x_n | x_1, ..., x_{n-1}, t)`. Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, termed the "*Time Transformer*", introduces a simple yet profound modification: an additional dimension, `φ(t)`, is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, `E(x, t)`, which then serves as input to the *Transformer* architecture. Consequently, the *Transformer* processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution `p_θ(x_n | x_1, ..., x_{n-1}, t)`. The training objective remains the standard maximisation of log likelihood across all sequences [@Goodfellow2016]. This direct injection of time into each token's representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.

## Empirical Validation: Data and Architecture

![Slide 16](images/ai-nepi_017_slide_16.jpg)

To empirically validate the *Time Transformer* concept, researchers required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service's digital archive, proved an ideal choice [@UKMetOffice]. Researchers scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The tokenisation process was intentionally simplistic, neglecting sub-word tokenisation, case, and interpunctuation, resulting in a compact vocabulary of only 3,395 unique words across the entire seven-year period. An alternative dataset, *TinyStories*, was also considered for its similar characteristics, offering short, synthetically generated narratives [@Xu2023].

A modest *Transformer* architecture, termed the "*Vanilla model*", underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output [@Vaswani2017]. Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like *GPT-4*, which boasts 1.8 trillion parameters distributed across 120 layers [@OpenAI2023]. Training occurred on an HPC cluster in Munich, utilising two H100 GPUs. Remarkably, each epoch completed in just 11 seconds—a testament to the dataset's small scale and the model's compact design. The code for this implementation is publicly available on GitHub [@Büttner2025GitHub], though it was developed primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a perfect ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as "During the night, a band ...", proved indistinguishable from authentic reports, confirming the model's proficiency in capturing the underlying linguistic patterns.

## Time Transformer: Implementation and Results

![Slide 15](images/ai-nepi_017_slide_15.jpg)

Implementing the *Time Transformer* required only a minimal architectural adjustment to the previously described *Vanilla model*. Engineers reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as `(day of year - 1) / (365 - 1)`. This specific encoding was chosen to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.

The first experiment aimed to demonstrate the model's capacity for learning synthetic temporal drift through synonymic succession. Researchers injected a time-dependent replacement rule into the training data: `rain` was replaced by `liquid sunshine` according to a sigmoid probability function, transitioning from zero replacement at the year's beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of `rain` versus `liquid sunshine`. The *Time Transformer* flawlessly reproduced the injected sigmoid pattern, exhibiting `rain` predominantly early in the year and `liquid sunshine` towards the end, with the transition occurring precisely mid-year.

The second experiment explored the model's ability to learn a more complex temporal pattern: a change in co-occurrence, or the "fixation of a collocation." Here, instances of `rain` not immediately followed by `and` were synthetically replaced with `rain and snow`. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of `rain and snow` against `rain only`. The model successfully acquired this pattern, generating `rain and snow` almost exclusively in the latter part of the year, whilst early-year occurrences of `rain` (sometimes accompanied by `snow`) reflected natural January weather patterns. Furthermore, introspection into the model's attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year `rain and snow` on the presence of a "cold system," underscoring the model's capacity for intricate pattern recognition even in this modest experimental setup.

## Proof of Concept, Applications, and Challenges

![Slide 21](images/ai-nepi_017_slide_21.jpg)

This research unequivocally establishes a proof of concept: *Transformer*-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation *Time Transformer*, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to "talk to a specific time," potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.

Several promising avenues for future research emerge from this work. Benchmarking the *Time Transformer* against explicit time-token approaches will quantify its performance advantages and identify optimal use cases. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.

Nevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained *LLMs* remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to *BERT* [@Devlin2019], built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward for historical language analysis.
