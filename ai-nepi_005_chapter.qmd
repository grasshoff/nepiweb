---
title: "Genre Classification for Historical Medical Periodicals"
author: "Vera Danilova, Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, Gijs Aangenendt"
date: "Forthcoming 2025"
bibliography: bibliography.bib
format:
  html:
    toc: true
    number-sections: true
    theme: sandstone
    code-fold: false
    link-citations: true
---


# Overview {.unnumbered}

This chapter delves into the ActDisease project, exploring its objectives, the dataset of historical medical periodicals it employs, and the inherent challenges encountered during dataset digitisation. Subsequently, it details a series of genre classification experiments. These experiments encompass the motivation behind genre classification, an examination of zero-shot and few-shot classification techniques, and specific trials involving few-shot prompting with the Llama-3.1 8b Instruct model. The chapter culminates in a conclusion that synthesises the findings and their implications.

![Outline of the chapter content, detailing sections on ActDisease, Genre Classification Experiments, and Conclusion.](images/ai-nepi_005_slide_02.jpg)

## About ActDisease

The ActDisease project, an initiative funded by the European Research Council (ERC), investigates the histories of patient organisations across Europe. Central to this research are the periodicals published by these organisations in England, Germany, France, and Great Britain. These documents serve as the primary source material for understanding the evolution and impact of patient advocacy.

![Title slide of the presentation 'GENRE CLASSIFICATION FOR HISTORICAL MEDICAL PERIODICALS', ActDisease Project, listing authors and affiliation.](images/ai-nepi_005_slide_01.jpg)

### About the Project

ActDisease, an acronym for 'Acting out Disease – How Patient Organizations Shaped Modern Medicine', is an ERC-funded research endeavour. Its core purpose is to study how patient organisations in 20th-century Europe contributed to shaping disease concepts, illness experiences, and medical practices. The project focuses on ten European patient organisations from Sweden, Germany, France, and Great Britain, covering a period from approximately 1890 to 1990. The principal source materials are the periodicals, mostly magazines, produced by these patient organisations.

![Slide describing the ActDisease project: its funding, purpose, focus, and main source material, with an image of Heligoland, Germany.](images/ai-nepi_005_slide_03.jpg)

### Dataset Description

The ActDisease dataset comprises a private, recently digitised collection of patient organisation magazines. This collection encompasses materials from Germany, Sweden, France, and the United Kingdom, covering diseases such as allergy/asthma, diabetes, multiple sclerosis, lung diseases, and rheumatism/paralysis. The accompanying image displays a table that summarises the magazines by country, disease, total page count, and year coverage, amounting to 96,186 pages in total. Initial explorations reveal a diverse array of text types within these materials, with notable similarities in content across all magazines.

![Slide detailing the ActDisease Dataset with a table summarising magazines by country, disease, size, and year coverage, alongside example magazine covers.](images/ai-nepi_005_slide_04.jpg)

### Dataset Digitisation Challenges

The digitisation process for the ActDisease dataset primarily involved Optical Character Recognition (OCR) using ABBYY FineReader Server 14. Whilst this software performed well on most common layouts and fonts, several challenges persist. Complex layouts, slanted text, rare fonts, and varying scan or photograph quality continue to pose difficulties for OCR accuracy. Consequently, remaining issues include OCR errors, particularly in German and French texts, and disrupted reading order. Researchers conducted experiments on post-OCR correction of German texts using instruction-tuned generative models to address some of these problems [@DanilovaAangenendt2025].

Furthermore, OCR errors appear frequently in creative texts, such as advertisements, humour pages, and poems. A significant challenge arises from the co-occurrence of different text types within a single page—for instance, an administrative report might appear alongside an advertisement and a humour section. This heterogeneity means that conventional topic models and term counts, which do not account for such juxtapositions, are likely biased towards the most frequent text type on a page.

![Slide outlining digitization challenges, including OCR issues with complex layouts and creative texts, and showing examples of historical periodical pages.](images/ai-nepi_005_slide_05.jpg)

## Genre Classification Experiments

The inherent diversity of texts within the historical medical periodicals necessitates a robust method for distinguishing between them. Genre classification emerges as a pivotal approach to address this need.

### Motivation for Genre Classification

An examination of the ActDisease materials reveals a wide variety of text types, which, interestingly, exhibit similarities across all magazines. Different text types, such as administrative reports, advertisements, and humour sections, often appear side-by-side on the same page. This textual diversity poses a challenge for analytical methods like yearly and decade-based topic models or term counts, as these methods typically do not account for such internal heterogeneity.

![Slide highlighting challenges in analysing diverse text types within historical magazines.](images/ai-nepi_005_slide_06.jpg)

Genre, therefore, presents itself as a useful concept for differentiating kinds of text. In Language Technology, genre is often defined as a class of documents sharing a communicative purpose [@Petrenz2004; @Kessler1997]—a definition that proves highly applicable here. The ability to classify genre is crucial for exploring the data from multiple perspectives to construct historical arguments. Specifically, genre classification enables the comparative study of communicative strategies across different countries, diseases, and publications over time [@Broersma2010]. It also facilitates a more fine-grained analysis of term distributions and topic models within distinct genre groups.

![Slide explaining why genre is a useful concept for classification and its benefits for historical analysis.](images/ai-nepi_005_slide_07.jpg)

The ActDisease data showcases a rich tapestry of genres. Examples include poetry, academic reports (such as studies on the pancreas), legal documents (like deeds of covenant), and advertisements (for instance, for chocolate aimed at diabetics). Instructive messages, including recipes or medical advice, feature prominently, alongside patient organisation reports detailing meetings and activities. Narratives about patients' lives also constitute a significant portion of the content.

![Slide illustrating the variety of genres found in the ActDisease dataset, such as patient experiences, advertisements, and instructive texts.](images/ai-nepi_005_slide_09.jpg)

### Zero-Shot and Few-Shot Classification

Given the scarcity of annotated data within the ActDisease project, researchers explored both zero-shot and few-shot learning approaches for genre classification [@DanilovaSoderfeldt2025]. For zero-shot learning, key research questions focused on whether genre labels from publicly available datasets could be efficiently mapped to the project's custom labels and how performance would vary across different datasets and models. For few-shot learning, the investigation centred on how performance changes with varying training set sizes across models and whether prior fine-tuning on the full dataset could substantially enhance performance.

![Slide outlining research questions for zero-shot and few-shot learning due to limited annotated data.](images/ai-nepi_005_slide_10.jpg)

#### Genre Definition and Annotation

The project team, under the supervision of the main historian, defined the genre labels. The aim was to create labels that are useful for separating content within the ActDisease materials and sufficiently general for potential application to similar datasets. The defined genres include:

*   Academic: Research-based reports or explanations of scientific ideas (e.g., research article, report).
*   Administrative: Documents on organisational activities (e.g., meeting minutes, reports, announcements).
*   Advertisement: Promotes products or services for commercial purposes.
*   Guide: Provides step-by-step instructions (e.g., health tips, legal advice, recipes).
*   Fiction: Entertains and emotionally engages (e.g., stories, poems, humour, myths).
*   Legal: Explains legal terms and conditions (e.g., contracts, rules, amendments).
*   News: Reports recent events and developments.
*   Nonfiction Prose: Narrates real events or describes cultural/historical topics (e.g., memoir, essay, documentary).
*   QA (Question & Answer): Structured as questions with expert answers, typically from periodical sections.

![Slide presenting a table of genres and their definitions used in the ActDisease project.](images/ai-nepi_005_slide_12.jpg)

Researchers selected the paragraph as the annotation unit, merging paragraphs from the ABBYY FineReader output based on font patterns (type, size, bold, italic) within a page. Annotators sampled from two periodicals: the Swedish "Diabetes" and the German "Diabetiker Journal", specifically focusing on the first and mid-year issues for each year. A team of four historians and two computational linguists, all either native or proficient in Swedish and German, performed the annotation. Each paragraph received two annotations, achieving an average inter-annotator agreement of 0.95 Krippendorff's alpha, indicating a high level of consistency. Annotators used a structured file format, assigning hard genre labels to each paragraph.

![Slide detailing the annotation process, including the annotation unit, periodicals used, annotator team, and inter-annotator agreement.](images/ai-nepi_005_slide_14.jpg)
![Slide showing an example of the annotation file used, with columns for genre assignment.](images/ai-nepi_005_slide_15.jpg)

#### Data Splits and Distribution

For the experiments, researchers first split the annotated data into training and held-out sets, with the held-out set comprising approximately 30% of the data. For few-shot experiments, they further divided the held-out set equally and balanced it by label. Researchers excluded the 'Legal' and 'News' genres from these few-shot experiments due to insufficient training data. Researchers utilised the entire test set for zero-shot experiments.
The distribution of genres across languages (German and Swedish) in the training and held-out samples reveals some imbalances. Notably, there is a strong imbalance in 'Advertisement' and 'Nonfiction Prose' across the two languages.

![Slide presenting bar charts of genre distribution in the ActDisease training and held-out samples for German and Swedish languages.](images/ai-nepi_005_slide_16.jpg)

#### External Datasets and Genre Mapping for Zero-Shot Learning

To facilitate zero-shot experiments, researchers incorporated external datasets. These included modern datasets from previous work on automatic web genre classification: the Corpus of Online Registers of English (CORE) and the Functional Text Dimensions (FTD) dataset, both annotated at the document level. Additionally, they used a sample from Universal Dependencies (UDM) Treebanks, which contains sentence-level annotations in multiple languages.

Two annotators independently performed the genre mapping from these external datasets to the ActDisease categories. For the final mapping, researchers selected only assignments with full agreement. This process revealed that for some ActDisease genres, no directly suitable labels existed in the available external datasets. The pipeline for creating training data involved this mapping, followed by preprocessing, chunking, and sampling in several configurations based on language family and label levels (ActDisease original vs. external dataset original).

![Slide showing a table mapping ActDisease genre categories to those in CORE, UDM, and FTD datasets.](images/ai-nepi_005_slide_18.jpg)

#### Models Employed

Researchers selected multilingual encoders for these experiments, models that have demonstrated success in previous automatic genre classification tasks. The chosen models were:

*   XLM-RoBERTa [@Conneau2020]
*   mBERT (multilingual BERT) [@Devlin2019]
*   historical mBERT (hmBERT) [@Schweter2022]

BERT-like models have seen extensive use in prior work on web register and genre classification [@LepekhinSharoff2022; @KuzmanLjubesic2023; @Laippala2023]. XLM-RoBERTa is recognised as a state-of-the-art web genre classifier [@Kuzman2023]. The inclusion of hmBERT was particularly pertinent as it is pretrained on a large corpus of multilingual historical newspapers, encompassing the languages in the ActDisease dataset. mBERT was included for comparison with hmBERT, as direct comparison with XLM-RoBERTa is not straightforward. Fine-tuning these models on all configurations of the training data (derived from FTD, CORE, UDM, and a merged set) yielded a total of 48 fine-tuned models. Researchers typically average subsequent metrics across these configurations.

![Slide listing the multilingual encoder models used: XLM-Roberta, mBERT, and historical mBERT, with justifications for their selection.](images/ai-nepi_005_slide_20.jpg)
![Diagram illustrating the creation of 48 fine-tuned models from different training set configurations and base models.](images/ai-nepi_005_slide_21.jpg)

#### Zero-Shot Learning Evaluation

In evaluating zero-shot learning, the imperfect overlap between label sets necessitated an analysis of individual genres and confusion matrices to avoid potential biases. The state-of-the-art web genre classifier, X-GENRE, served as a baseline, considering only the most similar labels.

![Introduction slide for Zero-Shot Learning Evaluation.](images/ai-nepi_005_slide_22.jpg)

Overall, models fine-tuned on the Functional Text Dimensions (FTD) dataset, using the established mapping, performed better. In most FTD configurations, researchers observed no systematic bias, and per-genre metrics were quite good. An interesting observation emerged: on certain datasets, some models handled specific genres much more effectively than others on average. For instance, XLM-RoBERTa demonstrated superior prediction of 'QA' (Question & Answer) texts compared to other models when fine-tuned on UDM. Conversely, hmBERT, when fine-tuned on UDM, showed a 16% average increase in correct 'Administrative' predictions over XLM-RoBERTa and mBERT. Models based on the CORE dataset proved adept at predicting the 'Legal' genre. However, researchers noted class-specific biases in other datasets: UDM fine-tuning tended towards 'News' (as the 'News' training data had the highest number of Germanic instances, mostly German), whilst CORE fine-tuning leaned towards 'Guide' (as only 'Guide' training data in CORE was multilingual).

![Slide summarising key results from zero-shot learning, highlighting performance with FTD and specific model-dataset-genre strengths.](images/ai-nepi_005_slide_24.jpg)

Confusion matrices for specific configurations illustrate this behaviour. For example, hmBERT fine-tuned on UDM (hmbert_UDM_True_True) shows strong performance for 'Administrative'. XLM-RoBERTa fine-tuned on CORE (xlmr_CORE_True_False) effectively identifies 'Legal' and 'Academic' texts. XLM-RoBERTa fine-tuned on UDM (xlmr_UDM_False_False) excels with 'QA'. Finally, XLM-RoBERTa fine-tuned on FTD (xlmr_FTD_False_False) accurately classifies 'Legal' texts.

![Slide displaying four confusion matrices for different model configurations in zero-shot learning, highlighting specific genre prediction strengths.](images/ai-nepi_005_slide_25.jpg)

The table below presents detailed average F1 scores per category, averaged across data configurations. Highlighted values in the original presentation (not reproduced here as bold text) indicate performance that is not a result of systematic biases towards those categories. Notably, models fine-tuned on FTD and CORE show strong F1 scores for the 'Legal' genre. hmBERT (UDM) performs well for 'Administrative', and XLM-RoBERTa (UDM) for 'QA'.

![Table of zero-shot per-category F1 scores averaged across data configurations for different models and datasets.](images/ai-nepi_005_slide_26.jpg)

Analysis of average performance across different training configurations (balancing strategies, language family inclusion) for each external dataset (FTD, CORE, UDM) reveals nuances. For FTD, balancing by original labels alongside ActDisease labels ([B2]) or including only Germanic languages ([G+]) decreased performance compared to balancing by ActDisease labels alone ([B1]) or including all language families ([G-]). For CORE, the small number of Finnish and French instances (in the 'Guide' genre) slightly decreased performance. For UDM, the presence of other language families and balancing generally improved performance in terms of macro F1.

![Slide showing average F1 scores for different training configurations within FTD, CORE, and UDM datasets.](images/ai-nepi_005_slide_27.jpg)

#### Few-Shot Learning Evaluation

The investigation then turned to few-shot learning scenarios.

![Introduction slide for Few-Shot Learning Evaluation.](images/ai-nepi_005_slide_28.jpg)

Experiments demonstrated how models performed with varying training data sizes, both with and without prior Masked Language Model (MLM) fine-tuning on the entire ActDisease dataset. This prior MLM fine-tuning (+MLM) proved clearly advantageous. F1 scores generally increased with the number of training instances, although they remained below 0.8 even with 1182 instances. Notably, hmBERT-MLM (the historical model with prior fine-tuning) outperformed other models, particularly at larger dataset sizes, boosting its performance significantly and even surpassing other models by a small margin.

![Line graph showing few-shot learning performance (F1 score vs. dataset size) for different models with and without MLM fine-tuning.](images/ai-nepi_005_slide_30.jpg)

A detailed examination of scores revealed that hmBERT-MLM's superior performance is largely attributable to its sustained ability to differentiate between 'Fiction' and 'Nonfiction Prose' as dataset size increases. In contrast, other models, especially XLM-RoBERTa-MLM, exhibited a drastic drop in performance for 'Fiction' when using the full-sized training dataset (1182 instances), often over-predicting 'Nonfiction Prose' for 'Fiction' instances. Both these genres in the ActDisease data frequently contain narratives about patient experiences, particularly concerning diabetes. It is plausible that with a larger data size, the linguistic features of these two genres become more similar, especially as they are confined to the specific domain of patient organisation magazines focused on diabetes and often share themes and narrative structures. This suggests that more data, or perhaps more nuanced features, might be necessary to improve discrimination between these closely related genres.

![Table showing few-shot learning per-category F1 scores and overall metrics for different models at training sizes of 500 and 1182 instances.](images/ai-nepi_005_slide_31.jpg)
![Confusion matrix for XLM-Roberta-MLM with the full-sized training dataset, illustrating confusion between Fiction and Nonfiction Prose.](images/ai-nepi_005_slide_32.jpg)

### Few-Shot Prompting Llama-3.1 8b Instruct

Recognising the limitations of available data for extensive instruction tuning, researchers also explored few-shot prompting with Llama-3.1 8b Instruct, a prominent multilingual generative model with open weights. The prompt structure incorporated genre definitions and two to three carefully selected examples for each genre. The instruction guided the model to label input text with one of the defined genres based on its perceived purpose and content.

![Slide illustrating the prompt structure used for few-shot prompting of Llama-3.1 8b Instruct, including genre definitions and example placeholders.](images/ai-nepi_005_slide_33.jpg)

The results from few-shot prompting Llama-3.1 8b Instruct on the zero-shot test set (the entire held-out set) indicate that the model handles certain labels reasonably well. For instance, 'Legal' texts achieved an F1-score of 0.84, and 'Academic' and 'Advertisement' texts scored 0.72 and 0.73, respectively. However, the provision of only two or three examples proved insufficient for the model to adequately represent and distinguish more nuanced genres such as 'Nonfiction Prose' (F1-score 0.49), 'Administrative' (F1-score 0.60), and 'News' (F1-score 0.08). The overall macro average F1-score was 0.59. The confusion matrix reveals particular difficulties in distinguishing 'Nonfiction Prose' from 'Fiction' and 'Administrative' texts, and 'Advertisement' from 'Administrative' texts.

![Slide presenting results (F1 scores and confusion matrix) for few-shot prompting of Llama-3.1 8b Instruct.](images/ai-nepi_005_slide_34.jpg)

## Conclusion

Historical periodicals, particularly popular magazines, represent a promising yet challenging source for research into the history of science and medicine. Their genre-rich nature reflects diverse communicative strategies employed over time. Accurately accounting for these genres is crucial for the detailed interpretation of text mining results.

This exploration demonstrates that genre classification can significantly enhance the accessibility of such complex historical sources for computational analysis. When faced with no training data, researchers can successfully leverage available modern datasets, provided the genre categories are sufficiently general-purpose. Alternatively, few-shot prompting of capable open generative models, like Llama-3.1 8b Instruct, can achieve decent quality for some genres, although performance may be limited for categories requiring more nuanced understanding with minimal examples.

However, if some annotated data is available, even in limited quantities, few-shot learning with multilingual encoders—such as XLM-RoBERTa or, notably, historical multilingual BERT (hmBERT)—especially when combined with prior Masked Language Model (MLM) fine-tuning on the target domain data, emerges as a superior strategy. For the ActDisease project, this approach yielded the most promising results, with hmBERT-MLM showing considerable gains in performance.

![Slide summarising the main conclusions regarding genre richness in popular magazines and effective strategies for genre classification.](images/ai-nepi_005_slide_35.jpg)

Ongoing and future efforts aim to further refine these methodologies and apply them to specific historical hypotheses. This includes developing a new annotation scheme with more fine-grained genres, an annotation project financed by Swe-CLARIN, exploring synthetic data generation techniques, and implementing active learning strategies to improve classifier quality efficiently. These endeavours seek to enhance the utility of these methods for both the ActDisease project and the broader digital humanities community.

![Slide outlining future and present work, including working with historical hypotheses, new annotation schemes, and advanced machine learning techniques.](images/ai-nepi_005_slide_36.jpg)

---
### Acknowledgements {.unnumbered}

The project team extends its gratitude to the annotators: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. We also thank Dr Maria Skeppstedt and the anonymous reviewers for their valuable feedback. This research received funding from the European Research Council (ERC-2021-STG, 101040999). The Centre for Digital Humanities and Social Sciences at Uppsala University provided essential support in the form of GPUs and data storage.

![Slide listing acknowledgements to the project team, reviewers, European Research Council, and Centre for Digital Humanities and Social Sciences.](images/ai-nepi_005_slide_37.jpg)

For further information, please visit the project website.

![Thank you slide with a QR code.](images/ai-nepi_005_slide_38.jpg)
