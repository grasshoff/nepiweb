---
title: "Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections"
author:
- name: "Arno Simons"
  affiliation: "AI-NEPI Conference Participant"
date: '2025'
bibliography: bibliography.bib
---
## Overview {.unnumbered}

This presentation systematically introduces the foundational architecture of large language models (*LLMs*), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (*HPSS*). As a co-organiser, the speaker initially provides a primer on the seminal *Transformer* architecture, explaining its encoder-decoder structure and original purpose in language translation.

Subsequently, the discussion differentiates between encoder-based models, such as *BERT*, which offer bidirectional full-context understanding, and decoder-based generative models, like *GPT*, capable of producing novel text. The presentation then charts the proliferation of domain-specific *LLMs* across various scientific fields, outlining diverse adaptation strategies including pre-training, fine-tuning, and the sophisticated *Retrieval Augmented Generation* (*RAG*) pipeline.

Crucially, the presentation categorises current *LLM* applications in *HPSS*, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, it offers critical reflections on *HPSS*-specific challenges, such as historical language evolution and sparse data. The speaker advocates for enhanced *LLM* literacy and a steadfast adherence to *HPSS* methodologies, highlighting new opportunities for bridging qualitative and quantitative research.

## The Foundational *Transformer* Architecture

![Slide 01](images/ai-nepi_003_slide_01.jpg)

At the core of all contemporary large language models (*LLMs*) lies the seminal *Transformer* architecture, pioneered by Vaswani et al. in 2017 [@Vaswani2017]. Originally designed for language translation, such as German to English, this architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.

The encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence's meaning. This allows the model to understand the full context of the input.

Conversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness and enabling more nuanced language processing.

## Differentiating *BERT* and *GPT* Models

![Slide 03](images/ai-nepi_003_slide_03.jpg)

Immediately following the *Transformer*'s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (*NLP*) tasks with minimal additional training.

From the encoder side, the *BERT* family of models emerged, standing for Bidirectional Encoder Representations from *Transformers*. *BERT* operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. This enables *BERT* to excel at tasks requiring deep contextual comprehension, such as sentiment analysis or question answering.

Conversely, the decoder side gave rise to the *GPT* models, or Generative Pre-trained *Transformers*, which now power applications like *ChatGPT*. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text. This generative function is not inherently present in *BERT*-like models.

Consequently, a fundamental distinction arises: generative models, exemplified by *GPT*, primarily produce language, whereas full-context models, such as *BERT*, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like *XLM* and *XLNet*.

## Scientific *LLM* Evolution and Adaptation Strategies

![Slide 06](images/ai-nepi_003_slide_06.jpg)

A comprehensive overview reveals the rapid evolution of large language models (*LLMs*), particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to *BERT*, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included *BioBERT*, *Specter*, and *SciBERT*. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.

Researchers employ several methods to adapt these models to specific scientific language. The initial phase is *pre-training*, where a model learns language by predicting the next token (a word or sub-word unit), as in *GPT* models, or by predicting randomly masked words, characteristic of *BERT* models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many.

Consequently, *continued pre-training* offers a viable alternative. Researchers utilise an already pre-trained model and subsequently train it on domain-specific language. For instance, a general *BERT* model can be adapted for physics texts by further training it on a large corpus of physics literature.

Beyond this, engineers can add extra layers atop pre-trained models, effectively fine-tuning them for specific classification tasks like sentiment analysis or named entity recognition. Crucially, *contrastive learning* emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with *Sentence BERT* serving as a widely adopted example for creating semantically meaningful sentence representations.

## *Retrieval Augmented Generation* and Key Distinctions

![Slide 10](images/ai-nepi_003_slide_10.jpg)

*Retrieval Augmented Generation* (*RAG*) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins *ChatGPT*'s internet search capabilities, allowing it to access and synthesise information beyond its initial training data.

The *RAG* process commences with a user query, such as "What are *LLMs*?". Subsequently, a *BERT*-type model encodes this query into a sentence embedding, a numerical representation of its meaning. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the *RAG* pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context. This approach significantly reduces the likelihood of "hallucinations" often associated with standalone *LLMs*.

Beyond *RAG*, advanced reasoning models and agents are emerging. These are not isolated *LLMs* but rather intricate systems that combine *LLMs* with a diverse array of other tools, enabling more complex problem-solving and decision-making.

Consequently, a clear understanding of key distinctions proves crucial for navigating the *LLM* landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone *LLMs*, complex pipelines like *RAG*, and sophisticated agents becomes paramount for effective application and responsible deployment.

## Applications and Trends in *HPSS* Research

![Slide 13](images/ai-nepi_003_slide_13.jpg)

A current survey explores the burgeoning uses of large language models (*LLMs*) as tools within history, philosophy, and sociology of science (*HPSS*) research. This investigation has identified four primary categories for these applications:

*   *LLMs* assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations. For example, they can efficiently identify and categorise funding sources in large corpora of scientific papers.
*   They contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields. This could involve identifying key concepts and their relationships within historical scientific texts.
*   *LLMs* illuminate knowledge dynamics, particularly through the study of conceptual histories of words. By tracking semantic shifts over time, researchers can trace the evolution of scientific concepts.
*   They support the analysis of knowledge practices, including citation context analysis—an older *HPSS* tradition now also employed for evaluatory purposes. This allows for a deeper understanding of how scientific ideas are built upon and referenced.

A notable trend indicates an accelerating interest in *LLMs*, with findings predominantly appearing in information science journals like *Scientometrics* and the *Journal of the Association for Information Science and Technology* (*JASIST*). Increasingly, however, papers featuring *LLM* applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers, bridging disciplinary divides. Furthermore, the degree of customisation in *LLM* deployment varies widely, spanning from straightforward off-the-shelf use of *ChatGPT* to the development of entirely new model architectures tailored for specific *HPSS* questions.

Despite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models (the "black box" problem), and persistent shortages of training data and benchmarks specific to *HPSS* domains. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by *BERTopic*, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.

## Critical Reflections and Future Directions for *HPSS*

![Slide 12](images/ai-nepi_003_slide_12.jpg)

Crucially, scholars must acknowledge the specific challenges inherent to *HPSS* when engaging with large language models (*LLMs*). Foremost amongst these is the historical evolution of concepts and language. Models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models on historical corpora or the judicious use of existing ones with a keen awareness of their limitations.

Furthermore, *HPSS* adopts a reconstructive and critically reflective perspective, often reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive *HPSS* "reading." Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts, which pose significant hurdles for *LLM* application.

Consequently, building robust *LLM* literacy becomes imperative for *HPSS* researchers. They must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing (*NLP*) may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.

Ultimately, *HPSS* researchers must remain true to their established methodologies. This involves carefully translating complex *HPSS* problems into specific *NLP* tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon *HPSS*'s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s [@Callon1986; @Rip1986], offers valuable theoretical grounding, particularly given the resonance of *Actor-Network Theory* (*ANT*) concepts with contemporary *LLM* developments.
