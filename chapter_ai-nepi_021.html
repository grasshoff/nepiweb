<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Schlattmann, Aleksandra Kaye &amp; Malte Vogl">
<meta name="dcterms.date" content="2025-01-01">

<title>19&nbsp; From Source to Structure: Extracting Knowledge Graphs with LLMs – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./chapter_ai-nepi_020.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-09b140d2d032adf2aedb8b099be3ee13.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_021.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_001_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_003_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_004_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Workflow and Utility of OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_005_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_006_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_007_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_008_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science Dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of <em>LDA</em> and <em>BERTopic</em> Performance across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Beyond Traditional Views of Science Funding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs" id="toc-transforming-unstructured-biographical-data-into-queryable-knowledge-graphs" class="nav-link" data-scroll-target="#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs"><span class="header-section-number">19.1</span> Transforming Unstructured Biographical Data into Queryable Knowledge Graphs</a></li>
  <li><a href="#illustrative-extraction-structuring-polish-biographical-entries" id="toc-illustrative-extraction-structuring-polish-biographical-entries" class="nav-link" data-scroll-target="#illustrative-extraction-structuring-polish-biographical-entries"><span class="header-section-number">19.2</span> Illustrative Extraction: Structuring Polish Biographical Entries</a></li>
  <li><a href="#pipeline-architecture-and-guiding-principles-for-knowledge-extraction" id="toc-pipeline-architecture-and-guiding-principles-for-knowledge-extraction" class="nav-link" data-scroll-target="#pipeline-architecture-and-guiding-principles-for-knowledge-extraction"><span class="header-section-number">19.3</span> Pipeline Architecture and Guiding Principles for Knowledge Extraction</a></li>
  <li><a href="#stage-1-workflow-open-information-extraction-in-detail" id="toc-stage-1-workflow-open-information-extraction-in-detail" class="nav-link" data-scroll-target="#stage-1-workflow-open-information-extraction-in-detail"><span class="header-section-number">19.4</span> Stage 1 Workflow: Open Information Extraction in Detail</a></li>
  <li><a href="#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail" id="toc-stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail" class="nav-link" data-scroll-target="#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail"><span class="header-section-number">19.5</span> Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail</a></li>
  <li><a href="#application-case-study-analysing-zielińskis-polish-biographical-compilations" id="toc-application-case-study-analysing-zielińskis-polish-biographical-compilations" class="nav-link" data-scroll-target="#application-case-study-analysing-zielińskis-polish-biographical-compilations"><span class="header-section-number">19.6</span> Application Case Study: Analysing Zieliński’s Polish Biographical Compilations</a></li>
  <li><a href="#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr" id="toc-application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr" class="nav-link" data-scroll-target="#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr"><span class="header-section-number">19.7</span> Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”</a></li>
  <li><a href="#achievements-current-challenges-and-future-trajectories" id="toc-achievements-current-challenges-and-future-trajectories" class="nav-link" data-scroll-target="#achievements-current-challenges-and-future-trajectories"><span class="header-section-number">19.8</span> Achievements, Current Challenges, and Future Trajectories</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Raphael Schlattmann, Aleksandra Kaye &amp; Malte Vogl <a href="mailto:raphael.schlattmann@tu-berlin.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Technische Universität Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Researchers Malte Vogl, Raphael Schlattmann, and Alex Kaye confront the formidable challenge of extracting structured knowledge from historically significant, yet computationally intractable, unstructured biographical sources. Their pioneering work introduces a novel method to transform such materials—typically printed books or dictionaries—into readily queryable knowledge graphs.</p>
<p>This innovative approach transcends the conventional reliance on pre-structured datasets, prevalent within History and Philosophy of Science (<em>HPSS</em>). Instead, it strategically employs <em>Large Language Models</em> (<em>LLMs</em>) not as infallible oracles, but as integral components within a meticulously engineered pipeline. The primary objective involves imposing rigorous structure upon unstructured data, such as Polish and German biographical dictionaries, thereby facilitating complex historical inquiries. These investigations might, for instance, explore the formation of intellectual networks or trace the evolution of professional connections over time.</p>
<p>The team expertly utilises tools like <em>Neo4j</em> for graph representation, having developed a sophisticated multi-stage pipeline that incorporates crucial human oversight to ensure both accuracy and relevance. Ultimately, this methodology enables the construction of controllable knowledge graphs, where entities—individuals, locations, organisations, and publications—become distinct nodes, and their interconnections, meticulously derived from textual evidence, form the relational edges.</p>
</section>
<section id="transforming-unstructured-biographical-data-into-queryable-knowledge-graphs" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="transforming-unstructured-biographical-data-into-queryable-knowledge-graphs"><span class="header-section-number">19.1</span> Transforming Unstructured Biographical Data into Queryable Knowledge Graphs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>Researchers confront a significant challenge in historical studies: many invaluable biographical sources persist in unstructured textual formats, such as printed dictionaries and compendia. Whilst existing History and Philosophy of Science (<em>HPSS</em>) datasets often comprise structured information—for instance, publication databases or email archives—these unstructured materials, despite their rich detail, resist computational analysis. Consequently, this project pioneers a methodology to systematically impose structure upon such data, specifically targeting biographical entries. The core idea leverages <em>Large Language Models</em> (<em>LLMs</em>), not as standalone solutions, but as crucial components within a larger, controllable pipeline engineered to construct knowledge graphs.</p>
<p>Historically, tools like <em>Get Grass</em> facilitated access to printed materials for digitisation. The current approach, however, aims for a deeper level of structuration. It conceptualises biographical information as a knowledge graph: a network where entities—individuals, geographical locations, countries, published works, or organisations—constitute the nodes. The relationships between these entities, as described in the source texts, form the connecting edges. Such a graph, once constructed, permits complex, structured queries. For instance, one might investigate how professional networks evolved within a specific discipline during a particular era, or trace the contacts an individual established over their career.</p>
<p>The team employs <em>LLMs</em> selectively, focusing on their utility for specific tasks within a broader information processing chain, rather than pursuing an elusive ‘perfect’ model. Source materials include Polish biographical collections and German-language resources such as the handbook <em>Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien</em>. An entry from the latter, for Alexander Abusch, might detail his role as Minister für Kultur alongside birth and death dates. An extraction pipeline processes these text-based entries, often derived from scanned documents, transforming them into a visual and queryable graph, potentially managed in a system like <em>Neo4j</em>. This process thus converts isolated, albeit information-dense, textual accounts into a connected, structured dataset ripe for scholarly exploration.</p>
</section>
<section id="illustrative-extraction-structuring-polish-biographical-entries" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="illustrative-extraction-structuring-polish-biographical-entries"><span class="header-section-number">19.2</span> Illustrative Extraction: Structuring Polish Biographical Entries</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>To illustrate the extraction methodology, consider a Polish biographical entry for Bartsch Henryk, an evangelical priest (<em>ks. ewang.</em>). The text records his birth on 12th December 1832 in Wladyslawowo, situated within the Konin district. Furthermore, it details his extensive travels to Italy (<em>Włochy</em>), Greece (<em>Grecja</em>), the Holy Land (<em>Ziemia Święta</em>), and Egypt (<em>Egipt</em>). His scholarly contributions encompass several publications: <em>Wspomnienia z podróży do Jerozolimy i Kairo</em> (Warsaw, 1873), <em>Listy z podróży po Grecji i Sycylji</em> (Warsaw, 1874), and <em>Z teki podróżniczej, szkice dawne i nowe oryginalne i tłumaczone</em> (Warsaw, 1883). The entry itself cites Bystron’s <em>Wielka Encyklopedja</em> as a source.</p>
<p>From this concise textual snippet, the system extracts key entities and their relationships. ‘Bartsch Henryk’ is identified as a PERSON, his ROLE as ‘<em>ks. ewang.</em>’, his birthdate as ‘12. XII. 1832’ (a DATE), and ‘Wladyslawowo’ along with the countries he visited as LOCATIONs. Relationships such as ‘born on’, ‘born in’, ‘located in’, and ‘travelled to’ link these entities. Consequently, this process yields a set of structured triples: (Bartsch Henryk, is a, <em>ks. ewang.</em>), (Bartsch Henryk, travelled to, <em>Włochy</em>), and so forth, effectively translating narrative information into a machine-readable format suitable for knowledge graph construction.</p>
</section>
<section id="pipeline-architecture-and-guiding-principles-for-knowledge-extraction" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="pipeline-architecture-and-guiding-principles-for-knowledge-extraction"><span class="header-section-number">19.3</span> Pipeline Architecture and Guiding Principles for Knowledge Extraction</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_08.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>Engineers have designed a sophisticated two-stage pipeline to transform raw textual data into structured knowledge graphs. This architecture systematically processes information, integrating both automated techniques and human expertise. The first stage, ‘Ontology-agnostic Open Information Extraction’ (<em>OIE</em>), focuses on identifying factual statements. It commences by loading and chunking the source data, then proceeds to <em>OIE</em> extraction, validates these extractions, and standardises them against benchmarks. A critical quality control checkpoint determines the sufficiency of the <em>OIE</em> output; insufficient output triggers a manual correction loop for a sample of triples.</p>
<p>Subsequently, the second stage, ‘Ontology-driven Knowledge Graph (<em>KG</em>) building’, refines and structures this information. This stage commences by formulating Competency Questions (<em>CQs</em>) that define the <em>KG</em>’s desired analytical capabilities. Based on these <em>CQs</em>, developers create an ontology and define <em>SHACL</em> (Shapes Constraint Language) shapes for validation. The pipeline then maps the extracted triples to this ontology, disambiguates entities (potentially linking them to <em>Wikidata IDs</em>), represents the data using <em>RDF-star</em>, and subsequently performs <em>SHACL</em> validation. Similar to Stage 1, a quality control loop allows for manual correction of <em>CQs</em>, the ontology, or <em>SHACL</em> shapes if necessary. Both stages integrate layers for <em>LLM</em> interaction and crucial ‘Human in the Loop’ interventions, ensuring robust and reliable outcomes.</p>
<p>Several core principles underpin this pipeline’s design. It is fundamentally research-driven and data-oriented; thus, ontology development directly addresses specific research questions and aligns with the data’s realistic provisions. The human-in-the-loop paradigm strategically combines <em>LLM</em> automation for efficiency with expert oversight at all critical junctures, balancing scale with quality. Transparency is paramount; researchers design each step for verifiability. Furthermore, task decomposition breaks the complex process into manageable, sequential units—notably performing <em>OIE</em> before aligning with an ontology. Finally, modularity ensures the system can adapt, allowing for the integration of improved models or components as technology evolves.</p>
</section>
<section id="stage-1-workflow-open-information-extraction-in-detail" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="stage-1-workflow-open-information-extraction-in-detail"><span class="header-section-number">19.4</span> Stage 1 Workflow: Open Information Extraction in Detail</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_09.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>The initial stage of the pipeline, <em>Open Information Extraction</em> (<em>OIE</em>), meticulously processes raw biographical texts to identify factual assertions. It commences by loading and chunking data: the system ingests pre-processed files containing biographical narratives and segments them into manageable units. This step yields semi-structured data, often organised tabularly, with each row corresponding to a text chunk and including identifiers such as name, role, the biographical snippet itself, and a unique chunk ID.</p>
<p>Following this preparation, <em>OIE</em> extraction commences. For each chunk—comprising, for example, a person’s name such as ‘Havemann, Robert’ and a segment of their biography like ‘… 1935 Prom. mit … an der Univ. Berlin…’—the system attempts to extract factual statements. This process generates raw Subject-Predicate-Object (<em>SPO</em>) triples, enriched with pertinent metadata such as an associated timeframe and a confidence score indicating the extraction’s reliability.</p>
<p>Subsequently, <em>OIE</em> validation scrutinises these raw triples. The original text chunk and its corresponding extracted triples serve as input. Human experts, or potentially other specialised <em>LLMs</em>, then assess the accuracy and relevance of each triple against the source text. This critical review produces a set of validated <em>SPO</em> triples. Finally, the <em>OIE</em> Standard step evaluates the overall quality of this extraction phase. Researchers compare the validated triples against a ‘Gold Standard’—a reference set of triples meticulously created or verified by domain experts. This comparison yields key performance indicators such as F1-score, Precision, and Recall, offering a quantitative measure of the <em>OIE</em> process’s success.</p>
</section>
<section id="stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail"><span class="header-section-number">19.5</span> Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>The second stage of the pipeline focuses on building the knowledge graph in an ontology-driven manner, ensuring the final structure aligns precisely with research objectives. This stage commences by formulating Competency Questions (<em>CQs</em>). Drawing upon a sample of validated triples from Stage 1, experts define the specific analytical questions the knowledge graph must be capable of answering. This process yields a set of manually refined <em>CQs</em> that guide subsequent development.</p>
<p>Next, developers create the ontology. Using the <em>CQs</em> and the sample of validated triples as inputs, they design a formal ontology. This ontology specifies the classes of entities (e.g., Person, Organisation, Event), the properties these entities can possess, and the types of relationships that can exist between them, all meticulously tailored to address the <em>CQs</em>. This process yields a comprehensive Ontology Definition.</p>
<p>With the ontology established, ontology mapping commences. The pipeline processes the validated triples from Stage 1, mapping their constituent subjects, predicates, and objects to the corresponding classes and properties within the defined ontology. This step transforms the relatively raw triples into conceptual <em>RDF</em> (Resource Description Framework) statements. Finally, disambiguation and <em>Wikidata ID</em> linking refine the graph. This crucial step involves resolving ambiguities in entity references—for instance, ensuring that different individuals sharing the same name are correctly distinguished. The system links entities to external identifiers, such as <em>Wikidata IDs</em>, where feasible. This phase also incorporates <em>RDF-star</em> statement generation, allowing annotations or contextual information to attach directly to individual triples, thereby enriching the graph’s expressive power. This process yields a set of disambiguated triples and <em>RDF-star</em> statements, ready for <em>SHACL</em> validation and subsequent querying.</p>
</section>
<section id="application-case-study-analysing-zielińskis-polish-biographical-compilations" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="application-case-study-analysing-zielińskis-polish-biographical-compilations"><span class="header-section-number">19.6</span> Application Case Study: Analysing Zieliński’s Polish Biographical Compilations</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_15.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>Researchers applied the knowledge graph extraction pipeline to a set of significant Polish historical sources: three complementary compilations by Stanisław Zieliński. These encompass <em>Mały słownik pionierów polskich kolonjalnych i morskich</em> (1933), a biographical dictionary of colonial and maritime pioneers; <em>Bibljografja czasopism polskich zagranicą, 1830-1934</em> (1935), a bibliography of Polish periodicals published abroad; and <em>Wybitne czyny Polaków na obczyźnie</em> (1935), a record of notable Polish achievements internationally.</p>
<p>The structured data extracted from these volumes enables the exploration of several nuanced research questions. For instance, the knowledge graph facilitates the identification of individuals or communities whose pivotal roles in developing ideas and practices might have been obscured by dominant historical narratives. It allows for an analysis of shifts in migration patterns, such as those occurring before and after the January Uprising of 1863. Furthermore, investigators can examine the function of specific journals, determining if they served as ‘boundary objects’ linking diverse intellectual or professional circles, or which publications proved most central to the communities of practice amongst Polish migrants.</p>
<p>Initial analysis of the Zieliński data yielded a substantial social network graph containing 3,598 nodes and 5,443 edges. Visualisations of this network distinguish editors (coloured green) from other individuals (coloured pink), offering a preliminary glimpse into the relational structures embedded within these historical texts.</p>
</section>
<section id="application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr" class="level2" data-number="19.7">
<h2 data-number="19.7" class="anchored" data-anchor-id="application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr"><span class="header-section-number">19.7</span> Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_17.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>Another significant application of the extraction methodology involves the German biographical lexicon, <em>Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien</em>. This extensive work, first published in the 1980s by the Bundesstiftung zur Aufarbeitung der SED-Diktatur and subsequently digitised in the 2000s, profiles approximately 4,000 prominent East German figures from diverse fields including politics, science, culture, and sports. Containing entries for individuals such as Gustav Hertz and Robert Havemann, it serves as an indispensable resource for researchers and journalists seeking to understand the complex legacy of the German Democratic Republic.</p>
<p>The structured data derived from this lexicon enables quantitative analyses of historical patterns. One such analysis, visually presented as a scatter plot, investigates the relationship between state award recipients, high-ranking positions, and affiliation with the Socialist Unity Party (<em>SED</em>). The plot maps individuals based on their award status (e.g., <em>Karl-Marx-Orden</em>, <em>Nationalpreis der DDR</em>) against their rate of holding high positions and their <em>SED</em> affiliation rate. Comparative statistics reveal striking correlations: for instance, 95.0% of the 38 <em>Karl-Marx-Orden</em> recipients were <em>SED</em> members, compared to only 38.5% of the 1,056 individuals in the sample without this award. Recipients of the <em>Karl-Marx-Orden</em> also held a significantly higher share of high positions (65.8%) compared to those with no awards (28.0%), and an average birth year of 1905.9, substantially earlier than the 1923.0 average for non-recipients. Further detailed breakdowns show that 100% of <em>Karl-Marx-Orden</em> recipients who held Politbüro positions were members, highlighting the award’s strong ties to the highest echelons of power.</p>
</section>
<section id="achievements-current-challenges-and-future-trajectories" class="level2" data-number="19.8">
<h2 data-number="19.8" class="anchored" data-anchor-id="achievements-current-challenges-and-future-trajectories"><span class="header-section-number">19.8</span> Achievements, Current Challenges, and Future Trajectories</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_20.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>The project has successfully demonstrated a significant advancement: the transformation of isolated biographical entries into a resource capable of supporting complex structural queries. This marks a crucial step towards unlocking the rich, latent information within historical texts. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques to ensure greater accuracy and in enhancing benchmarking methodologies to rigorously assess pipeline performance.</p>
<p>Looking ahead, researchers will immediately complete and finalise the current pipeline’s development. Subsequently, a systematic comparison of its outputs against alternative pipelines and existing software packages will provide valuable performance context. A key objective involves scaling the entire process to analyse full datasets, thereby enabling more comprehensive historical investigations.</p>
<p>Beyond these immediate goals, future ambitions include fine-tuning the pipeline for highly specific research use cases. The team also plans to explore the potential of <em>GraphRAG</em> (Graph Retrieval Augmented Generation), which could allow users to query the knowledge graphs using natural language. Furthermore, the team expresses interest in constructing multilayered networks, possibly employing frameworks such as <em>ModelSEN</em> (Multilayer Social-Epistemic Networks), to facilitate even deeper structural analyses of the complex relationships within the historical data. Interested parties may contact Raphael Schlattmann, Alex Kaye, or Malte Vogl via their provided email addresses.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_020.html" class="pagination-link" aria-label="Beyond Traditional Views of Science Funding">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Beyond Traditional Views of Science Funding</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>