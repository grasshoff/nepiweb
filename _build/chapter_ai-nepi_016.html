<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Francis Lareau &amp; Christophe Malaterre">
<meta name="dcterms.date" content="2025-01-01">

<title>14&nbsp; Text Granularity and Topic Model Performance – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_017.html" rel="next">
<link href="./chapter_ai-nepi_015.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-fe5eeb5af71a333b155c360431d06b9a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e463572c889c87c7eefd27e1777fa793.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="14&nbsp; Text Granularity and Topic Model Performance – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta property="og:description" content="This study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches, Latent Dirichlet Allocation (LDA) and BERTopic. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels. A corpus of scientific articles in Astrobiology serves as the material. Six topic models…">
<meta property="og:image" content="images/ai-nepi_016_slide_01.jpg">
<meta property="og:site_name" content="AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:title" content="14&nbsp; Text Granularity and Topic Model Performance – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:description" content="This study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches, Latent Dirichlet Allocation (LDA) and BERTopic. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels. A corpus of scientific articles in Astrobiology serves as the material. Six topic models…">
<meta name="twitter:image" content="images/ai-nepi_016_slide_01.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_016.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper: Transdisciplinary Investigations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computational HPSS: Tracing Ancient Wisdom’s Influence with VERITRACE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and Scientific Insights in Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems in Philosophy and HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural pursuit across scales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">LLMs for Chemical Knowledge Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Interpretable Models for Linguistic Change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">LLM for HPS Studies: Analyzing the NHGRI Archive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">14.1</span> Overview</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">14.2</span> Introduction</a></li>
  <li><a href="#study-design" id="toc-study-design" class="nav-link" data-scroll-target="#study-design"><span class="header-section-number">14.3</span> Study Design</a></li>
  <li><a href="#topic-modeling-approaches" id="toc-topic-modeling-approaches" class="nav-link" data-scroll-target="#topic-modeling-approaches"><span class="header-section-number">14.4</span> Topic Modeling Approaches</a></li>
  <li><a href="#material-and-qualitative-analysis" id="toc-material-and-qualitative-analysis" class="nav-link" data-scroll-target="#material-and-qualitative-analysis"><span class="header-section-number">14.5</span> Material and Qualitative Analysis</a></li>
  <li><a href="#quantitative-metrics" id="toc-quantitative-metrics" class="nav-link" data-scroll-target="#quantitative-metrics"><span class="header-section-number">14.6</span> Quantitative Metrics</a></li>
  <li><a href="#adjusted-rand-index-results" id="toc-adjusted-rand-index-results" class="nav-link" data-scroll-target="#adjusted-rand-index-results"><span class="header-section-number">14.7</span> Adjusted Rand Index Results</a></li>
  <li><a href="#lda-model-comparison" id="toc-lda-model-comparison" class="nav-link" data-scroll-target="#lda-model-comparison"><span class="header-section-number">14.8</span> LDA Model Comparison</a></li>
  <li><a href="#bertopic-model-comparison" id="toc-bertopic-model-comparison" class="nav-link" data-scroll-target="#bertopic-model-comparison"><span class="header-section-number">14.9</span> BERTopic Model Comparison</a></li>
  <li><a href="#comparing-top-words" id="toc-comparing-top-words" class="nav-link" data-scroll-target="#comparing-top-words"><span class="header-section-number">14.10</span> Comparing Top Words</a></li>
  <li><a href="#coherence-diversity-and-joint-recall-results" id="toc-coherence-diversity-and-joint-recall-results" class="nav-link" data-scroll-target="#coherence-diversity-and-joint-recall-results"><span class="header-section-number">14.11</span> Coherence, Diversity, and Joint Recall Results</a></li>
  <li><a href="#model-performance-summary" id="toc-model-performance-summary" class="nav-link" data-scroll-target="#model-performance-summary"><span class="header-section-number">14.12</span> Model Performance Summary</a></li>
  <li><a href="#discussion-and-future-directions" id="toc-discussion-and-future-directions" class="nav-link" data-scroll-target="#discussion-and-future-directions"><span class="header-section-number">14.13</span> Discussion and Future Directions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Show code</button></div></div>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Francis Lareau &amp; Christophe Malaterre <a href="mailto:francislareau@hotmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Université du Québec à Montréal; Université de Sherbrooke; CIRST
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    This study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches, Latent Dirichlet Allocation (LDA) and BERTopic. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels. A corpus of scientific articles in Astrobiology serves as the material. Six topic models…
  </div>
</div>


</header>


<section id="overview" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">14.1</span> Overview</h2>
<p>This study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches: <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) and <em>BERTopic</em>. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels.</p>
<p>A corpus of scientific articles in Astrobiology serves as the material for this study. Six topic models are generated: <em>LDA</em> on titles, abstracts, and full-texts, and <em>BERTopic</em> on titles, abstracts, and full-texts. These models are then analyzed and compared qualitatively and quantitatively using metrics such as Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.</p>
<p>The qualitative analysis involves comparing topic coherence and the stability of topics across models, referencing a previously established <em>LDA</em> full-text model with 25 topics and 4 thematic clusters. Quantitative results indicate that title-based models generally perform poorly, while abstract models show better coherence and diversity. Full-text models demonstrate superior joint recall.</p>
<p>Specifically, <em>BERTopic</em> Abstract emerges as a strong performer in coherence, and <em>BERTopic</em> Title in diversity, while <em>LDA</em> Fulltext and <em>BERTopic</em> Fulltext excel in joint recall. The study concludes that the optimal choice of text level and topic model depends on specific research objectives. Abstract-based models offer a good balance and consistency with full-text models, while title-based models, despite limitations, can identify robust core topics. The potential for leveraging structural information (titles, abstracts, full-texts) in future models is also discussed.</p>
</section>
<section id="introduction" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="introduction"><span class="header-section-number">14.2</span> Introduction</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>This presentation is delivered by Francis Lareau, a Postdoctoral Fellow affiliated with the University of Sherbrooke and the University of Quebec in Montreal (UQAM). This work is a comparative study conducted with Christophe Malaterre from the University of Quebec in Montreal.</p>
<p>The study focuses on topic modeling, a technique for extracting themes from a corpus. Topic modeling is recognized as an important tool for analyzing large volumes of scientific literature, especially within the history, philosophy, and sociology of science (HPSS).</p>
<p>A problem arises because existing studies utilize different textual structures for topic modeling, namely titles, abstracts, and full text. Obtaining, preprocessing, and analyzing full-text corpora demand significant resources. This prompts the central research question: Is applying topic modeling to titles or abstracts sufficient, or is full-text analysis necessary?</p>
</section>
<section id="study-design" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="study-design"><span class="header-section-number">14.3</span> Study Design</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_02.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>This study addresses the pressing question of whether analyzing titles or abstracts is sufficient for topic modeling, given the substantial resources needed for full-text corpora acquisition, preprocessing, and analysis. The methodology involves a structured workflow.</p>
<p>First, a corpus of scientific articles is constituted. Second, the distinct title, abstract, and full text sections are identified within this corpus. Third, two different topic modeling approaches, <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) and <em>BERTopic</em>, are applied separately to each of the three identified text levels: titles, abstracts, and full texts.</p>
<p>This process generates a total of six distinct topic models. Finally, these six resulting topic models undergo both qualitative and quantitative analysis and comparison to evaluate their performance across the different text levels.</p>
</section>
<section id="topic-modeling-approaches" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="topic-modeling-approaches"><span class="header-section-number">14.4</span> Topic Modeling Approaches</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_03.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The study compares two distinct topic modeling approaches: <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) and <em>BERTopic</em>. Both approaches share fundamental postulates: documents can be represented by numerical vectors, topics are identifiable through linguistic regularities manifested as repetitions, and machine learning facilitates the automatic detection of these regularities.</p>
<p><em>Latent Dirichlet Allocation</em> (<em>LDA</em>) is characterized as a classical statistical method. It employs a classical vector representation technique based on counting words within documents. In the <em>LDA</em> framework, topics are conceptualized as latent variables that adhere to Dirichlet’s law. A key advantage of <em>LDA</em> is its ability to handle long texts, making it suitable for analysis across titles, abstracts, and full texts.</p>
<p>In contrast, <em>BERTopic</em> is described as a modern, modular approach, developed by Martin Grootendorst. It utilizes an LLM-based vector representation method, originally based on <em>BERT</em>, which gives the approach its name. Topics in <em>BERTopic</em> correspond to topological densities of documents, typically identified using clustering algorithms like <em>HDBSCAN</em>.</p>
<p>Historically, <em>BERTopic</em> did not handle long texts efficiently, but recent advancements have addressed this limitation. For this study, a specific embedding model, <em>Stella EN 1.5B V5</em>, was selected for the <em>BERTopic</em> implementation. This model was chosen based on its high ranking on the Massive Text Embedding Benchmark on Hugging Face and its capacity to handle approximately 131,000 tokens, addressing the long text limitation.</p>
</section>
<section id="material-and-qualitative-analysis" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="material-and-qualitative-analysis"><span class="header-section-number">14.5</span> Material and Qualitative Analysis</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The material utilized in this study is an Astrobiology corpus, which was previously subjected to an in-depth topic analysis. This prior analysis, documented in <em>Malaterre &amp; Lareau (2023)</em>, resulted in the selection of a full-text <em>LDA</em> model comprising 25 topics. This model serves as a reference for the current comparative study.</p>
<p>The 25 topics of the reference model were analyzed meticulously. This analysis involved examining the most representative words and documents associated with each topic. Based on this examination, each topic was assigned a descriptive label derived from its key terms.</p>
<p>The relationships between topics were then assessed by calculating their mutual correlation, determined by the co-occurrence of topics within documents. Subsequently, a community detection algorithm was applied to the correlation data, identifying four distinct thematic clusters. These clusters were designated with letters (A, B, C, D) and assigned corresponding colors (red, green, yellow, blue) for visualization.</p>
<p>The results of this reference analysis are represented visually as a graph illustrating the correlations between the 25 topics. The graph displays the topic labels and the color variations indicating their thematic clusters. The thickness of the lines connecting topics represents the strength of their correlation, while the size of the circles representing topics indicates their overall presence across all documents in the corpus. This established and analyzed reference model provides a basis for qualitatively comparing the six topic models generated and investigated in the current comparative study.</p>
</section>
<section id="quantitative-metrics" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="quantitative-metrics"><span class="header-section-number">14.6</span> Quantitative Metrics</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_07.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>The quantitative analysis compares the six topic models using four specific metrics:</p>
<ul>
<li><p><em>Adjusted Rand Index</em>: This metric assesses the similarity between two different document clusterings. It is corrected for chance, meaning a value of zero corresponds to a random clustering.</p></li>
<li><p><em>Topic Diversity</em>: This measures the proportion of distinct top words utilized to describe the topics within a single topic model. A higher diversity indicates that topics are characterized by different sets of words.</p></li>
<li><p><em>Joint Recall</em>: This evaluates the extent to which the top words collectively represent the documents assigned to each topic. It provides an average measure of document-topic recall, indicating how well the topic’s representative words can retrieve the documents belonging to that topic.</p></li>
<li><p><em>Coherence CV</em>: This evaluates the semantic meaningfulness of the topic’s top words. It is computed as the average of the cosine relative distance between the top words within each topic, where a higher value suggests greater semantic relatedness among the words and thus more coherent topics.</p></li>
</ul>
</section>
<section id="adjusted-rand-index-results" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="adjusted-rand-index-results"><span class="header-section-number">14.7</span> Adjusted Rand Index Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_08.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>The <em>Adjusted Rand Index</em> is employed to quantitatively assess the similarities among the six generated topic models by comparing their document clusterings. A value of zero for this metric signifies a random clustering.</p>
<p>The results indicate that the <em>LDA</em> model trained on titles is the most distinct among all models, as evidenced by low <em>Adjusted Rand Index</em> values, specifically under 0.2, shown in the heatmap comparison.</p>
<p>Conversely, all other models exhibit a better overall match with each other, demonstrating values exceeding 0.2. A notable observation is that the <em>BERTopic</em> models tend to correspond more strongly with each other, with <em>Adjusted Rand Index</em> values generally above 0.35. Furthermore, the <em>BERTopic</em> Abstract model appears to be more central in its similarity profile, showing good correspondence to every other model, with values over 0.30, except for the <em>LDA</em> Title model.</p>
</section>
<section id="lda-model-comparison" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="lda-model-comparison"><span class="header-section-number">14.8</span> LDA Model Comparison</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_09.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>A more detailed qualitative analysis focuses on comparing the <em>LDA</em> Full-text model with the <em>LDA</em> Abstract and <em>LDA</em> Title models, using heatmaps that visualize the number of shared documents between topics. A reddish diagonal pattern in these heatmaps signifies a good correspondence between topics across models.</p>
<p>Comparing the <em>LDA</em> Full-text model with the <em>LDA</em> Abstract model (Table A) reveals a good overall fit. This is evident from the prominent reddish diagonal, indicating that topics in one model largely correspond to single topics in the other with a high proportion of shared documents.</p>
<p>However, the analysis also identifies specific topic transformations: three full-text topics disappear entirely in the abstract model (represented by long horizontal dark gray lines), three full-text topics split into multiple topics in the abstract model (short horizontal dark gray lines), and three abstract topics are formed by the merger of multiple full-text topics (short horizontal dark gray lines). Additionally, the <em>LDA</em> Abstract model exhibits one small class containing fewer than 50 documents.</p>
<p>In contrast, the comparison between the <em>LDA</em> Full-text model and the <em>LDA</em> Title model (Table B) indicates a poor overall fit, characterized by substantial reorganization of topics. This is visually represented by numerous vertical and horizontal dark lines in the heatmap, signifying that many full-text topics disappear and many new topics emerge in the title model, with little direct correspondence.</p>
</section>
<section id="bertopic-model-comparison" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="bertopic-model-comparison"><span class="header-section-number">14.9</span> BERTopic Model Comparison</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_11.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>The qualitative comparison extends to the <em>BERTopic</em> models, assessing their correspondence with the <em>LDA</em> Full-text reference model using heatmaps showing shared documents.</p>
<p>Comparing the <em>LDA</em> Full-text model with the <em>BERTopic</em> Full-text model (Table C) shows an average overall fit. The analysis reveals that 8 topics from the <em>LDA</em> Full-text model disappear, and 6 topics split into multiple topics in the <em>BERTopic</em> Full-text model (indicated on the horizontal axis). On the vertical axis, 5 new topics appear in the <em>BERTopic</em> model, and 1 topic results from the merger of <em>LDA</em> topics. The <em>BERTopic</em> Full-text model also exhibits issues with class size, including 4 small classes and 1 very large class.</p>
<p>The comparison between the <em>LDA</em> Full-text model and the <em>BERTopic</em> Abstract model (Table D) indicates a relatively good overall fit. Four <em>LDA</em> topics disappear, and 6 topics split in the <em>BERTopic</em> Abstract model (horizontal axis). Two new topics appear, and 4 topics result from mergers in the <em>BERTopic</em> Abstract model (vertical axis).</p>
<p>Finally, comparing the <em>LDA</em> Full-text model with the <em>BERTopic</em> Title model (Table E) shows an average overall fit. Seven <em>LDA</em> topics disappear, and 1 topic splits in the <em>BERTopic</em> Title model (horizontal axis). Seven new topics appear, and 1 topic results from a merger in the <em>BERTopic</em> Title model (vertical axis). The <em>BERTopic</em> Title model also presents class size issues, with 3 small classes and 1 large class.</p>
</section>
<section id="comparing-top-words" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="comparing-top-words"><span class="header-section-number">14.10</span> Comparing Top Words</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_13.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>A qualitative assessment involves comparing the top words associated with selected topics across the different models to understand the nature of the topics generated.</p>
<p>Within the <em>LDA</em> models (Full-text, Abstract, and Title), topics are observed to be relatively well-formed. A robust topic, exemplified by “A radiation spore,” demonstrates good correspondence in its top words across all three <em>LDA</em> models.</p>
<p>Splitting of topics is also observed: the “A life civilization” topic from the full-text model splits across the abstract and title models, which is considered sensible as it relates to a general theme of research in astrobiology. The “B chemistry” topic from the full-text model also splits across the abstract and title models, though this particular split is noted as being more challenging to interpret without deeper analysis. Merging of topics occurs as well, such as the “B amino acid” and “B protein gene RNA” topics from the full-text model merging into a single topic in other models, which is deemed sensible as it forms a more general thematic area.</p>
<p>Comparing the <em>BERTopic</em> models (Full-text, Abstract, and Title) with the <em>LDA</em> Full-text model also reveals relatively well-formed topics across all <em>BERTopic</em> models. The robustness of the “A radiation spore” topic is again observed, appearing consistently across all <em>BERTopic</em> models and the <em>LDA</em> Full-text reference. The “A life civilization” topic is relatively stable across the <em>BERTopic</em> models, although some splitting occurs, leading to narrower topics specifically focused on extraterrestrial life. The “B chemistry” topic also splits across the <em>BERTopic</em> models, resulting in more narrow thematic topics.</p>
</section>
<section id="coherence-diversity-and-joint-recall-results" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="coherence-diversity-and-joint-recall-results"><span class="header-section-number">14.11</span> Coherence, Diversity, and Joint Recall Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_15.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>Quantitative performance metrics are evaluated for all six topic models across a range of topic numbers, specifically from 5 to 50.</p>
<p>The <em>Coherence CV</em> metric, which assesses the meaningfulness of the topic top words, yields several findings:</p>
<ul>
<li><p>Models trained on titles exhibit the worst coherence.</p></li>
<li><p>Abstract models demonstrate better coherence compared to full-text models.</p></li>
<li><p>Overall, <em>BERTopic</em> models show better coherence than <em>LDA</em> models when applied to abstracts and titles, although this difference becomes less pronounced as the number of topics increases.</p></li>
<li><p>Based on this metric, <em>BERTopic</em> Abstract is identified as the clear winner.</p></li>
</ul>
<p>Regarding <em>Topic Diversity</em>, which measures the proportion of distinct top words, the results show that diversity generally decreases as the number of topics increases:</p>
<ul>
<li><p>Models trained on titles offer the best diversity.</p></li>
<li><p><em>BERTopic</em> models exhibit better diversity than <em>LDA</em> models.</p></li>
<li><p>The winner for diversity is <em>BERTopic</em> Title, closely followed by <em>BERTopic</em> Full-text.</p></li>
</ul>
<p>The <em>Joint Recall</em> metric evaluates how effectively the top words collectively represent the documents classified within each topic:</p>
<ul>
<li><p>Titles yield the worst joint recall.</p></li>
<li><p>Full-text models perform better than their abstract and title counterparts.</p></li>
<li><p><em>LDA</em> models generally show better joint recall than <em>BERTopic</em> models.</p></li>
<li><p>The winners for <em>Joint Recall</em> are <em>LDA</em> Fulltext and <em>BERTopic</em> Fulltext, with <em>BERTopic</em> Abstract performing very closely behind.</p></li>
</ul>
</section>
<section id="model-performance-summary" class="level2" data-number="14.12">
<h2 data-number="14.12" class="anchored" data-anchor-id="model-performance-summary"><span class="header-section-number">14.12</span> Model Performance Summary</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_17.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>A summary table consolidates the performance results, offering an overall view of each model’s strengths and weaknesses across various assessment criteria. The models evaluated are:</p>
<ul>
<li><p><em>LDA</em> Full-text (rated 4*)</p></li>
<li><p><em>LDA</em> Abstract (4.5*)</p></li>
<li><p><em>LDA</em> Title (2.5*)</p></li>
<li><p><em>BERTopic</em> Full-text (4.5*)</p></li>
<li><p><em>BERTopic</em> Abstract (4.5*)</p></li>
<li><p><em>BERTopic</em> Title (3*)</p></li>
</ul>
<p>The assessment criteria include Overall fit, Top-words quality, Coherence, Diversity, and Joint recall. Performance scores are visually represented using circle icons, where a full black circle signifies the highest score and an empty circle indicates a low score. Red crosses highlight specific problems, such as class imbalance.</p>
<p>The analysis indicates that there is no single absolute best model; the optimal choice is contingent upon the specific research objectives. Different objectives necessitate different model characteristics. For instance, if the primary goal is the discovery of main topics without requiring precise classification of every document, issues like poor recall or large classes might be acceptable. In such a scenario, the <em>BERTopic</em> Full-text model performs well, although it exhibits some class imbalance. The <em>BERTopic</em> Title model, while generally less optimal, is capable of producing some robust topics that are also identified by the other models.</p>
</section>
<section id="discussion-and-future-directions" class="level2" data-number="14.13">
<h2 data-number="14.13" class="anchored" data-anchor-id="discussion-and-future-directions"><span class="header-section-number">14.13</span> Discussion and Future Directions</h2>
<p>The discussion highlights several key observations and potential future directions. The poor performance observed in title-based models is primarily attributed to the inherent lack of information in titles compared to abstracts or full texts. This limitation can lead to inaccurate classification of documents, although title models are still capable of identifying meaningful core topics.</p>
<p>Full-text models exhibit distinct characteristics depending on the approach. <em>LDA</em> models applied to full text tend to produce topics that are more loosely defined and broader in coverage, potentially capturing transverse themes such as research methods. <em>BERTopic</em> full-text models, on the other hand, may result in some topics being too narrow, leading to poor document coverage, and can suffer from class-size imbalance problems.</p>
<p>Abstract models demonstrate notable consistency, both between the <em>LDA</em> and <em>BERTopic</em> implementations and in their correspondence with the <em>LDA</em> full-text model. A significant finding is the overall robustness of topics, with very similar thematic areas being identified across the board, regardless of the specific model or text level used.</p>
<p>Future research possibilities include exploiting meta-analysis techniques to systematically identify the most robust topics that consistently appear across multiple models and text levels. Another direction involves using relative distance metrics to determine which model is the most central or representative among the set. Furthermore, the study suggests the potential for developing new topic modeling approaches that explicitly leverage the structural information present in documents (i.e., the distinction between full text, abstract, and titles) to extract more meaningful sets of topics or top words.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Create burger menu button
  const toggleButton = document.createElement('button');
  toggleButton.className = 'sidebar-toggle';
  toggleButton.setAttribute('aria-label', 'Toggle sidebar');
  toggleButton.innerHTML = `
    <div class="burger-icon">
      <span></span>
      <span></span>
      <span></span>
    </div>
  `;
  
  // Create backdrop for mobile
  const backdrop = document.createElement('div');
  backdrop.className = 'sidebar-backdrop';
  
  // Add elements to page
  document.body.appendChild(toggleButton);
  document.body.appendChild(backdrop);
  
  // Get sidebar and main content elements
  const sidebar = document.querySelector('.sidebar') || 
                 document.querySelector('.quarto-sidebar') || 
                 document.querySelector('.sidebar-navigation');
  const mainContent = document.querySelector('main') || 
                     document.querySelector('.main-content') || 
                     document.querySelector('.quarto-container') || 
                     document.body;
  
  // State management
  let sidebarOpen = window.innerWidth > 768; // Start open on desktop, closed on mobile
  
  // Initialize sidebar state
  function initializeSidebar() {
    if (window.innerWidth <= 768) {
      sidebarOpen = false;
    }
    updateSidebarState();
  }
  
  // Update sidebar state and classes
  function updateSidebarState() {
    if (sidebar) {
      if (sidebarOpen) {
        sidebar.classList.remove('collapsed');
        toggleButton.classList.add('sidebar-open');
        mainContent.classList.add('sidebar-open');
        mainContent.classList.remove('sidebar-closed');
        if (window.innerWidth <= 768) {
          backdrop.classList.add('active');
        }
      } else {
        sidebar.classList.add('collapsed');
        toggleButton.classList.remove('sidebar-open');
        mainContent.classList.remove('sidebar-open');
        mainContent.classList.add('sidebar-closed');
        backdrop.classList.remove('active');
      }
    }
    
    // Store state in localStorage
    localStorage.setItem('sidebarOpen', sidebarOpen);
  }
  
  // Toggle sidebar
  function toggleSidebar() {
    sidebarOpen = !sidebarOpen;
    updateSidebarState();
  }
  
  // Close sidebar (for chapter links)
  function closeSidebar() {
    if (window.innerWidth <= 768) { // Only auto-close on mobile
      sidebarOpen = false;
      updateSidebarState();
    }
  }
  
  // Event listeners
  toggleButton.addEventListener('click', toggleSidebar);
  backdrop.addEventListener('click', toggleSidebar);
  
  // Auto-close sidebar when clicking chapter links
  if (sidebar) {
    const chapterLinks = sidebar.querySelectorAll('a[href]');
    chapterLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        // Small delay to allow navigation to start
        setTimeout(closeSidebar, 100);
      });
    });
  }
  
  // Handle window resize
  window.addEventListener('resize', function() {
    if (window.innerWidth > 768 && !sidebarOpen) {
      sidebarOpen = true;
      updateSidebarState();
    } else if (window.innerWidth <= 768 && sidebarOpen) {
      sidebarOpen = false;
      updateSidebarState();
    }
  });
  
  // Handle escape key
  document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape' && sidebarOpen && window.innerWidth <= 768) {
      closeSidebar();
    }
  });
  
  // Restore saved state from localStorage
  const savedState = localStorage.getItem('sidebarOpen');
  if (savedState !== null) {
    sidebarOpen = savedState === 'true';
  }
  
  // Initialize
  initializeSidebar();
  
  // Add keyboard navigation support
  toggleButton.addEventListener('keydown', function(e) {
    if (e.key === 'Enter' || e.key === ' ') {
      e.preventDefault();
      toggleSidebar();
    }
  });
  
  // Improve accessibility
  toggleButton.setAttribute('role', 'button');
  toggleButton.setAttribute('tabindex', '0');
  
  // Update aria-expanded attribute
  function updateAriaExpanded() {
    toggleButton.setAttribute('aria-expanded', sidebarOpen);
  }
  
  // Call updateAriaExpanded whenever sidebar state changes
  const originalUpdateSidebarState = updateSidebarState;
  updateSidebarState = function() {
    originalUpdateSidebarState();
    updateAriaExpanded();
  };
  
  updateAriaExpanded();
  
  // Ensure TOC sticky positioning works properly
  function ensureTOCSticky() {
    // Find all possible TOC elements
    const tocSelectors = [
      '#TOC',
      '.table-of-contents',
      '.quarto-sidebar-toc',
      '.toc',
      '.quarto-toc',
      'nav[role="doc-toc"]',
      '.margin-sidebar',
      '.sidebar-right',
      '.quarto-margin-sidebar',
      '.column-margin'
    ];
    
    let toc = null;
    for (const selector of tocSelectors) {
      toc = document.querySelector(selector);
      if (toc) break;
    }
    
    if (toc) {
      console.log('Found TOC element:', toc.className || toc.id);
      
      // Force sticky positioning with important styles
      toc.style.setProperty('position', 'sticky', 'important');
      toc.style.setProperty('top', '1rem', 'important');
      toc.style.setProperty('max-height', 'calc(100vh - 2rem)', 'important');
      toc.style.setProperty('overflow-y', 'auto', 'important');
      toc.style.setProperty('z-index', '100', 'important');
      
      // Ensure parent containers support sticky
      let parent = toc.parentElement;
      while (parent && parent !== document.body) {
        parent.style.setProperty('position', 'relative', 'important');
        parent.style.setProperty('height', 'auto', 'important');
        parent = parent.parentElement;
      }
      
      // Add scroll event listener to maintain visibility
      let lastScrollTop = 0;
      const scrollHandler = function() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        
        // Ensure TOC remains visible and properly positioned
        if (toc && window.innerWidth > 768) {
          toc.style.setProperty('position', 'sticky', 'important');
          toc.style.setProperty('top', '1rem', 'important');
        }
        
        lastScrollTop = scrollTop;
      };
      
      // Remove existing scroll listeners to avoid duplicates
      window.removeEventListener('scroll', scrollHandler);
      window.addEventListener('scroll', scrollHandler, { passive: true });
      
      // Also apply to any nested TOC elements
      const nestedTocs = toc.querySelectorAll('#TOC, .toc, .table-of-contents');
      nestedTocs.forEach(nestedToc => {
        nestedToc.style.setProperty('position', 'sticky', 'important');
        nestedToc.style.setProperty('top', '0', 'important');
      });
    } else {
      console.log('No TOC element found');
    }
  }
  
  // Initialize TOC sticky behavior
  ensureTOCSticky();
  
  // Re-initialize periodically to ensure it stays sticky
  setInterval(ensureTOCSticky, 2000);
  
  // Re-initialize on window resize
  window.addEventListener('resize', function() {
    setTimeout(ensureTOCSticky, 100);
  });
  
  // Re-initialize if content changes
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.type === 'childList') {
        setTimeout(ensureTOCSticky, 100);
      }
    });
  });
  
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });
  
  // Force re-initialization after page load
  window.addEventListener('load', function() {
    setTimeout(ensureTOCSticky, 500);
  });
});
</script>

<style>
/* Additional styles for better integration */
body {
  overflow-x: hidden;
}

.sidebar-toggle {
  -webkit-tap-highlight-color: transparent;
}

/* Ensure smooth transitions on all relevant elements */
.sidebar,
.sidebar-toggle,
.main-content,
.sidebar-backdrop {
  will-change: transform, opacity, margin;
}

/* Focus styles for accessibility */
.sidebar-toggle:focus {
  outline: 2px solid white;
  outline-offset: 2px;
}

/* Prevent text selection on burger icon */
.burger-icon {
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
</style> 
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_015.html" class="pagination-link" aria-label="Plural pursuit across scales">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural pursuit across scales</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_017.html" class="pagination-link" aria-label="Time-Aware Language Models">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">abstract:</span><span class="co"> "\n      This study investigates the impact of text granularity (titles,\</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">  \ abstracts, full-texts) on the performance of two distinct topic modeling approaches,\</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">  \ Latent Dirichlet Allocation (LDA) and BERTopic. The research addresses the practical\</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">  \ challenge of significant resource requirements for obtaining, preprocessing, and\</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  \ analyzing full-text corpora by comparing topic models derived from different text\</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  \ levels. A corpus of scientific articles in Astrobiology serves as the material.\</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">  \ Six topic models..."</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="an">author:</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">- affiliation: Université du Québec à Montréal; Université de Sherbrooke; CIRST</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">  email: francislareau@hotmail.com</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">  name: Francis Lareau &amp; Christophe Malaterre</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="an">bibliography:</span><span class="co"> bibliography.bib</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="an">date:</span><span class="co"> '2025'</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">---</span></span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="fu"># Text Granularity and Topic Model Performance</span></span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="fu">## Overview</span></span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>This study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches: *Latent Dirichlet Allocation* (*LDA*) and *BERTopic*. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels.</span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>A corpus of scientific articles in Astrobiology serves as the material for this study. Six topic models are generated: *LDA* on titles, abstracts, and full-texts, and *BERTopic* on titles, abstracts, and full-texts. These models are then analyzed and compared qualitatively and quantitatively using metrics such as Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a>The qualitative analysis involves comparing topic coherence and the stability of topics across models, referencing a previously established *LDA* full-text model with 25 topics and 4 thematic clusters. Quantitative results indicate that title-based models generally perform poorly, while abstract models show better coherence and diversity. Full-text models demonstrate superior joint recall.</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>Specifically, *BERTopic* Abstract emerges as a strong performer in coherence, and *BERTopic* Title in diversity, while *LDA* Fulltext and *BERTopic* Fulltext excel in joint recall. The study concludes that the optimal choice of text level and topic model depends on specific research objectives. Abstract-based models offer a good balance and consistency with full-text models, while title-based models, despite limitations, can identify robust core topics. The potential for leveraging structural information (titles, abstracts, full-texts) in future models is also discussed.</span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="fu">## Introduction</span></span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a><span class="al">![Slide 01](images/ai-nepi_016_slide_01.jpg)</span></span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a>This presentation is delivered by Francis Lareau, a Postdoctoral Fellow affiliated with the University of Sherbrooke and the University of Quebec in Montreal (UQAM). This work is a comparative study conducted with Christophe Malaterre from the University of Quebec in Montreal.</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>The study focuses on topic modeling, a technique for extracting themes from a corpus. Topic modeling is recognized as an important tool for analyzing large volumes of scientific literature, especially within the history, philosophy, and sociology of science (HPSS).</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>A problem arises because existing studies utilize different textual structures for topic modeling, namely titles, abstracts, and full text. Obtaining, preprocessing, and analyzing full-text corpora demand significant resources. This prompts the central research question: Is applying topic modeling to titles or abstracts sufficient, or is full-text analysis necessary?</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="fu">## Study Design</span></span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a><span class="al">![Slide 02](images/ai-nepi_016_slide_02.jpg)</span></span>
<span id="cb1-43"><a href="#cb1-43"></a></span>
<span id="cb1-44"><a href="#cb1-44"></a>This study addresses the pressing question of whether analyzing titles or abstracts is sufficient for topic modeling, given the substantial resources needed for full-text corpora acquisition, preprocessing, and analysis. The methodology involves a structured workflow.</span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a>First, a corpus of scientific articles is constituted. Second, the distinct title, abstract, and full text sections are identified within this corpus. Third, two different topic modeling approaches, *Latent Dirichlet Allocation* (*LDA*) and *BERTopic*, are applied separately to each of the three identified text levels: titles, abstracts, and full texts.</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a>This process generates a total of six distinct topic models. Finally, these six resulting topic models undergo both qualitative and quantitative analysis and comparison to evaluate their performance across the different text levels.</span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a><span class="fu">## Topic Modeling Approaches</span></span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a><span class="al">![Slide 03](images/ai-nepi_016_slide_03.jpg)</span></span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a>The study compares two distinct topic modeling approaches: *Latent Dirichlet Allocation* (*LDA*) and *BERTopic*. Both approaches share fundamental postulates: documents can be represented by numerical vectors, topics are identifiable through linguistic regularities manifested as repetitions, and machine learning facilitates the automatic detection of these regularities.</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>*Latent Dirichlet Allocation* (*LDA*) is characterized as a classical statistical method. It employs a classical vector representation technique based on counting words within documents. In the *LDA* framework, topics are conceptualized as latent variables that adhere to Dirichlet's law. A key advantage of *LDA* is its ability to handle long texts, making it suitable for analysis across titles, abstracts, and full texts.</span>
<span id="cb1-57"><a href="#cb1-57"></a></span>
<span id="cb1-58"><a href="#cb1-58"></a>In contrast, *BERTopic* is described as a modern, modular approach, developed by Martin Grootendorst. It utilizes an LLM-based vector representation method, originally based on *BERT*, which gives the approach its name. Topics in *BERTopic* correspond to topological densities of documents, typically identified using clustering algorithms like *HDBSCAN*.</span>
<span id="cb1-59"><a href="#cb1-59"></a></span>
<span id="cb1-60"><a href="#cb1-60"></a>Historically, *BERTopic* did not handle long texts efficiently, but recent advancements have addressed this limitation. For this study, a specific embedding model, *Stella EN 1.5B V5*, was selected for the *BERTopic* implementation. This model was chosen based on its high ranking on the Massive Text Embedding Benchmark on Hugging Face and its capacity to handle approximately 131,000 tokens, addressing the long text limitation.</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a><span class="fu">## Material and Qualitative Analysis</span></span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a><span class="al">![Slide 06](images/ai-nepi_016_slide_06.jpg)</span></span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a>The material utilized in this study is an Astrobiology corpus, which was previously subjected to an in-depth topic analysis. This prior analysis, documented in *Malaterre &amp; Lareau (2023)*, resulted in the selection of a full-text *LDA* model comprising 25 topics. This model serves as a reference for the current comparative study.</span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>The 25 topics of the reference model were analyzed meticulously. This analysis involved examining the most representative words and documents associated with each topic. Based on this examination, each topic was assigned a descriptive label derived from its key terms.</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a>The relationships between topics were then assessed by calculating their mutual correlation, determined by the co-occurrence of topics within documents. Subsequently, a community detection algorithm was applied to the correlation data, identifying four distinct thematic clusters. These clusters were designated with letters (A, B, C, D) and assigned corresponding colors (red, green, yellow, blue) for visualization.</span>
<span id="cb1-71"><a href="#cb1-71"></a></span>
<span id="cb1-72"><a href="#cb1-72"></a>The results of this reference analysis are represented visually as a graph illustrating the correlations between the 25 topics. The graph displays the topic labels and the color variations indicating their thematic clusters. The thickness of the lines connecting topics represents the strength of their correlation, while the size of the circles representing topics indicates their overall presence across all documents in the corpus. This established and analyzed reference model provides a basis for qualitatively comparing the six topic models generated and investigated in the current comparative study.</span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="fu">## Quantitative Metrics</span></span>
<span id="cb1-75"><a href="#cb1-75"></a></span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="al">![Slide 07](images/ai-nepi_016_slide_07.jpg)</span></span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a>The quantitative analysis compares the six topic models using four specific metrics:</span>
<span id="cb1-79"><a href="#cb1-79"></a></span>
<span id="cb1-80"><a href="#cb1-80"></a><span class="ss">-   </span>*Adjusted Rand Index*: This metric assesses the similarity between two different document clusterings. It is corrected for chance, meaning a value of zero corresponds to a random clustering.</span>
<span id="cb1-81"><a href="#cb1-81"></a></span>
<span id="cb1-82"><a href="#cb1-82"></a><span class="ss">-   </span>*Topic Diversity*: This measures the proportion of distinct top words utilized to describe the topics within a single topic model. A higher diversity indicates that topics are characterized by different sets of words.</span>
<span id="cb1-83"><a href="#cb1-83"></a></span>
<span id="cb1-84"><a href="#cb1-84"></a><span class="ss">-   </span>*Joint Recall*: This evaluates the extent to which the top words collectively represent the documents assigned to each topic. It provides an average measure of document-topic recall, indicating how well the topic's representative words can retrieve the documents belonging to that topic.</span>
<span id="cb1-85"><a href="#cb1-85"></a></span>
<span id="cb1-86"><a href="#cb1-86"></a><span class="ss">-   </span>*Coherence CV*: This evaluates the semantic meaningfulness of the topic's top words. It is computed as the average of the cosine relative distance between the top words within each topic, where a higher value suggests greater semantic relatedness among the words and thus more coherent topics.</span>
<span id="cb1-87"><a href="#cb1-87"></a></span>
<span id="cb1-88"><a href="#cb1-88"></a><span class="fu">## Adjusted Rand Index Results</span></span>
<span id="cb1-89"><a href="#cb1-89"></a></span>
<span id="cb1-90"><a href="#cb1-90"></a><span class="al">![Slide 08](images/ai-nepi_016_slide_08.jpg)</span></span>
<span id="cb1-91"><a href="#cb1-91"></a></span>
<span id="cb1-92"><a href="#cb1-92"></a>The *Adjusted Rand Index* is employed to quantitatively assess the similarities among the six generated topic models by comparing their document clusterings. A value of zero for this metric signifies a random clustering.</span>
<span id="cb1-93"><a href="#cb1-93"></a></span>
<span id="cb1-94"><a href="#cb1-94"></a>The results indicate that the *LDA* model trained on titles is the most distinct among all models, as evidenced by low *Adjusted Rand Index* values, specifically under 0.2, shown in the heatmap comparison.</span>
<span id="cb1-95"><a href="#cb1-95"></a></span>
<span id="cb1-96"><a href="#cb1-96"></a>Conversely, all other models exhibit a better overall match with each other, demonstrating values exceeding 0.2. A notable observation is that the *BERTopic* models tend to correspond more strongly with each other, with *Adjusted Rand Index* values generally above 0.35. Furthermore, the *BERTopic* Abstract model appears to be more central in its similarity profile, showing good correspondence to every other model, with values over 0.30, except for the *LDA* Title model.</span>
<span id="cb1-97"><a href="#cb1-97"></a></span>
<span id="cb1-98"><a href="#cb1-98"></a><span class="fu">## LDA Model Comparison</span></span>
<span id="cb1-99"><a href="#cb1-99"></a></span>
<span id="cb1-100"><a href="#cb1-100"></a><span class="al">![Slide 09](images/ai-nepi_016_slide_09.jpg)</span></span>
<span id="cb1-101"><a href="#cb1-101"></a></span>
<span id="cb1-102"><a href="#cb1-102"></a>A more detailed qualitative analysis focuses on comparing the *LDA* Full-text model with the *LDA* Abstract and *LDA* Title models, using heatmaps that visualize the number of shared documents between topics. A reddish diagonal pattern in these heatmaps signifies a good correspondence between topics across models.</span>
<span id="cb1-103"><a href="#cb1-103"></a></span>
<span id="cb1-104"><a href="#cb1-104"></a>Comparing the *LDA* Full-text model with the *LDA* Abstract model (Table A) reveals a good overall fit. This is evident from the prominent reddish diagonal, indicating that topics in one model largely correspond to single topics in the other with a high proportion of shared documents.</span>
<span id="cb1-105"><a href="#cb1-105"></a></span>
<span id="cb1-106"><a href="#cb1-106"></a>However, the analysis also identifies specific topic transformations: three full-text topics disappear entirely in the abstract model (represented by long horizontal dark gray lines), three full-text topics split into multiple topics in the abstract model (short horizontal dark gray lines), and three abstract topics are formed by the merger of multiple full-text topics (short horizontal dark gray lines). Additionally, the *LDA* Abstract model exhibits one small class containing fewer than 50 documents.</span>
<span id="cb1-107"><a href="#cb1-107"></a></span>
<span id="cb1-108"><a href="#cb1-108"></a>In contrast, the comparison between the *LDA* Full-text model and the *LDA* Title model (Table B) indicates a poor overall fit, characterized by substantial reorganization of topics. This is visually represented by numerous vertical and horizontal dark lines in the heatmap, signifying that many full-text topics disappear and many new topics emerge in the title model, with little direct correspondence.</span>
<span id="cb1-109"><a href="#cb1-109"></a></span>
<span id="cb1-110"><a href="#cb1-110"></a><span class="fu">## BERTopic Model Comparison</span></span>
<span id="cb1-111"><a href="#cb1-111"></a></span>
<span id="cb1-112"><a href="#cb1-112"></a><span class="al">![Slide 11](images/ai-nepi_016_slide_11.jpg)</span></span>
<span id="cb1-113"><a href="#cb1-113"></a></span>
<span id="cb1-114"><a href="#cb1-114"></a>The qualitative comparison extends to the *BERTopic* models, assessing their correspondence with the *LDA* Full-text reference model using heatmaps showing shared documents.</span>
<span id="cb1-115"><a href="#cb1-115"></a></span>
<span id="cb1-116"><a href="#cb1-116"></a>Comparing the *LDA* Full-text model with the *BERTopic* Full-text model (Table C) shows an average overall fit. The analysis reveals that 8 topics from the *LDA* Full-text model disappear, and 6 topics split into multiple topics in the *BERTopic* Full-text model (indicated on the horizontal axis). On the vertical axis, 5 new topics appear in the *BERTopic* model, and 1 topic results from the merger of *LDA* topics. The *BERTopic* Full-text model also exhibits issues with class size, including 4 small classes and 1 very large class.</span>
<span id="cb1-117"><a href="#cb1-117"></a></span>
<span id="cb1-118"><a href="#cb1-118"></a>The comparison between the *LDA* Full-text model and the *BERTopic* Abstract model (Table D) indicates a relatively good overall fit. Four *LDA* topics disappear, and 6 topics split in the *BERTopic* Abstract model (horizontal axis). Two new topics appear, and 4 topics result from mergers in the *BERTopic* Abstract model (vertical axis).</span>
<span id="cb1-119"><a href="#cb1-119"></a></span>
<span id="cb1-120"><a href="#cb1-120"></a>Finally, comparing the *LDA* Full-text model with the *BERTopic* Title model (Table E) shows an average overall fit. Seven *LDA* topics disappear, and 1 topic splits in the *BERTopic* Title model (horizontal axis). Seven new topics appear, and 1 topic results from a merger in the *BERTopic* Title model (vertical axis). The *BERTopic* Title model also presents class size issues, with 3 small classes and 1 large class.</span>
<span id="cb1-121"><a href="#cb1-121"></a></span>
<span id="cb1-122"><a href="#cb1-122"></a><span class="fu">## Comparing Top Words</span></span>
<span id="cb1-123"><a href="#cb1-123"></a></span>
<span id="cb1-124"><a href="#cb1-124"></a><span class="al">![Slide 13](images/ai-nepi_016_slide_13.jpg)</span></span>
<span id="cb1-125"><a href="#cb1-125"></a></span>
<span id="cb1-126"><a href="#cb1-126"></a>A qualitative assessment involves comparing the top words associated with selected topics across the different models to understand the nature of the topics generated.</span>
<span id="cb1-127"><a href="#cb1-127"></a></span>
<span id="cb1-128"><a href="#cb1-128"></a>Within the *LDA* models (Full-text, Abstract, and Title), topics are observed to be relatively well-formed. A robust topic, exemplified by "A radiation spore," demonstrates good correspondence in its top words across all three *LDA* models.</span>
<span id="cb1-129"><a href="#cb1-129"></a></span>
<span id="cb1-130"><a href="#cb1-130"></a>Splitting of topics is also observed: the "A life civilization" topic from the full-text model splits across the abstract and title models, which is considered sensible as it relates to a general theme of research in astrobiology. The "B chemistry" topic from the full-text model also splits across the abstract and title models, though this particular split is noted as being more challenging to interpret without deeper analysis. Merging of topics occurs as well, such as the "B amino acid" and "B protein gene RNA" topics from the full-text model merging into a single topic in other models, which is deemed sensible as it forms a more general thematic area.</span>
<span id="cb1-131"><a href="#cb1-131"></a></span>
<span id="cb1-132"><a href="#cb1-132"></a>Comparing the *BERTopic* models (Full-text, Abstract, and Title) with the *LDA* Full-text model also reveals relatively well-formed topics across all *BERTopic* models. The robustness of the "A radiation spore" topic is again observed, appearing consistently across all *BERTopic* models and the *LDA* Full-text reference. The "A life civilization" topic is relatively stable across the *BERTopic* models, although some splitting occurs, leading to narrower topics specifically focused on extraterrestrial life. The "B chemistry" topic also splits across the *BERTopic* models, resulting in more narrow thematic topics.</span>
<span id="cb1-133"><a href="#cb1-133"></a></span>
<span id="cb1-134"><a href="#cb1-134"></a><span class="fu">## Coherence, Diversity, and Joint Recall Results</span></span>
<span id="cb1-135"><a href="#cb1-135"></a></span>
<span id="cb1-136"><a href="#cb1-136"></a><span class="al">![Slide 15](images/ai-nepi_016_slide_15.jpg)</span></span>
<span id="cb1-137"><a href="#cb1-137"></a></span>
<span id="cb1-138"><a href="#cb1-138"></a>Quantitative performance metrics are evaluated for all six topic models across a range of topic numbers, specifically from 5 to 50.</span>
<span id="cb1-139"><a href="#cb1-139"></a></span>
<span id="cb1-140"><a href="#cb1-140"></a>The *Coherence CV* metric, which assesses the meaningfulness of the topic top words, yields several findings:</span>
<span id="cb1-141"><a href="#cb1-141"></a></span>
<span id="cb1-142"><a href="#cb1-142"></a><span class="ss">-   </span>Models trained on titles exhibit the worst coherence.</span>
<span id="cb1-143"><a href="#cb1-143"></a></span>
<span id="cb1-144"><a href="#cb1-144"></a><span class="ss">-   </span>Abstract models demonstrate better coherence compared to full-text models.</span>
<span id="cb1-145"><a href="#cb1-145"></a></span>
<span id="cb1-146"><a href="#cb1-146"></a><span class="ss">-   </span>Overall, *BERTopic* models show better coherence than *LDA* models when applied to abstracts and titles, although this difference becomes less pronounced as the number of topics increases.</span>
<span id="cb1-147"><a href="#cb1-147"></a></span>
<span id="cb1-148"><a href="#cb1-148"></a><span class="ss">-   </span>Based on this metric, *BERTopic* Abstract is identified as the clear winner.</span>
<span id="cb1-149"><a href="#cb1-149"></a></span>
<span id="cb1-150"><a href="#cb1-150"></a>Regarding *Topic Diversity*, which measures the proportion of distinct top words, the results show that diversity generally decreases as the number of topics increases:</span>
<span id="cb1-151"><a href="#cb1-151"></a></span>
<span id="cb1-152"><a href="#cb1-152"></a><span class="ss">-   </span>Models trained on titles offer the best diversity.</span>
<span id="cb1-153"><a href="#cb1-153"></a></span>
<span id="cb1-154"><a href="#cb1-154"></a><span class="ss">-   </span>*BERTopic* models exhibit better diversity than *LDA* models.</span>
<span id="cb1-155"><a href="#cb1-155"></a></span>
<span id="cb1-156"><a href="#cb1-156"></a><span class="ss">-   </span>The winner for diversity is *BERTopic* Title, closely followed by *BERTopic* Full-text.</span>
<span id="cb1-157"><a href="#cb1-157"></a></span>
<span id="cb1-158"><a href="#cb1-158"></a>The *Joint Recall* metric evaluates how effectively the top words collectively represent the documents classified within each topic:</span>
<span id="cb1-159"><a href="#cb1-159"></a></span>
<span id="cb1-160"><a href="#cb1-160"></a><span class="ss">-   </span>Titles yield the worst joint recall.</span>
<span id="cb1-161"><a href="#cb1-161"></a></span>
<span id="cb1-162"><a href="#cb1-162"></a><span class="ss">-   </span>Full-text models perform better than their abstract and title counterparts.</span>
<span id="cb1-163"><a href="#cb1-163"></a></span>
<span id="cb1-164"><a href="#cb1-164"></a><span class="ss">-   </span>*LDA* models generally show better joint recall than *BERTopic* models.</span>
<span id="cb1-165"><a href="#cb1-165"></a></span>
<span id="cb1-166"><a href="#cb1-166"></a><span class="ss">-   </span>The winners for *Joint Recall* are *LDA* Fulltext and *BERTopic* Fulltext, with *BERTopic* Abstract performing very closely behind.</span>
<span id="cb1-167"><a href="#cb1-167"></a></span>
<span id="cb1-168"><a href="#cb1-168"></a><span class="fu">## Model Performance Summary</span></span>
<span id="cb1-169"><a href="#cb1-169"></a></span>
<span id="cb1-170"><a href="#cb1-170"></a><span class="al">![Slide 17](images/ai-nepi_016_slide_17.jpg)</span></span>
<span id="cb1-171"><a href="#cb1-171"></a></span>
<span id="cb1-172"><a href="#cb1-172"></a>A summary table consolidates the performance results, offering an overall view of each model's strengths and weaknesses across various assessment criteria. The models evaluated are:</span>
<span id="cb1-173"><a href="#cb1-173"></a></span>
<span id="cb1-174"><a href="#cb1-174"></a><span class="ss">-   </span>*LDA* Full-text (rated 4*)</span>
<span id="cb1-175"><a href="#cb1-175"></a></span>
<span id="cb1-176"><a href="#cb1-176"></a><span class="ss">-   </span>*LDA* Abstract (4.5*)</span>
<span id="cb1-177"><a href="#cb1-177"></a></span>
<span id="cb1-178"><a href="#cb1-178"></a><span class="ss">-   </span>*LDA* Title (2.5*)</span>
<span id="cb1-179"><a href="#cb1-179"></a></span>
<span id="cb1-180"><a href="#cb1-180"></a><span class="ss">-   </span>*BERTopic* Full-text (4.5*)</span>
<span id="cb1-181"><a href="#cb1-181"></a></span>
<span id="cb1-182"><a href="#cb1-182"></a><span class="ss">-   </span>*BERTopic* Abstract (4.5*)</span>
<span id="cb1-183"><a href="#cb1-183"></a></span>
<span id="cb1-184"><a href="#cb1-184"></a><span class="ss">-   </span>*BERTopic* Title (3*)</span>
<span id="cb1-185"><a href="#cb1-185"></a></span>
<span id="cb1-186"><a href="#cb1-186"></a>The assessment criteria include Overall fit, Top-words quality, Coherence, Diversity, and Joint recall. Performance scores are visually represented using circle icons, where a full black circle signifies the highest score and an empty circle indicates a low score. Red crosses highlight specific problems, such as class imbalance.</span>
<span id="cb1-187"><a href="#cb1-187"></a></span>
<span id="cb1-188"><a href="#cb1-188"></a>The analysis indicates that there is no single absolute best model; the optimal choice is contingent upon the specific research objectives. Different objectives necessitate different model characteristics. For instance, if the primary goal is the discovery of main topics without requiring precise classification of every document, issues like poor recall or large classes might be acceptable. In such a scenario, the *BERTopic* Full-text model performs well, although it exhibits some class imbalance. The *BERTopic* Title model, while generally less optimal, is capable of producing some robust topics that are also identified by the other models.</span>
<span id="cb1-189"><a href="#cb1-189"></a></span>
<span id="cb1-190"><a href="#cb1-190"></a><span class="fu">## Discussion and Future Directions</span></span>
<span id="cb1-191"><a href="#cb1-191"></a></span>
<span id="cb1-192"><a href="#cb1-192"></a>The discussion highlights several key observations and potential future directions. The poor performance observed in title-based models is primarily attributed to the inherent lack of information in titles compared to abstracts or full texts. This limitation can lead to inaccurate classification of documents, although title models are still capable of identifying meaningful core topics.</span>
<span id="cb1-193"><a href="#cb1-193"></a></span>
<span id="cb1-194"><a href="#cb1-194"></a>Full-text models exhibit distinct characteristics depending on the approach. *LDA* models applied to full text tend to produce topics that are more loosely defined and broader in coverage, potentially capturing transverse themes such as research methods. *BERTopic* full-text models, on the other hand, may result in some topics being too narrow, leading to poor document coverage, and can suffer from class-size imbalance problems.</span>
<span id="cb1-195"><a href="#cb1-195"></a></span>
<span id="cb1-196"><a href="#cb1-196"></a>Abstract models demonstrate notable consistency, both between the *LDA* and *BERTopic* implementations and in their correspondence with the *LDA* full-text model. A significant finding is the overall robustness of topics, with very similar thematic areas being identified across the board, regardless of the specific model or text level used.</span>
<span id="cb1-197"><a href="#cb1-197"></a></span>
<span id="cb1-198"><a href="#cb1-198"></a>Future research possibilities include exploiting meta-analysis techniques to systematically identify the most robust topics that consistently appear across multiple models and text levels. Another direction involves using relative distance metrics to determine which model is the most central or representative among the set. Furthermore, the study suggests the potential for developing new topic modeling approaches that explicitly leverage the structural information present in documents (i.e., the distinction between full text, abstract, and titles) to extract more meaningful sets of topics or top words.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>