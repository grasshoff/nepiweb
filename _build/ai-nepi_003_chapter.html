<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lea Stengel">

<title>3&nbsp; The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ai-nepi_004_chapter.html" rel="next">
<link href="./ai-nepi_001_chapter.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-09b140d2d032adf2aedb8b099be3ee13.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai-nepi_003_chapter.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_001_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_003_chapter.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_004_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Workflow and Utility of OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_005_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_006_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_007_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_008_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science Dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of <em>LDA</em> and <em>BERTopic</em> Performance across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Beyond Traditional Views of Science Funding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#a-primer-on-large-language-models" id="toc-a-primer-on-large-language-models" class="nav-link" data-scroll-target="#a-primer-on-large-language-models"><span class="header-section-number">3.1</span> A Primer on Large Language Models</a>
  <ul class="collapse">
  <li><a href="#evolution-into-pre-trained-language-models" id="toc-evolution-into-pre-trained-language-models" class="nav-link" data-scroll-target="#evolution-into-pre-trained-language-models"><span class="header-section-number">3.1.1</span> Evolution into Pre-trained Language Models</a></li>
  <li><a href="#encoder-based-models-the-bert-family" id="toc-encoder-based-models-the-bert-family" class="nav-link" data-scroll-target="#encoder-based-models-the-bert-family"><span class="header-section-number">3.1.2</span> Encoder-based Models: The BERT Family</a></li>
  <li><a href="#decoder-based-models-the-gpt-lineage" id="toc-decoder-based-models-the-gpt-lineage" class="nav-link" data-scroll-target="#decoder-based-models-the-gpt-lineage"><span class="header-section-number">3.1.3</span> Decoder-based Models: The GPT Lineage</a></li>
  </ul></li>
  <li><a href="#adapting-and-specialising-language-models" id="toc-adapting-and-specialising-language-models" class="nav-link" data-scroll-target="#adapting-and-specialising-language-models"><span class="header-section-number">3.2</span> Adapting and Specialising Language Models</a>
  <ul class="collapse">
  <li><a href="#strategies-for-domain-adaptation" id="toc-strategies-for-domain-adaptation" class="nav-link" data-scroll-target="#strategies-for-domain-adaptation"><span class="header-section-number">3.2.1</span> Strategies for Domain Adaptation</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag"><span class="header-section-number">3.2.2</span> Retrieval Augmented Generation (RAG)</a></li>
  <li><a href="#key-distinctions-to-remember" id="toc-key-distinctions-to-remember" class="nav-link" data-scroll-target="#key-distinctions-to-remember"><span class="header-section-number">3.2.3</span> Key Distinctions to Remember</a></li>
  </ul></li>
  <li><a href="#applications-of-llms-in-hpss-research" id="toc-applications-of-llms-in-hpss-research" class="nav-link" data-scroll-target="#applications-of-llms-in-hpss-research"><span class="header-section-number">3.3</span> Applications of LLMs in HPSS Research</a>
  <ul class="collapse">
  <li><a href="#categorising-hpss-applications" id="toc-categorising-hpss-applications" class="nav-link" data-scroll-target="#categorising-hpss-applications"><span class="header-section-number">3.3.1</span> Categorising HPSS Applications</a></li>
  <li><a href="#observed-trends-and-recurring-concerns" id="toc-observed-trends-and-recurring-concerns" class="nav-link" data-scroll-target="#observed-trends-and-recurring-concerns"><span class="header-section-number">3.3.2</span> Observed Trends and Recurring Concerns</a></li>
  </ul></li>
  <li><a href="#critical-reflections-and-future-pathways" id="toc-critical-reflections-and-future-pathways" class="nav-link" data-scroll-target="#critical-reflections-and-future-pathways"><span class="header-section-number">3.4</span> Critical Reflections and Future Pathways</a>
  <ul class="collapse">
  <li><a href="#acknowledging-hpss-specific-challenges" id="toc-acknowledging-hpss-specific-challenges" class="nav-link" data-scroll-target="#acknowledging-hpss-specific-challenges"><span class="header-section-number">3.4.1</span> Acknowledging HPSS-Specific Challenges</a></li>
  <li><a href="#the-imperative-of-llm-literacy" id="toc-the-imperative-of-llm-literacy" class="nav-link" data-scroll-target="#the-imperative-of-llm-literacy"><span class="header-section-number">3.4.2</span> The Imperative of LLM Literacy</a></li>
  <li><a href="#upholding-hpss-methodological-integrity" id="toc-upholding-hpss-methodological-integrity" class="nav-link" data-scroll-target="#upholding-hpss-methodological-integrity"><span class="header-section-number">3.4.3</span> Upholding HPSS Methodological Integrity</a></li>
  <li><a href="#the-evolving-landscape-agents-and-beyond" id="toc-the-evolving-landscape-agents-and-beyond" class="nav-link" data-scroll-target="#the-evolving-landscape-agents-and-beyond"><span class="header-section-number">3.4.4</span> The Evolving Landscape: Agents and Beyond</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lea Stengel </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter navigates the evolving landscape of Large Language Models (LLMs), commencing with a foundational primer on their core architecture. It then explores their diverse applications within History and Philosophy of Science and Science Studies (HPSS), considering various adaptation strategies. Finally, the chapter offers critical reflections on the specific challenges and opportunities these powerful tools present for HPSS research, emphasising the need for methodological rigour and LLM literacy. The discussion aims to equip readers with a nuanced understanding of LLMs, fostering informed engagement with these transformative technologies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Today’s Menu slide outlining the chapter’s structure: Primer on LLMs, Applications in HPSS, and Reflections.</figcaption>
</figure>
</div>
</section>
<section id="a-primer-on-large-language-models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="a-primer-on-large-language-models"><span class="header-section-number">3.1</span> A Primer on Large Language Models</h2>
<p>The journey into contemporary Large Language Models invariably begins with the Transformer architecture, a pivotal development that underpins nearly all modern LLMs. Researchers at Google Brain introduced this model in 2017, initially for machine translation tasks, such as converting German text to English <span class="citation" data-cites="Vaswani2017">(<a href="#ref-Vaswani2017" role="doc-biblioref"><strong>Vaswani2017?</strong></a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_05.jpg" class="img-fluid figure-img"></p>
<figcaption>Figure 1: The Transformer - model architecture, illustrating the encoder and decoder components side-by-side. The left diagram details BERT (Encoder) and the right details a generic Decoder architecture.</figcaption>
</figure>
</div>
<p>The Transformer comprises two primary interconnected streams: an encoder and a decoder. In its original translation application, the encoder processes the input sentence (e.g., in German), converting words into numerical representations. These numbers undergo several layers of processing—or ‘crunching’—where contextualised word embeddings become progressively more refined layer by layer. Subsequently, these numerical representations transfer to the decoder stream. The decoder then generates the output sentence (e.g., in English) word by word. Each generated word feeds back into the decoder, influencing the prediction of subsequent words until the complete translated sentence emerges.</p>
<p>A crucial distinction exists between the operational modes of the encoder and decoder. The encoder reads the entire input sentence simultaneously, allowing each word to interact with every other word in the sentence. This mechanism enables the model to construct a comprehensive representation of the sentence’s complete meaning, capturing what is often termed “bidirectional full-context”. Conversely, the decoder operates sequentially; when generating an English word, it can only consider the words previously generated in that sentence. It cannot ‘look into the future’ because its fundamental task is to predict the next word based on the preceding context.</p>
<section id="evolution-into-pre-trained-language-models" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="evolution-into-pre-trained-language-models"><span class="header-section-number">3.1.1</span> Evolution into Pre-trained Language Models</h3>
<p>Shortly after Vaswani and colleagues published their seminal paper <span class="citation" data-cites="Vaswani2017">(<a href="#ref-Vaswani2017" role="doc-biblioref"><strong>Vaswani2017?</strong></a>)</span>, researchers began re-engineering the encoder and decoder streams individually. This work led to the development of pre-trained language models (PLMs). These PLMs represent a shift away from translation per se, towards models possessing a profound general understanding or generative capacity for language. Such models can subsequently undergo minor additional training, or fine-tuning, to perform a wide array of specific Natural Language Processing (NLP) tasks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram illustrating the general Transformer model architecture, showing inputs, embeddings, positional encoding, multi-head attention, and feed-forward layers within both encoder and decoder stacks.</figcaption>
</figure>
</div>
</section>
<section id="encoder-based-models-the-bert-family" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="encoder-based-models-the-bert-family"><span class="header-section-number">3.1.2</span> Encoder-based Models: The BERT Family</h3>
<p>The encoder component of the Transformer architecture gave rise to models like BERT (Bidirectional Encoder Representations from Transformers), first introduced by Devlin and colleagues in 2018 <span class="citation" data-cites="Devlin2018">(<a href="#ref-Devlin2018" role="doc-biblioref"><strong>Devlin2018?</strong></a>)</span>. The BERT family of models remains highly influential.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram comparing BERT (Encoder) on the left with a generic Transformer block on the right, illustrating data flow and key components like multi-head attention and feed-forward networks.</figcaption>
</figure>
</div>
<p>BERT’s defining characteristic, inherited from the encoder, is its bidirectionality. Each word in an input sequence can ‘attend’ to all other words in both directions (left and right). This allows BERT to build a deep, contextual understanding of the entire input at once. While the specifics of the acronym “Bidirectional Encoder-based Representations from Transformers” are less critical now, the core principle of full-context understanding remains paramount for these models.</p>
</section>
<section id="decoder-based-models-the-gpt-lineage" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="decoder-based-models-the-gpt-lineage"><span class="header-section-number">3.1.3</span> Decoder-based Models: The GPT Lineage</h3>
<p>On the other side of the architectural spectrum, researchers developed models based on the Transformer’s decoder component. Prominent amongst these are the GPT (Generative Pre-trained Transformer) models, which power systems like ChatGPT <span class="citation" data-cites="Radford2018">(<a href="#ref-Radford2018" role="doc-biblioref"><strong>Radford2018?</strong></a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram comparing BERT (Encoder) and GPT (Decoder) architectures within the broader Transformer framework, highlighting bidirectional full-context for BERT and unidirectional generative for GPT.</figcaption>
</figure>
</div>
<p>GPT models, due to their decoder-based structure, can only consider preceding tokens when generating new text. This unidirectional constraint, however, is precisely what enables them to generate novel text, a capability generally lacking in BERT-like models. Consequently, BERT and GPT models serve fundamentally different purposes: BERT excels at understanding language coherently, whilst GPT excels at producing language.</p>
<p>Beyond these two primary families, a diverse ecosystem of models exists. Some models combine encoder and decoder functionalities. Others employ sophisticated techniques to make decoders behave more like encoders for specific tasks, such as the XLM and XLNet architectures. Understanding the core distinction between generative models (like GPT) that produce language and full-context models (like BERT) that comprehensively understand sentences provides a crucial foundation.</p>
</section>
</section>
<section id="adapting-and-specialising-language-models" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="adapting-and-specialising-language-models"><span class="header-section-number">3.2</span> Adapting and Specialising Language Models</h2>
<p>The proliferation of scientific language models, as surveyed by Ho and colleagues <span class="citation" data-cites="Ho2024">(<a href="#ref-Ho2024" role="doc-biblioref"><strong>Ho2024?</strong></a>)</span>, underscores the diverse efforts to tailor these technologies for specific research domains.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.jpg" class="img-fluid figure-img"></p>
<figcaption>A landscape diagram of Scientific Large Language Models, showing a timeline and categorisation of various models by type (Encoders, Decoders, Enc-Dec) and indicating open-source versus closed-source status.</figcaption>
</figure>
</div>
<p>Adapting these powerful pre-trained models to specific scientific language or tasks involves several strategies.</p>
<section id="strategies-for-domain-adaptation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="strategies-for-domain-adaptation"><span class="header-section-number">3.2.1</span> Strategies for Domain Adaptation</h3>
<p>Initial pre-training, where a model first encounters language, demands substantial computational resources and data. During this phase, models learn language by predicting the next token (as in GPT models) or by predicting randomly masked words within a sentence (as in BERT models). For many research groups, undertaking full pre-training from scratch is infeasible.</p>
<p>A more accessible approach involves continued pre-training. Researchers can take an existing pre-trained model, such as a general BERT model, and continue its training on a specialised corpus, for instance, a collection of physics texts. This allows the model to adapt its parameters to the nuances of that specific domain.</p>
<p>Alternatively, one can use pre-trained models as feature extractors. By adding a few extra layers on top of a pre-trained model, researchers can train these new layers for specific downstream tasks, such as sentiment classification or named entity recognition.</p>
<p>Contrastive learning offers another key method, particularly for generating sentence or document embeddings. While word embeddings capture semantic relationships between words, many applications require representations for entire sentences or documents. SentenceBERT, for example, employs contrastive learning to fine-tune BERT-like models to produce meaningful sentence embeddings, placing semantically similar sentences close together in the embedding space. This technique is vital for tasks requiring semantic similarity assessment at a level beyond individual words.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">3.2.2</span> Retrieval Augmented Generation (RAG)</h3>
<p>Retrieval Augmented Generation (RAG) has emerged as a significant technique for adapting LLMs to specific domains or tasks, often without requiring extensive model retraining. RAG systems typically involve multiple models acting in concert.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram illustrating the Retrieval Augmented Generation (RAG) process, showing a ‘Retrieval’ phase (querying documents, pooling) and a ‘Generation’ phase (using retrieved context with a generative model).</figcaption>
</figure>
</div>
<p>In a RAG pipeline, a user query (e.g., “What are LLMs?”) is first encoded, often by a BERT-like model, into a sentence embedding. This embedding is then used to search a database of relevant documents, retrieving passages most similar to the query. These retrieved passages provide specific context. The pipeline then integrates this retrieved information into the prompt supplied to a generative model (like GPT). The generative model uses this augmented context to produce a more informed and domain-specific answer. Many contemporary applications, including some functionalities within ChatGPT that involve searching the internet, utilise RAG principles. Reasoning models and the increasingly discussed ‘agents’ are also typically not single LLMs but rather complex systems of LLMs combined with various other tools and data sources.</p>
</section>
<section id="key-distinctions-to-remember" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="key-distinctions-to-remember"><span class="header-section-number">3.2.3</span> Key Distinctions to Remember</h3>
<p>To navigate the LLM landscape effectively, several core distinctions warrant reiteration. These include the fundamental differences between encoder, decoder, and encoder-decoder architectures. Grasping various fine-tuning strategies is also essential. Furthermore, understanding the distinction between word embeddings and sentence (or document) embeddings is crucial, as they serve different analytical purposes. Finally, appreciating the different levels of abstraction at which these models operate—from token processing to document-level understanding—helps in selecting and applying them appropriately.</p>
</section>
</section>
<section id="applications-of-llms-in-hpss-research" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="applications-of-llms-in-hpss-research"><span class="header-section-number">3.3</span> Applications of LLMs in HPSS Research</h2>
<p>A nascent but growing body of work explores the utility of LLMs as tools within History and Philosophy of Science and Science Studies (HPSS) research. Preliminary surveys reveal several emerging categories of application.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide listing applications of LLMs in HPSS, categorised into: Dealing with data and sources, Knowledge structures, Knowledge dynamics, and Knowledge practices.</figcaption>
</figure>
</div>
<section id="categorising-hpss-applications" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="categorising-hpss-applications"><span class="header-section-number">3.3.1</span> Categorising HPSS Applications</h3>
<p>Researchers are employing LLMs for a variety of tasks:</p>
<ul>
<li>Dealing with data and sources: This includes parsing and extracting structured information from texts, such as publication types, acknowledgements, or citations. Interacting with sources through summarisation or RAG-type ‘chatting with your documents’ also falls into this category.</li>
<li>Analysing knowledge structures: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds. They also aid in mapping complex relationships, such as those between disciplines, interdisciplinary fields, or science-policy discourses.</li>
<li>Investigating knowledge dynamics: Conceptual histories of terms (e.g., “theory” in Digital Humanities, or “virtual” and “Planck” in physics) can be traced using LLM-derived embeddings. Identifying novelty, such as breakthrough papers or emerging technologies, represents another application.</li>
<li>Examining knowledge practices: LLMs can support argument reconstruction by identifying premises and conclusions or causal relationships. Citation context analysis, an established HPSS method, can be enhanced to determine the purpose or sentiment of citations. Discourse analysis, focusing on elements like hedge sentences, jargon, or boundary work, also benefits from these tools.</li>
</ul>
</section>
<section id="observed-trends-and-recurring-concerns" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="observed-trends-and-recurring-concerns"><span class="header-section-number">3.3.2</span> Observed Trends and Recurring Concerns</h3>
<p>The application of LLMs in HPSS exhibits several notable trends and prompts recurring concerns amongst researchers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide summarising trends and concerns in LLM use for HPSS: accelerating interest, varying customisation, repeating concerns (resources, opaqueness, data, benchmarks, model trade-offs), and a trend toward accessibility.</figcaption>
</figure>
</div>
<p>An accelerating interest in LLMs is evident, with studies appearing not only in information science journals like Scientometrics and JASIST but also increasingly in journals traditionally less focused on computational methods. This suggests that the semantic capabilities of LLMs are attracting qualitative researchers and philosophers.</p>
<p>The degree of customisation varies widely. Some researchers develop new architectures or undertake custom pre-training, whilst others fine-tune existing models or use off-the-shelf tools like ChatGPT.</p>
<p>Several concerns consistently surface. The substantial computational resources required for training and, in some cases, running large models pose a significant barrier. The ‘opaqueness’ or lack of interpretability of some models remains a challenge. A scarcity of suitable training data and domain-specific benchmarks for HPSS tasks is frequently noted. Researchers also grapple with trade-offs between different model types (e.g., BERT-like versus GPT-like). The potential for generative models to ‘hallucinate’ or produce plausible but incorrect information is another significant concern, although this issue is gradually improving with newer models and techniques like RAG.</p>
<p>Despite these challenges, a trend towards greater accessibility is apparent. Tools like BERTopic, which simplifies topic modelling, are gaining popularity due to their ease of use and robust maintenance.</p>
</section>
</section>
<section id="critical-reflections-and-future-pathways" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="critical-reflections-and-future-pathways"><span class="header-section-number">3.4</span> Critical Reflections and Future Pathways</h2>
<p>Engaging with LLMs in HPSS necessitates careful consideration of specific disciplinary challenges, a commitment to building LLM literacy, and adherence to core HPSS methodologies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide outlining key reflections: Acknowledging HPSS-specific challenges, Building LLM literacy, and Staying true to HPSS methodologies.</figcaption>
</figure>
</div>
<section id="acknowledging-hpss-specific-challenges" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="acknowledging-hpss-specific-challenges"><span class="header-section-number">3.4.1</span> Acknowledging HPSS-Specific Challenges</h3>
<p>Several challenges are particular to HPSS contexts. The historical evolution of concepts and language is crucial; LLMs are typically trained on modern language, which may lead to biases or misinterpretations when applied to historical texts. HPSS often adopts a reconstructive, critical, and reflective perspective, seeking to read between the lines and understand texts within their situated socio-historical contexts, including subtle discursive strategies. Current LLMs are not inherently trained for this type of nuanced reading. Furthermore, HPSS research frequently contends with sparse data, multiple languages, archaic scripts, and incompletely digitised archives.</p>
</section>
<section id="the-imperative-of-llm-literacy" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="the-imperative-of-llm-literacy"><span class="header-section-number">3.4.2</span> The Imperative of LLM Literacy</h3>
<p>To address these challenges effectively, HPSS researchers must cultivate LLM literacy. This involves familiarising themselves with the underlying principles of LLMs, NLP, and Deep Learning—encompassing both the tools and their theoretical underpinnings. It requires learning to identify the most appropriate model architectures and training regimes for specific HPSS research questions and data. The terminology itself is in flux; the term “LLM” may become less adequate as models become increasingly multimodal, incorporating images, sound, and other data types. The definition of “large” in “Large Language Model” also shifts rapidly with technological advancements. Developing shared datasets and benchmarks tailored to HPSS problems is another vital aspect of building collective literacy and capability. For tasks involving multilinguality, understanding which models are suitable or whether custom training is feasible given available resources becomes paramount.</p>
</section>
<section id="upholding-hpss-methodological-integrity" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="upholding-hpss-methodological-integrity"><span class="header-section-number">3.4.3</span> Upholding HPSS Methodological Integrity</h3>
<p>Whilst embracing new tools, it is essential to remain true to established HPSS methodologies. HPSS research problems must be thoughtfully translated into NLP tasks (e.g., classification, generation, summarisation) without allowing the technical task to overshadow or distort the original research question. Simultaneously, LLMs offer new opportunities for bridging qualitative and quantitative approaches, potentially fostering richer, mixed-methods research designs.</p>
<p>LLMs may offer novel ways to address core HPSS questions. For instance, contextualised word embeddings can track the evolving meanings of concepts like “Planck” across different contexts (Max Planck the person, Planck institutes, the Planck satellite, Planck length), revealing shifts in scientific discourse over time. There is potential, though requiring careful exploration, to use LLMs to investigate complex phenomena such as paradigm shifts and incommensurability.</p>
<p>Finally, HPSS can reflect on its own pre-history concerning some concepts now central to LLMs. For example, co-word analysis, developed in the 1980s by science studies scholars like Michel Callon and Arie Rip, shares intellectual roots with current embedding-based approaches to mapping knowledge landscapes.</p>
</section>
<section id="the-evolving-landscape-agents-and-beyond" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="the-evolving-landscape-agents-and-beyond"><span class="header-section-number">3.4.4</span> The Evolving Landscape: Agents and Beyond</h3>
<p>The field of language modelling is developing at a rapid pace. The rise of ‘agents’—systems where LLMs interact with other tools and data sources to perform complex tasks autonomously—signals a further evolution. Interestingly, some of the language used by computer scientists to describe these emerging agentic systems echoes concepts from Actor-Network Theory (ANT) and other STS frameworks, suggesting that HPSS theories may offer valuable conceptual tools for understanding and critically engaging with these technological advancements. The journey requires continuous learning, critical assessment, and a commitment to harnessing these powerful models responsibly in the pursuit of insightful HPSS research.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ai-nepi_001_chapter.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ai-nepi_004_chapter.html" class="pagination-link" aria-label="The Workflow and Utility of OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Workflow and Utility of OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>