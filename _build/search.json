[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "",
    "text": "Overview\nThis presentation systematically introduces the foundational architecture of large language models (LLMs), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (HPSS). As a co-organiser, the speaker initially provides a primer on the seminal Transformer architecture, explaining its encoder-decoder structure and original purpose in language translation.\nSubsequently, the discussion differentiates between encoder-based models, such as BERT, which offer bidirectional full-context understanding, and decoder-based generative models, like GPT, capable of producing novel text. The presentation then charts the proliferation of domain-specific LLMs across various scientific fields, outlining diverse adaptation strategies including pre-training, fine-tuning, and the sophisticated Retrieval Augmented Generation (RAG) pipeline.\nCrucially, the presentation categorises current LLM applications in HPSS, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, it offers critical reflections on HPSS-specific challenges, such as historical language evolution and sparse data. The speaker advocates for enhanced LLM literacy and a steadfast adherence to HPSS methodologies, highlighting new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#foundations-of-large-language-models-the-transformer-architecture-bert-and-gpt",
    "href": "chapter_ai-nepi_003.html#foundations-of-large-language-models-the-transformer-architecture-bert-and-gpt",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.1 Foundations of Large Language Models: The Transformer Architecture, BERT, and GPT",
    "text": "2.1 Foundations of Large Language Models: The Transformer Architecture, BERT, and GPT\n\n\n\nSlide 02\n\n\nThis presentation commenced by outlining its objectives: to provide a primer on Large Language Models (LLMs) and their scientific adaptations, to summarise current HPSS applications, and to offer critical reflections. At the core of modern LLMs lies the Transformer architecture, a pivotal development introduced by Vaswani and colleagues in 2017. Initially, researchers designed this architecture for language translation tasks, such as converting German to English. This ingenious architecture employs two interconnected streams: an encoder and a decoder.\nThe encoder processes an input sentence, for instance, in German, transforming its words into numerical representations. Crucially, within the encoder, every word interacts with all other words in the sentence, thereby constructing a comprehensive understanding of the sentence’s complete meaning. This mechanism operates bidirectionally. These numerical data then transition to the decoder stream. Here, the decoder generates words in the target language, say English, one by one. Each generated word references only its predecessors to predict the subsequent word; it cannot foresee future words in the sequence. This iterative process feeds each newly produced word back into the decoder until the entire translated sentence emerges. This part of the architecture functions unidirectionally. Both streams contain multiple layers that progressively refine and contextualise word embeddings.\nFrom this foundational design, researchers began re-engineering the encoder and decoder streams independently, leading to the advent of pre-trained language models (PLMs). These models possess a robust capacity to understand or generate language and serve as versatile bases for various Natural Language Processing (NLP) tasks, often requiring only minimal further training.\nOne prominent family of PLMs, derived from the encoder stream, is BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin and associates in 2018. BERT models distinguish themselves by allowing each input word to consider every other word in the context simultaneously, thereby achieving a thorough, full-context understanding. The term “bidirectional” encapsulates this ability to look both forwards and backwards within the text. Whilst powerful for comprehension, BERT models are not primarily designed for generating novel text. Conversely, GPT (Generative Pre-trained Transformers), developed by Radford and colleagues in 2018 and originating from the decoder stream, excels at text generation. Models like ChatGPT are powered by this technology. GPT’s architecture, where tokens only attend to their predecessors, underpins its generative strength. Beyond these, other architectures exist, including combined encoder-decoder models and sophisticated decoder applications that emulate encoder functionalities, such as XLM, which itself builds upon XLNet. A key takeaway highlights the fundamental difference between generative models like GPT, which produce language, and full-context models like BERT, which focus on coherent sentence understanding.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-and-adaptation-of-llms-for-scientific-applications",
    "href": "chapter_ai-nepi_003.html#evolution-and-adaptation-of-llms-for-scientific-applications",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.2 Evolution and Adaptation of LLMs for Scientific Applications",
    "text": "2.2 Evolution and Adaptation of LLMs for Scientific Applications\n\n\n\nSlide 06\n\n\nThe landscape of Large Language Models has evolved significantly, particularly with a focus on scientific domains, as documented by Ho and colleagues in their 2024 survey. This evolution reveals a diverse ecosystem where encoder models, such as BERT-types, are more prevalent than decoder, or GPT-type, models, alongside hybrid encoder-decoder systems. Early influential models in this scientific niche include BioBERT, Specter, and Cyber. Specialised LLMs now cater to a wide array of disciplines, including biomedicine, chemistry, material science, climate science, mathematics, physics, and the social sciences.\nAdapting these powerful models to the nuances of specific scientific language involves several distinct techniques. Full pre-training, where a model learns language from scratch by predicting next tokens (as in GPT) or masked words (as in BERT), demands vast computational power and data, often rendering it impractical for many research teams. A more feasible approach involves continued pre-training, where an existing pre-trained model undergoes further training on a specialised corpus; for instance, a general BERT model can be fine-tuned on physics literature, a method employed by the presenter. Another strategy involves adding extra parameters or layers atop a pre-trained model, then training these new components for specific downstream tasks like sentiment classification or Named Entity Recognition. Prompt-based adaptation offers another avenue, though its specifics were not elaborated upon. Furthermore, contrastive learning stands out as a crucial method for deriving sentence or document embeddings from word-level embeddings, with Sentence-BERT being a prominent example; the work of Irina Gurevich is notable in this area.\nBeyond direct model training, Retrieval Augmented Generation (RAG) offers a powerful pipeline for domain adaptation without necessitating complete model retraining. RAG systems are not monolithic; rather, they orchestrate multiple LLMs and other tools. A typical RAG workflow begins with a user query, such as “What are LLMs?”. A BERT-like model then encodes this query into a sentence embedding. This embedding facilitates a search across a database of relevant documents, retrieving the most similar passages. These retrieved texts subsequently augment the prompt fed to a generative model, which then formulates an answer based on this enriched context. This mechanism is commonly observed in systems like ChatGPT when they integrate information from internet searches. Indeed, the concept extends to more complex reasoning models and agents, which are sophisticated systems combining LLMs with a variety of other tools, representing a significant step beyond individual model capabilities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#llm-applications-trends-and-concerns-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#llm-applications-trends-and-concerns-in-hpss-research",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.3 LLM Applications, Trends, and Concerns in HPSS Research",
    "text": "2.3 LLM Applications, Trends, and Concerns in HPSS Research\n\n\n\nSlide 09\n\n\nTo navigate the complexities of LLMs effectively, researchers must bear in mind several key distinctions. These include the fundamental differences in architecture and pre-training methodologies (encoder-based, decoder-based, or hybrid), the variety of fine-tuning strategies available, the crucial conceptual gap between word embeddings and sentence embeddings, and the varying levels of abstraction from individual LLMs to integrated pipelines and sophisticated agents.\nA survey conducted on the utilisation of LLMs within HPSS research reveals applications falling into four broad categories:\n\nData and Source Management: Scholars employ LLMs for parsing and extracting specific information—such as publication types, acknowledgements, or citations—and for facilitating interaction with source materials through summarisation or RAG-style conversational interfaces.\nKnowledge Structure Analysis: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds, and in mapping intricate networks such as disciplinary formations, interdisciplinary fields, or science-policy discourses.\nKnowledge Dynamics Exploration: This domain benefits from LLM applications in tracing conceptual histories—for example, the evolution of terms like “theory” in Digital Humanities or “virtual” and “Planck” in physics—and in identifying novelty, such as breakthrough papers or emerging technologies.\nKnowledge Practice Investigation: LLMs are applied to reconstruct arguments by identifying premises and conclusions, to analyse citation contexts for purpose and sentiment (revitalising an older HPSS tradition), and to conduct discourse analysis to detect features like hedge sentences, jargon, or instances of boundary work.\n\nObservations indicate an accelerating interest in LLMs within HPSS, with research now appearing even in journals not traditionally associated with computational methods. This broadening appeal may stem from the enhanced semantic capabilities of modern LLMs, attracting qualitative researchers and philosophers. The degree of technical engagement varies widely, from researchers undertaking architectural modifications and custom pre-training, through those performing custom fine-tuning, to others using off-the-shelf tools like ChatGPT. Despite the enthusiasm, several concerns recur: the substantial computational resources required, the inherent opaqueness of many models, shortages of appropriate training data and standardised benchmarks for HPSS-specific tasks, and the inevitable trade-offs when selecting between model types, such as the comprehension strengths of BERT-like models versus the generative power of GPT-like alternatives. Nina’s pertinent point highlights that no single model suits all purposes; rather, one must find an adequate model for a given purpose. Encouragingly, a trend towards greater accessibility is evident, exemplified by tools like BERTTopic for topic modelling, which is gaining traction due to its user-friendliness and robust developer support, potentially emulating the widespread adoption previously seen with tools like pyLDAvis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#reflections-hpss-specific-challenges-and-methodological-considerations",
    "href": "chapter_ai-nepi_003.html#reflections-hpss-specific-challenges-and-methodological-considerations",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.4 Reflections: HPSS-Specific Challenges and Methodological Considerations",
    "text": "2.4 Reflections: HPSS-Specific Challenges and Methodological Considerations\n\n\n\nSlide 12\n\n\nEngaging with LLMs in HPSS necessitates acknowledging several unique challenges inherent to the discipline. A primary concern is the historical evolution of concepts and language. As Nina also highlighted, LLMs are predominantly trained on contemporary language, which can lead to biases or misinterpretations when applied to historical texts where meanings and linguistic conventions have shifted. Researchers must therefore devise strategies to train their own models on historical data or critically adapt existing models, remaining acutely aware of their inherent limitations.\nFurthermore, HPSS scholarship often adopts a reconstructive and critical perspective, seeking to read “between the lines,” understand the socio-historical context of texts, discern authorial intent, and identify subtle discursive strategies like boundary work. Current LLMs are not generally equipped for such nuanced analytical tasks, compelling the HPSS community to explore how models might be guided towards these deeper forms of interpretation. Data-specific issues also abound, including:\n\nThe frequent sparseness of relevant data.\nThe complexities of multilingual sources.\nThe difficulties presented by archaic scripts and orthographies.\nThe persistent lack of digitalisation for many historical archives.\n\nTo navigate these complexities, building robust LLM literacy within the HPSS community is paramount. This involves familiarising scholars not only with the practical application of LLMs, Natural Language Processing (NLP), and Deep Learning tools but also with their theoretical underpinnings. Researchers must cultivate the expertise to determine the most suitable model architectures and training approaches for their specific research questions. Developing shared datasets and benchmarks tailored to HPSS inquiries is also vital, preventing a reliance on off-the-shelf tools that might produce visually impressive but ultimately uninterpretable results.\nCrucially, whilst embracing these new technologies, HPSS scholars must remain true to their own methodologies. The objective should be to translate HPSS problems into solvable NLP tasks—such as classification, generation, or summarisation—without allowing the technical task to obscure or distort the original research focus. If approached thoughtfully, LLMs present new opportunities, particularly for bridging qualitative and quantitative research paradigms, a prospect the presenter finds especially compelling. Finally, it is worth reflecting on HPSS’s own “pre-history” in relation to LLM development. Concepts like co-word analysis, pioneered in the 1980s by Actor-Network Theory scholars such as Callon and Rip, demonstrate a longstanding disciplinary engagement with theoretically informed tool development, offering a rich intellectual heritage upon which to build.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  ```",
    "section": "",
    "text": "4.1 OpenAlex Mapper: Technical Foundations\ntitle: “Philosophy at Scale: Introducing OpenAlex Mapper” author: - name: “Maximilian Noichl & Andrea Loettgers” affiliation: “Utrecht University” email: “noichlmax@hotmail.co.uk” date: ‘2025’ bibliography: bibliography.bib — ## Overview {.unnumbered}\nMaximilian Noichl, Andrea Loettgers, and Taya Knuuttila introduced OpenAlex Mapper, a novel tool designed to facilitate transdisciplinary investigations within the history, philosophy, and sociology of science (HPSS). Developed by researchers at Utrecht University and the University of Vienna, this instrument received funding from an ERC grant focused on “possible life”. The presentation meticulously detailed the tool’s technical architecture, its operational workflow, and its diverse applications in scholarly analysis.\nOpenAlex Mapper directly addresses the inherent challenges of generalising findings from small samples and case studies prevalent in HPSS research. The tool leverages a fine-tuned Specter 2 language model, specifically adapted to discern disciplinary boundaries. This model processes a vast dataset of 300,000 randomly sampled English-language abstracts from the OpenAlex database. Subsequently, the system employs Uniform Manifold Approximation and Projection (UMAP) to reduce these high-dimensional embeddings into a two-dimensional, interactive base map.\nUsers can submit arbitrary queries to OpenAlex; the tool then embeds the retrieved abstracts and projects them onto this pre-existing map, thereby revealing their disciplinary locations. Crucially, OpenAlex Mapper supports qualitative, heuristic investigations by grounding them in extensive quantitative data, whilst always linking back to original textual sources.\nDemonstrations highlighted the tool’s utility in mapping the distribution of model templates, such as the Ising, Hopfield, and Sherrington-Kirkpatrick models, across scientific fields. Furthermore, the tool effectively visualises the spread of key concepts, exemplified by “phase transition” and “emergence”. It also analyses the interdisciplinary adoption patterns of specific methods, including “Random Forest” and “Logistic Regression”. The developers acknowledged certain limitations, notably OpenAlex’s data quality, the current English-only language model, the requirement for abstracts or robust titles, and the inherent stochasticity and dimensionality reduction trade-offs of the UMAP algorithm.\nMaximilian Noichl, Andrea Loettgers, and Taya Knuuttila introduced OpenAlex Mapper, a sophisticated tool developed through collaborative research. Noichl, a PhD candidate at Utrecht University’s Theoretical Philosophy Department, collaborated with Loettgers and Knuuttila from the University of Vienna’s Philosophy Department. This endeavour received funding from an ERC grant specifically supporting research on “possible life”.\nThe presentation systematically unfolded, first elucidating the tool’s core functions and high-level technical specifications. Subsequently, a live demonstration illustrated its practical application, culminating in a detailed discussion of its utility within the History, Philosophy, and Sociology of Science (HPSS) domain.\nAt the heart of OpenAlex Mapper’s functionality lies a meticulously fine-tuned Specter 2 language model. Researchers adapted this model to enhance its capacity for discerning disciplinary boundaries, training it on a dataset of articles from closely related fields to improve distinction. The resulting embeddings from this training process are then visually represented using a UMAP dimensionality reduction technique. Crucially, these modifications represent minor adjustments to the language model, rather than a comprehensive retraining effort.\nFor base-map preparation, the team drew upon the extensive OpenAlex database. This resource is renowned for its size and inclusivity, surpassing proprietary databases such as Web of Science and Scopus. Its open data policy facilitates easy, batch-query access, making it an invaluable asset for large-scale scholarly analysis. From this vast repository, researchers sampled 300,000 random articles, selecting only those with well-formed English abstracts. The fine-tuned Specter 2 model then processed these abstracts, generating high-dimensional embeddings.\nTo render these complex embeddings visually interpretable, the system employs Uniform Manifold Approximation and Projection (UMAP). This dimensionality reduction algorithm transforms the high-dimensional data into a two-dimensional base map, simultaneously producing a trained UMAP model. Consequently, when users submit arbitrary queries through the OpenAlex search interface, the tool first downloads the initial 1,000 records. It then embeds their abstracts using the same fine-tuned language model, and subsequently projects these new embeddings onto the pre-existing 2D map via the trained UMAP model. This inherent feature of UMAP ensures consistent and accurate positioning of new documents within the established disciplinary landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#introduction-project-context",
    "href": "chapter_ai-nepi_004.html#introduction-project-context",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.1 Introduction: Project Context",
    "text": "3.1 Introduction: Project Context\n\n\n\nSlide 01\n\n\nMaximilian Noichl, a doctoral candidate in theoretical philosophy at Utrecht University, presented collaborative work undertaken with Andrea Loettgers and Taja Knutila from the University of Vienna’s philosophy department. This research received funding from an European Research Council (ERC) grant, supporting the “Possible Life” project. The presentation primarily introduced OpenAlex Mapper, a novel tool for scholarly investigation.\nTo foster engagement, attendees accessed the presentation slides interactively via the website maxnechel.eu/talk, enabling local exploration of elements. The presentation first elucidated the tool’s operational principles and its high-level technical architecture. Following this explanation, a practical demonstration showcased its capabilities. Finally, the discussion turned towards the tool’s intended applications and its significance for research in the History and Philosophy of Science and Technology (HPSS).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.2 OpenAlex Mapper: Architecture and Workflow",
    "text": "3.2 OpenAlex Mapper: Architecture and Workflow\n\n\n\nSlide 02\n\n\nResearchers developed OpenAlex Mapper by first refining the Specter 2 language model. They meticulously fine-tuned this model, enhancing its capacity to discern and delineate disciplinary boundaries. Training involved a curated dataset of articles from closely related fields, thereby improving differentiation. This process entailed minor adjustments rather than extensive retraining; UMAP dimensionality reduction visualised its progress.\nSubsequently, the team leveraged the OpenAlex database, a vast, open repository of scholarly material renowned for its inclusivity and accessibility, surpassing alternatives such as Web of Science or Scopus. From OpenAlex, they sampled 300,000 random articles, stipulating only that these possessed well-formed abstracts and were in English. The fine-tuned Specter 2 model then embedded these selected abstracts. Following embedding, Uniform Manifold Approximation and Projection (UMAP) reduced the high-dimensional data to a two-dimensional representation. Crucially, the team preserved this UMAP model.\nOpenAlex Mapper, accessible at https://m7n-openalex-mapper.hf.space, leverages this foundational work. The tool permits users to submit arbitrary queries to the OpenAlex database. It then downloads the results, embeds them using the identical Specter 2 model, and projects them onto the pre-existing two-dimensional UMAP map. A key feature of UMAP facilitates this, ensuring new data points position themselves as if they had formed part of the original map’s construction. As its informational page describes, OpenAlex Mapper projects search queries onto a background map of randomly sampled papers. This enables users to investigate interdisciplinary connections, for instance, by searching for topics such as the Kuramoto model or Wittgenstein’s Philosophical Investigations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#demonstration-interactive-features",
    "href": "chapter_ai-nepi_004.html#demonstration-interactive-features",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.3 Demonstration: Interactive Features",
    "text": "3.3 Demonstration: Interactive Features\n\n\n\nSlide 07\n\n\nA live demonstration vividly illustrated OpenAlex Mapper’s functionality. Users access the tool by navigating to OpenAlex, conducting a search—for instance, for “scale-free network models”—and then copying the resultant URL. They subsequently paste this URL into OpenAlex Mapper’s designated input field. Users can adjust various settings, including sample size and selection methods, before initiating the query.\nUpon query execution, the system commences its backend operations. Initially, it downloads a specified number of records from the search results; for demonstration purposes, this was limited to the first thousand to conserve time. The tool then embeds the abstracts of these downloaded articles. If selected, it also processes and prepares the citation graph associated with these results. The outcome presents a visual projection: search results appear as points on a grey base map, indicating their distribution across the scholarly landscape.\nThis fully interactive map empowers users to delve into specific areas and individual data points. One might, for example, investigate unexpected occurrences, such as references to “coriander” within epidemiological literature. Furthermore, clicking any paper within the visualisation directs the user to its original online source, ensuring a constant link to the underlying textual data. Additional configurable options include visualising temporal distributions of publications or overlaying the citation network. An alternative, more powerful version of the tool, utilising a higher latency GPU setup for larger queries, was also briefly made available.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#rationale-applications-in-hpss",
    "href": "chapter_ai-nepi_004.html#rationale-applications-in-hpss",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.4 Rationale: Applications in HPSS",
    "text": "3.4 Rationale: Applications in HPSS\n\n\n\nSlide 13\n\n\nThe development of OpenAlex Mapper addresses specific challenges prevalent within the History and Philosophy of Science and Technology (HPSS). Primarily, it assists researchers in grappling with the inherent limitations of small samples and individual case studies. HPSS often yields rich, detailed understandings of scientific processes through methods such as close textual analysis, direct engagement with scientists, or ethnographic studies. A persistent concern, however, centres on generalising these nuanced insights or validating them against the backdrop of contemporary science, characterised by its global scale, vast output, and rapid pace of discovery.\nOpenAlex Mapper offers a potent means to bridge this gap. It enables researchers to pose questions such as: “Where did the Hopfield model, initially developed in a specific context, truly gain traction and see continued use across various scientific domains?” The tool facilitates tracing the dissemination and adoption of models, methods, or concepts. Consequently, it supports sophisticated quantitative methodologies, underpinning essentially qualitative and heuristic investigations. Its design fosters an iterative analytical process, allowing users to move fluidly between a broad overview on the map and in-depth scrutiny of individual papers, whilst always maintaining a direct link back to the original textual sources. Further technical details and discussions are available in a working paper and the online presentation slides.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nResearchers at Uppsala University have pioneered a procedural investigation into genre classification for historical medical periodicals. This endeavour forms a core component of the ERC-funded ActDisease project, which meticulously examines the history of patient organisations in 20th-century Europe. The European Research Council (ERC) provides funding for this initiative. The project’s central aim involves scrutinising the profound influence of these organisations on disease concepts, illness experiences, and prevailing medical practices. Primarily, the project leverages a private, recently digitised collection of patient organisation magazines from Sweden, Germany, France, and Great Britain, encompassing an impressive 96,186 pages published between approximately 1890 and 1990.\nDigitisation efforts, employing ABBYY FineReader Server 14, successfully processed most common layouts. Nevertheless, complex layouts, slanted text, and rare fonts posed persistent challenges, frequently leading to Optical Character Recognition (OCR) errors, particularly in German and French texts. Recognising the diverse and co-occurring text types within these periodicals, the team identified genre classification as a crucial methodological advancement. This approach directly addresses the limitations and potential biases of traditional topic models and term counts, which often fail to account for the varied communicative purposes embedded within single pages.\nTo address the scarcity of annotated data, the project explored both zero-shot and few-shot learning paradigms. Zero-shot learning involves applying a model to data from categories it has not seen during training, relying on its understanding of the categories’ descriptions. Few-shot learning, conversely, trains a model with only a very small number of examples per category. Researchers defined a bespoke set of nine genre labels—Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and Question and Answer (QA)—under the direct supervision of a specialist historian. Annotation involved six project members, achieving a high inter-annotator agreement of 0.95 Krippendorff’s alpha on paragraphs sampled from Swedish and German periodicals.\nFor zero-shot experiments, the team leveraged publicly available datasets, including the Corpus of Online Registers of English (CORE), Functional Text Dimensions (FTD), and UD-MULTIGENRE (UDM). They performed a rigorous cross-dataset genre mapping to align these external datasets with their custom labels. Multilingual encoder models, specifically XLM-Roberta, mBERT, and historical mBERT, underwent fine-tuning across 48 configurations. Findings indicated that models fine-tuned on FTD performed optimally with the custom mapping, whilst historical mBERT demonstrated particular efficacy in distinguishing between fiction and nonfiction prose in few-shot settings.\nFurthermore, the project investigated few-shot prompting with Llama 3.1 8b Instruct, a large language model (LLM), revealing its capacity to handle certain genre labels effectively. However, a limited number of examples proved insufficient for comprehensive representation across all categories. Ultimately, the research concludes that genre classification significantly enhances the accessibility of historical periodical sources for text mining. Few-shot learning of multilingual encoders, particularly historical mBERT with prior Masked Language Model (MLM) fine-tuning, offers the most robust performance. Ongoing work includes developing a more fine-grained annotation scheme, generating synthetic data, and implementing active learning strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#overview",
    "href": "chapter_ai-nepi_005.html#overview",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "",
    "text": "Researchers associated with the ActDisease project investigate the historical role of patient organisations in shaping modern medicine, primarily through analysing their periodicals. The project involves a substantial dataset of 96,186 pages from magazines published by patient organisations in Sweden, Germany, France, and Great Britain between approximately 1890 and 1990. Digitisation of these materials, using ABBYY FineReader Server 14 for Optical Character Recognition (OCR), encountered challenges such as complex layouts, varied fonts, and inconsistent scan quality, leading to OCR errors, particularly in German and French texts, and disrupted reading order. Consequently, the team conducted experiments on post-OCR correction for historical German periodicals using instruction-tuned generative models (Danilova  Aangenendt, 2023). A key analytical challenge arises from the diverse range of text genres within these periodicals—including administrative reports, advertisements, and patient narratives often appearing on the same page—which can bias standard text mining approaches like topic modelling and term counts.\n\n  To address this, investigators developed a genre classification system. They defined nine distinct genres (Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and Question  Answer) under the guidance of a historian specialising in patient organisations. For annotation, project members utilised paragraphs merged by font patterns from two specific periodicals: the Swedish \"Diabetes\" and the German \"Diabetiker Journal\", achieving a high inter-annotator agreement of 0.95 Krippendorff's alpha. Given the limited availability of annotated data (1,182 paragraphs for training and 552 for a held-out set), the research explored both zero-shot and few-shot learning techniques.\n\n  Zero-shot classification experiments involved mapping labels from publicly available datasets—CORE, FTD, and UD-MULTIGENRE—to the custom ActDisease genres. Multilingual encoder models, including XLM-Roberta, mBERT, and a historical mBERT variant (hmBERT), underwent fine-tuning on these mapped datasets. Models fine-tuned on the FTD dataset generally demonstrated better performance with the project's mapping. Notably, specific model-dataset pairings exhibited superior efficacy for certain genres; for instance, XLM-Roberta fine-tuned on UDM excelled at identifying QA sections, whilst hmBERT fine-tuned on UDM performed well on Administrative texts.\n\n  Few-shot learning experiments revealed that additional training on the ActDisease dataset, particularly with prior Masked Language Model (MLM) fine-tuning, significantly improved performance. The hmBERT-MLM model emerged as the top performer, showing particular strength in distinguishing between fiction and nonfiction, a common point of confusion for other models. Although F1 scores improved with increased training instances, they generally remained below 0.8 even with the full training set of 1,182 paragraphs. Further experiments involved few-shot prompting of the Llama-3.1 8b Instruct model, which managed some labels reasonably well but highlighted the insufficiency of only two to three examples per genre for capturing the complexity of categories like nonfiction prose.\n\n  The findings underscore the genre-rich nature of popular historical magazines, which complicates text mining efforts compared to more uniform sources like scientific journals. Genre classification offers a pathway to make these diverse sources more accessible for detailed analysis, enabling comparisons of communicative strategies across different countries, diseases, and publications over time. When training data is scarce, leveraging existing modern datasets or employing few-shot prompting with generative models present viable options. However, few-shot learning with multilingual encoders, especially historical mBERT enhanced by prior MLM fine-tuning, yielded the most promising results, with hmBERT showing substantial gains. Current and future work focuses on applying these methods to specific historical hypotheses, developing a new, more fine-grained annotation scheme (supported by Swe-CLARIN), exploring synthetic data generation, and implementing active learning strategies to refine the classification quality.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-and-dataset",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-and-dataset",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.1 The ActDisease Project and Dataset",
    "text": "4.1 The ActDisease Project and Dataset\n\n\n\nSlide 01\n\n\n    Researchers at Uppsala University are engaged in the ActDisease project, an ERC-funded initiative titled \"Acting out Disease - How Patient Organizations Shaped Modern Medicine.\" This project delves into the histories of patient organisations across 20th-century Europe, specifically examining their contributions to evolving disease concepts, illness experiences, and medical practices. The investigation centres on approximately ten patient organisations located in Sweden, Germany, France, and Great Britain, with a temporal focus spanning roughly from 1890 to 1990. Periodicals, predominantly magazines, published by these organisations constitute the main source material; an early example of such an organisation is the Hay Fever Association of Heligoland, established in Germany in 1897.\n\n    The ActDisease dataset comprises a private, recently digitised collection of these patient organisation magazines, amounting to a substantial 96,186 pages. This corpus includes materials from Germany covering Allergy/Asthma (10,926 pages, 1901-1985), Diabetes (19,324 pages, 1931-1990), and Multiple Sclerosis (5,646 pages, 1954-1990). Swedish contributions include periodicals on Allergy/Asthma (4,054 pages, 1957-1990), Diabetes (7,150 pages, 1949-1990), and Lung Diseases (16,790 pages, 1938-1991). From France, the dataset contains materials on Diabetes (6,206 pages, 1947-1990) and Rheumatism/Paralysis (9,317 pages, 1935-1990). Finally, UK sources cover Diabetes (11,127 pages, 1935-1990) and Rheumatism (5,646 pages, 1950-1990).\n\n    Engineers employed ABBYY FineReader Server 14 for the Optical Character Recognition (OCR) process. Whilst this tool performed well on most common layouts and fonts, significant challenges arose from complex page designs, slanted text, rare typefaces, and inconsistent quality of scans or photographs. Consequently, residual issues such as OCR errors—particularly prevalent in German and French documents—and disrupted reading order affect the digitised collection. To mitigate these problems, the team undertook experiments in post-OCR correction of historical German texts using instruction-tuned generative models (Danilova  Aangenendt, 2023). Observations also indicate a higher frequency of OCR errors within creative textual forms, including advertisements, humour sections, and poetry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-and-manifestations-of-genre-in-actdisease-periodicals",
    "href": "chapter_ai-nepi_005.html#rationale-and-manifestations-of-genre-in-actdisease-periodicals",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.2 Rationale and Manifestations of Genre in ActDisease Periodicals",
    "text": "4.2 Rationale and Manifestations of Genre in ActDisease Periodicals\n\n\n\nSlide 11\n\n\n    Investigators exploring the ActDisease materials discovered a wide diversity of textual content, which, paradoxically, exhibited similarities in composition across the various magazines. A significant challenge arises from this diversity: different text types, such as administrative reports, advertisements, and humour sections, frequently appear side-by-side on a single page. This heterogeneity means that conventional analytical methods, like yearly or decade-based topic models and term counts, fail to capture such intra-page variations. Consequently, these methods are likely to produce results biased towards the most prevalent text types within the corpus.\n\n    To address these analytical limitations, the concept of 'genre' proved useful for distinguishing between different kinds of texts, particularly as genres are intrinsically linked to the communicative purposes of authors. Adopting a definition from Language Technology, a genre is understood as a class of documents sharing a common communicative purpose (Petrenz, 2004; Kessler, 1997). The ability to classify texts by genre is crucial for achieving a key research objective: exploring the dataset from multiple perspectives to construct robust historical arguments. Specifically, genre classification facilitates the study of evolving communicative strategies over time, across different countries, diseases, and publications (Broersma, 2010). Furthermore, it allows for more nuanced, fine-grained analyses of term distributions and topic models within distinct genre categories.\n\n    The ActDisease periodicals manifest a rich tapestry of genres. Examples include poetry, academic reports detailing scientific studies (such as research on the pancreas), and legal documents like deeds of covenant. Commercial content appears in the form of advertisements, for instance, promoting chocolate suitable for diabetics. Instructive or guidance texts offer practical advice, encompassing recipes or medical counsel on diet. Patient organisation reports document internal activities, such as meetings. Crucially for the project, narratives detailing patient experiences and aspects of their lives also form a distinct category.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#methodology-for-genre-classification-experiments",
    "href": "chapter_ai-nepi_005.html#methodology-for-genre-classification-experiments",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.3 Methodology for Genre Classification Experiments",
    "text": "4.3 Methodology for Genre Classification Experiments\n\n\n\nSlide 04\n\n\n    Owing to a scarcity of annotated data, researchers embarked on an exploration of zero-shot and few-shot learning methodologies for genre classification. The zero-shot learning investigation posed two primary questions: firstly, whether genre labels from publicly available datasets could be efficiently mapped to the project's custom labels, and secondly, how classification performance would vary when using different datasets and models. For few-shot learning, the inquiry focused on how performance changes with varying training set sizes across different models, and whether prior fine-tuning on the complete dataset could substantially boost performance. Details of this experimental design are documented by Danilova and Söderfeldt (2025).\n\n    The genre labels themselves were meticulously defined under the supervision of the project's principal historian, an expert on patient organisations. These labels were crafted to be effective in segregating content within the historical materials for detailed analysis, whilst also aiming for a degree of general-purpose applicability to similar datasets. Nine distinct genres were established: Academic (research-based reports, aiming to disseminate scientific information), Administrative (documents on organisational activities), Advertisement (commercial promotions), Guide (instructional texts), Fiction (entertaining narratives), Legal (documents explaining terms and conditions), News (reports on recent events), Nonfiction Prose (narratives of real events or cultural topics), and QA (question-and-answer sections from periodicals).\n\n    For the annotation process, the fundamental unit was the paragraph, derived from ABBYY OCR output and subsequently merged based on font patterns (type, size, bold, italic) at the page level. Annotators sampled content from first and mid-year issues of two periodicals: the Swedish \"Diabetes\" and the German \"Diabetiker Journal\". A team of four historians and two computational linguists, all either native or proficient in Swedish and German, undertook the annotation, with two independent annotations collected for every paragraph. This process yielded a high average inter-annotator agreement of 0.95, measured by Krippendorff's alpha. Annotators utilised spreadsheet files (e.g., .numbers) where they made hard assignments of genres to paragraphs.\n\n    The annotated data was partitioned into a training set of 1,182 paragraphs and a held-out set of 552 paragraphs (approximately 30% of the total), with stratification by label. For few-shot experiments, six different training set sizes (100, 200, 300, 400, 500, and 1,182 paragraphs) were created by random, balanced sampling from the main training set. The held-out set was further divided equally into validation and test sets, also balanced by label. However, the 'legal' and 'news' genres were excluded from these few-shot experiments due to an insufficient number of training instances. For zero-shot experiments, the entire test portion of the held-out set served as the evaluation data. Analysis of genre distribution within the ActDisease training and held-out samples revealed a strong imbalance for the 'advertisement' and 'nonfictional prose' categories across the Swedish and German language materials.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-genre-classification-approach-and-results",
    "href": "chapter_ai-nepi_005.html#zero-shot-genre-classification-approach-and-results",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.4 Zero-Shot Genre Classification: Approach and Results",
    "text": "4.4 Zero-Shot Genre Classification: Approach and Results\n\n\n\nSlide 36\n\n\n    For the zero-shot experiments, investigators utilised several publicly available datasets. These included the Corpus of Online Registers of English (CORE), featuring English texts with main categories also available in Swedish, Finnish, and French, annotated at the document level (Egbert et al., 2015). Another source was the Functional Text Dimensions (FTD) dataset, a balanced collection of English and Russian web genres also annotated at document level (Sharoff, 2018), previously employed in web genre classification work (Kuzman et al., 2023). Additionally, UD-MULTIGENRE (UDM), a subset of Universal Dependencies spanning 38 languages with recovered sentence-level genre annotations, provided further training material (de Marneffe et al., 2021; Danilova and Stymne, 2023).\n\n    Two annotators independently performed the critical task of mapping genre labels from these external datasets to the ActDisease project's custom genre schema. Only mappings where both annotators reached full agreement were incorporated into the final schema. This process revealed that some ActDisease genres, such as 'Administrative' and 'QA' in certain datasets, lacked direct equivalents in the external sources. For instance, 'Academic' in ActDisease mapped to 'research article' in CORE, 'academic' in UDM, and 'academic (A14)' in FTD.\n\n    The creation of training data followed a pipeline involving mapping, preprocessing, chunking, and sampling. Each external dataset yielded training sets in four distinct configurations: considering only Germanic languages [G+], balancing by ActDisease labels [B1], including all language families [G-], and balancing by both ActDisease and original dataset labels [B2]. This resulted in four training samples each from FTD, CORE, and UDM. Researchers selected three multilingual encoder models for these experiments: XLM-Roberta (Conneau et al., 2020), noted as a state-of-the-art web genre classifier (Kuzman et al., 2023); mBERT (Devlin et al., 2019), included for comparison; and historical mBERT (hmBERT) (Schweter et al., 2022), which is pretrained on a large corpus of multilingual historical newspapers that include the languages present in the ActDisease data. These BERT-like models have seen extensive use in prior work on web register and genre classification. Fine-tuning these models across all configurations produced a total of 48 distinct fine-tuned models, with reported metrics representing averages across these configurations.\n\n    Evaluating zero-shot predictions presented a challenge due to the imperfect overlap between the label sets, making direct comparison of overall performance metrics problematic. Therefore, analysts examined the performance for each genre individually and scrutinised confusion matrices. The X-GENRE web genre classifier (Kuzman et al., 2023) served as a baseline, with predictions made on the most similar labels directly mappable to the ActDisease schema. The experimental setup was entirely cross-lingual for the FTD dataset and the X-GENRE baseline (which lack German or Swedish training data) and partially cross-lingual for the UDM and CORE datasets.\n\n    Overall, models fine-tuned on the FTD dataset, using the ActDisease mapping, generally performed better and exhibited less bias in most configurations, with per-genre metrics indicating good performance. Conversely, models trained on UDM and CORE datasets displayed class-specific biases: UDM-trained models tended to favour 'news' (as its news training data contained the highest number of Germanic instances, predominantly German), whilst CORE-trained models showed a bias towards 'guide' (its only multilingual training data). Interestingly, certain model-dataset combinations demonstrated particular strengths: XLM-Roberta fine-tuned on UDM achieved, on average, 32% more correct predictions for the QA genre compared to mBERT and hmBERT. Similarly, hmBERT fine-tuned on UDM yielded 16% more correct predictions for the 'Administrative' genre than XLM-Roberta and mBERT. Models based on CORE data proved effective at predicting the 'legal' genre. Analysis of per-category F1 scores, averaged across data configurations, helped identify performances not attributable to systematic biases.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-for-genre-classification-performance-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-for-genre-classification-performance-evaluation",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.5 Few-Shot Learning for Genre Classification: Performance Evaluation",
    "text": "4.5 Few-Shot Learning for Genre Classification: Performance Evaluation\n\n\n\nSlide 13\n\n\n    The investigation into few-shot learning revealed that further training on the ActDisease dataset yields clear advantages, particularly when models undergo prior fine-tuning using a Masked Language Model (MLM) objective. Performance, measured by F1 scores, consistently increased with the number of training instances. Nevertheless, even with the largest training set of 1,182 paragraphs, F1 scores generally remained below the 0.8 threshold.\n\n    Amongst the models tested, historical mBERT with MLM pre-training (hmBERT-MLM) demonstrated superior performance. A key factor in its success appears to be its sustained ability to differentiate between the 'fiction' and 'nonfiction' genres, even when trained on the full dataset. In contrast, other models, most notably XLM-Roberta, exhibited a significant decline in their capacity to distinguish these two genres as the training data size increased. Examination of detailed scores and confusion matrices for XLM-Roberta-MLM trained on the full dataset showed that it frequently misclassified 'fiction' instances as 'nonfictional prose'.\n\n    Researchers hypothesise that this growing confusion between 'fiction' and 'nonfictional prose' may stem from their increasing similarity within the specific domain of the ActDisease corpus. Since all genres are confined to patient organisation magazines largely focused on diabetes, both fictional narratives and (auto)biographical accounts often centre on the experiences of individuals with diabetes. This thematic and structural overlap likely makes them harder to distinguish as more examples are seen by the models. This suggests that acquiring even more annotated data might be necessary to enhance the classifier's ability to resolve these nuanced distinctions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#investigating-few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#investigating-few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.6 Investigating Few-Shot Prompting with Llama-3.1 8b Instruct",
    "text": "4.6 Investigating Few-Shot Prompting with Llama-3.1 8b Instruct\n\n\n\nSlide 36\n\n\n    Given the current insufficiency of annotated data for comprehensive instruction tuning, investigators explored few-shot prompting as an alternative. They selected Llama-3.1 8b Instruct, a widely recognised multilingual generative model with open weights, for this experiment. The prompt design incorporated definitions for each genre along with two or three carefully selected examples intended to illustrate the target category.\n\n    Evaluation of Llama-3.1 8b Instruct's few-shot prediction performance on the entire held-out set (the zero-shot test set) yielded varied results across genres. The model achieved an F1-score of 0.84 for 'legal' texts, 0.73 for 'advertisement', and 0.72 for 'academic' content. Other scores included 0.64 for 'fiction', 0.62 for 'QA', 0.61 for 'guide', and 0.60 for 'administrative'. Performance was notably lower for 'nonfictional prose' (0.49) and especially for 'news' (0.08). The overall accuracy reached 0.62, with a macro average F1-score of 0.59 and a weighted average F1-score of 0.63.\n\n    These outcomes indicate that the model can handle certain labels with a fair degree of success using only a few examples. However, the experiment also underscored a significant limitation: providing merely two or three examples per genre proved insufficient for the model to adequately represent and distinguish more complex or nuanced categories such as 'nonfictional prose', 'advertisement', and 'administrative' texts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#summary-of-findings-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#summary-of-findings-and-future-research-directions",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.7 Summary of Findings and Future Research Directions",
    "text": "4.7 Summary of Findings and Future Research Directions\n\n\n\nSlide 36\n\n\n    The research culminates in several key conclusions regarding genre classification in historical periodicals. Firstly, popular magazines, unlike more uniform scientific journals or books, often contain a multitude of genres. This richness, reflecting deliberate communicative strategies, complicates text mining efforts. Secondly, genre classification emerges as a vital tool to render these complex sources accessible for text mining, facilitating more accurate and detailed interpretations of results and enabling comparisons across diverse sources from novel perspectives.\n\n    For scenarios lacking specific training data, investigators found it possible to successfully leverage existing modern datasets, provided the target genre categories are sufficiently general-purpose. Another viable approach involves few-shot prompting of open generative models, which can achieve decent classification quality even with limited examples. However, when some annotated data is available, few-shot learning using multilingual encoders such as XLM-Roberta or historical multilingual BERT, particularly when augmented with prior MLM fine-tuning, proves to be a superior strategy; indeed, this was the most effective method in the reported experiments. Notably, historical multilingual BERT demonstrated particularly strong performance gains from MLM fine-tuning, exhibiting a 24% improvement, which surpassed the gains seen for mBERT-MLM (14.5%) and XLM-RoBERTa-MLM (16.9%).\n\n    Current and future endeavours aim to build upon these findings. The team is now working with specific historical hypotheses, leveraging the developed classification methods. A new annotation scheme featuring more fine-grained genres is under development, supported by a new annotation project financed by Swe-CLARIN. Furthermore, researchers are exploring synthetic data generation and the implementation of active learning techniques to enhance classification quality and efficiency. The project acknowledges the contributions of the annotation team (Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, Gijs Aangenendt), funding from the European Research Council (ERC-2021-STG 10104099), support from the Centre for Digital Humanities and Social Sciences (for GPUs and data storage), and valuable input from reviewers including Dr Maria Skeppstedt and anonymous reviewers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC (European Research Council) Starting Grant initiative, commenced in 2023 and is set to conclude in 2028. Researchers at the Vrije Universiteit Brussel (VUB) lead this ambitious undertaking, which aims to trace the profound influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition on the development of natural philosophy and science. A dedicated team of five, comprising Professor Cornelis J. Schilt (Principal Investigator), Dr. Eszter Kovács, Dr. Jeffrey Wolf, Niccolò Cantoni, and Demetrios Paraschos, drives this interdisciplinary effort, combining rigorous historical scholarship with advanced digital humanities methodologies.\nThe project identifies key texts embodying this tradition, such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence confirms the tradition’s significant impact on prominent figures; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum [Citation Needed for Historical Claims]. Beyond these well-known examples, the VERITRACE team aims to uncover a broader, often overlooked network of texts and authors, which scholars collectively term the ‘great unread’ [Citation Needed for “great unread” coinage].\nTo achieve this ambitious goal, the project pioneers a computational approach to the history and philosophy of science (HPSS). This involves large-scale multilingual exploration, employing both keyword search and sophisticated text matching techniques. The methodology seeks to identify both direct lexical reuse (e.g., direct quotations) and indirect semantic influence (e.g., paraphrases or subtle allusions) across a vast corpus. Essentially, the system functions as an ‘early modern plagiarism detector’, designed to reveal hidden networks of texts, passages, themes, topics, and authors, thereby potentially uncovering novel patterns in intellectual history.\nThe project’s substantial dataset comprises approximately 430,000 printed texts, spanning nearly two centuries from 1540 to 1728. These digital texts originate from three primary multilingual sources—Early English Books Online (EEBO), Gallica (from the French National Library), and the Bavarian State Library—encompassing at least six languages. Researchers apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to analyse this extensive corpus.\nThe team confronts several significant challenges inherent in processing historical texts at scale. Notably, the variable quality of Optical Character Recognition (OCR) text poses a primary concern. Libraries provide this raw text directly in formats such as XML, HOCR, and HTML, often without corresponding page images. Early modern typography and the evolving semantics across multiple languages further complicate processing. Moreover, the sheer volume of data—hundreds of thousands of texts printed across Europe—presents a formidable computational hurdle.\nThe project strategically leverages Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as ‘judges’ for enriching and cleaning metadata. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages, facilitating advanced text matching. An alpha version of the VERITRACE web application currently serves as an internal testing ground, showcasing the project’s ambitious capabilities. This application features a complex 15-stage data processing pipeline, transforming raw text into an Elasticsearch database. It offers functionalities for exploring corpus statistics, examining rich metadata, conducting advanced keyword searches, and, crucially, performing lexical and semantic text matching. Whilst the current BERT-based embedding model (LaBSE) demonstrates promise for semantic comparisons, particularly across languages, its limitations with historical and out-of-domain data necessitate further investigation into alternative models or fine-tuning strategies. Scaling the system to accommodate the entire corpus and managing the evolving semantics of historical languages remain key issues for future development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-veritrace-context-and-objectives",
    "href": "chapter_ai-nepi_006.html#project-veritrace-context-and-objectives",
    "title": "5  VERITRACE",
    "section": "5.1 Project VERITRACE: Context and Objectives",
    "text": "5.1 Project VERITRACE: Context and Objectives\n\n\n\nSlide 02\n\n\nResearchers at Vrije Universiteit Brussel spearhead the VERITRACE project, a five-year initiative generously funded by an ERC Starting Grant (101076836) from 2023 to 2028. Formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy”, the project involves a dedicated team of five. Professor Cornelis J. Schilt serves as Principal Investigator, supported by classicist Dr Eszter Kovács, historians Niccolò Cantoni and Demetrios Paraschos, and Dr Jeffrey Wolf, a historian of science and medicine who functions as the project’s digital humanities specialist. Although the core team is based in Brussels, Dr Wolf resides in Berlin.\nThe central objective of VERITRACE is to meticulously trace the influence of an early modern ‘ancient wisdom’ or ‘Prisca Sapientia’ tradition upon the burgeoning field of natural philosophy during that era. This profound tradition manifests in significant texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the widely recognised Corpus Hermeticum. Indeed, a curated collection of 140 works forms the project’s close reading corpus for this tradition.\nWhilst established connections, such as Newton’s engagement with the Sibylline Oracles or Kepler’s knowledge of the Corpus Hermeticum, provide a foundational understanding, VERITRACE seeks to extend far beyond these known instances. The team endeavours to uncover a substantially broader network of texts and authors who interacted with this ancient wisdom, including many overlooked works often referred to as “the great Unread.” Further project details are available via their website: HTTPS://VERITRACE.EU.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-framework-and-multilingual-data-corpus",
    "href": "chapter_ai-nepi_006.html#computational-framework-and-multilingual-data-corpus",
    "title": "5  VERITRACE",
    "section": "5.2 Computational Framework and Multilingual Data Corpus",
    "text": "5.2 Computational Framework and Multilingual Data Corpus\n\n\n\nSlide 04\n\n\nTo address its ambitious research questions, the VERITRACE project employs a computational History and Philosophy of Science and Scholarship (HPSS) framework. This framework facilitates large-scale, multilingual exploration of the extensive textual data. Core to this approach is the identification of textual reuse, encompassing both direct lexical borrowings—such as unacknowledged quotations—and more subtle, indirect semantic appropriations, like paraphrases or allusions that contemporary readers would have readily recognised. Effectively, the team aims to construct what might be termed an “Early Modern Plagiarism Detector.” Beyond merely identifying reuse, the project seeks to uncover previously overlooked networks connecting texts, specific passages, overarching themes, topics, and authors, thereby hoping to reveal novel patterns in the intellectual history and philosophy of science.\nThe foundation for this computational inquiry is a substantial and diverse multilingual dataset. Researchers focus exclusively on digital texts of printed works, setting aside handwritten materials for this project. The corpus spans approximately two centuries, from 1540 to 1728, a period concluding just after Isaac Newton’s death. These texts originate from at least six different languages and are drawn from three primary sources: the freely downloadable Early English Books Online (EEBO), Gallica (the digital repository of the French National Library), and, most significantly, the Bavarian State Library. Collectively, these sources contribute to a corpus of around 430,000 texts. Analysis of this vast collection will leverage a suite of digital techniques, including keyword searching, sophisticated text matching algorithms, topic modelling, and sentiment analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-project-challenges-and-utilisation-of-large-language-models",
    "href": "chapter_ai-nepi_006.html#core-project-challenges-and-utilisation-of-large-language-models",
    "title": "5  VERITRACE",
    "section": "5.3 Core Project Challenges and Utilisation of Large Language Models",
    "text": "5.3 Core Project Challenges and Utilisation of Large Language Models\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several significant challenges inherent in working with historical textual data at scale. A primary concern is the variable quality of Optical Character Recognition (OCR) in the texts, which libraries provide in raw digital formats like XML, HOCR, or HTML, often without the corresponding page images that could serve as ground truth. This variability directly affects all subsequent analytical processes. Furthermore, the project must navigate the complexities of early modern typography and semantics across at least six different languages. Compounding these issues is the sheer volume of data: hundreds of thousands of texts printed across Europe over a span of roughly two hundred years.\nTo address some of these complexities, particularly in text analysis and metadata management, researchers are incorporating Large Language Models (LLMs). On the decoder side, GPT-based LLMs function as “‘LLMs-as-Judges’” to aid in the enrichment and cleaning of metadata, although this presentation did not delve into the specifics of this application. The main focus here is on the encoder-side utilisation of LLMs. Specifically, BERT-based models generate embeddings to capture the semantic meaning of sentences and passages within the vast textual corpus, a crucial step for enabling effective text matching.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-a-brief-overview",
    "href": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-a-brief-overview",
    "title": "5  VERITRACE",
    "section": "5.4 Metadata Enrichment via “LLMs-as-Judges”: A Brief Overview",
    "text": "5.4 Metadata Enrichment via “LLMs-as-Judges”: A Brief Overview\n\n\n\nSlide 08\n\n\nResearchers explored the use of Large Language Models as adjudicators to enrich the project’s metadata, a task critical for ensuring data quality. The primary motivation involves mapping VERITRACE’s records, which initially possess variable quality metadata, against the high-quality records of the Universal Short Title Catalogue (USTC), accessible at https://www.ustc.ac.uk. This mapping, if successful, would yield “enriched” metadata, thereby diminishing the extensive manual cleaning otherwise required. Whilst some record matching can be automated using external identifiers, a substantial portion necessitates more sophisticated comparison, a process made more complex by the uncleaned nature of the initial VERITRACE data. The sheer scale of manual review—with team members potentially facing 10,000 pairs of records each—underscores the urgent need for an automated solution.\nConsequently, the team developed an “‘LLM Bench’”, a system employing a chain of LLMs to evaluate these bibliographic pairs. This panel of judges, including models such as Llama 3 (8B as primary), Qwen 2.5 (7B as secondary for architectural diversity), Mixtral (8x7B as a more powerful tiebreaker), and Llama 3.1 (the latest version for expert review of edge cases), assesses whether two records pertain to the same underlying printed text. Crucially, these LLMs are prompted to provide detailed reasoning and confidence levels for their decisions, guided by extensive “‘MATCHING_GUIDELINES’” that specify field priorities and criteria for matches or non-matches.\nDespite its promise, this “LLM-as-judge” system remains a work in progress. A significant hurdle is the occurrence of hallucinations, where the models generate information about records not actually presented to them. Attempts to mitigate this by enforcing more structured output have, paradoxically, often resulted in overly generic and less insightful responses, especially concerning the reasoning. Achieving the delicate balance between structured, reliable output and genuinely helpful, nuanced reasoning proves to be more an art than a science. Nevertheless, the potential for this approach to streamline the metadata enrichment process remains considerable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-data-processing-pipeline",
    "href": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-data-processing-pipeline",
    "title": "5  VERITRACE",
    "section": "5.5 The VERITRACE Web Application: Architecture and Data Processing Pipeline",
    "text": "5.5 The VERITRACE Web Application: Architecture and Data Processing Pipeline\n\n\n\nSlide 13\n\n\nEngineers are developing the VERITRACE web application, currently an alpha version restricted to internal use and not yet publicly accessible. This early iteration serves as a proof-of-concept, representing the project’s aspirations for its digital research environment. A key component under current testing is a BERT-based Large Language Model, LaBSE (Language-agnostic BERT Sentence Embedding), tasked with generating vector embeddings for every passage within the corpus. However, initial assessments suggest that LaBSE, whilst functional in some scenarios, may not possess the requisite sophistication for the project’s full demands.\nUnderpinning the web application is a complex, 15-stage data processing pipeline. This pipeline is essential for transforming the raw textual data—received from libraries in formats such as XML, HOCR, and HTML—into a structured format suitable for analysis. The processed data ultimately populates an Elasticsearch database, which functions as the application’s backend. The pipeline encompasses numerous critical steps, including:\n\nBatch processing\nCharacter position mapping\nPage extraction\nLanguage analysis and mapping\nOCR quality assessment\nDocument segmentation\nSegment filtering\nRelationship tracking between segments\nData enrichment within a MongoDB instance before final sequence enrichment\n\nVector embeddings are generated towards the culmination of this multi-stage process. Each stage demands meticulous optimisation to ensure data integrity and processing efficiency.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#navigating-the-corpus-the-explore-and-search-functionalities",
    "href": "chapter_ai-nepi_006.html#navigating-the-corpus-the-explore-and-search-functionalities",
    "title": "5  VERITRACE",
    "section": "5.6 Navigating the Corpus: The Explore and Search Functionalities",
    "text": "5.6 Navigating the Corpus: The Explore and Search Functionalities\n\n\n\nSlide 15\n\n\nThe VERITRACE web application offers users several distinct modules for interacting with the corpus, including sections for Exploration, Searching, Matching, Analysis, and Reading. The “Explore” section furnishes users with statistical insights into the dataset, drawing live information from a MongoDB backend. As an example, on 20 March 2025, the system reported 427,395 metadata records. This section presents visualisations such as language distribution and data source breakdowns, alongside charts illustrating document distribution by decade and common publication places.\nFurthermore, an Elasticsearch Metadata Explorer enables detailed examination of individual records, showcasing the rich metadata compiled for each text. Notably, this includes granular language identification, capable of discerning multiple languages within a single document down to segments of about 50 characters—for example, identifying a text as 85% Latin and 15% Greek. The system also attempts a page-by-page OCR quality assessment, a challenging task without access to ground truth images.\nFor direct interrogation of the corpus, the “Search” section provides robust keyword search functionality. Currently, the prototype operates on a test set of 132 files, yet the index for this small subset already consumes 15 gigabytes, hinting at the terabytes of storage the full corpus will require. A simple keyword search for “hermes” within this prototype, for instance, identified 22 documents with 332 matches. Leveraging the power of Elasticsearch, the search interface supports more sophisticated queries. Users can perform field-specific searches, such as locating instances of “hermes” in works authored by “Kepler” (e.g., author:kepler 'hermes'). The system also accommodates complex Boolean queries, nested searches, and proximity searches, allowing, for example, the discovery of texts where “Hermes” and “Plato” appear within ten words of each other.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prospective-analytical-tools-and-integrated-reading-environment",
    "href": "chapter_ai-nepi_006.html#prospective-analytical-tools-and-integrated-reading-environment",
    "title": "5  VERITRACE",
    "section": "5.7 Prospective Analytical Tools and Integrated Reading Environment",
    "text": "5.7 Prospective Analytical Tools and Integrated Reading Environment\n\n\n\nSlide 14\n\n\nBeyond exploration and search, the VERITRACE web application plans to incorporate an “Analyse” section, although its features are yet to be fully implemented. This module will offer advanced analytical tools, including Topic Modelling capabilities to uncover thematic patterns across the entire corpus or user-selected document sets. Additionally, Latent Semantic Analysis (LSA) will be available for determining document similarity based on shared semantic content. The team also intends to integrate Diachronic Analysis tools, enabling researchers to visualise linguistic and conceptual changes as they unfold over the historical period covered by the corpus.\nComplementing these analytical functions, a “Read” section is already operational. This feature provides scholars with direct access to the source texts. It integrates a Mirador viewer, allowing users to read PDF versions of the documents seamlessly within the application. This reading experience is designed to be comparable to that of major digital library websites, offering the text alongside its pertinent metadata.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#detecting-textual-reuse-the-veritrace-match-functionality",
    "href": "chapter_ai-nepi_006.html#detecting-textual-reuse-the-veritrace-match-functionality",
    "title": "5  VERITRACE",
    "section": "5.8 Detecting Textual Reuse: The VERITRACE Match Functionality",
    "text": "5.8 Detecting Textual Reuse: The VERITRACE Match Functionality\n\n\n\nSlide 20\n\n\nA cornerstone of the VERITRACE web application is its “Match” section, engineered to detect textual reuse across documents. Users can select a query text and then compare it against a single comparison document, multiple documents (for instance, all works by a specific author like Kepler within the database), or, ambitiously, the entire corpus—though the latter presents significant computational challenges. Crucially, the system exposes numerous parameters, such as minimum similarity thresholds, allowing users to fine-tune the matching process.\nThe tool offers three primary types of matching. Lexical matching relies on keyword overlap and BM25 relevance ranking to identify passages with similar vocabulary. Semantic matching, by contrast, employs vector embeddings—currently generated by the LaBSE model—to find conceptually akin passages, even if they share little direct wording; this is vital for comparing texts across different languages. A hybrid approach, combining both lexical and semantic techniques with potentially adjustable weights, is also available. Users can further select a matching mode: “Standard” for default operation, “Comprehensive” for more exhaustive (and computationally intensive) searches, or “Selective” for quicker, less detailed results.\nTo illustrate its capabilities, several sanity checks were performed using Isaac Newton’s Latin Optice (1719) and his English Opticks (1718). A lexical match between the Latin and English versions, as anticipated, yielded no significant results in standard mode due to the language barrier. However, the comprehensive mode did identify three matches, likely corresponding to English text segments, perhaps in the preface of the Latin edition. Conversely, a lexical match of the English Opticks against itself produced perfect scores (100% normalised match, 99.7% coverage, 100% quality), based on nearly 1.3 million passage comparisons, with the interface highlighting identical terms.\nWhen performing a semantic match between the Latin and English Opticks, the system identified passages that seemed reasonably connected (e.g., discussions of colours), despite OCR imperfections. The resulting normalised match score was 58%, with a coverage score of 36.9% and a quality score of 91.2%. The lower coverage might accurately reflect that the Latin edition is considerably longer and potentially contains material not present in the English version. Nevertheless, these tests also underscored potential limitations of the LaBSE model for nuanced semantic comparisons across such historical and linguistically diverse texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#anticipated-challenges-and-future-directions-in-historical-text-analysis",
    "href": "chapter_ai-nepi_006.html#anticipated-challenges-and-future-directions-in-historical-text-analysis",
    "title": "5  VERITRACE",
    "section": "5.9 Anticipated Challenges and Future Directions in Historical Text Analysis",
    "text": "5.9 Anticipated Challenges and Future Directions in Historical Text Analysis\n\n\n\nSlide 21\n\n\nLooking ahead, the VERITRACE team anticipates several significant challenges that will require careful consideration and innovative solutions. The choice and refinement of embedding models remain a critical area. Whilst LaBSE served as an initial model, its limitations are apparent. Researchers are evaluating alternatives such as XLM-Roberta, intfloat multilingual-e5-large, and various historical mBERT implementations, each carrying its own balance of accuracy, computational overhead, and storage demands. An important strategic question is whether the unique characteristics of the early modern corpus—with its specific linguistic features and OCR artefacts—necessitate fine-tuning a base language model to achieve optimal performance, rather than relying on pre-trained models.\nThe phenomenon of semantic change over time presents another profound challenge. Current embedding models, largely trained on contemporary language data, may struggle to adequately represent the evolving meanings of words and concepts across several centuries (e.g., from 1540 to 1700) and across multiple languages. This raises fundamental questions about the coherence of a single vector space for such diachronically and linguistically diverse material.\nFurthermore, the pervasive issue of poor OCR quality continues to cast a long shadow. Errors in the digitised text directly impede crucial downstream processes, including accurate sentence and passage segmentation. Given that re-OCRing the entire corpus of 430,000 texts is impracticable, the team is considering more targeted interventions. These might include selectively re-processing the most problematic texts or dedicating resources to locate pre-existing, higher-quality digital versions.\nFinally, issues of scaling and performance loom large. Current queries on a test set of just 132 texts already take around 15 seconds to complete. Extrapolating this to the full corpus of 430,000 texts highlights an urgent need to optimise algorithms and infrastructure to maintain acceptable response times for users. The project actively welcomes advice and insights from the wider research community on tackling these multifaceted challenges.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter introduces a pioneering framework for historical and scientific inquiry. It meticulously harnesses advanced Large Language Models (LLMs) whilst systematically mitigating their inherent limitations, particularly concerning factual hallucination and validation. The authors propose a novel “computational epistemology” paradigm, which fundamentally prioritises robust validation mechanisms over the mere expansion of contextual understanding or the simulated “thinking” capabilities of contemporary LLMs.\nA bespoke research infrastructure, aptly named Scholarium, forms the bedrock of this methodology. This sophisticated system seamlessly integrates meticulously curated scholarly sources, exemplified by the exhaustive Opera Omnia Euler, with a structured registry database. This registry systematically records historical activities and communications, thereby offering a validated alternative to conventional embedding-based approaches. The team implemented this intricate system within the Cursor environment, leveraging a suite of multimodal LLMs, including Gemini 2.5, Claude, and Llama, alongside a dedicated AI agent christened Bernoulli.\nFurthermore, the project strategically employs Zenodo, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository, for enduring data preservation and dissemination. Open Science Technology provides crucial technical support, operating a Model Context Protocol Application Programming Interface (MCP API) server. This server ensures global access to the curated data via standardised APIs for artificial intelligence models, fostering principles of open access, open data, and open collaboration. Ultimately, the system aims to deliver complete, rigorously validated answers to complex historical queries—a critical capability currently absent in existing LLM applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#overview",
    "href": "chapter_ai-nepi_008.html#overview",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "",
    "text": "Firstly, a “Scholarium” provides curated scholarly evidence from editorial board-approved sources, including the Opera Omnia Euler, Kepler Gesammelte Werke, and Brahe Opera Omnia.\nSecondly, this Scholarium employs a registry of content items—detailing personal actions, communication acts, statements, and terminology usage, all validated against historical records—as a robust alternative to embedding-based methods.\nThirdly, advanced multimodal LLMs, including Gemini 2.5, Claude, and Llama, integrate into the system via “LettreAI on Cursor.”\nFourthly, a FAIR infrastructure, utilising Zenodo (hosted by CERN), ensures long-term data storage and publication.\nFinally, technical support from the Open Science Technology startup underpins the system, offering an MCP API server to facilitate open collaboration and standardised data access for AI models worldwide.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-evolving-trajectory-and-persistent-deficiencies-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#the-evolving-trajectory-and-persistent-deficiencies-of-large-language-models",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.1 The Evolving Trajectory and Persistent Deficiencies of Large Language Models",
    "text": "6.1 The Evolving Trajectory and Persistent Deficiencies of Large Language Models\n\n\n\nSlide 01\n\n\nLarge Language Models (LLMs) underwent a rapid developmental trajectory. Initial advancements centred upon the principle that “attention is all you need.” Subsequently, the necessity for broader context gained prominence, leading to the idea that “context is all you need,” often addressed through techniques like Retrieval Augmented Generation (RAGs) designed to incorporate larger information stores. Current thinking posits that “thinking is all you need,” adding another dimension to their operational framework.\nDespite these advancements, critical deficiencies remain inherent in contemporary LLMs. A significant issue is the lack of an internal “opponent” or verification mechanism to effectively counter hallucinations. Furthermore, a fundamental misunderstanding persists if one considers embedding vectors as true representations of linguistic meaning; they are not. LLMs also exhibit a tendency to generate statements that, whilst sounding coherent or plausible, are demonstrably false. Their outputs often reflect repetitions of information found across internet media, rather than constituting genuine, verified knowledge.\nCrucially, these models currently lack the ability to systematically seek what is best justified or to formulate coherent and effective plans for scientific inquiry. Indeed, little indication suggests that current technological approaches will overcome these deep-seated limitations in the near future.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#validation-as-a-cornerstone-introducing-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#validation-as-a-cornerstone-introducing-computational-epistemology",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.2 Validation as a Cornerstone: Introducing Computational Epistemology",
    "text": "6.2 Validation as a Cornerstone: Introducing Computational Epistemology\n\n\n\nSlide 03\n\n\nA new guiding principle, “validation is all you need,” emerges as a critical requirement to address the shortcomings of current AI. Validation, in this context, encompasses several core functions. It involves providing substantive reasons both for and against the veracity of any given proposition. Furthermore, it requires the capacity to furnish cogent arguments and supply verifiable evidence that either supports or refutes a proposition’s truth. Beyond propositions, validation extends to actions, offering reasoned justifications for or against their pursuit.\nTo systematically develop these capabilities, researchers propose a new discipline termed “Computational Epistemology.” This nascent field would concern itself with the intricate methods and methodologies required to instil robust validation mechanisms within AI. A key outcome of such a discipline would be the cultivation of “epistemic agency” in AI systems. This agency necessitates the ability to identify underlying propositions, moving beyond superficial sentence interpretation. It also demands the capacity to recognise and deconstruct argumentation present in texts and historical inquiries. Crucially, epistemic agency involves discerning the intentions, plans, and actions of individuals as they are documented and leave their traces in historical records.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#a-practical-framework-the-bernoulli-agent-and-curation-driven-inquiry",
    "href": "chapter_ai-nepi_008.html#a-practical-framework-the-bernoulli-agent-and-curation-driven-inquiry",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.3 A Practical Framework: The Bernoulli Agent and Curation-Driven Inquiry",
    "text": "6.3 A Practical Framework: The Bernoulli Agent and Curation-Driven Inquiry\n\n\n\nSlide 04\n\n\nResearchers have developed a practical working environment to illustrate the principles of validated scholarly inquiry. This environment operates on the Cursor platform and features a specialised AI agent, named “Bernoulli,” tasked with conducting deep inquiries into historical sources. An illustrative application involves the complex history of the Sanssouci castle’s construction under Frederick the Great, specifically addressing the long-debated question of mathematician Leonhard Euler’s involvement and potential culpability in the failure of its water features—a significant engineering challenge of the 18th century.\nWithin this environment, users can pose specific questions. For instance, using a source document such as “Manger1789.pdf” (a historical German text), a query like “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain) prompts the system. The AI agent, Bernoulli, then endeavours to provide a validated and qualifying answer. This answer details, with supporting evidence, the individuals involved (such as Nahl, Benkert and Heymüller, and Giese), their specific tasks, the payments they received, and their contributions or shortcomings related to the water fountain or the Neptune group.\nA core challenge this system addresses is the inadequacy of traditional methods, like simple indexing or token concentration, for comprehensively searching and synthesising information from a multitude of available sources, moving far beyond single-document analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#architectural-components-for-robust-epistemic-systems",
    "href": "chapter_ai-nepi_008.html#architectural-components-for-robust-epistemic-systems",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.4 Architectural Components for Robust Epistemic Systems",
    "text": "6.4 Architectural Components for Robust Epistemic Systems\n\n\n\nSlide 08\n\n\nEngineers and scholars designed a multi-component architecture to support robust epistemic systems.\n\nThe first component, the “Scholarium,” serves as a repository of curated scholarly evidence. A dedicated scholarly editorial board oversees the validation of these sources. A prime example is the Opera Omnia Euler, an extensive collection of 86 volumes compiled over approximately 120 years of scholarly labour, with editing completed in 2022; this encompasses all 866 of Euler’s publications and his entire correspondence. Other significant sources integrated include the Kepler Gesammelte Werke and the Brahe Opera Omnia.\nSecondly, the Scholarium features a registry-based content management system, offering a structured alternative to conventional embeddings. This system functions as a curated database, meticulously cataloguing items such as chronologies of personal actions, communication acts (letters, publications, reports), formal statements (including implications, arguments, and inquiries), and detailed records of an individual’s use of language, terminology, concepts, models, methods, tools, and data. Each entry undergoes rigorous validation against original historical sources, thereby creating a comprehensive inventory of historically substantiated activities. Access to this registry is facilitated through an AI API and an MCP API, referencing Anthropic’s work on MCP.\nThirdly, the architecture integrates advanced multimodal Large Language Models. Gemini 2.5 is particularly favoured for its capacity to combine information from textual and visual sources. Other models, such as Claude and Llama, also form part of this integrated LLM suite, operating within the “LettreAI on Cursor” platform.\nFourthly, a FAIR (Findable, Accessible, Interoperable, Reusable) infrastructure ensures responsible data management. Researchers selected Zenodo, hosted by CERN in Geneva, as the repository for long-term storage and publication of the project’s data.\nFinally, technical support and an open collaboration framework are provided by Open Science Technology, a startup founded by the presenter. This organisation offers crucial technical support for the infrastructure’s operation, including an MCP API server. This server enables AI models worldwide to access the curated data through a standardised API, fostering an environment of Open Source, Open Access, Open Data, and Open Collaboration.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "",
    "text": "Overview\nResearchers investigated the application of Large Language Models (LLMs) to assess biases within publications classified under Sustainable Development Goals (SDGs). This inquiry spanned three major bibliometric databases: Web of Science, Scopus, and OpenAlex. The core motivation stemmed from these databases’ critical role in the sociology of science, influencing academic behaviour, funding, and policy. Crucially, they also reflect political and commercial interests, possessing a performative nature.\nThis project built upon previous work that identified minimal overlap in SDG-labelled publications across different providers. It aimed to understand the aggregate effects of LLM-based tools on the representation of SDG-related research. Furthermore, the study conducted a proof-of-concept exercise on automating information extraction for research decision-making. Investigators selected five SDGs directly related to socioeconomic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure).\nFor classification, the team utilised a shared corpus of 15,471,336 jointly indexed publications, spanning January 2015 to July 2023. Regarding the LLM component, researchers fine-tuned DistilGPT2, a lightweight, open-source model, separately on publication abstracts for each SDG and database combination. This process yielded 15 distinct models, a choice that minimised pre-existing knowledge about SDGs within the model. Prompts, derived from SDG targets (8-12 targets per SDG, 10 diverse questions per target), benchmarked the fine-tuned LLMs, employing three decoding strategies: top-k, nucleus, and contrastive search. Noun phrase extraction from LLM responses facilitated analysis across four dimensions: locations, actors, data/metrics, and focuses.\nKey findings revealed a systematic oversight in the data concerning disadvantaged individuals, the poorest countries, and underrepresented topics explicitly mentioned in SDG targets. Conversely, economic superpowers received considerable attention. The study highlighted the decisive impact of bibliometric SDG classification and the sensitivity of LLMs to training data, model architecture, parameters, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.1 Background and Motivation: The Performative Nature of Bibliometric Databases",
    "text": "7.1 Background and Motivation: The Performative Nature of Bibliometric Databases\n\n\n\nSlide 01\n\n\nMatteo Ottaviani and Stephan Stahlschmidt initiated an investigation into the application of Large Language Models (LLMs) for assessing biases within scientific publications, as classified by major bibliometric databases. This work acknowledges the critical role that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, fulfil within the sociology of science. Indeed, these platforms significantly influence the behaviours and decisions of academics, researchers, funding bodies, and policymakers alike.\nThese databases, however, are far from neutral entities; they respond to diverse political and commercial interests, inherently possessing a performative nature. This performativity shapes how the science system is understood and how value is attributed within it—a concept explored by scholars such as Whitley (2000) and Winkler (1988). The current study specifically considers Web of Science, Scopus, and OpenAlex.\nBuilding upon prior research examining the labelling of Sustainable Development Goals (SDGs) and the construction of search queries, this study addresses a persistent challenge. Armitage et al. (2020), for instance, observed that SDG labelling by various providers yielded disparate results with minimal overlap. Such classification discrepancies can foster divergent perceptions of research priorities. Consequently, these disparities may profoundly impact resource allocation and policy decisions, frequently intertwined with underlying political and commercial interests. This investigation, therefore, scrutinises the aggregate effects arising from how bibliometric databases process metadata and how this subsequently influences diverse stakeholders.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "href": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.2 Case Study: LLMs for SDG Research Analysis",
    "text": "7.2 Case Study: LLMs for SDG Research Analysis\n\n\n\nSlide 04\n\n\nThis investigation centres on a case study analysing the representation of United Nations Sustainable Development Goals (SDGs) within bibliometric data, as detailed by Ottaviani & Stahlschmidt (2024). A primary motivation for this research stems from a desire to comprehend the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly given the prospective integration of LLM-based analytical tools. To this end, the investigators employed relatively small pre-trained Large Language Models, selecting DistilGPT2 for its specific characteristics.\nThe core methodological approach involved fine-tuning these LLMs. Researchers trained separate models on distinct subsets of publication abstracts, with each subset corresponding to a particular SDG classification provided by one of the bibliometric databases under review. This strategy enabled the LLM technology to fulfil a dual role: firstly, as an instrument for detecting inherent data biases; and secondly, as a demonstration of concept. This latter role explored the feasibility of LLMs in automating information extraction processes, thereby informing decision-making within the research domain. Ultimately, the project aimed to conduct a broadly applicable exercise, assessing these aggregate effects and gauging their potential impact on research policy.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "href": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons",
    "text": "7.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons\n\n\n\nSlide 03\n\n\nThe researchers conceptualised a chain of dependencies to frame their analysis. This chain posits that SDG classification practices define “Research on SDGs,” which in turn informs decision-making aimed at aligning with these goals, ultimately impacting socioeconomic inequalities. Various actors—including researchers, small and medium-sized enterprises (SMEs), and governments—process this “Research on SDGs.” The introduction of an LLM as a bias detector, it is posited, influences the adoption of LLMs in research policy, which itself can affect socioeconomic inequalities.\nThe study focused on three principal bibliometric databases: the proprietary Web of Science (Clarivate, US) and Scopus (Elsevier, UK), alongside the open-access OpenAlex (formerly Microsoft, US). Researchers selected five SDGs directly relating to socioeconomic inequalities. These comprised SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduced Inequalities) for the socio-equity dimension, complemented by SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure) for the economic development dimension.\nA substantial dataset formed the basis of the analysis: a jointly indexed subset of 15,471,336 publications. These publications, shared across all three databases and identified via exact DOI matching, spanned from January 2015 to July 2023. Investigators then applied the distinct SDG classification standards of Web of Science, Scopus, and OpenAlex to this common corpus for the five chosen SDGs. This process yielded three unique subsets of publications for each SDG, one corresponding to each database’s classification.\nInitial comparisons of these SDG-classified papers revealed a strikingly low overlap amongst the databases, a finding consistent with earlier work by Armitage (2020). For instance, concerning SDG4 (Quality Education), only 7.2% of the relevant publications in the shared corpus were classified as such by all three databases. Similarly low intersection rates were observed for SDG5 (Gender Equality) at 4.8%, SDG10 (Reduced Inequalities) at 2.9%, SDG8 (Decent Work) at 2.5%, and SDG9 (Industry/Innovation) at a mere 2.0%. An interesting anomaly noted was that Web of Science classified approximately 10% of its SDG5-related publications as originating from the field of mathematics, including topics like geometrical differential equations, indicating potential classification idiosyncrasies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "href": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning",
    "text": "7.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning\n\n\n\nSlide 07\n\n\nInvestigators initially conceived of building bespoke Large Language Models, each trained exclusively on publications classified under a specific SDG by a particular bibliometric database. However, developing LLMs entirely from scratch proved a prohibitively resource-intensive endeavour. Consequently, the team adopted a pragmatic compromise: fine-tuning an existing, pre-trained LLM known for having limited prior knowledge, using publication abstracts as the training material.\nThe choice fell upon DistilGPT2. This selection was deliberate, as prominent commercial and large open-source LLMs were deemed ineligible. Such models often possess pre-existing knowledge about SDGs and strong semantic associations derived from their extensive training datasets, which can include sources like Wikipedia and Reddit discussions. DistilGPT2, in contrast, is a lightweight, English-speaking variant of the open-source GPT2 model that utilises a technique called “distillation,” as described by Sanh (2019). With 82 million parameters—significantly fewer than models like GPT-4—DistilGPT2 offered feasibility for working with proprietary datasets; importantly, it was assessed to have no significant prior semantic understanding of the specific publication domain or the prompts to be used.\nThe fine-tuning procedure involved creating 15 distinct LLM instances: one for each of the five selected SDGs, replicated across the three bibliometric databases. For this fine-tuning, researchers utilised the titles and abstracts of the classified publications. The task was structured such that the LLM, when given a new title as a prompt, would generate a new abstract, with the training aimed at maximising the similarity of this output to the characteristics of the source corpus.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "href": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.5 Benchmarking: SDG Targets and Prompt Engineering",
    "text": "7.5 Benchmarking: SDG Targets and Prompt Engineering\n\n\n\nSlide 10\n\n\nTo evaluate the fine-tuned LLMs, researchers developed a benchmarking methodology rooted in the official structure of the UN Sustainable Development Goals. Each SDG is defined by a series of specific targets; for the SDGs under analysis, this typically ranged from eight to twelve targets per goal. For instance, SDG4 (Quality Education) includes targets such as ensuring all children complete primary and secondary education (Target 4.1), providing access to early childhood development, guaranteeing equal access to vocational and tertiary education for all, enhancing youth and adult skills for employment, eliminating gender disparities in education, and ensuring literacy and numeracy for all learners by 2030, as outlined in the UN’s 2030 Agenda for SDGs.\nBased on this structure, the team implemented a systematic prompt generation strategy. For every individual target within each of the five selected SDGs, ten distinct questions, or prompts, were carefully crafted. Each of these prompts was designed to probe different aspects and nuances of its corresponding target. This meticulous process yielded a specific set of 80 to 120 prompts for each SDG.\nThese target-derived prompts formed the cornerstone of the benchmarking standard. Their primary purpose was to establish a ground truth against which the LLM responses could be measured, thereby defining compliance with the stated objectives of the SDGs. Furthermore, this approach facilitated the identification of “biases” or significant informational omissions. The underlying rationale is straightforward: if an LLM, fine-tuned on a corpus of literature purportedly related to a specific SDG, cannot generate relevant responses to prompts directly addressing that SDG’s official targets, it indicates that information crucial to those targets is either missing or substantially underrepresented within the dataset upon which the LLM was trained. This method provides a systematic way to assess both the completeness of the information captured by the database classifications and the potential biases therein.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "href": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.6 Research Design and Analytical Workflow",
    "text": "7.6 Research Design and Analytical Workflow\n\n\n\nSlide 11\n\n\nThe comprehensive research design involved several distinct stages, beginning with the input data: sets of publication abstracts classified under a specific Sustainable Development Goal (SDG#) by one of the three bibliometric databases (DB#). Each of these curated sets of abstracts then served as the training material to fine-tune an instance of the DistilGPT-2 model. This procedure resulted in a collection of specialised LLMs, each uniquely adapted to the content associated with a particular SDG as represented by a specific database (denoted as Fine-tuned DistilGPT-2 SDG# DB#).\nSubsequently, the prompting process commenced. Researchers utilised the previously developed sets of prompts, each tailored to a specific SDG#. These prompts were systematically inputted into the corresponding fine-tuned DistilGPT-2 model. To explore the variability in LLM output and ensure robustness, the team applied three distinct decoding strategies for generating responses: top-k sampling, nucleus (or top-p) sampling, and contrastive search. This approach yielded three distinct sets of responses for every SDG and database combination, reflecting the nuances of each decoding method.\nFor the initial analysis of these generated responses, researchers applied a filter based on the words used in the original prompts. Following this, noun phrases were extracted from the filtered responses, creating a structured dataset of key terms (Noun phrases SDG# DB#). However, the analytical scrutiny extended beyond this. To ensure a thorough and nuanced comparison, investigators also conducted direct searches within the full text of the LLM-generated responses, complementing the insights derived from noun phrase analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "href": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.7 LLM Response Analysis: Uncovering Biases",
    "text": "7.7 LLM Response Analysis: Uncovering Biases\n\n\n\nSlide 12\n\n\nResearchers analysed the LLM-generated responses by matching the extracted noun phrases against the official targets of each Sustainable Development Goal. This analysis was structured around four key data dimensions: Locations, Actors, Data/Metrics, and Focuses. For every SDG under review, the team assessed the degree of compliance with its targets and identified any discernible biases. Importantly, this process also highlighted differences in how the three bibliometric databases represented SDG-related research.\nAn illustrative example using SDG4 (Quality Education) revealed significant omissions. The LLM responses, reflecting the underlying database classifications, inadequately addressed numerous geographical areas, including most African countries (with the exception of South Africa), other developing nations, Least Developed Countries (LDCs), and Small Island Developing States (SIDS). Similarly, critical groups of actors were systematically overlooked, such as vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Many crucial thematic focuses pertinent to SDG4 were also underrepresented or entirely missing from the LLM outputs. These included vocational training, scholarships, the creation of safe and inclusive learning environments, education for sustainable lifestyles, human rights education, the promotion of peace and non-violence, global citizenship, the appreciation of cultural diversity, and even fundamental aspects like free primary and secondary education and tertiary education.\nExtending these observations across all five selected SDGs, several patterns emerged. Regarding locations, LDCs received scant attention, with Sub-Saharan Africa being mentioned primarily in the context of SDG8. The United States held an “undoubted monopoly” in terms of mentions, followed by South Africa and China, and then the UK and Australia. In the realm of metrics and data, the LLMs frequently recalled specific surveys like the Demographic and Health Surveys (DHS) and World Values Survey (WVS) as data sources. Various indicators, benchmarks, and research methodologies—spanning theoretical, empirical, and thematic analyses, as well as market dynamics and macroeconomics—were also identified, with semantic networks formed after fine-tuning indicating recurrent survey data.\nA consistent and concerning finding related to actors: discriminated and vulnerable categories were systematically overlooked across all analysed SDGs. Even when prompts specifically targeted these groups for different SDGs, the LLMs failed to generate substantial, macro-level responses. In terms of thematic focuses, many SDG-specific sensitive topics, such as human trafficking, human exploitation, and migration, were notably absent. Furthermore, the analysis discerned database-specific tendencies. Across three different SDGs, Web of Science’s classified literature leaned towards a more theoretical approach. Conversely, both Scopus and OpenAlex appeared to favour and represent more empirical research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "href": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.8 Synthesis and Limitations",
    "text": "7.8 Synthesis and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s findings highlight a critical issue: employing Large Language Models as an analytical instrument, mediating between the SDG classifications provided by bibliometric databases and their interpretation by policymakers, uncovers a systematic deficiency in the underlying data. Specifically, scientific publications, as classified under various SDGs, frequently overlook the most disadvantaged categories of individuals, the poorest nations, and numerous underrepresented topics that are, in fact, explicit focuses of the SDG targets themselves. In stark contrast, the classified literature demonstrates considerable attention towards economic superpowers and rapidly developing countries. These results unequivocally show how an ostensibly objective, science-informed practice such as the bibliometric classification of SDGs can wield a decisive influence on perceived research landscapes and priorities.\nResearchers also acknowledged several inherent methodological limitations. Large Language Models exhibit high sensitivity to a range of factors. These include the specific model architecture chosen, although DistilGPT2 was selected for its suitability to the task. The nature of the training data is also paramount; this was partly addressed by utilising three distinct databases, which provided varied training corpora for the LLMs. Furthermore, hyper-parameters, general model parameters, and the chosen decoding strategy all significantly influence LLM behaviour. The impact of decoding strategy was partially mitigated by employing three different recognised methods (top-k, nucleus sampling, and contrastive search). Finally, whilst the study employed a general framework, the use of more developed or specialised LLM architectures could potentially reveal different or more nuanced outcomes. Despite efforts to account for variations in training data and decoding strategies, these elements remain influential variables in LLM performance and output.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "",
    "text": "Overview\nResearchers have long grappled with the persistent challenge of extracting citation data from the complex footnotes prevalent in law and humanities scholarship. Historically, bibliometric databases have offered inadequate coverage for these domains. This deficiency stems primarily from a lack of commercial interest, a focus on impact factors over intellectual history, and the inherent complexity of humanities footnotes. Moreover, traditional machine learning tools consistently demonstrate poor performance in parsing these intricate structures. Consequently, this project explores the utility of Large Language Models (LLMs) and Vision Language Models (VLMs) as a more effective solution.\nA central tenet of this research involves establishing a robust testing and evaluation framework. To this end, scholars are developing a high-quality gold standard dataset, meticulously annotated using TEI XML encoding. This standard, well-established within the digital humanities, facilitates comprehensive representation of citation phenomena, including crucial contextual information. Furthermore, it ensures interoperability with existing tools such as Grobid, enabling direct performance comparisons.\nTo operationalise this approach, engineers crafted Llamore, a lightweight Python package. Llamore extracts citation data from raw text or PDFs, exporting it into TEI-formatted XML files. Crucially, it also evaluates extraction performance against gold standard references using an F1-score metric, which accounts for precision and recall through an unbalanced assignment problem. Initial evaluations reveal that whilst Llamore’s resource consumption exceeds that of traditional tools like Grobid for biomedical literature, it significantly outperforms Grobid when processing the challenging, footnoted humanities data. Future work aims to expand the training data, refine evaluation metrics, and enhance Llamore’s capabilities to capture contextual citation information and resolve complex stylistic variations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "href": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities",
    "text": "8.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities\n\n\n\nSlide 01\n\n\nResearchers embark upon the challenge of parsing footnotes within law and humanities scholarship, a task with which current Large Language Models (LLMs) and other algorithms often struggle. Their primary objective involves generating the specific data required to construct comprehensive citation graphs. Such graphs offer powerful tools for intellectual historians, enabling the discovery of patterns and intricate relationships within the production of knowledge. Moreover, they facilitate the reconstruction of scholarly influences and allow for the measurement of how published ideas are received over time. An illustrative application involves tracking shifts in the most-cited authors, exemplified by an interactive web application analysing the Journal of Law and Society between 1994 and 2003.\nAn extremely poor coverage of historical Social Sciences and Humanities (SSH) material by established bibliometric datasources significantly impedes this research. Prominent databases like Web of Science, Scopus, and even the more accessible OpenAlex, prove largely inadequate for this domain, as they simply do not contain the requisite data. Compounding this issue, Web of Science and Scopus prove prohibitively expensive and operate under highly restrictive licences, creating dependencies undesirable for open scholarly inquiry. Whilst OpenAlex offers an open-access alternative, its coverage for the specialised content needed—particularly non-“A-journals,” pre-digital publications, and non-English language works—remains insufficient. For instance, data for the Zeitschrift für Rechtssoziologie, a German journal for law and society, reveals a stark lack of citation information prior to the 2000s in both Dimensions and OpenAlex.\nSeveral factors contribute to this poor coverage. Primarily, commercial interest in humanities scholarship pales in comparison to that for STEM fields, medicine, and economics, which dominate these large bibliometric databases. Furthermore, these platforms typically prioritise the “impact factor” as a metric for science evaluation, a concern quite distinct from the nuanced inquiries of intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "href": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools",
    "text": "8.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools\n\n\n\nSlide 04\n\n\nBeyond database limitations, researchers identify the inherent complexity of humanities footnotes—aptly termed “footnotes from hell”—as a core challenge. These footnotes frequently feature extensive commentary and disordered data, all embedded within a significant amount of textual “noise,” as examples of German and English academic texts illustrate. Consequently, creating accurate training data for these intricate structures becomes an arduous task. Traditional annotation methods demand a laborious process of manually identifying and tagging various bibliographic elements, such as author, title, and publication date, often within specialised software interfaces.\nFurthermore, existing tools, predominantly reliant on Conditional Random Forests and similar machine learning approaches, prove incapable of effectively handling such complex footnotes. Their performance significantly degrades when confronted with this type of data. For instance, performance metrics for the ExCite tool, detailed by Boulanger and Iurshina (2022), demonstrate variable extraction and segmentation accuracy across different training datasets, highlighting the difficulties with footnoted material. The challenges are multifaceted, encompassing varying citation styles, the complexities of multilingual terminology, and the pervasive use of ellipses, abbreviations (like idem or derselbe), and cross-references. Ambiguities, such as discerning whether an initial numeral signifies a volume or a page number, can perplex even human readers. Misleading capitalisation and the appearance of personal names within titles, which are then erroneously identified as authors, further complicate automated extraction. Language models may also struggle with specialised terminology with which they are unacquainted.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "href": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation",
    "text": "8.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation\n\n\n\nSlide 10\n\n\nScientists now explore Large Language Models (LLMs) as a promising avenue for tackling reference extraction. Early experiments conducted in 2022 with models like text-davinci-003 already indicated the considerable power of LLMs to extract references from disordered textual data. Newer models, including Vision Language Models (VLMs) capable of directly processing PDF documents, hold the promise of even greater efficacy. Researchers investigate various methods, such as prompt engineering, Retrieval Augmented Generation (RAG), and finetuning, to harness these capabilities.\nNevertheless, a critical question looms: can one trust the results generated by these models? The potential for error, exemplified by a widely reported incident of a lawyer misusing ChatGPT in federal court, underscores this concern. A guiding principle for the research, therefore, necessitates avoiding attempts to solve problems for which no validation data exists. This requires developing a robust testing and evaluation solution. Such a solution must rest upon three pillars:\n\na high-quality Gold Standard dataset\na flexible framework that can readily adapt to the fast-moving landscape of AI technology\nsolid testing and evaluation algorithms capable of producing comparable and reliable metrics",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "href": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.4 Crafting a High-Quality Gold Standard: A TEI XML Approach",
    "text": "8.4 Crafting a High-Quality Gold Standard: A TEI XML Approach\n\n\n\nSlide 10\n\n\nAndreas Wagner detailed the team’s efforts to compile a high-quality dataset suitable for both training and evaluation, opting for TEI XML encoding. This choice, whilst perhaps less common in contemporary machine learning circles, stands as the preeminent standard within text-based humanities and digital editorics. Several compelling reasons underpin this decision. TEI XML provides a well-established, comprehensively specified standard for text interchange, surpassing the capabilities of purely bibliographical standards like CSL or BibTeX by covering a broader range of textual phenomena. Crucially, it extends beyond mere reference management to include citations, cross-references, and other forms of contextual markup, which can prove invaluable for tasks such as classifying citation intention. Furthermore, adopting TEI allows researchers to tap into a wealth of existing text collections and corpora from digital editorics projects, many of which publish their source data in this format, sometimes including detailed reference encodings.\nAnother significant advantage of TEI XML lies in the extensive tooling available. Grobid, a prominent tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. Utilising the same data format enables direct performance comparisons with Grobid, facilitates the sharing of training data with the Grobid team and others, and allows the project to leverage Grobid’s existing training resources.\nThe dataset currently under development draws from open-access journals. It involves the meticulous encoding of over 1,100 footnotes extracted from 25 articles, encompassing a diverse range of languages—French, German, Spanish, Italian, and Portuguese—and spanning a considerable period from 1958 to 2018. This collection anticipates yielding over 1,600 individual references; importantly, multiple references to the same work are encoded separately to capture the context of each occurrence. This endeavour remains a work in progress, having adapted its strategy midway to focus on Open Access journals and to incorporate PDFs alongside text, reference strings, and parsed TEI structures. Despite its strengths, TEI XML is no panacea; conceptual challenges, such as distinguishing pointers from references, and technical complexities, like handling constrained elements versus elliptic material, persist. These considerations lead to a fundamental question: how precisely should “performance” be defined and measured in this context?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment",
    "text": "8.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment\n\n\n\nSlide 14\n\n\nDavid Carreto Fidalgo introduced Llamore, an acronym for Large Language Models for Reference Extraction. Engineers developed this Python package to perform two primary functions: firstly, to extract citation data from raw input text or PDF documents utilising (multimodal) LLMs, and secondly, to evaluate the performance of this extraction process. Llamore processes textual or PDF inputs and outputs references formatted as TEI XML. When provided with gold standard references, it generates an F1 score as an evaluation metric.\nTwo principal objectives guided Llamore’s creation. It needed to be lightweight, containing fewer than 2000 lines of code and functioning as an interface to a user’s chosen model rather than embedding models itself. Concurrently, compatibility with both open and closed-source LLMs and VLMs formed a key design consideration. Users can install Llamore via pip. For extraction, one defines an extractor based on the desired model (e.g., GeminiExtractor, OpenAIExtractor). Notably, the OpenAIExtractor ensures broad compatibility with open model serving frameworks like Ollama and VLLM, which typically offer OpenAI-compatible API endpoints. The chosen extractor then processes a PDF or a raw text string, returning references that can be exported to an XML file in TEI biblStruct format. For evaluation, the F1 class is imported and used to compute a macro-average F1 score by comparing the extracted references against gold standard references; users can specify parameters like Levenshtein distance for matching.\nThe evaluation hinges on the F1 score, a well-established metric for structured data comparison, deriving from precision (matches divided by predicted elements) and recall (matches divided by gold elements). An F1 score of 1 signifies perfect extraction, whilst 0 indicates no matches. A crucial aspect of evaluation involves aligning the set of extracted references with the set of gold references. Llamore tackles this by formulating it as an Unbalanced Assignment Problem, employing a solver from the SciPy library. This process involves calculating F1 scores for every possible pairing of extracted and gold references, constructing a matrix of these scores, and then identifying the assignment that maximises the total F1 score under the constraint of unique pairings. This sophisticated alignment ensures accurate macro-averaging, with missing or hallucinated references appropriately penalised with an F1 score of zero. This alignment methodology strongly resembles recent work by Baka and colleagues.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "href": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.6 Comparative Performance, Key Insights, and Future Directions",
    "text": "8.6 Comparative Performance, Key Insights, and Future Directions\n\n\n\nSlide 20\n\n\nTo assess Llamore’s efficacy, researchers conducted comparative performance evaluations. On the PLOS 1000 dataset, comprising 1000 PDFs from the biomedical domain, Llamore (utilising Gemini 2.0 Flash) achieved an F1 score (macro average, exact match) of 0.62, performing on par with Grobid’s score of 0.61. This result is notable given that Grobid was trained on portions of this type of biomedical literature. However, a stark contrast emerged during evaluation on the team’s custom humanities dataset. Here, Grobid’s F1 score plummeted to 0.14, indicating significant difficulty in extracting references. In contrast, Llamore achieved an F1 score of 0.45, demonstrating substantially better, indeed threefold improved, performance on this challenging, footnoted material.\nThese findings lead to several key takeaways. Grobid remains a preferable option for literature similar to its training data, primarily because it operates much faster and consumes fewer resources. Nevertheless, for the complex, footnoted literature characteristic of the humanities, experiments with Llamore paired with Gemini models reveal a significant performance advantage. One must note that these current performance metrics pertain to pure reference extraction and do not yet encompass more nuanced analyses such as citation context or cross-referencing.\nLooking ahead, the team plans to expand their efforts by producing more training data and further refining test metrics. A significant focus will augment Llamore’s capabilities to support more sophisticated analyses. This includes identifying citations in their context (e.g., determining if a citation is approving or critical), resolving abbreviations like op cit., extracting specific pages cited, and accurately counting multiple citations to the same work.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview\nResearchers at DANS, the data archive of the Royal Netherlands Academy of Arts and Science, in collaboration with GESIS, a research-engaged archive, have pioneered an AI-driven solution to manage the escalating volume of scientific information. This initiative directly addresses the challenges of growth and increasing differentiation within the sciences, which complicate the review, evaluation, and selection of relevant content. A fundamental precondition for creating new knowledge, whether individually or across academia, involves efficiently finding and understanding existing information. Consequently, the project investigates whether contemporary Artificial Intelligence (AI), particularly Large Language Models (LLMs), can support the knowledge production process through advanced information retrieval.\nThe core research question explores the feasibility of constructing an AI solution capable of facilitating conversational interaction with academic papers from specific collections. Developers have crafted a dual-component system: Ghostwriter, serving as the user interface, and EverythingData, encompassing the comprehensive backend operations. This architecture integrates principles of information retrieval, human-machine interaction, and Retrieval-Augmented Generation (RAG). The project employs the method-data-analysis (mda) journal as a practical use case, demonstrating a ‘local’ and ‘tailored’ AI solution workflow.\nThe Ghostwriter interface redefines information retrieval by enabling simultaneous interaction with structured data—analogous to a librarian—and natural language—akin to an expert. This approach leverages Knowledge Organisation Systems (KOS) and classification schemes whilst interpreting natural language queries. The underlying technical infrastructure, EverythingData, processes input document collections, such as articles from the mda journal, by ingesting them into a vector store, specifically Qdrant. It performs crucial operations including term extraction, embedding construction, and crucially, coupling these with knowledge graphs. This integration enriches embeddings by contextualising them, thereby adding significant value to the information.\nThe system prioritises factual accuracy, aiming to prevent hallucinations by relying exclusively on the ingested source material. It segments papers into small, identifiable blocks, employing LLM techniques to intelligently connect and retrieve relevant sections. Knowledge graphs further enhance this process by predicting which text segments will best address specific queries. A key innovation links extracted entities to Wikidata, transforming free strings into multilingual identifiers. This mechanism supports immediate multilinguality, allowing users to query in one language and retrieve information from documents in another. Moreover, decoupling knowledge from the model and storing it as Wikidata identifiers establishes a robust ground truth for benchmarking and validating future AI models. The project envisions this knowledge organisation system as a sustainable future for scientific information management, fostering collaborations with industry leaders like Google and Meta.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "href": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "title": "9  Science Dynamics and AI",
    "section": "9.1 Context: Science Dynamics, Information Overload, and AI",
    "text": "9.1 Context: Science Dynamics, Information Overload, and AI\n\n\n\nSlide 01\n\n\nThis exploration into AI’s pivotal role in science dynamics stems from a collaborative endeavour between DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, a research-active archive. Whilst DANS primarily focuses on data archiving, GESIS also conducts extensive research. The initiative originated from the profound experimentation of Slava Tikhonov, a senior research engineer at DANS, who has meticulously developed intricate data processing pipelines—complex systems aptly characterised by Arno Simons as a ‘tangle of interwoven components’.\nModern sciences evolve with remarkable rapidity and increasing differentiation. This trajectory, however, poses a significant challenge: how can researchers effectively review, evaluate, and select pertinent information from an ever-expanding corpus? Consequently, scholars confront a veritable ‘flood of information’. The ability to locate and comprehend existing knowledge forms a fundamental precondition for creating new insights, whether by individuals or broader academic communities. A central question thus emerges: can machines, particularly the latest AI technologies that have paradoxically contributed to this information proliferation, also assist in the knowledge production process itself? This chapter investigates this crucial question through the lens of Information Retrieval, aiming to elucidate complex AI solutions comprehensively via a practical use case.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "href": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "title": "9  Science Dynamics and AI",
    "section": "9.2 Research Objectives and System Introduction: Ghostwriter and EverythingData",
    "text": "9.2 Research Objectives and System Introduction: Ghostwriter and EverythingData\n\n\n\nSlide 02\n\n\nInvestigators pursued a specific research question: could they construct an AI solution enabling users to interact conversationally with a selected collection of academic papers? To address this, they developed a system comprising two primary components, internally designated Ghostwriter and EverythingData. Ghostwriter functions as the user-facing interface, whilst EverythingData encompasses the intricate array of backend processes.\nThis chapter introduces foundational concepts such as Information Retrieval, human-machine interaction, and the Retrieval-Augmented Generation (RAG) technique in generative AI. Subsequently, it details the compelling use case involving the method-data-analysis (mda) journal. A core segment elucidates the workflow underpinning this ‘local’ or ‘tailored’ AI solution, providing illustrations of both front-end and back-end operations, before concluding with a summary and outlook.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "title": "9  Science Dynamics and AI",
    "section": "9.3 The Ghostwriter Interface: A Novel Information Retrieval Approach",
    "text": "9.3 The Ghostwriter Interface: A Novel Information Retrieval Approach\n\n\n\nSlide 03\n\n\nResearchers conceptualise the Ghostwriter system as a novel interface for Information Retrieval. Its design philosophy draws inspiration from Slava Tikhonov’s insightful metaphors, which distinguish between two modes of interaction: ‘talking to the librarian’ and ‘talking to the expert’. The ‘librarian’ symbolises engagement with structured data, knowledge organisation systems, and pre-existing classifications. Conversely, the ‘expert’ represents interaction through natural language.\nCrucially, the Ghostwriter interface enables users to converse with both these symbolic entities simultaneously. This capability rests upon a local Large Language Model (LLM) operating on a target data collection, which is further embedded within a network of supplementary data interpretation sources accessible via APIs. This innovative approach seeks to overcome the classic Information Retrieval challenge, where users often require prior knowledge of a database’s schema and typical values to formulate effective queries and obtain optimal results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "href": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "title": "9  Science Dynamics and AI",
    "section": "9.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG",
    "text": "9.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG\n\n\n\nSlide 04\n\n\nThe system’s development firmly situates itself within the broader scientific discourse of Retrieval Augmented Generation (RAG). For a comprehensive introduction to this topic, particularly the integration of knowledge graphs, readers are highly encouraged to consult Philip Rattliff’s seminal paper, ‘GenAI Knowledge Graph The GraphRAG Manifesto: Adding Knowledge to GenAI’ (Neo4j, 11 July 2024). Arno Simons also merits acknowledgement for his significant contributions in this domain, particularly concerning the RAG ‘tool box’.\nThis RAG approach fundamentally comprises three main ingredients. Firstly, researchers construct a vector space from the content of data files, encoding them as embeddings that meticulously capture properties and their attributes; various Machine Learning algorithms and diverse LLMs compute these embeddings. Secondly, a graph forms a metadata layer, seamlessly integrating with diverse ontologies and controlled vocabularies, including those pertinent to responsible AI, and is expressed using the Croissant ML standard. The overarching vision, termed GraphRAG, aims to unify these graph and vector components within a single model. Developers plan to implement this ‘locally’, conceptualising it as a form of Distributed AI where the LLM serves as both an ‘interface’ between human and AI and a ‘reasoning engine’. In practice, the LLM connects to a ‘RAG library’ (representing the graph), navigates datasets, and consumes embeddings (the vectors) to provide essential context for its operations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "href": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "title": "9  Science Dynamics and AI",
    "section": "9.5 System Workflow: From Document Ingestion to Query Response",
    "text": "9.5 System Workflow: From Document Ingestion to Query Response\n\n\n\nSlide 05\n\n\nThe system processes information through a clearly defined workflow, commencing with a collection of input documents. For demonstration purposes, researchers utilised articles from the mda journal, scraping a small number, though any document collection can serve as input. These documents first enter the EverythingData backend, where a series of sophisticated operations transform them. Initially, the system ingests information into a vector store, employing Qdrant for this purpose. Subsequent operations include meticulous term extraction, the construction of precise embeddings, and various enrichments. Notably, selected terms become structured data within a graph, further enriched by linking to external resources such as Wikidata. This coupling with knowledge graphs proves crucial, as it significantly enhances the value of words, phrases, and embeddings by adding layers of contextualisation.\nAll processed data then populates a ‘vector space RAG-Graph’, which forms the core reasoning substrate. When a user poses a question in natural language via the Ghostwriter interface, this query directly interacts with the vector space RAG-Graph. In response, the system delivers not only a list of relevant documents, typical of conventional information retrieval systems, but also a generated summary or explanatory text that the underlying ‘machinery’ deems pertinent to the user’s question. An accompanying diagram from TheAidedge.io compellingly illustrates the comparative roles of vector and graph databases within RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "href": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "title": "9  Science Dynamics and AI",
    "section": "9.6 Implementation and Use Case: Interacting with MDA Journal Articles",
    "text": "9.6 Implementation and Use Case: Interacting with MDA Journal Articles\n\n\n\nSlide 07\n\n\nSlava Tikhonov, a senior research engineer at DANS, meticulously detailed the system’s implementation, drawing upon his early engagement with Large Language Models (LLMs) since testing GPT-2 in 2020. His methodology involves deconstructing the LLM training process into smaller, adaptable components. This approach yields a versatile system applicable not only to academic papers but also to diverse web content; for instance, it can interact with spreadsheets, enabling users to query specific values and receive factual, non-hallucinated responses derived exclusively from the spreadsheet’s data. For complex queries, the system employs a one-billion-parameter LLM, significantly enhanced by integrated knowledge graphs.\nThe primary use case centres on the ‘mda methods, data, analyses’ journal, a GESIS publication. Engineers ingested papers from this journal into the Ghostwriter tool, thereby creating a distinct collection. A core design principle dictates that the system must not rely on any general knowledge pre-loaded into the LLM; instead, it must answer questions using only factual information present within the specified papers. If the requested information is absent, the system transparently states ‘I don’t know’, thereby avoiding speculation. For testing, developers utilised a collection of 100 articles scraped from the mda website. The Ghostwriter instance for these mda papers remains accessible at https://gesis.now.museum. The GESIS ‘Ask Questions’ interface, presented as part of the broader ecosystem, allows users to add new content collections via various means, including single webpages, website crawlers, or RSS feeds.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "href": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "title": "9  Science Dynamics and AI",
    "section": "9.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement",
    "text": "9.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement\n\n\n\nSlide 10\n\n\nThe Ghostwriter system compellingly demonstrates its core functionalities through practical examples. When a user queries, for instance, ‘explain male breadwinner model to me’, the system furnishes a detailed explanation and, crucially, provides precise references to the source documents. This implementation actively prevents hallucination by meticulously locating information within the ingested texts. Engineers achieve this accuracy by splitting each paper into small, identifiable blocks, each assigned a unique identifier. LLM-based techniques then intelligently connect and retrieve these blocks, whilst weights and other methods further refine the process. Furthermore, knowledge graphs assist in predicting which specific text segments are most likely to provide a relevant answer to a given question.\nShould a query be refined—for instance, to ‘explain how data was collected on male breadwinner model’—and the information proves absent from the current corpus, the system responds transparently: ‘According to the provided text, there is no direct information about how data was collected on the male breadwinner model.’ This honesty represents a key feature. The system also supports iterative improvement; an ‘add paper’ button enables users to integrate new documents they discover externally. Subsequently, if the same question is posed, the system can readily utilise this newly incorporated information. For all queries, the interface displays source papers, including their titles, direct links (e.g., to mda.gesis.org), and relevance scores, although not all listed documents may contain the exact query terms in their full text.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "title": "9  Science Dynamics and AI",
    "section": "9.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities",
    "text": "9.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities\n\n\n\nSlide 11\n\n\nSeveral advanced backend mechanisms empower the Ghostwriter system, particularly in entity management and multilingual support. An entity extraction pipeline annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby effectively bridging vector spaces and knowledge graphs. These entities then link to richer knowledge graph representations, with Wikidata serving as a prime example; this linkage proves vital for establishing ground truth. The system also offers immediate multilinguality, enabling seamless interaction even when query and document languages differ. Finally, the LLM processes the retrieved, relevant text pieces to generate a coherent summary or ‘explanatory text’.\nDelving deeper into fact extraction, a user’s query undergoes mapping to a graph representation, and its constituent strings are meticulously annotated with ‘facts’. For instance, terms such as ‘gender roles’ or ‘male breadwinner model’ connect to concepts like ‘societal expectations’ or ‘economic systems’. This process relies on a Knowledge Organisation System (KOS) that can be iteratively applied to reveal progressively deeper semantic layers beneath a term. Instead of relying on free-text strings, the system links entities to Wikidata, thereby obtaining unique identifiers. These identifiers intrinsically connect to multilingual translations and a wealth of properties, allowing, for example, the term ‘male’ to resolve to its specific Wikidata entry (e.g., Q12308941), with LLM embeddings providing similarity scores for disambiguation.\nMultilingual capability is robustly implemented. The system identifies the core concept of a query, such as ‘male breadwinner model’ (represented as ‘bread winner mo’). An LLM, specifically Gemma3, then generates translations of this core concept into a multitude of languages, including Czech, Danish, German, and Japanese, amongst others. These translations effectively broaden the search parameters for the primary LLM.\nThis sophisticated approach underpins a broader vision for Knowledge Organisation Systems. By converting concepts into Wikidata identifiers, knowledge becomes decoupled from the specifics of individual questions or papers. Such abstracted knowledge can be stored independently of any single LLM, thereby fostering model agnosticism. This decoupling also facilitates a novel benchmarking methodology: different LLMs, even those yet to be developed, can be evaluated by their ability to return the same set of identifiers for identical conceptual queries. Any deviations would signal potential issues with a model’s suitability for certain tasks. Collaborations with industry leaders like Google and Meta aim to establish this KOS-centric methodology as a sustainable and foundational element for future scientific endeavours, positioning KOS as a cornerstone of future knowledge management.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "",
    "text": "Overview\nPhilosophers frequently grapple with complex research questions that demand precise linguistic and semantic accuracy. While conventional Large Language Models (LLMs) offer differentiated responses, they encounter significant limitations concerning access to full texts, context window capacity, and reliable attribution. Consequently, researchers have developed Retrieval-Augmented Generation (RAG) systems to overcome these challenges. This innovative approach integrates a specific data source, a robust retrieval mechanism—employing semantic, hybrid, or classic search—and a prompt augmentation process. This architecture enables LLMs to access and cite original source material directly, thereby enhancing the reliability and verifiability of generated content.\nThe utility of RAG systems extends across both didactics and research. For pedagogical purposes, these systems facilitate interactive engagement with extensive philosophical corpora, such as Locke’s Oeuvre, allowing students to delve deeply into complex texts and progressively refine their understanding. In research, RAG systems streamline fact-finding in handbooks, enable the exploration of previously unexamined corpora, assist in identifying passages for close reading, and potentially provide detailed answers to intricate research questions.\nA practical implementation involved coding a RAG system using the Stanford Encyclopedia of Philosophy as its primary data source. Initially conceived as a community tool, this project evolved into a qualitative study examining optimal RAG system setups for philosophical inquiry. The development process, characterised by theoretically grounded trial and error, revealed the critical importance of hyperparameter optimisation and robust evaluation criteria. Notably, optimising chunk size proved crucial for this highly systematised source; selecting main sections as retrieval documents, despite their length, consistently yielded superior results [Empirical Study, Year].\nRAG systems demonstrably integrate verbatim corpora and domain-specific knowledge, significantly reducing hallucinations and enabling the citation of relevant documents. Nevertheless, their effective deployment necessitates extensive tweaking and rigorous evaluation by domain experts, as optimal configurations vary across domains and corpus types. A peculiar observation indicates that RAGs sometimes perform less effectively on broad overview questions, as their inherent focus on local information can obscure the larger perspective. Future developments aim to create more flexible, agentic RAG systems capable of discerning question types and adapting their approach accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "href": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation",
    "text": "10.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation\n\n\n\nSlide 01\n\n\nPhilosophical inquiry frequently grapples with intricate questions. Consider, for instance, elucidating Aristotle’s theory of matter within the Physics, or tracing the evolution of Einstein’s concept of locality from his early works on relativity to his 1948 paper addressing quantum mechanics and ‘Wirklichkeit’. Whilst standard Large Language Models (LLMs) like ChatGPT can generate superficially plausible and differentiated responses to such queries, they exhibit significant limitations when applied to rigorous philosophical research.\nA primary constraint involves access to textual sources. LLMs typically lack dynamic access to the full text of scholarly works, even if those texts formed part of their training data. Consequently, requests for specific quotations from chapters or papers may lead to hallucinations or an admission of inability. Even when online search capabilities are activated, copyright restrictions can prevent the reproduction of material.\nFurthermore, the fundamental training mechanisms of LLMs are engineered to avoid mere parroting of texts. Instead, they learn generalisable statistical patterns of language production, with explicit mechanisms preventing verbatim memorisation. This contrasts sharply with the needs of philosophical research, which hinges on meticulous engagement with original source materials and their precise, fine-grained formulations. The limited context window of current LLMs—for instance, ChatGPT-4o’s 128,000 tokens—also poses a significant hurdle when dealing with extensive philosophical corpora.\nTo surmount these challenges, researchers propose Retrieval Augmented Generation (RAG) systems. A RAG system’s architecture typically involves a curated data source, such as the complete corpus of Aristotle’s or Einstein’s writings. From this source, documents are retrieved using methods like semantic search, hybrid approaches, or traditional keyword search. These retrieved documents, or relevant chunks thereof, then augment the user’s original prompt before processing by the LLM. This setup directly tackles the problem of text access, provides a mechanism for managing large context sizes by focusing on relevant segments, and crucially, facilitates attribution by enabling the system to cite the sources for its generated claims, much like the numbered citations seen in tools such as Perplexity AI.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "href": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.2 Applications of RAG Systems in Philosophical Scholarship",
    "text": "10.2 Applications of RAG Systems in Philosophical Scholarship\n\n\n\nSlide 09\n\n\nRAG systems offer a promising avenue for interacting with philosophical corpora, distinguished by their capacity to integrate detailed domain knowledge and rely on verbatim textual evidence. This capability opens several valuable applications within philosophical scholarship, broadly categorised into didactic and research uses.\nIn the realm of didactics, RAG systems can transform how students engage with challenging philosophical texts. For instance, students approaching Locke’s Essay concerning Human Understanding can benefit immensely from the ability to pose repeated questions. This iterative process proves highly instructive, allowing them to start with broad inquiries, such as “What is Locke’s general idea?”, and progressively delve into more specific aspects, like “What is his idea in epistemology?” or “What is his theory of matter?”. Such interactions foster a deeper, more nuanced understanding of the material.\nBeyond educational settings, RAG systems hold significant potential for research. They can streamline the process of looking up facts in handbooks, a task that traditionally involved manually searching physical volumes for information, perhaps for a footnote. RAG offers a more efficient method, potentially with greater reliability than relying on the unverified outputs of standard LLMs. Furthermore, researchers can employ RAG systems to explore unexamined corpora; once digitised, collections of unpublished manuscripts or less-studied texts can be “chatted with” to gain an overview and deeper insights into their content. Another key research application is the identification of specific passages relevant to a particular research question, thereby facilitating focused close reading. Ultimately, the aspiration is that RAG systems might, at some point, become capable of providing detailed answers to at least certain components of complex philosophical research questions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "href": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project",
    "text": "10.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project\n\n\n\nSlide 09\n\n\nResearchers embarked on a project to construct an example RAG system, selecting the Stanford Encyclopedia of Philosophy (SEP)—a widely respected online handbook—as the primary data source. The initial step involved scraping the SEP’s content and converting it into markdown format. Originally, the ambition was to develop a directly useful tool for the philosophical community. However, early attempts to implement a standard textbook RAG system, comprising distinct retrieval and generation components, produced answers of surprisingly poor quality; indeed, these initial outputs were often inferior to those obtainable from a standalone LLM like ChatGPT.\nThis experience prompted a significant shift in focus. The project evolved into a qualitative study aimed at determining the optimal configuration for RAG systems tailored to the specific demands of philosophical texts. Achieving improved performance necessitated a meticulous process of refining numerous aspects of the system. This involved careful selection of both the generative LLM and the embedding model responsible for understanding text semantics. Extensive hyperparameter tuning became essential, covering parameters such as top-k (the number of documents retrieved), maximum input and output token lengths, the temperature or top-p settings influencing the creativity of the generated text, and the strategies for chunk size and overlap in document segmentation. Beyond parameter adjustments, researchers explored more complex algorithmic solutions, such as implementing reranking mechanisms to mitigate problems like semantic mismatch between the query and retrieved documents.\nThe methodology for enhancing the system was predominantly one of trial and error, guided by theoretical insights into how RAG components interact. A significant hurdle encountered throughout this process was the evaluation of the system’s output. Philosophical RAG systems generate answers in free, unstructured text, often articulating complex propositions rather than simple atomic facts (unlike, for example, a historical query seeking Wittgenstein’s last place of living, which expects a city name). Consequently, robust evaluation standards are paramount to assess whether these generated propositions accurately convey the intended philosophical concepts and facts, a non-trivial task.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.4 SEP RAG System: Interface and Functionality Details",
    "text": "10.4 SEP RAG System: Interface and Functionality Details\n\n\n\nSlide 09\n\n\nThe developed Stanford Encyclopedia of Philosophy RAG (SEP RAG) system features a user-facing frontend and a backend constructed with Python, amounting to a few thousand lines of code. The frontend interface provides users with considerable control over the generation process. Within its input section, users can configure several key parameters: they can select the generative model (with gpt-4o-mini shown as an example), view the chosen model’s maximum prompt token limit (e.g., 128,000 tokens), and set a specific prompt token limit for the RAG system’s input (e.g., 15,000 tokens). Additionally, users can define a persona for the LLM—for instance, “You are an expert philosopher. You answer meticulously and precisely”—and specify the number of texts to retrieve for context (e.g., 15). A dedicated field allows for the input of a philosophical question, such as “What is priority monism?”, followed by a “Generate answer” button to initiate the process.\nUpon generation, the system presents its output in a structured manner. Notably, it offers a comparative view, displaying the answer from a standalone LLM (serving as a benchmark) alongside the answer produced by the SEP RAG system. This side-by-side presentation facilitates a more effective assessment of the RAG system’s contribution. Furthermore, the output includes a detailed list of the texts retrieved from the SEP. This list specifies the names of the articles and the particular section headings that the system identified as relevant. Crucially, it also indicates which of these retrieved texts were ultimately included in the augmented prompt passed to the LLM and which, if any, were truncated due to the imposed prompt length limitations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.5 Optimising Document Chunking for Philosophical Corpora",
    "text": "10.5 Optimising Document Chunking for Philosophical Corpora\n\n\n\nSlide 15\n\n\nResearchers dedicated particular attention to optimising the hyperparameter of chunk size, which dictates how documents are segmented for retrieval and processing. They explored several distinct options for this segmentation. One approach involved a fixed number of words or tokens, for example, 500, a method often favoured in computer science for its straightforward implementation. Another considered using natural paragraph breaks as delimiters. A third strategy focused on segmenting the source material by its inherent sections, potentially at various levels of the document hierarchy.\nThrough experimentation with the Stanford Encyclopedia of Philosophy, a clear finding emerged: the most effective results were achieved when entire main sections of SEP articles were treated as the individual “documents” for retrieval. This outcome was somewhat surprising because the average length of these main sections—around 3,000 words—considerably surpassed the input limit of the embedding model, which could process only a little over 500 words at a time.\nThe proposed explanation for this counterintuitive success rests on the specific nature of the SEP. It is a highly systematised and meticulously structured encyclopedic work. Within such a well-organised corpus, the initial portion of a main section (the first 500 words or so that the embedding model can ingest) likely contains enough concentrated information to represent the semantic core of the entire section adequately. However, it is important to note a caveat: this successful strategy of using large, section-level chunks may not be universally applicable. Its efficacy is probably tied to the SEP’s unique characteristics and might not translate effectively to more heterogeneous textual collections or corpora that lack such a high degree of internal systematisation and clear structural demarcation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "href": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.6 Implementing Reranking to Enhance Retrieval Relevance",
    "text": "10.6 Implementing Reranking to Enhance Retrieval Relevance\n\n\n\nSlide 18\n\n\nResearchers identified that an initial retrieval process, even one based on semantic similarity, can sometimes include documents that are not genuinely relevant to the user’s specific question—these are known as false positives. To address this limitation, they incorporated an additional step: reranking. The primary aim of reranking is to re-evaluate and reorder the initially retrieved set of documents, arranging them according to their true relevance to the posed query.\nThe implemented solution involves leveraging a generative Large Language Model (gLLM) to perform this relevance assessment. This choice stems from the understanding that gLLMs exhibit more sophisticated semantic differentiation capabilities than embedding models can offer on their own. Consequently, a gLLM can provide a more nuanced and accurate judgement of how well each retrieved text pertains to the question. During the reranking process, the gLLM scores each candidate document based on specific categories, notably its informativeness concerning the query and the length of the relevant passage contained within it. These individual scores are then aggregated into a “Total Score,” which quantifies the overall relevance of each document.\nThe introduction of this reranking stage proved highly effective. Evaluations demonstrated that it leads to very good results, significantly enhancing the quality and relevance of the documents ultimately used to generate the answer. However, this improvement comes at a cost: the reranking step, by invoking a powerful gLLM for each retrieved document, substantially multiplies the computational resources required, which in turn can increase the monetary expense of operating the RAG system.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "href": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy",
    "text": "10.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy\n\n\n\nSlide 18\n\n\nRetrieval Augmented Generation systems present several distinct advantages for philosophical scholarship. They can seamlessly integrate verbatim corpora, ensuring that answers are grounded in authentic textual evidence, and can effectively incorporate domain-specific and specialised knowledge. These capabilities lead to the generation of more detailed answers and, crucially, a dramatic reduction in the incidence of hallucinations. Furthermore, the ability of RAG systems to cite the relevant documents underpinning their responses directly supports scientific rigour and verifiability, making them, in principle, well-suited for assisting in scholarly tasks.\nNevertheless, several points of caution warrant consideration. RAG systems are not “plug-and-play” solutions; they inherently demand extensive and continuous tweaking to achieve optimal performance. The ideal settings for hyperparameters and model choices are not universal but are instead highly contingent upon the specific characteristics of the corpus in use and the nature of the questions typically posed to the system. Rigorous evaluation of RAG outputs is paramount. This requires establishing a representative set of test questions along with clearly defined expected or ideal answers. Such evaluation processes become particularly challenging when working with unexplored or novel corpora, and the active involvement of domain experts—in this case, philosophers—is indispensable for any meaningful assessment of quality and accuracy.\nResearchers also identified specific challenges and limitations. A significant issue arises if the retrieval mechanism fails to locate any relevant documents; in such instances, the quality of the generated answer tends to decrease substantially, often necessitating adjustments to the user’s prompt. An intriguing, somewhat counterintuitive finding was that RAG systems can sometimes produce worse results for broad, widely discussed overview questions, such as “What are the central arguments against scientific realism?”. The hypothesised reason for this phenomenon is that the RAG system’s operational prompt directs it to focus intently on the local information contained within the retrieved texts. This localised focus, whilst beneficial for specific queries, can inadvertently distract from or fail to adequately synthesise the broader, more encompassing perspective required to answer overview questions comprehensively.\nLooking ahead, these observations underscore the need for more flexible RAG systems. Future developments may involve systems capable of discerning between different types of questions and adapting their strategies accordingly, potentially moving in the direction of more sophisticated “agentic” RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  ```markdown",
    "section": "",
    "text": "13.1 Philosophical Framing and Research Trajectory\ntitle: “Quantum gravity and plural pursuit in science” author: - name: “Lucas Gautheron & Mike D. Schneider” affiliation: “Wuppertal / ENS” email: “lucas.gautheron@gmail.com” date: ‘2025’ bibliography: bibliography.bib — ## Overview {.unnumbered}\nThis chapter, a collaboration with Mike Schneider of the University of Missouri, addresses fundamental questions in the philosophy of science. Their methodology integrates computational linguistic techniques with social network analysis. This investigation unfolds in three distinct phases: first, the authors present a quantum gravity case study, establishing its philosophical framework; next, they propose a bottom-up reconstruction of the quantum gravity research landscape; finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.\nFormulating a quantum theory of gravity, which aims to reconcile knowledge across disparate scales (from the subatomic to the cosmological), presents an enduring challenge and constitutes the core problem addressed. Numerous theoretical solutions have emerged, including string theory, supergravity, loop quantum gravity, spin foams, causal set theory, and asymptotic safety. To characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. This is defined as distinct yet concurrent instances of ‘normal science’ dedicated to a common problem-solving goal. Each instance articulates through a social community intertwined with an intellectual disciplinary matrix, drawing upon established concepts such as Kuhnian paradigms [Kuhn, Year], Laudan’s research traditions [Laudan, Year], and Lakatos’ research programmes [Lakatos, Year]. This framework consequently poses an empirical question: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?\nTo address this empirical question, the authors undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape, encompassing both its linguistic/intellectual and social structures. The dataset comprised 228,748 theoretical physics abstracts and titles, sourced from Inspire HEP (High-Energy Physics Information System). Their methodology involved two principal stages.\nFirst, linguistic analysis employed the Bertopic pipeline [Grootendorst, 2020] for spatialisation into an embedding space, unsupervised clustering (K=611 topics), and specialty assignment to physicists. Second, social network analysis utilised a co-authorship graph of 30,000 physicists, alongside community detection (C=819 communities).\nA key challenge arose from the scale-dependency inherent in computational notions of topics and communities, exacerbated by the intrinsically nested nature of research programmes. To address this, the authors developed a hierarchical reconstruction strategy. They employed Ward agglomerative clustering for topics and a hierarchical stochastic block model [Peixoto, 2014] for communities. Subsequently, they devised an adaptive topic coarse-graining strategy, guided by the Minimum Description Length (MDL) criterion [Rissanen, 1978], to identify the optimal scale by balancing model fit to social structure against model complexity.\nUltimately, the bottom-up analysis yielded 50 coarse-grained topics, which were then correlated with community structures. Findings revealed that whilst some topics aligned well with specific communities, others proved universally relevant. Notably, the bottom-up approach identified a large string theory cluster encompassing supergravity. This aligned precisely with physicists’ intuitions that these, despite historical differences, are not meaningfully separable at certain scales. Such a finding suggests that linguistic nuances lacking social consequences are effectively stripped away. The study concludes that socio-epistemic systems operate at multiple scales, necessitating cross-scale matching for identifying plural pursuit. It thus underscores the transformative potential of computational methods in revisiting and challenging long-held philosophical insights.\nThis chapter, a collaboration with Mike Schneider of the University of Missouri, addresses fundamental questions in the philosophy of science. Their methodology integrates computational linguistic techniques, previously explored in related work [Gautheron & Schneider, Year], with social network analysis.\nThis investigation unfolds through three distinct phases. Initially, the authors present a case study focusing on quantum gravity, establishing its philosophical framework. Subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape. Finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.1 Conceptualising Plural Pursuit in Quantum Gravity",
    "text": "11.1 Conceptualising Plural Pursuit in Quantum Gravity\n\n\n\nSlide 01\n\n\nResearchers initiated an inquiry into the structure of scientific fields, specifically focusing on a long-standing challenge in fundamental physics: the formulation of a quantum theory of gravity. This ambitious endeavour seeks to reconcile our understanding of phenomena at very small scales with those at very large scales. The field of quantum gravity currently presents a multitude of attempted solutions, with string theory standing as the most prominent amongst others, including supergravity, loop quantum gravity (encompassing spin foams), causal set theory, and asymptotic safety.\nTo characterise this situation of multiple, simultaneous research efforts, the investigators, in collaboration with philosopher Mike Schneider, introduce the notion of “plural pursuit”. They define plural pursuit as the existence of distinct yet concurrent instances of normal science, all dedicated to a common problem-solving goal—in this case, the unification of quantum mechanics and general relativity. Furthermore, each such instance of normal science articulates through a specific social community intertwined with an intellectual disciplinary matrix. This conceptualisation draws upon established frameworks in the philosophy of science, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’s research programmes.\nConsequently, a central empirical question arises: does quantum gravity research indeed constitute an instance of plural pursuit? Answering this involves determining whether the field comprises independent communities, each pursuing different paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.2 Bottom-Up Reconstruction: Linguistic and Social Networks",
    "text": "11.2 Bottom-Up Reconstruction: Linguistic and Social Networks\n\n\n\nSlide 03\n\n\nTo empirically investigate the structure of quantum gravity research, scientists embarked on a bottom-up reconstruction of its landscape. This reconstruction aimed to delineate not only the intellectual and linguistic fabric of the field but also its underlying social structure. For this purpose, they gathered a substantial corpus comprising 228,748 abstracts and titles from theoretical physics publications listed on the Inspire HEP database.\nThe analytical process proceeded in two main stages. Firstly, a linguistic analysis sought to map the intellectual structure. Researchers employed the Bertopic pipeline, initially spatialising the documents into an embedding space (L.1). Subsequently, they performed unsupervised clustering on this space (L.2), yielding a highly fine-grained partition of 611 distinct topics. This level of detail was deemed necessary to capture niche research approaches within quantum gravity, some of which might encompass only around one hundred papers. Based on this classification, researchers then assigned each physicist a specialty (σ), defined as the most prevalent topic appearing across their individual publications (L.3).\nIn parallel, a social network analysis scrutinised the co-authorship graph of the field. In this graph, physicists represent nodes, and co-authorship relationships form the edges. Applying community detection methods to this network, which included approximately 30,000 physicists, researchers identified around 819 distinct communities (S.1). This dual analysis thus provided two distinct ways of partitioning the scientists: one based on their topical specialisations and another based on their collaborative communities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.3 Addressing Scale in Plural Pursuit",
    "text": "11.3 Addressing Scale in Plural Pursuit\n\n\n\nSlide 06\n\n\nInvestigators conceptualise plural pursuit, in terms of their computational constructs, as an intuitive one-to-one mapping between social communities and intellectual topics. If such a mapping existed perfectly, a correlation matrix plotting communities against topics would appear block-diagonal. This would signify that each community specialises exclusively in a distinct topical domain, thereby indicating a clear division of intellectual labour.\nHowever, when researchers directly correlated their initial fine-grained partitions of 819 communities and 611 topics, the resulting matrix—visualised using a measure related to normalised pointwise mutual information, npmic,k—proved exceedingly complex and difficult to interpret. Several factors contribute to this intricacy. Firstly, the level of fine-graining in the topic partition is somewhat arbitrary; a broad research programme like string theory, for instance, might be fragmented across numerous fine-grained topics. Secondly, multiple, distinct communities can simultaneously pursue large research programmes, influenced by various micro-social dynamics.\nMore fundamentally, both computational notions of “topic” and “community” are scale-dependent. This implies that literature and social networks can be partitioned at different levels of granularity. This technical issue mirrors a conceptual reality: research programmes themselves are often nested. For instance, one can hierarchically categorise string theory into families like Superstring Theory (further branching into Type II and Heterotic theories, with sub-branches such as Type IIA, Type IIB, Heterotic SO(32), and Heterotic E8 × E8), Bosonic String Theory, and Type I string theory. Consequently, to robustly identify instances of plural pursuit, one must confront and resolve the ambiguity introduced by these multiple, interacting scales.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "href": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining",
    "text": "11.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining\n\n\n\nSlide 09\n\n\nTo navigate the complexities of scale, researchers proposed a hierarchical reconstruction of the quantum gravity landscape. For topics, they implemented Ward agglomerative clustering, starting with the initial 611 fine-grained topics and iteratively merging them based on an objective function, visualised as a dendrogram. For communities, they employed hierarchical stochastic block modelling (drawing on Peixoto, 2014), which inherently learns a multi-level partition of the co-authorship network into increasingly coarse-grained communities. These hierarchical structures permit observation of the system—comprising physicists, their specialties, and their community affiliations—at various scales. However, the choice of which specific scale to analyse for topics and communities remains initially arbitrary, with different choices leading to different correlation patterns and interpretations.\nAn adaptive topic coarse-graining strategy was developed to address this challenge. The underlying idea is to simplify the detailed topic structure by merging topics, provided such merging does not discard information crucial for understanding the social organisation of the field. Many subtle linguistic distinctions, whilst conceptually valid, might not influence how scientists collaborate. This strategy relies on the Minimum Description Length (MDL) criterion, which seeks a partition σ that minimises the sum of two terms: one representing the negative log-likelihood of the social graph G given the partition σ (model fit), and another representing the negative log-probability of the partition σ itself (model complexity). This balances the explanatory power of the topic partition regarding social structure against the desire for a less complex, more parsimonious partition. Applying this, the initial ~600 topics were refined to just 50. Notably, this process preserved some small-scale linguistic topics deemed important for social structure, whilst amalgamating others into broader categories.\nWith these 50 adaptively coarse-grained topics, investigators re-examined the correlation matrix, attempting to match each topic to community structures across different hierarchical levels. Some very broad topics, such as one encompassing general quantum field theory and quantum gravity aspects, appeared ubiquitous and not tied to any specific community. In contrast, the string theory topic demonstrated a strong correspondence with a community structure at the third level of the community hierarchy. Other research programmes, like loop quantum gravity, seemed to align with communities at much finer-grained levels. These findings suggest a complex interplay rather than a simple, clear-cut instance of plural pursuit; nested structures were evident, such as a smaller community focused on holography existing within the larger string theory community. This indicates an entanglement of different scales and a lack of straightforward division of intellectual labour.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.5 Confronting Bottom-Up with Physicists’ Intuitions",
    "text": "11.5 Confronting Bottom-Up with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nResearchers then sought to confront their bottom-up reconstruction of the quantum gravity landscape with the intuitions of physicists working within the field. They conducted a survey targeting the founding members of the International Society for Quantum Gravity, asking them to list the quantum gravity approaches they perceived as structuring the overall research landscape. Although consensus was not universal, this exercise yielded a consolidated list of approaches, including asymptotic safety, causal sets, string theory, supergravity, and holography, amongst many others. For subsequent detailed analysis, the investigators focused on string theory, supergravity, and holography, particularly because some physicists expressed uncertainty about whether these should be considered entirely separate. Indeed, some argued that supergravity and aspects of holography are fundamentally rooted in string theory, despite historical and certain conceptual distinctions.\nTo operationalise this top-down perspective, they trained a Support Vector Machine (SVM) classifier. The classifier’s task was to predict the categorisation of papers into these physicist-defined approaches, using text embeddings derived from titles and abstracts (via the all-MiniLM-L6-v2 model) and trained on a set of hand-coded labels. The resulting supervised, top-down classification was then compared against the 50 coarse-grained bottom-up topics using a correlation heatmap.\nThis comparison revealed that certain physicist-defined approaches aligned well with the emergent bottom-up topics, especially those that were well-defined and conceptually autonomous. Conversely, approaches that were more phenomenological or lacked a fully developed conceptual framework showed poorer correspondence. A significant observation concerned a large “string theory” cluster identified through the bottom-up, scale-aware analysis; this cluster appeared to encompass both supergravity and string theory. This finding resonated with feedback from the survey, exemplified by one physicist who noted the substantial overlap in personnel working on supergravity and string theory, questioning whether the communities could be meaningfully separated. This suggests that the bottom-up methodology, by filtering out linguistic nuances that lack significant correlates in the social structure, can reveal how fields evolve socio-epistemically, sometimes merging entities that were once more distinct. The initial, finer-grained linguistic clustering, however, does correctly capture these underlying conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "title": "13  ```markdown",
    "section": "13.8 Conclusions and Philosophical Implications",
    "text": "13.8 Conclusions and Philosophical Implications\n\n\n\nSlide 21\n\n\nSocio-epistemic systems demonstrably manifest across multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Consequently, identifying configurations of plural pursuit—characterised by a one-to-one mapping between communities and their intellectual substrate—necessitates the careful alignment of these structures across varying scales.\nIn the specific context of quantum gravity, a bottom-up reconstruction of the research landscape offers a powerful means to either confirm or re-assess existing physicists’ intuitions. This study’s key contributions include: (1) the development of a hierarchical reconstruction strategy for both intellectual and social structures in scientific fields; (2) the introduction of an adaptive scale selection method based on the MDL criterion to identify optimal granularity; and (3) an empirical demonstration of how computational methods can reveal the multi-scale, nested nature of research programmes, challenging simplistic notions of disciplinary boundaries.\nCrucially, the increasing potency of computational methods empowers researchers to revisit and even challenge long-held philosophical insights, particularly those concerning the nature of paradigms and communities within scientific fields. Indeed, as one might paraphrase, computation emerges as the continuation of philosophy by other means. ```",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThis chapter presents a comprehensive comparative study systematically evaluating the performance of Latent Dirichlet Allocation (LDA) and BERTopic across distinct textual granularities: titles, abstracts, and full texts. This investigation addresses a pressing question within topic modelling, a crucial analytical tool for navigating vast volumes of scientific literature, particularly within the history, philosophy, and sociology of science. Topic modelling, indeed, extracts thematic content from corpora, thereby enabling the identification of research trends, paradigm shifts, substructures, thematic interrelations, and the evolution of scientific vocabulary.\nThe study’s core objective was to ascertain whether analysing titles or abstracts suffices for effective topic modelling, or if full-text analysis remains indispensable. This inquiry is particularly pertinent given the substantial resources required for obtaining, preprocessing, and analysing comprehensive corpora. To achieve this, the researchers meticulously constituted a corpus of scientific articles, precisely identifying and segmenting title, abstract, and full-text sections.\nSubsequently, they applied both LDA and BERTopic approaches to each textual level. A dual analytical framework, encompassing both qualitative and quantitative methods, then facilitated the comparison of the resulting topic models. This rigorous methodology involved assessing model similarities, topic diversity, joint recall, and coherence, whilst leveraging a well-known astrobiology corpus for qualitative validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "href": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels",
    "text": "12.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels\n\n\n\nSlide 01\n\n\nInvestigators initiated an inquiry to ascertain the most effective textual level—titles, abstracts, or full-texts—for applying topic modelling techniques to scientific literature. This area holds particular relevance for the history, philosophy, and sociology of science. Topic modelling has, indeed, emerged as a crucial instrument for dissecting substantial volumes of scholarly publications.\nThis powerful technique enables diverse analytical tasks, such as identifying research trends and paradigm shifts, uncovering thematic substructures and their interrelations, and tracing the evolution of scientific terminology. Observations from existing literature, however, reveal a varied application of topic modelling across these different textual components.\nThis background prompts a central research question: can analyses restricted to titles or abstracts yield sufficient insights, or does comprehensive full-text analysis remain essential? The considerable resources demanded for obtaining, preprocessing, and analysing complete full-text corpora lend urgency to this question.\nTo address this, investigators first assembled a corpus of scientific articles. Subsequently, they meticulously identified the title, abstract, and full-text sections for each document. Two distinct topic modelling methodologies, Latent Dirichlet Allocation (LDA) and BERTopic, were then applied to each of these three textual levels. Finally, the six generated topic models underwent rigorous qualitative and quantitative comparison to evaluate their respective performances.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.2 Methodology: Topic Modelling Approaches – LDA and BERTopic",
    "text": "12.2 Methodology: Topic Modelling Approaches – LDA and BERTopic\n\n\n\nSlide 05\n\n\nInvestigators employed two distinct topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches operate on the premise that documents can be translated into numerical vectors. This transformation allows topics to be identified through the analysis of repetitions that highlight linguistic regularities. Machine learning techniques then automate the detection of these underlying patterns.\nLatent Dirichlet Allocation, a well-established statistical method, constructs simple vector representations by counting word occurrences within documents. Within this framework, topics manifest as latent variables governed by Dirichlet’s probability distribution. A key advantage of LDA is its capacity to handle extensive texts, rendering it applicable to titles, abstracts, and full-text documents alike.\nConversely, BERTopic offers a more recent, modular alternative. This approach leverages vector representations derived from Large Language Models, with BERT (Bidirectional Encoder Representations from Transformers) serving as its foundational model. In BERTopic, topics emerge as clusters of similar documents.\nWhilst earlier iterations of BERTopic faced limitations with long texts, this study incorporated new embedding techniques. These advancements enable the processing of substantial textual inputs, up to approximately 131,000 tokens, thereby extending BERTopic’s utility to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "href": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus",
    "text": "12.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus\n\n\n\nSlide 07\n\n\nFor the qualitative comparison, researchers utilised material from a prior in-depth topic analysis of an astrobiology corpus, detailed in Malaterre & Lareau (2023). Following a thorough evaluation process, they selected an existing Latent Dirichlet Allocation (LDA) full-text model, which featured 25 distinct topics, as a reference. Each of these 25 topics had undergone meticulous analysis, examining its most representative words and associated documents, leading to the generation of a descriptive label for each topic using pertinent keywords.\nSubsequently, the interrelations between these topics were quantified. Researchers calculated the mutual correlation based on how topics appeared together within documents. A community detection algorithm then processed these correlations, successfully identifying four overarching thematic clusters. These clusters received designations using letters (A, B, C, D) and distinct colours (red, green, yellow, and blue) for clarity.\nThe study presented these findings visually, employing a graph that illustrated the correlations between the 25 topics, complete with their assigned labels and colour-coded cluster memberships. In this graphical representation, the thickness of the lines signified the strength of the correlation between connected topics, whilst the size of the circles indicated the overall prevalence of each topic throughout the entire document collection. In essence, this pre-existing, detailed analysis provided a robust qualitative foundation against which the six topic models generated in the current investigation could be systematically compared.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.4 Methodology: Quantitative Analysis Metrics",
    "text": "12.4 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nResearchers employed four distinct metrics for the quantitative analysis to compare the topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings produced by the models, with a crucial correction for agreements that might occur by chance. An ARI value of zero typically signifies a random clustering.\nSecondly, topic diversity was assessed. This metric quantifies the proportion of distinct top words that characterise the topics within a given topic model, indicating whether different topics are described by unique sets of terms. Thirdly, joint recall provided a measure of how well the top words collectively represent the documents classified under each topic. Specifically, it evaluates the average document-topic recall, considering the relationship between a topic’s top words and its associated documents.\nFinally, coherence, specifically Coherence CV, was measured. This metric aims to determine if the top words constituting a topic are semantically related and form a meaningful group. Its calculation involves averaging the cosine relative distance between the top words within each topic.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.5 Results: Adjusted Rand Index Analysis of Model Similarity",
    "text": "12.5 Results: Adjusted Rand Index Analysis of Model Similarity\n\n\n\nSlide 09\n\n\nThe application of the Adjusted Rand Index (ARI) across all six topic models revealed varying degrees of similarity between them. As a reminder, an ARI score of zero signifies that the agreement between two clusterings is no better than random. The results, often visualised as a heatmap, indicated that the Latent Dirichlet Allocation (LDA) model applied to titles (LDA Title) was the most distinct. It showed the lowest similarity to the other models, with ARI values generally falling below 0.2.\nIn contrast, all other models demonstrated a better overall match with one another, achieving ARI values consistently above 0.2. Notably, the BERTopic models exhibited a stronger internal coherence; they tended to align more closely with each other, yielding ARI values that surpassed 0.35. Within this group, the BERTopic model applied to abstracts (BERTopic Abstract) emerged as a somewhat central figure, as it corresponded well with nearly every other model, the only significant exception being the divergent LDA Title model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.6 Results: Qualitative Comparison of LDA Models",
    "text": "12.6 Results: Qualitative Comparison of LDA Models\n\n\n\nSlide 09\n\n\nA more detailed qualitative analysis focused on the Latent Dirichlet Allocation (LDA) models. When comparing the LDA full-text model with the LDA abstract model (Table A), researchers observed a good overall fit. This conclusion arose because topics from one model generally found a corresponding match in the other, evidenced by a high proportion of shared documents, which formed a noticeable reddish diagonal in the suitably organised heatmap.\nNevertheless, some transformations occurred: three topics present in the full-text LDA model disappeared in the abstract model, whilst another three full-text topics split into multiple, more granular topics within the abstract representation. Conversely, three entirely new topics emerged in the LDA abstract model, and three other abstract topics appeared to be the result of mergers from the full-text model. An additional observation was the presence of one small class, or topic, in the LDA abstract model, encompassing fewer than 50 documents.\nThe comparison between the LDA full-text model and the LDA title model (Table B) revealed a starkly different picture. Here, the fit was poor, indicating substantial reorganisation of thematic structures. Numerous topics from the full-text model disappeared when moving to the title-based model, and concurrently, many new topics emerged that were specific to the LDA title analysis. The heatmap for this comparison displayed a profusion of dark vertical and horizontal lines, visually underscoring the extensive restructuring of topics.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.7 Results: Qualitative Comparison Involving BERTopic Models",
    "text": "12.7 Results: Qualitative Comparison Involving BERTopic Models\n\n\n\nSlide 11\n\n\nInvestigators then examined the BERTopic models in comparison. When contrasting the BERTopic full-text model with the original LDA full-text model (Table C), they found an average overall fit. From the perspective of the LDA full-text topics, eight disappeared in the BERTopic full-text representation, and six were split into more granular topics. Conversely, five new topics emerged within the BERTopic full-text model, and one topic appeared to be the result of a merger. This model, however, presented class size issues: specifically, four small classes and one extremely large class.\nNext, comparing the BERTopic abstract model against the LDA abstract model (Table D), researchers noted a relatively good overall fit. In this transition, four topics from the LDA abstract model disappeared, whilst six were split. The BERTopic abstract model introduced two new topics and featured four topics that resulted from mergers. Importantly, the class sizes in this BERTopic abstract model were generally balanced.\nFinally, the comparison between the BERTopic title model and the LDA title model (Table E) indicated an average fit. Seven topics from the LDA title model were absent in the BERTopic title model, and one LDA title topic was split. The BERTopic title model, in turn, presented seven new topics and one topic formed by a merger. This model also exhibited class size concerns, with three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.8 Results: LDA Top-Words Qualitative Analysis",
    "text": "12.8 Results: LDA Top-Words Qualitative Analysis\n\n\n\nSlide 13\n\n\nA qualitative assessment of the top words defining topics within the Latent Dirichlet Allocation (LDA) models revealed that, generally, the topics were relatively well-formed across the full-text, abstract, and title variations. Investigators observed instances of robust topics that maintained a strong correspondence across all three LDA models. A notable example was the topic labelled “A-Radiation spore” in the LDA full-text model, which aligned closely with semantically similar topics in both the LDA abstract model (characterised by top words such as “radiation,” “spore,” and “space”) and the LDA title model (with top words including “space,” “simulated,” and “spore”).\nFurthermore, some topics identified in the full-text model underwent splitting, fragmenting into several distinct topics within the abstract and title models. For instance, the “A-Life civilization” topic from the full-text analysis split, and one of its resultant components in the abstract model cohered into a general theme concerning research and astrobiology; this particular split was deemed logical. Another full-text topic, “B-Chemistry,” also fragmented, though its resulting divisions proved more challenging to interpret readily without deeper investigation.\nConversely, the analysis also identified instances of topic merging. Certain topics from the full-text model consolidated into new, more encompassing topics in the other LDA models. For example, the distinct full-text topics “B-Amino-acid” and “B-Protein-gene-rna” merged within the LDA abstract model. This fusion created a broader, more generalised topic, a development considered to be a sensible thematic consolidation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.9 Results: BERTopic Top-Words Qualitative Analysis",
    "text": "12.9 Results: BERTopic Top-Words Qualitative Analysis\n\n\n\nSlide 14\n\n\nContinuing the top-words assessment with the three BERTopic models (full-text, abstract, and title), researchers again found that the topics were, on the whole, relatively well-formed when compared against the LDA full-text baseline. The previously identified robust topic, “A-Radiation spore,” demonstrated its stability by maintaining good correspondence across all BERTopic model variations as well.\nThe topic “A-Life-civilization,” also from the LDA full-text model, showed relative stability when analysed with BERTopic across the different text levels. However, it did undergo some degree of splitting here and there. These divisions typically resulted in the formation of narrower, more specific topics pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic from the LDA full-text model, when subjected to BERTopic analysis across the full-text, abstract, and title inputs, also tended to split. This fragmentation consistently led to the emergence of more narrowly focused chemical themes.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.10 Results: Quantitative Analysis – Coherence (CV)",
    "text": "12.10 Results: Quantitative Analysis – Coherence (CV)\n\n\n\nSlide 15\n\n\nResearchers then presented the performance metrics for all six models, considering a range of topic numbers from 5 to 50. Beginning with coherence (specifically Coherence CV), which assesses the semantic relatedness of a topic’s top words, several patterns emerged. The analysis revealed that models based on titles consistently yielded the poorest coherence scores.\nComparing text levels, abstract-based models generally demonstrated superior coherence to their full-text counterparts. When contrasting the modelling techniques, BERTopic typically outperformed Latent Dirichlet Allocation (LDA) in terms of coherence for both abstract and title inputs. However, this advantage for BERTopic tended to lessen as the specified number of topics for the models rose. Across all configurations, the BERTopic Abstract model clearly emerged as the top performer for this particular metric.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.11 Results: Quantitative Analysis – Topic Diversity",
    "text": "12.11 Results: Quantitative Analysis – Topic Diversity\n\n\n\nSlide 16\n\n\nThe analysis of topic diversity, which measures the extent to which topics are described by distinct sets of words, showed a general trend: diversity tended to decrease as the number of topics in the models increased. Models constructed from titles offered better diversity scores when compared to their abstract or full-text equivalents.\nRegarding the modelling techniques, BERTopic consistently achieved higher diversity scores than Latent Dirichlet Allocation (LDA). The BERTopic Title model emerged as the winner for this metric, although the BERTopic Full-text model followed very closely in performance.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.12 Results: Quantitative Analysis – Joint Recall",
    "text": "12.12 Results: Quantitative Analysis – Joint Recall\n\n\n\nSlide 17\n\n\nJoint recall, a metric that evaluates how effectively the top words of a topic collectively represent all documents classified within that topic, yielded further insights. Models based on titles demonstrated the poorest performance in terms of joint recall. Conversely, full-text models generally outperformed their abstract and title-based counterparts on this measure.\nWhen comparing the two primary modelling techniques, Latent Dirichlet Allocation (LDA) tended to achieve better joint recall than BERTopic. The LDA Full-text model and the BERTopic Full-text model emerged as the top performers for joint recall, with the BERTopic Abstract model also demonstrating strong results, following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.13 Results: Summary of Model Performance",
    "text": "12.13 Results: Summary of Model Performance\n\n\n\nSlide 17\n\n\nTo provide a consolidated view, researchers assembled the various results into a summary table. This table depicted the performance of each of the six models—LDA Full-text, LDA Abstract, LDA Title, BERTopic Full-text, BERTopic Abstract, and BERTopic Title—across several assessment categories: overall fit, top-words quality, coherence, diversity, and joint recall. Performance levels were visually represented using circles, where a fully black circle indicated the highest score and a white circle denoted the lowest.\nIt is crucial to recognise that these results do not point to a single, universally superior model. The optimal choice invariably depends on the specific research objectives and needs of the investigator. For instance, if the primary aim involves discovering major thematic trends, and the precise classification of every single document is not paramount, then metrics like poor recall or the presence of a large class of unassigned documents might not present critical drawbacks.\nConversely, if the objective demands that all identified topics comprehensively cover the maximum number of relevant documents, then certain models become less suitable. Specifically, researchers do not recommend BERTopic Full-text and BERTopic Title for such tasks, as they both tended to produce large groups of unclassified documents; BERTopic Title also suffered from poor recall. The LDA Title model is likewise not advised for this scenario, given its generally weak performance across almost all assessment criteria.\nIn light of these findings, the researchers generally recommend performing topic modelling on either abstracts or full-texts, using either LDA or BERTopic. This recommendation comes with the important proviso that the chosen combination does not lead to significant misclassification of documents pertinent to the topics of interest.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.14 Discussion and Conclusion: Implications and Future Directions",
    "text": "12.14 Discussion and Conclusion: Implications and Future Directions\n\n\n\nSlide 17\n\n\nThe research culminated in several key findings and pointed towards future avenues of exploration. Firstly, title-based models generally exhibited poor performance. A plausible explanation for this lies in the inherent lack of informational content within titles alone, which can consequently lead to the misclassification of documents. Nevertheless, it is noteworthy that even the BERTopic Title model managed to identify a number of meaningful topics, suggesting that the utility of titles is not entirely negligible. This highlights a potential need to strike a balance between achieving well-defined topics and ensuring adequate document coverage for each topic.\nSecondly, full-text models presented their own set of challenges. With Latent Dirichlet Allocation (LDA) applied to full-texts, topics sometimes appeared more loosely defined and broader in their thematic scope. Furthermore, such models occasionally identified transverse topics—for instance, those related to methodology—which might be secondary to the primary research themes of interest. BERTopic, when applied to full-texts, sometimes produced topics that were overly narrow. This specificity could lead to poor document coverage and contribute to problems with class size, such as the emergence of extremely large, undifferentiated clusters of documents.\nThirdly, abstract-based models demonstrated commendable performance. The results derived from abstracts showed consistency between both LDA and BERTopic approaches. Moreover, these abstract models aligned well with the LDA full-text model, indicating that abstracts often provide a balanced and effective summary of information suitable for topic modelling.\nA fourth significant observation concerned the robustness of topics. Overall, the study found that very similar thematic structures emerged across the diverse range of models and text levels analysed. This consistency opens possibilities for employing meta-analytic techniques to pinpoint the most robust and consistently identified topics. Furthermore, the relative distances or similarities between models (such as those measured by the Adjusted Rand Index) could potentially be used to identify an optimal or most central model. In this particular study, the BERTopic Abstract model appeared to fulfil such a role, performing strongly across various metrics.\nLastly, the findings prompt consideration of new modelling approaches. Researchers hypothesise that it might be feasible to develop novel models, or refine existing ones, by explicitly leveraging the structural information inherent in scientific articles—that is, the distinct characteristics of full-texts, abstracts, and titles. Such an approach could potentially lead to the extraction of an even more meaningful and nuanced set of topics or defining top-words.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  ```",
    "section": "",
    "text": "15.1 Addressing Implicit Temporal Understanding\ntitle: “Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis” author: - name: “Jochen Büttner” affiliation: “Max Planck Institute of Geoanthropology” email: “buettner@gea.mpg.de” date: ‘2025’ bibliography: bibliography.bib — ## Overview {.unnumbered}\nThis report details a novel approach for imbuing large language models (LLMs) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, LLMs derive their understanding of time implicitly from statistical patterns within training texts [@Brown2020; @Devlin2019]. However, this implicit method proves insufficient for tasks demanding precise temporal context, often leading to “recency bias” or an inability to reconcile temporally contradictory information.\nTo overcome this, researchers propose the “Time Transformer”, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities arising from temporally contradictory information within training data.\nTo validate this concept, engineers developed a modest Transformer model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the Time Transformer’s efficacy. Experiments involved injecting synthetic temporal drifts—both synonymic succession (e.g., replacing “rain” with “liquid sunshine”) and co-occurrence changes (e.g., rain becoming rain and snow)—into the training data. The Time Transformer consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.\nBeyond this proof of concept, the Time Transformer holds significant implications for historical analysis, offering a robust foundation for downstream tasks on historical data and enabling instruction-tuned models to “talk to a specific time.” Whilst this architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking against explicit time-token approaches and investigating the utility of a modest, targeted encoder model. This work represents a crucial step towards developing more temporally intelligent and historically accurate language models.\nCurrent large language models (LLMs) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora [@Brown2020]. Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.\nA critical challenge arises when training data contains temporally contradictory information. Consider, for instance, two sentences: “The primary architectures for processing text through NNs are LSTMs” (true in 2017) and “The primary architectures for processing text through NNs are Transformers” (true in 2025) [@Vaswani2017; @Hochreiter1997]. Without explicit temporal context, an LLM treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a “recency bias,” favouring more recent information in next-token prediction. Current workarounds, such as prompt engineering—inserting explicit temporal cues like “In 2017”—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge [@Liu2023]. A more robust solution necessitates an architecture that enables LLMs to explicitly learn and reproduce these changing patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "href": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "title": "13  Time-Aware Language Models",
    "section": "13.1 The Temporal Deficit in Current Language Models",
    "text": "13.1 The Temporal Deficit in Current Language Models\n\n\n\nSlide 01\n\n\nJochen Büttner, from the Max Planck Institute of Geoanthropology, introduced a foundational concept aimed at enhancing language models. His presentation formalised an idea with potential applications in historical analysis (HPSS), though the speaker acknowledged its basic nature and solicited information regarding any pre-existing similar work.\nResearchers argued that current Large Language Models operate with merely an implicit comprehension of time, a comprehension statistically distilled from the vast quantities of text encountered during training. Whilst these models demonstrate a considerable, albeit indirect, grasp of temporal concepts, explicit time-awareness promises significant benefits, particularly for historical analysis and potentially broader applications. Consider, for instance, two statements: “The primary architectures for processing text through NNs are LSTMs,” accurate around 2017, and “The primary architectures for processing text through NNs are Transformers,” pertinent circa 2025. Humans effortlessly resolve the apparent contradiction by understanding the different temporal contexts. However, within an LLM’s training data, which lacks explicit temporal markers, these statements directly compete, compelling the model towards an unavoidable error in at least one instance.\nConsequently, during inference, an LLM prompted with “The primary architectures for processing text through NNs are” will likely predict “Transformers,” influenced by an inherent recency bias from its training. Eliciting an older truth, such as “LSTMs,” often necessitates careful prompt engineering—perhaps by adding “In 2017” or altering verb tenses—a process researchers describe as somewhat haphazard. The central objective, therefore, involves engineering explicitly time-aware LLMs, empowering them to learn and reproduce evolving patterns within training data as a direct function of time.\nFormally, standard LLMs estimate the probability of a subsequent token given a sequence of preceding tokens, denoted p(xn | x1, …, xn-1). In reality, this probability remains non-static; it dynamically changes with time, correctly represented as p(xn | x1, …, xn-1, t). For instance, the likelihood of “Transformers” completing the aforementioned sentence in 2017 was effectively zero. One can express the probability of an entire token sequence uttered at a specific time t as the product of these conditional probabilities: p(x1, x2, …, xn | t) = Πk=1 to n p(xk | x1, …, xk-1, t). Current models can only mirror temporal shifts in these underlying distributions through in-context learning during inference, a less direct mechanism.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "title": "13  Time-Aware Language Models",
    "section": "13.2 Architecting Time-Awareness: The Time Transformer",
    "text": "13.2 Architecting Time-Awareness: The Time Transformer\n\n\n\nSlide 13\n\n\nAddressing the challenge of modelling the time-dependent probability distribution p(xn | x1, …, xn-1, t) necessitated a novel approach. One existing strategy, time slicing, involves training distinct models for separate temporal segments, assuming distributions remain relatively static within each slice. However, this technique proves exceptionally data-inefficient.\nConsequently, researchers conceived the “Time Transformer”, an architecture distinguished by its elegant simplicity. Standard Natural Language Processing tasks commence by transforming words or tokens into vectorial representations—embeddings—which models refine during training. The Time Transformer innovates by appending an additional dimension to these latent semantic token features, specifically encoding the token’s origin time. Thus, every token in a sequence, uttered at a particular time, receives this explicit temporal information. For instance, the representation for “cat” would subtly differ in this dimension depending on whether it was uttered recently or several years prior.\nOne can formalise this time-aware embedding as E(x, t) = {e1(x), e2(x), …, ed-1(x), φ(t)}, where φ(t) represents the encoded time. The Transformer model then processes a sequence of these augmented embeddings, [E(x1, t), E(x2, t), …, E(xn-1, t)], to predict the time-conditioned probability pθ(xn | x1, …, xn-1, t). The training objective remains the minimisation of the negative log likelihood across the dataset: minθ - Σi=1 to N Σk=1 to n(i) log pθ(xk(i) | x1(i), …, xk-1(i), t(i)). Through this mechanism, temporal information directly ‘injects’ into every token’s representation, enabling the model to learn precisely how significantly the time dimension influences each individual token.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "href": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "title": "13  Time-Aware Language Models",
    "section": "13.3 Empirical Validation: Experimental Design and Implementation",
    "text": "13.3 Empirical Validation: Experimental Design and Implementation\n\n\n\nSlide 10\n\n\nTo validate the Time Transformer concept, researchers required a dataset characterised by a limited vocabulary and simple, repetitive language, thereby facilitating the training of a small generative LLM. Met Office weather reports from the UK’s National Meteorological Service, accessible via their digital archive (https://digital.nmla.metoffice.gov.uk/), fulfilled these criteria admirably. Researchers scraped data spanning 2018 to 2024, yielding approximately 2,500 reports, each comprising around 150-200 words. They also noted an alternative dataset, TinyStories. Preprocessing involved extracting daily reports from monthly PDFs and applying a straightforward tokenisation strategy: no sub-word units, and a disregard for case and interpunctuation. This yielded a modest vocabulary of 3,395 unique words.\nResearchers first constructed a baseline ‘vanilla’ Transformer model. This decoder-only architecture comprised an embedding layer, positional encoding, and dropout, followed by four decoder blocks—each containing multi-head attention (with eight heads), residual connections with layer normalisation, and a feed-forward network—culminating in a final dense layer for output probability distribution. This relatively small model, with 39 million parameters (150MB), contrasts sharply with models such as GPT-4 (1.8 trillion parameters). Training occurred on an HPC cluster in Munich, utilising two NVIDIA A100 GPUs, achieving a rapid 11 seconds per epoch owing to the dataset’s and model’s compactness. The associated code is available on GitHub (j-buettner/time_transformer), though primarily serving as a learning tool. This vanilla model demonstrated proficiency in replicating the language of the weather reports.\nTransitioning to the Time Transformer involved a minimal architectural adjustment. Instead of a standard embedding, researchers incorporated time data by reserving one dimension within the, for example, 512-dimensional latent semantic space for a temporal signal. They concatenated this time value with the token’s semantic embedding before positional encoding. Specifically, a non-trainable, min-max normalised day of the year (calculated as (day of year - 1) / 364) served as the time embedding, a choice made to exploit natural seasonal variations in weather patterns. Researchers acknowledged that alternative methods for encoding time could also be employed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "href": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "title": "13  Time-Aware Language Models",
    "section": "13.4 Learning Temporal Dynamics: Experimental Outcomes",
    "text": "13.4 Learning Temporal Dynamics: Experimental Outcomes\n\n\n\nSlide 16\n\n\nThe primary inquiry guiding these experiments sought to determine whether the Time Transformer could efficiently learn temporal drift within the underlying data distribution. A first experiment, termed “synonymic succession,” involved injecting a synthetic temporal drift. Researchers implemented a time-dependent replacement of the word “rain” with “liquid sunshine,” where the probability of replacement followed a sigmoid function across the days of the year—commencing at zero and culminating at one by year’s end. By generating a weather prediction for each day and analysing the monthly frequencies of these terms, they found the model accurately reproduced this injected pattern: “rain” predominated early in the year, whilst “liquid sunshine” emerged towards the end, with a clear mid-year transition, all subject to expected statistical fluctuations.\nBeyond synthetic changes, the model also captured naturally occurring seasonal patterns, such as the increased frequency of terms like “snow” and “sleet” in winter months, and “hot” or “warm” in summer. However, researchers viewed these as simpler instances of temporal influence, primarily affecting word frequencies. To explore a more complex scenario, a second experiment focused on altering a co-occurrence pattern, which they described as the “fixation of a collocation.” Here, they synthetically replaced instances of “rain” not immediately followed by “and” with “rain and snow” in a time-dependent manner. This aimed to render “rain and snow” an obligatory pairing by the year’s end, akin to how “bread and butter” functions as a fixed phrase. Again, analysis of daily predictions across the year confirmed the model’s success: towards the year’s end, predictions almost exclusively featured “rain and snow,” whilst earlier in the year, “rain” could appear alone—though “rain and snow” also occurred, reflecting genuine meteorological conditions for periods like January.\nInvestigations into the model’s internal workings, specifically its attention mechanisms (using techniques alluded to as ‘excite’), revealed that certain attention heads had specialised in capturing these temporal dependencies. For instance, the attention paid from “snow” back to “rain” (when generating “rain and snow”) varied appropriately with the time of year. Furthermore, early-year co-occurrences of “rain and snow” often correctly conditioned on contextual cues like “cold system,” underscoring the model’s ability to learn nuanced patterns. These findings collectively provided a proof of concept: Transformer-based LLMs can indeed be rendered efficiently time-aware through the simple addition of a temporal dimension to their token embeddings.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "href": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "title": "13  Time-Aware Language Models",
    "section": "13.5 Broader Implications and Future Trajectories",
    "text": "13.5 Broader Implications and Future Trajectories\n\n\n\nSlide 21\n\n\nThe successful proof of concept for the Time Transformer opens several avenues for application and further research. A foundational Time Transformer could provide a robust basis for numerous downstream tasks reliant on historical data. Furthermore, an instruction-tuned version might enable users to interact with information as it existed at a specific point in time, potentially even enhancing present-focused interactions by providing a richer temporal context. This architectural principle could, moreover, extend to model dependencies on other metadata dimensions, such as geographical origin or textual genre.\nRegarding future work, researchers identified several promising directions. Benchmarking the Time Transformer against approaches that treat time as an explicit token within the input sequence would prove valuable. Another important investigation involves testing whether the inclusion of an explicit temporal dimension enhances training efficiency; the hypothesis posits that it could aid the model in more readily deciphering complex temporal patterns that are otherwise only implicitly cued.\nNevertheless, translating this concept into widespread practical application faces notable challenges. The architectural modification—the addition of a temporal dimension to embeddings—raises questions about the feasibility and efficiency of fine-tuning existing pre-trained models; indeed, it may necessitate training new models from scratch. This, in turn, implies significant computational costs for any application beyond the small-scale demonstration. A crucial shift from current practices involves the loss of metadata-free self-supervised learning; the Time Transformer requires meticulous data curation to assign a temporal marker to every token sequence. For historians, accurately determining the ‘generation date’ of textual material can prove complex, involving considerations of original utterance, reprints, and publication lags.\nAs a concluding reflection, the presenter suggested that a more modest, targeted encoder model, akin to BERT, built upon the same time-aware principle, might offer a pragmatic path for specific tasks that do not require full generative capabilities. Such a model could focus on learning relevant temporal patterns without the overhead of modelling all linguistic intricacies. Collaboration on exploring these targeted applications is welcomed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nThis research pioneers a comprehensive approach to enhance the metadata of historical scientific texts and conduct diachronic analyses of chemical knowledge. Presented by Diego Alves, Sergei Bagdasarov, and Badr Abdullah, this initiative addresses the inherent challenges of managing extensive historical corpora.\nOur project unfolds in two distinct phases. Initially, we leverage Large Language Models (LLMs) to refine text metadata, focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Subsequently, the work delves into a detailed case study, meticulously analysing the evolution of chemical discourse across various disciplines over time. This second phase specifically identifies periods of heightened interdisciplinarity and knowledge transfer, employing advanced computational methods.\nThe Philosophical Transactions of the Royal Society of London, a foundational and continuously published scientific journal spanning over three centuries, serves as the primary data source. This rich historical context underpins our innovative research.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#overview",
    "href": "chapter_ai-nepi_018.html#overview",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Researchers presented a two-part investigation into leveraging Large Language Models (LLMs) for enhancing metadata and conducting diachronic analyses of chemical knowledge within historical scientific texts. The first part detailed the application of LLMs to improve metadata for a diachronic corpus, specifically focusing on categorising articles by scientific discipline, assigning semantic tags (topics), and generating abstractive summaries. The second part presented a case study analysing the evolution of the chemical space over time across different disciplines, aiming to identify periods of heightened interdisciplinarity and knowledge transfer. This work utilised the Philosophical Transactions of the Royal Society of London, a corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and almost 300 million tokens. For metadata enrichment, the team employed Llama 3, particularly the Hermes-2-Pro-Llama-3-8B model, fine-tuned for structured output. For the diachronic analysis of chemical terms, ChemDataExtractor, a Python module, identified chemical substances, and Kullback-Leibler Divergence (KLD) measured changes in their usage over time, both within and between disciplines such as chemistry, biology, and physics. Key findings include the successful application of LLMs for article categorisation, the identification of distinct evolutionary patterns in chemical terminology across disciplines, and the detection of knowledge transfer instances between scientific fields.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "href": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.1 Project Objectives and Structure",
    "text": "14.1 Project Objectives and Structure\n\n\n\nSlide 01\n\n\n    Researchers outlined a project, presented by Diego Alves and Sergey and developed in collaboration with LLM expert Badr Abdullah, titled \"Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.\" The investigation divides into two distinct parts. Part one explores the application of Large Language Models (LLMs) to enhance the metadata associated with historical texts, particularly within a diachronic corpus. This enhancement focuses on categorising articles by scientific discipline, assigning relevant semantic tags or topics, and creating abstractive summaries. Subsequently, part two delves into a case study analysing the evolution of the chemical space across different disciplines over time. This analysis specifically seeks to identify periods marked by significant interdisciplinarity and instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "href": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.2 Data Source: The Philosophical Transactions of the Royal Society",
    "text": "14.2 Data Source: The Philosophical Transactions of the Royal Society\n\n\n\nSlide 01\n\n\n    The project investigates how scientific English evolved over time to become an optimised medium for expert-to-expert communication, concurrently analysing phenomena such as knowledge transfer and the identification of influential papers and authors. For this purpose, researchers utilised the Philosophical Transactions of the Royal Society of London. First published in 1665, this journal holds the distinction of being the oldest scientific journal in continuous publication. It played a pivotal role in the development of scientific communication, notably establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge, and it continues to be a highly respected publication today.\n\n    Throughout its history, the journal has featured numerous influential contributions. For instance, in the 17th century, Isaac Newton published his \"New Theory about Light and Colours\" (1672). The 18th century saw Benjamin Franklin's account of \"The 'Philadelphia Experiment'\" concerning the electrical kite. Later, in the 19th century, James Clerk Maxwell detailed his \"Dynamical Theory of the Electromagnetic Field\" (1865). Beyond these seminal works, the corpus also contains more curious papers, such as Monsieur Autour's speculations on the inhabitants of the Earth and Moon. However, the current research refrains from fact-checking or assessing the scientific validity of these historical papers, focusing instead on other analytical dimensions.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "href": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata",
    "text": "14.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata\n\n\n\nSlide 08\n\n\n    Researchers employed the Royal Society Corpus (RSC) version 6.0 full for their analysis. This extensive collection covers over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 individual texts and amounting to almost 300 million tokens. The corpus already possesses some encoded metadata, including author names, century, year, and volume information.\n\n    A previous study attempted to define research disciplines and classify papers within this corpus using Latent Dirichlet Allocation (LDA) topic modelling. The output from this LDA analysis, however, revealed a mixture of disciplines, sub-disciplines, and even types of texts, such as \"Observation\" and \"Reporting,\" rather than purely thematic categories. A visual representation of this earlier classification showed a hierarchical structure with categories like \"LifeScience1,\" \"LifeScience2,\" alongside \"Chemistry\" and \"Physics,\" highlighting the need for a more refined categorisation approach.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation",
    "text": "14.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation\n\n\n\nSlide 10\n\n\n    The first part of the project focused on employing Large Language Models (LLMs) to enhance existing metadata and generate new metadata types for the historical texts. Researchers aimed to leverage LLMs for several information management and knowledge organisation tasks. These included text clean-up, summarisation, improved information extraction, categorisation, and enhanced access and retrieval capabilities, with the potential to feed structured information into knowledge graphs.\n\n    To illustrate the process, an example article titled \"A Spot in one of the Belts of Jupiter\" was considered. Such historical texts often present syntactic complexities characteristic of older writing styles. For this article, the LLM was tasked with providing a hierarchical categorisation (e.g., Discipline: Astronomy, Sub-discipline: Planetary Science), a list of relevant index terms (e.g., Astronomy, Jupiter, Telescopes), and a concise \"Too Long; Didn't Read\" (TL;DR) summary. An example summary provided was: \"The author reports observing a spot in one of Jupiter's belts using a 12-foot telescope. The spot was found to move from east to west within two hours, indicating movement on the planet's surface.\"",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "href": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.5 LLM Selection and Prompt Engineering for Metadata Generation",
    "text": "14.5 LLM Selection and Prompt Engineering for Metadata Generation\n\n\n\nSlide 13\n\n\n    Researchers selected Llama 3 for their metadata enrichment tasks, a new release in the Llama LLM family offering 8 billion (8B) and 70 billion (70B) parameter versions, with a 400B parameter model currently in training. This model, accessible via Hugging Face, reportedly offers significant improvements over predecessors like Mistral and Llama 2. The team specifically utilised instruction-tuned versions, deemed suitable for their objectives, and employed Hermes-2-Pro-Llama-3-8B, a variant fine-tuned for generating structured output, particularly JSON and YAML.\n\n    A detailed system prompt guided the LLM. The prompt first established a role: \"Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.\" It then defined the objective: to read, analyse, and organise the corpus to create a structured database facilitating research. The input was described as OCR-extracted text snippets with existing metadata.\n\n    Four main tasks were specified. Task A involved reading and analysing the article to suggest an alternative, more content-reflective title. Task B required writing a concise 3-4 sentence TL;DR summary in simple language, suitable for a high school student. Task C mandated the identification of exactly five main topics, conceptualised as Wikipedia-style keywords for scientific sub-fields. Finally, Task D involved identifying a primary scientific discipline from a predefined list of nine (Physics, Chemistry, Environmental  Earth Sciences, Astronomy, Biology  Life Sciences, Medicine  Health Sciences, Mathematics  Statistics, Engineering  Technology, Social Sciences  Humanities) and a corresponding second-level sub-discipline, which the LLM could freely define but could not be one of the primary disciplines.\n\n    An example input provided to the LLM included metadata for Isaac Newton's 1672 letter, which has a very long original title, along with a text snippet. The desired output was specified in YAML format, exemplified by a revised title (\"A New Theory of Light and Colours\"), five topics (e.g., \"Optics,\" \"Refraction\"), a TL;DR summary, and the categorisation (\"Physics,\" \"Optics  Light\"). The prompt concluded by strictly requiring valid YAML output.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "href": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.6 LLM Output Validation and Discipline Distribution Analysis",
    "text": "14.6 LLM Output Validation and Discipline Distribution Analysis\n\n\n\nSlide 15\n\n\n    Initial sanity checks on the LLM's output revealed promising results. An impressive 99.81% of the generated outputs (17,486 out of 17,520) conformed to a valid YAML structure, with only a minor 0.19% failing this validation. Regarding the prediction of scientific disciplines, 94% fell within the predefined set of nine categories. However, some hallucinations and errors occurred; for instance, the LLM occasionally produced minor variations like \"Earth Sciences\" instead of the specified \"Environmental  Earth Sciences,\" or invented entirely novel categories such as \"Music.\" In other cases, it mistakenly included the numerical index of a discipline as part of its name or classified sub-disciplines like \"Neurology\" or \"Zoology\" as primary disciplines. Despite these anomalies, the researchers concluded that the majority of papers received correct discipline assignments.\n\n    An analysis of the distribution of scientific disciplines over time, based on the LLM's categorisation, revealed distinct trends. Up until the end of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. A notable peak in chemical articles emerged in the late 18th century, coinciding with the chemical revolution, after which chemistry established itself as a principal pillar of the Royal Society's publications. Progressing into the 19th and 20th centuries, three main pillars—Biology, Physics, and Chemistry—became dominant.\n\n    Furthermore, researchers visualised the TL;DR summaries using t-SNE projections. This technique demonstrated how different disciplines clustered in the semantic space. Chemistry appeared centrally, with significant overlap with Physics and Biology. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters. This static visualisation hinted at the potential for diachronic analysis, enabling the observation of shifts and evolving overlaps between disciplines over time. These LLM-derived discipline categorisations subsequently informed the diachronic analysis of the chemical space.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "href": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.7 Part II: Diachronic Analysis of the Chemical Space Methodology",
    "text": "14.7 Part II: Diachronic Analysis of the Chemical Space Methodology\n\n\n\nSlide 13\n\n\n    The second part of the research concentrated on a diachronic analysis of the chemical space, focusing on three disciplines most prevalent in the corpus: Chemistry, Biology, and Physics. To achieve this, researchers first needed to extract chemical terms from the texts. They employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initially, applying ChemDataExtractor to the full text of articles produced a significant amount of noise. Consequently, a refined two-pass approach was adopted: the tool was first run on the texts, and then re-applied to the list of substances generated in the initial pass, which effectively reduced the noisy output.\n\n    For analysing the evolution of this chemical space, the team utilised Kullback-Leibler Divergence (KLD). KLD, or relative entropy, serves to detect changes across different situational contexts. It quantifies the number of additional bits required to encode a given dataset (A) when using a non-optimal model based on a different dataset (B). Higher KLD values indicate greater dissimilarity between the datasets, whilst lower values suggest similarity.\n\n    Researchers applied KLD in two distinct ways. Firstly, they conducted an intra-discipline diachronic analysis by comparing a future period (dataset A) with a past period (dataset B) within each of the three disciplines (Chemistry, Physics, and Biology) independently. This involved a sliding window technique: for a given central year, chemical term frequencies from a 20-year window preceding it were compared against those from a 20-year window succeeding it. The central year was then advanced by five years, and the comparison repeated, allowing them to trace the evolution of the chemical space within each discipline over the entire timeline. Secondly, they performed an inter-discipline comparative analysis, making pairwise comparisons of the chemical space in Chemistry versus Physics, and Chemistry versus Biology, based on 50-year segments of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.8 KLD Analysis Results: Intra-Disciplinary Evolution",
    "text": "14.8 KLD Analysis Results: Intra-Disciplinary Evolution\n\n\n\nSlide 21\n\n\n    The Kullback-Leibler Divergence (KLD) analysis per discipline revealed that the evolutionary trend of the chemical space was quite similar across Chemistry, Biology, and Physics. Peaks and troughs in KLD values, indicating periods of significant change or stability respectively, occurred at roughly the same times for all three fields. Notably, towards the end of the analysed timeline, the KLD plots tended to flatten, and the overall KLD values decreased, suggesting less variation in chemical terminology between successive future and past periods.\n\n    Researchers then focused on a prominent KLD peak observed in the late 18th century (approximately 1740-1810). The KLD methodology permitted a closer examination of the specific chemical substances contributing most to this divergence. During the sub-period of 1776-1816, in both Biology and Physics, one or two particular elements exhibited extremely high KLD values, indicating they were primary drivers of change in the chemical lexicon of those fields. Despite these drivers, the analysis showed that largely the same set of chemical elements featured prominently across Chemistry, Biology, and Physics during this era.\n\n    This pattern contrasted significantly with observations from a later period, specifically the second half of the 19th century (approximately 1850-1900). Here, the KLD graphs for Biology and Physics were much more populated with influential chemical substances, and the individual contributions of these elements to the overall KLD were more uniform. A differentiation in the types of substances also became apparent: Biology's chemical lexicon began to evolve distinctly towards terms associated with biochemistry. Simultaneously, Chemistry and Physics showed an increasing focus on noble gases and radioactive elements, many of which were discovered towards the end of the 19th century.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer",
    "text": "14.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer\n\n\n\nSlide 24\n\n\n    Pairwise comparisons using Kullback-Leibler Divergence (KLD) further illuminated the distinct chemical focuses of the disciplines, particularly evident in word clouds representing the second half of the 20th century. When comparing Chemistry and Biology, the word cloud for Biology featured a greater prominence of substances related to biochemical processes within living organisms. In contrast, Chemistry's word cloud highlighted substances associated with organic chemistry, such as hydrocarbons and benzene. A comparison between Chemistry and Physics revealed that Physics's chemical lexicon was characterized by a higher frequency of metals—including rare earth metals, semi-metals, and radioactive metals—alongside noble gases. These interdisciplinary comparisons effectively identified thematic divergences in chemical terminology.\n\n    Crucially, this pairwise KLD analysis also enabled the detection of instances termed \"knowledge transfer.\" This phenomenon describes situations where a chemical element, initially ranked as highly distinctive of Chemistry in an earlier period, subsequently becomes more characteristic of either Biology or Physics in a later period. For example, when comparing Chemistry and Physics, the element tin was found to be distinctive of Chemistry during the first half of the 18th century but shifted to become more distinctive of Physics in the second half of that century. Similar shifts for other elements were observed in the early 20th century. A comparable pattern emerged in the Chemistry versus Biology comparison during the 20th century, where elements becoming distinctive of Biology were, once again, frequently related to biochemical processes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Conclusion and Future Work",
    "text": "16.7 Conclusion and Future Work\n\n\n\nSlide 21\n\n\nIn conclusion, the project successfully demonstrated the utility of Large Language Models for categorising historical scientific articles. Researchers conducted a diachronic analysis of the chemical knowledge space within Chemistry, Physics, and Biology, whilst also performing interdisciplinary comparisons of this space, yielding insights into its evolution.\nLooking ahead, the team plans several extensions to this work. For the LLM-driven metadata enrichment (Part I), they intend to test alternative LLMs and undertake a formal evaluation of the results obtained thus far. Regarding the diachronic chemical analysis (Part II), future efforts will involve a more fine-grained interdisciplinary analysis employing diachronic sliding windows. They also aim to broaden the scope by including additional disciplines, such as comparing Chemistry with Medicine. Finally, investigators plan to explore the use of surprisal measures to trace the evolution of the chemical space, potentially offering another lens on conceptual innovation and change.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers are exploring the computational analysis of semantic change, focusing on the intricate modelling of diverse contextual factors and their dynamic interplay. This investigation forms an integral part of the Cascade project, a prestigious Marie Curie doctoral network, with significant contributions from PhD student Sofía Aguilar. Previous work meticulously modelled different context types in isolation; the current objective, however, seeks to synthesise these approaches to illuminate their complex interactions.\nThe chemical revolution, specifically the profound conceptual shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a compelling pilot study. Linguists involved in this endeavour examine how language adapts to real-world transformations, drawing upon established register theory and principles of rational communication. The study aims to detect periods of linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately uncover the linguistic mechanisms and communicative drivers underpinning these transformations. A novel framework, employing Graph Convolutional Networks (GCNs), is proposed to model language dynamics by treating context as a central signal, thereby aiming to overcome limitations of existing methods in capturing the nuanced interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot",
    "text": "15.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot\n\n\n\nSlide 01\n\n\nWithin the Cascade project, a Marie Curie doctoral network, researchers are rigorously investigating the computational analysis of semantic change. PhD student Sofía Aguilar spearheads efforts to model context comprehensively, meticulously examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more holistic understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing extensively upon the Royal Society Corpus (RSC). This pivotal historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation richly documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the iconic painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors: situational (where), temporal (when), experiential (what), interpersonal (who), textual (how), and causal (why).\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers inherent variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.2 Detecting Temporal Shifts through Kullback-Leibler Divergence",
    "text": "15.2 Detecting Temporal Shifts through Kullback-Leibler Divergence\n\n\n\nSlide 04\n\n\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal substantially higher divergence due to profound linguistic evolution.\nResearchers Degaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. This technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time (e.g., from 1725 to 1845) reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 precisely coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can meticulously track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "href": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.3 Paradigmatic Change and Scientific Influence: Cascade Models",
    "text": "15.3 Paradigmatic Change and Scientific Influence: Cascade Models\n\n\n\nSlide 08\n\n\nBeyond temporal detection, this investigation delves into paradigmatic context and the dynamics of conceptual change, referencing seminal work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand precisely who spearheads and propagates these linguistic and conceptual shifts, researchers Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that elegantly illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "href": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal",
    "text": "15.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal\n\n\n\nSlide 10\n\n\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with valuable contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is directly proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, researchers meticulously examine shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, vividly visualised through graphs plotting surprisal against year, robustly supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community utilising that form.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics",
    "text": "15.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics\n\n\n\nSlide 12\n\n\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the profound understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. The proposed framework positions context as a central signal for modelling language dynamics, with Graph Convolutional Networks (GCNs) identified as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four distinct stages:\n\nData Sampling: This initial stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: Researchers begin by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, network size is refined using community detection algorithms, such as that proposed by Riolo and Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering profound insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.6 Reflections: Limitations and Future Directions",
    "text": "15.6 Reflections: Limitations and Future Directions\n\n\n\nSlide 16\n\n\nThis research acknowledges several profound questions that delineate its current limitations and chart compelling future directions. A primary concern involves the very nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context truly integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "",
    "text": "Overview\nResearchers delve into the intricate complexities of science funding, moving beyond conventional analyses of publications and grants. Their work explores the internal processes of funding agencies, offering a nuanced perspective. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, given its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH).\nAn interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This remarkable collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets. To manage and interpret this vast dataset, investigators have developed advanced computational tools.\nThese tools include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data. This innovation significantly improves Optical Character Recognition (OCR) whilst enabling separate analysis of handwritten content. Furthermore, multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. Such capabilities prove crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies vividly demonstrate the power of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect.\nThe overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge. This endeavour seeks to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "href": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.1 Limitations of Current Science Funding Analysis",
    "text": "16.1 Limitations of Current Science Funding Analysis\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds support research endeavours, expecting them to yield societal benefits such as informed policy, clinical advancements, and new technologies. Scholars in the science of science predominantly study this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Indeed, analyses of these sources, including bibliometrics, have offered valuable insights into diverse aspects of science, including its long-term impact, the evolution of team sizes, the emergence of interdisciplinary fields, and the career trajectories of scientists.\nNevertheless, relying solely on the scientific article presents an incomplete, even skewed, representation of the complex scientific process. To assume that bibliometrics fully encapsulates the essence of science constitutes an oversimplification. A deeper understanding necessitates investigating the processes that precede publication, moving beyond the flawed picture painted by articles alone. Such an approach could illuminate critical questions: Does scientific inquiry shape funding priorities, or do funding agendas dictate the direction of science? Within the innovation pipeline, from initial ideation to eventual long-term impact, where do innovations flourish, diffuse across domains, or ultimately falter?\nNotably, the focus on published articles means that failed projects, which could offer significant learning opportunities, often remain unexamined. Beyond direct financial support, funders may also contribute through public data provision, community engagement initiatives, technology development, and cooperative agreements. All these factors influence both the creation of knowledge and the scholars involved. A dynamic interplay also exists between grant funding and technological development.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.2 The Human Genome Project: A Paradigm of ‘Big Science’",
    "text": "16.2 The Human Genome Project: A Paradigm of ‘Big Science’\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a landmark example of ‘big science’ in biology, analogous to large-scale projects in fields like particle physics. This colossal undertaking brought together tens of countries and thousands of researchers with the shared goal of sequencing the entire human genome. Its significance extends across multiple dimensions. Firstly, the HGP captured public interest to an extent previously unseen for a biological research programme, shifting focus from laboratory-based studies of organisms like Drosophila and C. elegans to a grand human-centric endeavour. Secondly, its impact resonates profoundly today; a vast majority of modern biological research, particularly omics methodologies, would be unfeasible without the reference genome it produced. Indeed, the HGP effectively established genomics as a distinct scientific discipline.\nFurthermore, the project pioneered new data-sharing practices, now widely adopted, and forged a powerful synergy between computational science and biology. Two principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, crucially for this study, the National Human Genome Research Institute (NHGRI), which served as the HGP division within the US National Institutes of Health (NIH). Dr Francis Collins, then Director of the NHGRI and later Director of the NIH, played a pivotal leadership role.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "href": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.3 NHGRI: An Innovative Funding Agency",
    "text": "16.3 NHGRI: An Innovative Funding Agency\n\n\n\nSlide 04\n\n\nAnalysis reveals the National Human Genome Research Institute (NHGRI) as a particularly innovative funding body within the National Institutes of Health (NIH). Several bibliometric indicators support this assessment when comparing NHGRI to other NIH institutes. For instance, NHGRI-funded research accounts for a larger share of manuscripts in the top 5% most cited publications. Moreover, its output demonstrates substantial long-term citation impact (measured after ten years) and generates significant citations from patents, indicating translation into clinical applications. The research funded by NHGRI also exhibits high ‘disruption’ scores, suggesting it often pioneers new directions.\nWhilst these metrics establish NHGRI’s innovative capacity, the underlying reasons for this success remain less understood. Consequently, an interdisciplinary research team has assembled to investigate the processes and practices that enable NHGRI to lead innovation. This team comprises experts from diverse fields, including history, physics, ethics, and computer science, and notably includes Dr Francis Collins, a former Director of both NHGRI and NIH. Their collective aim is to unravel the factors contributing to the rise of genomics, identify potential failure points and innovation spillovers, and understand the collaborative mechanisms between funding agencies and academic scientists that foster scientific advancement.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.4 The NHGRI Archive: A Rich and Complex Source",
    "text": "16.4 The NHGRI Archive: A Rich and Complex Source\n\n\n\nSlide 07\n\n\nOwing to the recognised historical importance of the Human Genome Project, the NHGRI meticulously preserved a substantial collection of its internal documentation, spanning from the 1980s and 1990s into subsequent years. This internal archive constitutes a rich repository, containing diverse materials such as the daily meeting notes of scientists coordinating the genome project, handwritten annotations from correspondence, conference agendas, formal presentations, detailed spreadsheets, and newspaper clippings chronicling the period. Additionally, it includes various internal forms, research proposals, and extensive email communications.\nThe sheer volume of this collection, currently exceeding two million pages and expanding by approximately 5% each year due to continuous digitisation, presents a formidable challenge. Effectively analysing such a vast and complex born-physical and born-digital artefact at scale necessitates innovative approaches, forming the core of the research team’s methodological development.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "href": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.5 Internal Archives Versus Public Data",
    "text": "16.5 Internal Archives Versus Public Data\n\n\n\nSlide 09\n\n\nThe internal documents housed within the NHGRI archive possess characteristics and content fundamentally distinct from publicly accessible information, such as Requests for Applications (RFAs) and peer-reviewed publications. Whilst scholars can readily access RFAs and publications through databases like PubMed or NIH RePORTER, the internal records offer a different perspective. Visualisations, such as t-SNE plots, demonstrate that these internal materials form distinct clusters, separate from the clusters representing public RFAs and publications.\nThese internal records provide detailed accounts of numerous large-scale genomic projects initiated and funded by NHGRI. Examples include the Ethical, Legal, and Social Implications (ELSI) Program’s LSAC, modENCODE, eMERGE, ENCODE, the foundational Human Genome Project itself, PAGE, the International HapMap Project, H3Africa, and the NHGRI-EBI GWAS Catalog. Many of these initiatives represented substantial investments, often amounting to tens or hundreds of millions of dollars, and mobilised thousands of researchers globally. Their collective purpose was to develop crucial resources for the genomics community, thereby catalysing the advancement of the entire field.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.6 Computational Methodologies for Archive Processing",
    "text": "16.6 Computational Methodologies for Archive Processing\n\n\n\nSlide 10\n\n\nResearchers developed specialised computational methods to manage the born-physical archive, a significant portion of which contains handwritten material. The use of AI for handwriting analysis presents not only technical hurdles but also ethical considerations regarding the unknown nature of handwritten content. To address this, the team trained a custom-built handwriting model, employing a U-Net architecture, specifically to identify and remove handwritten portions from documents. This process offers a dual benefit: it enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text and simultaneously enables the creation of a distinct processing pipeline dedicated solely to handwriting recognition.\nBeyond handwriting, the project leverages advances in multimodal models, drawing from the document intelligence research community. These models ingeniously combine visual information (the document image), textual content, and layout structure. Such an integrated approach supports diverse tasks, including sophisticated entity extraction. Furthermore, it facilitates the generation of synthetic documents, which serve as valuable training data for developing and refining new classification algorithms, thereby improving the system’s analytical capabilities over time.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "href": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.7 Entity and PII Recognition",
    "text": "16.7 Entity and PII Recognition\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous handling of sensitive information. The documents contain genuine Personally Identifiable Information (PII), including details of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles today. Consequently, the researchers implemented robust methods for entity and PII recognition. Their system effectively identifies, masks, and disambiguates such sensitive data throughout the vast collection.\nThe performance of these recognition models proves strong, as evidenced by F1 scores that improve with increased fine-tuning data for various entity types. These include persons (PERSON), organisations (ORG), email addresses (EMAIL), locations (LOC), and identification numbers (IDNUM). This careful approach ensures privacy and ethical compliance whilst enabling scholarly analysis of the archive’s content.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "href": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.8 Reconstructing the NHGRI Correspondence Network",
    "text": "16.8 Reconstructing the NHGRI Correspondence Network\n\n\n\nSlide 13\n\n\nResearchers conducted a detailed case study involving the reconstruction of email networks from the NHGRI archive. By extracting entities from thousands of scanned paper copies of emails and linking them, they successfully recreated the intricate web of correspondence that occurred during the Human Genome Project era. This reconstructed network encompasses 62,511 distinct email conversations originating from 5,414 individual scanned emails. Each node in the visualised network typically represents an individual involved in these communications, with affiliations to NIH (National Institutes of Health) or external entities (such as other funding agencies, companies, or universities) clearly delineated.\nApplying network analysis techniques, such as stochastic block modelling for community detection, yielded significant insights, particularly concerning the coordination of large-scale initiatives like the International HapMap Project. The HapMap Project, a major genomics endeavour following the HGP, focused on human genetic variation and laid the groundwork for genome-wide association studies (GWAS). Managing such a complex project, involving numerous universities and agencies, relied on formal structures like a steering committee. However, the computational analysis, performed in an unsupervised manner, revealed a hitherto undocumented informal leadership circle, dubbed the ‘Kitchen Cabinet’. This group, referencing a term from the Nixonian political era, apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "href": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.9 Information Brokerage and Leadership Dynamics",
    "text": "16.9 Information Brokerage and Leadership Dynamics\n\n\n\nSlide 16\n\n\nFurther investigation into the NHGRI’s operational dynamics focused on comparing the communication patterns of the informal ‘Kitchen Cabinet’ with those of the formal Steering Committee. Utilising brokerage role analysis, which assesses how individuals or groups mediate information flow within a network, researchers identified distinct behavioural patterns. This analysis categorises nodes based on their interaction styles, such as ‘consultant’ (receiving information and disseminating it back within their own group) or ‘gatekeeper’ (receiving information but not sharing it back with the originating group).\nThe findings indicate that the ‘Kitchen Cabinet’ primarily functioned in a consultant capacity, a pattern that distinguished it from other formal leadership structures active at the time. Notably, individuals like Francis Collins appeared to play significant consultant roles within this informal group. This suggests a leadership style at NHGRI that favoured consultation and open information exchange rather than restrictive gatekeeping, potentially contributing to the agency’s collaborative successes.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.10 Modelling Funding Decisions for Genome Sequencing",
    "text": "16.10 Modelling Funding Decisions for Genome Sequencing\n\n\n\nSlide 13\n\n\nThe research extended to analysing the funding agency’s decision-making processes through portfolio analysis, specifically modelling the choices made when selecting non-human organisms for genome sequencing following the completion of the Human Genome Project. Funding agencies like NHGRI faced complex decisions in allocating resources amongst numerous proposals from different organismal research communities, each advocating for their chosen species (e.g., various primates). To understand these decisions, scientists developed a machine learning model designed to recapitulate the actual funding outcomes.\nThis computational model incorporated a diverse array of features. Biological characteristics, such as the proposed organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (Area Under Curve, AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, measures of gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors submitting the proposal, the overall size of the relevant research community, the proposers’ centrality within the NHGRI network, and the breadth of community support, showed strong predictive power (AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, also contributed to the model’s accuracy (AUC: 0.85 ± 0.04).\nCrucially, when all these feature sets were combined, the model achieved a high level of performance in predicting funding decisions (AUC: 0.94 ± 0.03). This indicates that a multifaceted approach, considering biological, project-related, reputational, and linguistic factors, is necessary to understand the complexities of such funding allocations.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "href": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.11 Interpreting Funding Decisions: The Matthew Effect",
    "text": "16.11 Interpreting Funding Decisions: The Matthew Effect\n\n\n\nSlide 15\n\n\nTo delve deeper into the factors driving funding decisions, researchers employed feature interpretability techniques. These methods help to elucidate how individual features within the computational model influence the predicted outcome, thereby identifying characteristics associated with a higher or lower probability of receiving funding. One significant observation from this analysis is the presence of a ‘Matthew Effect’, a phenomenon where initial advantages tend to accumulate further advantages.\nSpecifically, the analysis revealed that proposals submitted by authors with a higher maximum H-index were more likely to secure funding. Similarly, organisms supported by a larger, more established research community also had a greater chance of being selected for sequencing. This pattern aligns with the strategic objectives of funding agencies, which often prioritise projects perceived as having a higher potential for significant downstream impact and eventual clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "href": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.12 Broader Applications: From Archives to Knowledge",
    "text": "16.12 Broader Applications: From Archives to Knowledge\n\n\n\nSlide 16\n\n\nThe methodologies and insights gained from the NHGRI archive project extend to a broader vision of leveraging born-physical archives through computational analysis. This specific study serves as a compelling example of how such historical data, when processed with advanced tools, can yield valuable knowledge. The research consortium collaborates with various partners and utilises diverse data sources beyond the NHGRI, including federal court records from the United States and seismological data (seismograms) from the EarthScope Consortium.\nA generalised workflow underpins these efforts. It begins with the acquisition of data and metadata from these archives. Subsequently, a sophisticated knowledge creation pipeline applies a series of computational processes. These include:\n\npage stream segmentation to delineate document structures\nhandwriting extraction\nentity disambiguation to resolve identities\nlayout modelling to understand document formats\ndocument categorisation\nfurther entity recognition\nredaction of personal information for privacy\nmodelling of decisions or processes captured in the records\n\nThe ultimate aim of this comprehensive approach is to transform raw archival data into actionable insights that can address pressing scientific questions, inform policy-making, and significantly enhance the accessibility of these rich historical resources. This transformation relies on robust algorithms and a well-developed cyberinfrastructure.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "href": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.13 Preservation, Collaboration, and NHGRI’s Significance",
    "text": "16.13 Preservation, Collaboration, and NHGRI’s Significance\n\n\n\nSlide 17\n\n\nA significant challenge remains in the preservation of born-physical data, much of which currently resides in vulnerable conditions, such as shipping containers susceptible to damage and neglect. The imperative to safeguard these invaluable historical records for future scholarly and scientific inquiry cannot be overstated. Recognising this, the ‘Born Physical, Studied Digitally’ consortium actively seeks collaboration, inviting testers, partners, and users to engage with their tools and methodologies. This initiative receives support from prominent organisations including the National Institutes of Health (specifically NHGRI), NVIDIA, and the National Science Foundation (NSF).\nThe speaker also highlighted a pertinent contemporary issue: recent proposals within the United States to dissolve the NHGRI. This underscores the critical need to appreciate the agency’s historical contributions. Evidence suggests NHGRI stands as one of the most innovative funding agencies in the annals of science. Consequently, the rich data contained within its archives promise to unlock answers to numerous significant scientific questions, reinforcing the importance of its continued study and preservation.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K., participating via Zoom, explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. This approach facilitates sophisticated querying and analysis. Their work directly addresses the challenge of computationally accessing the rich information embedded within traditional formats, such as printed books and archival materials, which typically lack inherent digital structure.\nThe core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline. This pipeline is meticulously designed for specific tasks, aiming to impose structure on unstructured data in a controllable manner. The process commences with diverse sources, including Polish biographical materials and German biographical handbooks, such as Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, and works—and their intricate relationships, representing them as nodes and edges within a knowledge graph. Tools like Neo4j enable the visualisation of these complex networks.\nThis structured representation significantly facilitates complex queries. Researchers can, for instance, investigate network formations amongst professionals during specific historical periods or meticulously trace the evolution of ideas. The methodology champions a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies precisely tailored to research questions, and disambiguated entities linked to external resources like Wikidata. Ultimately, the project aims to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through advanced technologies such as GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "href": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs",
    "text": "17.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs\n\n\n\nSlide 01\n\n\nResearchers confront a significant challenge in historical studies: many invaluable biographical sources persist in unstructured textual formats, such as printed dictionaries and compendia. Whilst existing History and Philosophy of Science (HPSS) datasets often comprise structured information—for instance, publication databases or email archives—these unstructured materials, despite their rich detail, resist computational analysis. Consequently, this project pioneers a methodology to systematically impose structure upon such data, specifically targeting biographical entries. The core idea leverages Large Language Models (LLMs), not as standalone solutions, but as crucial components within a larger, controllable pipeline engineered to construct knowledge graphs.\nHistorically, tools like Get Grass facilitated access to printed materials for digitisation. The current approach, however, aims for a deeper level of structuration. It conceptualises biographical information as a knowledge graph: a network where entities—individuals, geographical locations, countries, published works, or organisations—constitute the nodes. The relationships between these entities, as described in the source texts, form the connecting edges. Such a graph, once constructed, permits complex, structured queries. For instance, one might investigate how professional networks evolved within a specific discipline during a particular era, or trace the contacts an individual established over their career.\nThe team employs LLMs selectively, focusing on their utility for specific tasks within a broader information processing chain, rather than pursuing an elusive ‘perfect’ model. Source materials include Polish biographical collections and German-language resources such as the handbook Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. An entry from the latter, for Alexander Abusch, might detail his role as Minister für Kultur alongside birth and death dates. An extraction pipeline processes these text-based entries, often derived from scanned documents, transforming them into a visual and queryable graph, potentially managed in a system like Neo4j. This process thus converts isolated, albeit information-dense, textual accounts into a connected, structured dataset ripe for scholarly exploration.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "href": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.2 Illustrative Extraction: Structuring Polish Biographical Entries",
    "text": "17.2 Illustrative Extraction: Structuring Polish Biographical Entries\n\n\n\nSlide 06\n\n\nTo illustrate the extraction methodology, consider a Polish biographical entry for Bartsch Henryk, an evangelical priest (ks. ewang.). The text records his birth on 12th December 1832 in Wladyslawowo, situated within the Konin district. Furthermore, it details his extensive travels to Italy (Włochy), Greece (Grecja), the Holy Land (Ziemia Święta), and Egypt (Egipt). His scholarly contributions encompass several publications: Wspomnienia z podróży do Jerozolimy i Kairo (Warsaw, 1873), Listy z podróży po Grecji i Sycylji (Warsaw, 1874), and Z teki podróżniczej, szkice dawne i nowe oryginalne i tłumaczone (Warsaw, 1883). The entry itself cites Bystron’s Wielka Encyklopedja as a source.\nFrom this concise textual snippet, the system extracts key entities and their relationships. ‘Bartsch Henryk’ is identified as a PERSON, his ROLE as ‘ks. ewang.’, his birthdate as ‘12. XII. 1832’ (a DATE), and ‘Wladyslawowo’ along with the countries he visited as LOCATIONs. Relationships such as ‘born on’, ‘born in’, ‘located in’, and ‘travelled to’ link these entities. Consequently, this process yields a set of structured triples: (Bartsch Henryk, is a, ks. ewang.), (Bartsch Henryk, travelled to, Włochy), and so forth, effectively translating narrative information into a machine-readable format suitable for knowledge graph construction.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction",
    "text": "17.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction\n\n\n\nSlide 08\n\n\nEngineers have designed a sophisticated two-stage pipeline to transform raw textual data into structured knowledge graphs. This architecture systematically processes information, integrating both automated techniques and human expertise. The first stage, ‘Ontology-agnostic Open Information Extraction’ (OIE), focuses on identifying factual statements. It commences by loading and chunking the source data, then proceeds to OIE extraction, validates these extractions, and standardises them against benchmarks. A critical quality control checkpoint determines the sufficiency of the OIE output; insufficient output triggers a manual correction loop for a sample of triples.\nSubsequently, the second stage, ‘Ontology-driven Knowledge Graph (KG) building’, refines and structures this information. This stage commences by formulating Competency Questions (CQs) that define the KG’s desired analytical capabilities. Based on these CQs, developers create an ontology and define SHACL (Shapes Constraint Language) shapes for validation. The pipeline then maps the extracted triples to this ontology, disambiguates entities (potentially linking them to Wikidata IDs), represents the data using RDF-star, and subsequently performs SHACL validation. Similar to Stage 1, a quality control loop allows for manual correction of CQs, the ontology, or SHACL shapes if necessary. Both stages integrate layers for LLM interaction and crucial ‘Human in the Loop’ interventions, ensuring robust and reliable outcomes.\nSeveral core principles underpin this pipeline’s design. It is fundamentally research-driven and data-oriented; thus, ontology development directly addresses specific research questions and aligns with the data’s realistic provisions. The human-in-the-loop paradigm strategically combines LLM automation for efficiency with expert oversight at all critical junctures, balancing scale with quality. Transparency is paramount; researchers design each step for verifiability. Furthermore, task decomposition breaks the complex process into manageable, sequential units—notably performing OIE before aligning with an ontology. Finally, modularity ensures the system can adapt, allowing for the integration of improved models or components as technology evolves.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.4 Stage 1 Workflow: Open Information Extraction in Detail",
    "text": "17.4 Stage 1 Workflow: Open Information Extraction in Detail\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, Open Information Extraction (OIE), meticulously processes raw biographical texts to identify factual assertions. It commences by loading and chunking data: the system ingests pre-processed files containing biographical narratives and segments them into manageable units. This step yields semi-structured data, often organised tabularly, with each row corresponding to a text chunk and including identifiers such as name, role, the biographical snippet itself, and a unique chunk ID.\nFollowing this preparation, OIE extraction commences. For each chunk—comprising, for example, a person’s name such as ‘Havemann, Robert’ and a segment of their biography like ‘… 1935 Prom. mit … an der Univ. Berlin…’—the system attempts to extract factual statements. This process generates raw Subject-Predicate-Object (SPO) triples, enriched with pertinent metadata such as an associated timeframe and a confidence score indicating the extraction’s reliability.\nSubsequently, OIE validation scrutinises these raw triples. The original text chunk and its corresponding extracted triples serve as input. Human experts, or potentially other specialised LLMs, then assess the accuracy and relevance of each triple against the source text. This critical review produces a set of validated SPO triples. Finally, the OIE Standard step evaluates the overall quality of this extraction phase. Researchers compare the validated triples against a ‘Gold Standard’—a reference set of triples meticulously created or verified by domain experts. This comparison yields key performance indicators such as F1-score, Precision, and Recall, offering a quantitative measure of the OIE process’s success.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail",
    "text": "17.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail\n\n\n\nSlide 12\n\n\nThe second stage of the pipeline focuses on building the knowledge graph in an ontology-driven manner, ensuring the final structure aligns precisely with research objectives. This stage commences by formulating Competency Questions (CQs). Drawing upon a sample of validated triples from Stage 1, experts define the specific analytical questions the knowledge graph must be capable of answering. This process yields a set of manually refined CQs that guide subsequent development.\nNext, developers create the ontology. Using the CQs and the sample of validated triples as inputs, they design a formal ontology. This ontology specifies the classes of entities (e.g., Person, Organisation, Event), the properties these entities can possess, and the types of relationships that can exist between them, all meticulously tailored to address the CQs. This process yields a comprehensive Ontology Definition.\nWith the ontology established, ontology mapping commences. The pipeline processes the validated triples from Stage 1, mapping their constituent subjects, predicates, and objects to the corresponding classes and properties within the defined ontology. This step transforms the relatively raw triples into conceptual RDF (Resource Description Framework) statements. Finally, disambiguation and Wikidata ID linking refine the graph. This crucial step involves resolving ambiguities in entity references—for instance, ensuring that different individuals sharing the same name are correctly distinguished. The system links entities to external identifiers, such as Wikidata IDs, where feasible. This phase also incorporates RDF-star statement generation, allowing annotations or contextual information to attach directly to individual triples, thereby enriching the graph’s expressive power. This process yields a set of disambiguated triples and RDF-star statements, ready for SHACL validation and subsequent querying.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "href": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations",
    "text": "17.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations\n\n\n\nSlide 15\n\n\nResearchers applied the knowledge graph extraction pipeline to a set of significant Polish historical sources: three complementary compilations by Stanisław Zieliński. These encompass Mały słownik pionierów polskich kolonjalnych i morskich (1933), a biographical dictionary of colonial and maritime pioneers; Bibljografja czasopism polskich zagranicą, 1830-1934 (1935), a bibliography of Polish periodicals published abroad; and Wybitne czyny Polaków na obczyźnie (1935), a record of notable Polish achievements internationally.\nThe structured data extracted from these volumes enables the exploration of several nuanced research questions. For instance, the knowledge graph facilitates the identification of individuals or communities whose pivotal roles in developing ideas and practices might have been obscured by dominant historical narratives. It allows for an analysis of shifts in migration patterns, such as those occurring before and after the January Uprising of 1863. Furthermore, investigators can examine the function of specific journals, determining if they served as ‘boundary objects’ linking diverse intellectual or professional circles, or which publications proved most central to the communities of practice amongst Polish migrants.\nInitial analysis of the Zieliński data yielded a substantial social network graph containing 3,598 nodes and 5,443 edges. Visualisations of this network distinguish editors (coloured green) from other individuals (coloured pink), offering a preliminary glimpse into the relational structures embedded within these historical texts.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "href": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”",
    "text": "17.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”\n\n\n\nSlide 17\n\n\nAnother significant application of the extraction methodology involves the German biographical lexicon, Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. This extensive work, first published in the 1980s by the Bundesstiftung zur Aufarbeitung der SED-Diktatur and subsequently digitised in the 2000s, profiles approximately 4,000 prominent East German figures from diverse fields including politics, science, culture, and sports. Containing entries for individuals such as Gustav Hertz and Robert Havemann, it serves as an indispensable resource for researchers and journalists seeking to understand the complex legacy of the German Democratic Republic.\nThe structured data derived from this lexicon enables quantitative analyses of historical patterns. One such analysis, visually presented as a scatter plot, investigates the relationship between state award recipients, high-ranking positions, and affiliation with the Socialist Unity Party (SED). The plot maps individuals based on their award status (e.g., Karl-Marx-Orden, Nationalpreis der DDR) against their rate of holding high positions and their SED affiliation rate. Comparative statistics reveal striking correlations: for instance, 95.0% of the 38 Karl-Marx-Orden recipients were SED members, compared to only 38.5% of the 1,056 individuals in the sample without this award. Recipients of the Karl-Marx-Orden also held a significantly higher share of high positions (65.8%) compared to those with no awards (28.0%), and an average birth year of 1905.9, substantially earlier than the 1923.0 average for non-recipients. Further detailed breakdowns show that 100% of Karl-Marx-Orden recipients who held Politbüro positions were members, highlighting the award’s strong ties to the highest echelons of power.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.8 Achievements, Current Challenges, and Future Trajectories",
    "text": "17.8 Achievements, Current Challenges, and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project has successfully demonstrated a significant advancement: the transformation of isolated biographical entries into a resource capable of supporting complex structural queries. This marks a crucial step towards unlocking the rich, latent information within historical texts. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques to ensure greater accuracy and in enhancing benchmarking methodologies to rigorously assess pipeline performance.\nLooking ahead, researchers will immediately complete and finalise the current pipeline’s development. Subsequently, a systematic comparison of its outputs against alternative pipelines and existing software packages will provide valuable performance context. A key objective involves scaling the entire process to analyse full datasets, thereby enabling more comprehensive historical investigations.\nBeyond these immediate goals, future ambitions include fine-tuning the pipeline for highly specific research use cases. The team also plans to explore the potential of GraphRAG (Graph Retrieval Augmented Generation), which could allow users to query the knowledge graphs using natural language. Furthermore, the team expresses interest in constructing multilayered networks, possibly employing frameworks such as ModelSEN (Multilayer Social-Epistemic Networks), to facilitate even deeper structural analyses of the complex relationships within the historical data. Interested parties may contact Raphael Schlattmann, Alex Kaye, or Malte Vogl via their provided email addresses.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "18  References",
    "section": "",
    "text": "19 References\n{.unlisted}",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html",
    "href": "ai-nepi_001_chapter.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview\nThis workshop convenes researchers to explore the burgeoning applications of Large Language Models (LLMs) within the history, philosophy, and sociology of science (HPSS). It represents a confluence of scholarly interests, aiming to foster discussion on novel AI-assisted methodologies and their potential to transform research in these disciplines. The event, held from April 2nd to 4th, 2025, at TU Berlin and online, brings together diverse perspectives to examine how computational tools can offer fresh insights into scientific discovery, conceptual evolution, and the intricate dynamics of knowledge production.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-genesis-of-the-workshop",
    "href": "ai-nepi_001_chapter.html#the-genesis-of-the-workshop",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.1 The Genesis of the Workshop",
    "text": "2.1 The Genesis of the Workshop\nThe conception of this workshop arose from two distinct yet complementary wellsprings of research and intellectual curiosity. One origin lies within our project, Network Epistemology in Practice (NEPI). Within this framework, Arno Simons pioneered the training of one of the initial large language models specifically on physics texts, a domain of particular interest to the project. His proposal to discuss such work in a broader academic forum found ready support. Michael Zichert, also a member of the project team, readily agreed that this was a worthwhile endeavour and a pertinent topic, given his own work employing LLMs to analyse conceptual issues in physics.\nA second impetus came from Gerd Graßhoff, a cooperation partner of the NEPI project and a long-standing acquaintance. For many years, Professor Graßhoff has advocated the application of artificial intelligence in the history and philosophy of science, particularly for analysing scientific discovery processes. He independently conceived a similar idea for a workshop focused on novel AI-assisted methods for HPSS. Consequently, a decision to join forces culminated in the organisation of this event. The workshop is a collaborative effort by Gerd Graßhoff, Arno Simons, Michael Zichert, and myself, Adrian Wüthrich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-nepi-project-a-nexus-for-llm-application",
    "href": "ai-nepi_001_chapter.html#the-nepi-project-a-nexus-for-llm-application",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 The NEPI Project: A Nexus for LLM Application",
    "text": "2.2 The NEPI Project: A Nexus for LLM Application\nThe European Research Council (ERC) funds the “Network Epistemology in Practice” (NEPI) project, which serves as a significant contextual background for this workshop. Researchers in the NEPI project investigate the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The primary objective is to achieve a deeper understanding of how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual methodological approach. On one hand, network analysis techniques illuminate the communication structures within the Atlas collaboration. On the other hand, semantic tools, including large language models, are utilised to trace the flow of ideas through these intricate network structures. This latter application, exploring how LLMs can reveal patterns of conceptual development and transmission, represents a particularly compelling area of current research and directly informs the themes of this workshop. Beyond this specific focus, the workshop aims to showcase a wide array of other innovative LLM applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#workshop-modalities-and-acknowledgements",
    "href": "ai-nepi_001_chapter.html#workshop-modalities-and-acknowledgements",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.3 Workshop Modalities and Acknowledgements",
    "text": "2.3 Workshop Modalities and Acknowledgements\nThe smooth execution of this event relies on careful planning and the invaluable contributions of several individuals.\n\n2.3.1 Participant Engagement and Conduct\nTo ensure a comprehensive record and wider dissemination of the discussions, all sessions are being recorded. Participants provided consent for this during registration. A camera is directed towards the speaker, complemented by four roving microphones and an iPhone serving as a backup audio recorder. Subject to presenters’ approval, videos of the talks, including the audio of ensuing discussions (though only video of the presenter, not the audience), will later be uploaded to our YouTube channel. Should any participant require further information or wish to discuss their consent, they are encouraged to approach the organisers.\nGiven the considerable number of attendees and the relatively constrained time for presentations and subsequent dialogue, all participants are kindly requested to keep their questions and comments concise and to the point. Following each presentation, approximately four questions or comments will be collected, after which the presenter can respond collectively. This approach aims to optimise time and facilitate a broader range of interactions. For discussions extending beyond the sessions, an Etherpad (or Cryptpad) is available; a QR code provides access. This platform allows for continued engagement, where presenters can also read and respond to queries. Additionally, attendees—both online and in person—may use the Zoom chat function to post questions or comments at any time during the sessions.\n\n\n2.3.2 Networking and Logistics\nBeyond the formal presentations, the workshop programme incorporates ample opportunities for informal networking. Lunch breaks, coffee breaks, a modest reception, and a workshop dinner (for which seating is unfortunately limited to those who received confirmation) are designed to facilitate interaction amongst researchers and fellows. Coffee and refreshments are served in the main workshop area. Lunch and the reception will take place in room H2051. To reach this location, proceed down the hall to the very last staircase of the building on the other side, descend one floor, and the room will be nearby. We can also proceed there together after today’s final presentation.\n\n\n2.3.3 Gratitude\nThis event would not have been possible without the dedicated assistance of several individuals. Particular thanks are due to Svenja Goetz, Lea Stengel, and Julia Kim, who helped to think through various aspects and managed numerous administrative and organisational tasks. We also extend our gratitude to Oliver Ziegler and his Unicam team for their expertise in recording the keynotes and for their crucial support in setting up the technical infrastructure for all sessions, ensuring a smooth experience for both in-person and Zoom participants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "href": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Introducing the Keynote Speakers",
    "text": "2.5 Introducing the Keynote Speakers\nThe workshop features distinguished keynote speakers who bring exceptional expertise to the exploration of LLMs in HPSS.\n\n2.5.1 Pierluigi Cassotti and Nina Tahmasebi: Exploring Semantic Change\nThe first keynote address will be delivered by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi is the Principal Investigator of the “Change is Key!” research programme in Gothenburg, and Pierluigi Cassotti is a researcher within this project. Their collective work on semantic change detection is highly regarded, encompassing technical contributions such as benchmark creation and broader considerations regarding the application of data science methods to humanities research questions. This focus aligns perfectly with the workshop’s thematic concerns.\n\n\n\nSlide introducing Keynote 1: ‘Large-scale text analysis for the study of cultural and societal change’ by Pierluigi Cassotti and Nina Tahmasebi, with their portraits and the ‘Change is Key!’ tagline.\n\n\n\n\n2.5.2 Iryna Gurevych: Advancing Cross-Document NLP\nThe second keynote, scheduled for the late afternoon of the following day, will be presented by Iryna Gurevych. Professor Gurevych heads the Ubiquitous Knowledge Processing Lab at the Technical University Darmstadt. Her research focuses on information extraction, semantic text processing, and machine learning, alongside the application of Natural Language Processing (NLP) to the social sciences and humanities. Her expertise in elevating NLP to the cross-document level offers another compelling perspective for the workshop attendees.\n\n\n\nSlide introducing Keynote 2: ‘How to InterText? Elevating NLP to the cross-document level’ by Iryna Gurevych, with her photograph.\n\n\nWith these introductions complete, the stage is set for an engaging exploration of Large Language Models and their transformative potential for the History, Philosophy, and Sociology of Science.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html",
    "href": "ai-nepi_003_chapter.html",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "",
    "text": "Overview\nThis chapter navigates the evolving landscape of Large Language Models (LLMs), commencing with a foundational primer on their core architecture. It then explores their diverse applications within History and Philosophy of Science and Science Studies (HPSS), considering various adaptation strategies. Finally, the chapter offers critical reflections on the specific challenges and opportunities these powerful tools present for HPSS research, emphasising the need for methodological rigour and LLM literacy. The discussion aims to equip readers with a nuanced understanding of LLMs, fostering informed engagement with these transformative technologies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "href": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.1 A Primer on Large Language Models",
    "text": "3.1 A Primer on Large Language Models\nThe journey into contemporary Large Language Models invariably begins with the Transformer architecture, a pivotal development that underpins nearly all modern LLMs. Researchers at Google Brain introduced this model in 2017, initially for machine translation tasks, such as converting German text to English (Vaswani2017?).\n\n\n\nFigure 1: The Transformer - model architecture, illustrating the encoder and decoder components side-by-side. The left diagram details BERT (Encoder) and the right details a generic Decoder architecture.\n\n\nThe Transformer comprises two primary interconnected streams: an encoder and a decoder. In its original translation application, the encoder processes the input sentence (e.g., in German), converting words into numerical representations. These numbers undergo several layers of processing—or ‘crunching’—where contextualised word embeddings become progressively more refined layer by layer. Subsequently, these numerical representations transfer to the decoder stream. The decoder then generates the output sentence (e.g., in English) word by word. Each generated word feeds back into the decoder, influencing the prediction of subsequent words until the complete translated sentence emerges.\nA crucial distinction exists between the operational modes of the encoder and decoder. The encoder reads the entire input sentence simultaneously, allowing each word to interact with every other word in the sentence. This mechanism enables the model to construct a comprehensive representation of the sentence’s complete meaning, capturing what is often termed “bidirectional full-context”. Conversely, the decoder operates sequentially; when generating an English word, it can only consider the words previously generated in that sentence. It cannot ‘look into the future’ because its fundamental task is to predict the next word based on the preceding context.\n\n3.1.1 Evolution into Pre-trained Language Models\nShortly after Vaswani and colleagues published their seminal paper (Vaswani2017?), researchers began re-engineering the encoder and decoder streams individually. This work led to the development of pre-trained language models (PLMs). These PLMs represent a shift away from translation per se, towards models possessing a profound general understanding or generative capacity for language. Such models can subsequently undergo minor additional training, or fine-tuning, to perform a wide array of specific Natural Language Processing (NLP) tasks.\n\n\n\nDiagram illustrating the general Transformer model architecture, showing inputs, embeddings, positional encoding, multi-head attention, and feed-forward layers within both encoder and decoder stacks.\n\n\n\n\n3.1.2 Encoder-based Models: The BERT Family\nThe encoder component of the Transformer architecture gave rise to models like BERT (Bidirectional Encoder Representations from Transformers), first introduced by Devlin and colleagues in 2018 (Devlin2018?). The BERT family of models remains highly influential.\n\n\n\nDiagram comparing BERT (Encoder) on the left with a generic Transformer block on the right, illustrating data flow and key components like multi-head attention and feed-forward networks.\n\n\nBERT’s defining characteristic, inherited from the encoder, is its bidirectionality. Each word in an input sequence can ‘attend’ to all other words in both directions (left and right). This allows BERT to build a deep, contextual understanding of the entire input at once. While the specifics of the acronym “Bidirectional Encoder-based Representations from Transformers” are less critical now, the core principle of full-context understanding remains paramount for these models.\n\n\n3.1.3 Decoder-based Models: The GPT Lineage\nOn the other side of the architectural spectrum, researchers developed models based on the Transformer’s decoder component. Prominent amongst these are the GPT (Generative Pre-trained Transformer) models, which power systems like ChatGPT (Radford2018?).\n\n\n\nDiagram comparing BERT (Encoder) and GPT (Decoder) architectures within the broader Transformer framework, highlighting bidirectional full-context for BERT and unidirectional generative for GPT.\n\n\nGPT models, due to their decoder-based structure, can only consider preceding tokens when generating new text. This unidirectional constraint, however, is precisely what enables them to generate novel text, a capability generally lacking in BERT-like models. Consequently, BERT and GPT models serve fundamentally different purposes: BERT excels at understanding language coherently, whilst GPT excels at producing language.\nBeyond these two primary families, a diverse ecosystem of models exists. Some models combine encoder and decoder functionalities. Others employ sophisticated techniques to make decoders behave more like encoders for specific tasks, such as the XLM and XLNet architectures. Understanding the core distinction between generative models (like GPT) that produce language and full-context models (like BERT) that comprehensively understand sentences provides a crucial foundation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#harnessing-large-language-models-in-hpss-research",
    "href": "ai-nepi_003_chapter.html#harnessing-large-language-models-in-hpss-research",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "4.2 Harnessing Large Language Models in HPSS Research",
    "text": "4.2 Harnessing Large Language Models in HPSS Research\nThe capabilities of LLMs extend beyond general language tasks, offering promising avenues for research in the History and Philosophy of Science and Science Studies (HPSS). Adapting these models to the specific textual data and analytical questions prevalent in HPSS requires careful consideration of various techniques and an awareness of the evolving landscape of scientific LLMs.\n\n4.2.1 Domain Adaptation through Retrieval Augmented Generation (RAG)\nOne prominent method for tailoring LLMs to specific domains without extensive retraining is Retrieval Augmented Generation (RAG). This approach creates a pipeline that enhances the model’s knowledge base with domain-specific information at the point of inference.\n Figure 4: The RAG pipeline, combining retrieval mechanisms with generative models for domain-specific responses.\nA RAG system typically involves at least two models, or a model coupled with a retrieval mechanism, operating in concert. For example, when a user poses a query, such as “What are LLMs?”, a retrieval component—often powered by a BERT-like model proficient in semantic similarity—searches a database of relevant documents (e.g., HPSS scholarly articles). This component identifies and retrieves passages most similar to the query. These retrieved passages are then incorporated into the prompt provided to a generative LLM. The generative model uses this augmented context to formulate a more informed and domain-specific answer. This dynamic integration of external knowledge is a feature in many contemporary LLM applications, including instances where ChatGPT searches the internet to answer queries.\nIt is important to recognise that many advanced LLM applications, including reasoning models and “agents,” are not monolithic LLMs. Rather, they are complex systems comprising multiple LLMs, traditional algorithms, and various other tools working together. Understanding these distinctions—different architectures, varied fine-tuning strategies, the critical difference between word versus sentence embeddings, and the levels of abstraction involved—is fundamental for effectively applying LLMs in research.\n\n\n4.2.2 A Survey of LLM Applications in HPSS\nCurrent investigations are actively exploring the utility of LLMs as research tools within HPSS. A preliminary survey of existing applications reveals several emerging categories where these models demonstrate considerable potential (Ho2024?).\n Figure 5: Overview of LLM applications in HPSS research.\nThese categories include:\n\nDealing with data and sources:\n\nParsing and extracting structured information from texts, such as publication types, acknowledgements, and citations.\nInteracting with source materials through functionalities like automated summarisation or conversational interfaces built using RAG-type systems.\n\nKnowledge structures:\n\nExtracting specific entities relevant to HPSS, for example, scientific instruments, celestial bodies, or chemical compounds.\nMapping complex relationships, such as disciplinary connections, the contours of interdisciplinary fields, or the dynamics of science-policy discourses.\n\nKnowledge dynamics:\n\nTracing the conceptual histories of key terms, for instance, analysing the evolution of “theory” in Digital Humanities or the terms “virtual” and “Planck” in physics.\nIdentifying markers of novelty, such as detecting breakthrough papers or tracking the emergence of new technologies within a corpus.\n\nKnowledge practices:\n\nReconstructing arguments within texts by identifying premises, conclusions, and causal claims.\nAnalysing citation contexts to understand the purpose, sentiment, or function of a citation.\nConducting discourse analysis to identify linguistic features like hedge sentences, specialised jargon, or instances of boundary work.\n\n\n\n\n4.2.3 Emerging Trends and Persistent Concerns in HPSS LLM Adoption\nThe adoption of LLMs within HPSS reflects an accelerating interest. Research utilising these models is increasingly appearing not only in computationally focused journals like Scientometrics and JASIST but also in traditional HPSS journals that have historically been less engaged with computational methods. This broadening appeal suggests that the enhanced semantic capabilities of modern LLMs are attracting qualitative researchers, historians, and philosophers.\n Figure 6: Trends and concerns in the application of LLMs within HPSS.\nThe degree of customisation in these applications varies considerably. Some researchers undertake sophisticated architectural modifications or custom pre-training efforts. Others perform custom fine-tuning on existing models, whilst some may use off-the-shelf tools like ChatGPT for specific tasks.\nDespite the enthusiasm, several recurring concerns temper the adoption of LLMs in HPSS:\n\nComputational Resources: The significant computational power required for training and, in some cases, running large models remains a barrier.\nOpaqueness: The “black box” nature of many LLMs, where the internal decision-making processes are not easily interpretable, poses challenges for scholarly rigour and validation.\nData Scarcity: A lack of large, high-quality, digitised datasets specific to HPSS research questions can hinder model training and adaptation.\nBenchmark Deficiencies: The absence of standardised benchmarks tailored for HPSS tasks makes it difficult to compare model performance and progress.\nModel Trade-offs: Researchers must navigate the trade-offs between different model types (e.g., BERT-like for understanding versus GPT-like for generation), as no single model is universally optimal. The most effective model is always contingent upon the specific research purpose and available data.\n\nNevertheless, a positive trend towards greater accessibility is evident. Tools like BERTopic, for instance, have simplified complex tasks like topic modelling, largely due to excellent developer support and user-friendly interfaces, making advanced computational methods more approachable for a wider range of HPSS scholars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models: Foundations, HPSS Applications, and Reflections</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#critical-reflections-and-future-directions-for-llms-in-hpss",
    "href": "ai-nepi_003_chapter.html#critical-reflections-and-future-directions-for-llms-in-hpss",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "4.3 Critical Reflections and Future Directions for LLMs in HPSS",
    "text": "4.3 Critical Reflections and Future Directions for LLMs in HPSS\nAs Large Language Models become increasingly integrated into the research toolkit, it is imperative for the HPSS community to engage in critical reflection. This involves acknowledging discipline-specific challenges, fostering LLM literacy, upholding methodological integrity, and addressing the rapidly evolving nature of these technologies.\n Figure 7: Critical reflections for the HPSS community concerning LLM adoption.\n\n4.3.1 Navigating HPSS-Specific Challenges\nThe application of LLMs in HPSS is not without its unique hurdles. Firstly, the historical evolution of concepts and language presents a significant complication. LLMs are predominantly trained on contemporary text, which means they may misinterpret or lack understanding of archaic language, historical terminologies, or shifts in conceptual meaning over time. Researchers must develop strategies to address this, perhaps through targeted fine-tuning on historical corpora or by employing critical interpretive frameworks when analysing model outputs.\nSecondly, HPSS scholarship often embodies a reconstructive and critical perspective. Historians and philosophers of science frequently aim to “read between the lines,” understand the situated context of historical actors, analyse social implications, and identify subtle discursive strategies such as boundary work. Standard LLMs are not inherently designed for such nuanced, interpretative readings. Innovative approaches are therefore needed to guide models towards performing these more sophisticated analytical tasks that are central to HPSS inquiry.\nThirdly, practical data-related issues persist. HPSS researchers often contend with sparse data, particularly for niche historical topics. Source materials may exist in multiple languages, involve old scripts or non-standard orthography, and suffer from a lack of comprehensive digitalisation, all of which pose challenges for LLM processing.\n\n\n4.3.2 Cultivating LLM Literacy within the HPSS Community\nTo effectively navigate these complexities and harness the potential of LLMs, fostering a robust LLM literacy within the HPSS community is paramount. This extends beyond merely using the tools; it involves a deeper engagement with their underlying principles and implications.\nResearchers should familiarise themselves with the fundamental concepts of LLMs, Natural Language Processing (NLP), and Deep Learning (DL), encompassing both the theoretical underpinnings and the practical capabilities and limitations of available tools. Understanding which model architectures, training regimes, and fine-tuning strategies are most appropriate for specific HPSS research problems is essential. This may, for some, involve acquiring or enhancing coding skills, although the trend towards more intuitive natural language interfaces for interacting with models may gradually lower this barrier.\nFurthermore, the development and sharing of domain-specific datasets and benchmarks tailored to HPSS research questions would significantly advance the field. Without a solid foundation of LLM literacy, there is a risk of misinterpreting model outputs, being swayed by superficially impressive visualisations without a critical understanding of their generation, or inappropriately applying tools to research questions.\n\n\n4.3.3 Upholding Methodological Integrity whilst Embracing Innovation\nA crucial aspect of integrating LLMs into HPSS research is the commitment to maintaining established scholarly methodologies and epistemic standards. Whilst these new tools offer powerful analytical capabilities, they must serve, not supplant, the core research objectives of the discipline.\nThe challenge lies in effectively translating HPSS research problems into tractable NLP tasks—such as classification, generation, or summarisation—without allowing the technical specificities of the NLP task to hijack or distort the original scholarly inquiry. The goal is to use LLMs to answer HPSS questions, not merely to find HPSS data upon which to apply LLM techniques.\nSimultaneously, LLMs present exciting new opportunities for bridging traditional qualitative and quantitative approaches within HPSS. They can enable the analysis of textual data at scales previously unimaginable, potentially revealing patterns and connections that complement close reading and interpretative scholarship. This could foster greater interdisciplinary collaboration and methodological pluralism.\nIt is also valuable for the HPSS community to reflect on its own intellectual pre-history concerning computational methods. For instance, techniques like co-word analysis, developed in the 1980s by scholars such as Michel Callon and Arie Rip, emerged from specific theoretical commitments within science and technology studies (STS) and sought to map the socio-cognitive dynamics of scientific fields. Acknowledging this lineage can inform contemporary engagements with LLMs, encouraging a theoretically grounded and critically reflective application of these new tools.\n\n\n4.3.4 Addressing Emergent Questions and the Evolving Landscape\nThe field of LLMs is characterised by exceptionally rapid development, prompting ongoing questions and requiring continuous adaptation from the research community.\nThe issue of model ‘hallucinations’—the propensity of generative models to produce plausible-sounding but factually incorrect or nonsensical information—remains a pertinent concern. Whilst the accuracy of leading models is improving, the outputs of generative LLMs always necessitate careful critical scrutiny and fact-checking, particularly when employed in scholarly contexts.\nThe expanding capacity for multilingual support in LLMs opens new avenues for comparative and cross-linguistic HPSS research. However, selecting or training appropriate models for specific multilingual tasks demands careful consideration of data availability, linguistic nuances, and the specific research goals. The principle remains: the choice of model must align with the research problem and the nature of the available data.\nIndeed, the very term ‘Large Language Model’ may soon prove insufficient, as models increasingly demonstrate multimodal capabilities, processing and generating not only text but also images, sound, and other forms of data. This rapid evolution, where the definition of ‘large’ itself is a constantly moving target with model parameter counts escalating dramatically, underscores the need for researchers to maintain an awareness of ongoing developments.\nThe emergence of ‘agents’—more autonomous systems that can combine LLMs with other tools to perform complex, multi-step tasks—signals a potential new frontier. These agents could transform research practices, assist in data analysis, or even participate in scholarly communication in novel ways. Intriguingly, theoretical frameworks developed within HPSS and STS, such as Actor-Network Theory (ANT), offer robust conceptual language for understanding and analysing these evolving socio-technical assemblages, where human and non-human actors interact and co-construct knowledge.\nUltimately, a core question for the HPSS community is whether, and how, LLMs can contribute to solving longstanding intellectual challenges within the discipline. Early applications, such as the use of contextualised word embeddings to trace the nuanced evolution of concepts like ‘Planck’ through scientific literature over time, suggest significant potential. By carefully mapping word senses and their changing prominence, researchers can gain new insights into conceptual dynamics. Such capabilities offer pathways to address core problems in HPSS that were previously intractable through manual methods alone, promising a richer and more detailed understanding of the history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models: Foundations, HPSS Applications, and Reflections</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html",
    "href": "ai-nepi_004_chapter.html",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "",
    "text": "Overview\nOpenAlex Mapper facilitates the exploration of scholarly literature by projecting search queries from the OpenAlex database onto a pre-computed base-map of scientific articles. The underlying workflow encompasses three principal stages. Initially, an embedding model, Specter 2 (Singh2022?), undergoes fine-tuning to enhance its ability to distinguish between academic disciplines. Subsequently, a base-map materialises from a random sample of 300,000 articles drawn from OpenAlex; their abstracts are embedded and then projected into a two-dimensional space using Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?). This process yields both the visual base-map and a trained UMAP model. Finally, for an individual user query, the system retrieves relevant records from OpenAlex, embeds their abstracts using the fine-tuned Specter 2 model, and projects these new embeddings onto the existing base-map via the trained UMAP model. The outcome is an interactive map, accessible online, which visually situates the user’s query results within the broader landscape of science.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#introduction",
    "href": "ai-nepi_004_chapter.html#introduction",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThe proliferation of scholarly literature presents both immense opportunities and significant challenges for researchers seeking to understand the structure and dynamics of science. Navigating this vast corpus to identify connections, trace influences, and map intellectual terrains requires innovative tools. OpenAlex Mapper, a project developed by Maximilian Noichl (Utrecht University) and Andrea Loettgers, with contributions from Taya Knuuttila (University of Vienna) under an ERC grant focused on Possible Life, offers such a solution. This chapter introduces OpenAlex Mapper, detailing its technical architecture, demonstrating its interactive capabilities, and exploring its applications within the History and Philosophy of Science and Scholarship (HPSS). The tool aims to bridge the gap between detailed qualitative analysis and large-scale quantitative perspectives on scientific knowledge.\n\n\n\nIntroducing OpenAlex Mapper, a tool for visualising connections within scholarly literature.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#the-openalex-mapper-a-technical-architecture",
    "href": "ai-nepi_004_chapter.html#the-openalex-mapper-a-technical-architecture",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.2 The OpenAlex Mapper: A Technical Architecture",
    "text": "4.2 The OpenAlex Mapper: A Technical Architecture\nThe power of OpenAlex Mapper resides in its carefully constructed technical pipeline, which transforms textual data from scholarly articles into spatial representations on an interactive map. This process unfolds in three principal phases: enhancing a language model for disciplinary specificity, constructing a foundational map of science, and dynamically projecting user-specific queries onto this map.\n\n4.2.1 Fine-tuning the Embedding Model\nAt the core of OpenAlex Mapper lies a sophisticated language model, Specter 2 (Singh2022?). Researchers initiated the process by fine-tuning this model. The primary objective of this fine-tuning was to improve the model’s ability to recognise and respect disciplinary boundaries. By training the model on a dataset of articles with closely related disciplinary backgrounds, its capacity to distinguish subtle yet significant differences between fields of study was enhanced. This step ensures that the subsequent mapping process reflects genuine thematic and disciplinary distinctions within the scholarly literature. Visualisations of the embedding space before and after fine-tuning, often produced using UMAP, typically illustrate a clearer separation of disciplinary clusters post-tuning.\n\n\n4.2.2 Base-map Preparation\nWith a discipline-aware embedding model established, the next phase focuses on creating a base-map representing a broad overview of science. For this, the system draws upon the OpenAlex database, a large and openly accessible repository of scholarly material. A random sample of 300,000 article abstracts constitutes the base data. Each abstract in this sample is then converted into a high-dimensional vector representation—an embedding—using the fine-tuned Specter 2 model.\nTo visualise these complex embeddings, engineers employ Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?), a powerful dimensionality reduction technique. UMAP projects the high-dimensional abstract embeddings into a two-dimensional space, arranging them such that similar articles appear closer together. This projection forms the static base-map. Importantly, the UMAP algorithm, once trained on this base data, is preserved. This trained UMAP model retains the knowledge of how to position articles within this specific two-dimensional representation of science.\n\n\n4.2.3 Processing Individual User Queries\nThe true dynamism of OpenAlex Mapper emerges when users introduce their own research interests. The system allows users to input arbitrary queries directly to the OpenAlex database, typically by providing a URL generated from an OpenAlex search. Upon receiving a query, OpenAlex Mapper downloads the relevant scholarly records, often facilitated by tools like PyAlex.\nThese newly acquired records, specifically their abstracts, then undergo the same embedding process as the base-map data, utilising the fine-tuned Specter 2 model. The crucial step follows: the trained UMAP model, developed during the base-map preparation, projects these new embeddings into the existing two-dimensional map. This ensures that the queried articles are positioned coherently relative to the initial 300,000 articles, as if they had been part of the original layout process. The outcome is an interactive map, accessible online and available for download via the data-mapplot library, where the user’s query results are highlighted against the backdrop of the broader scientific landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#navigating-the-scholarly-landscape-using-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#navigating-the-scholarly-landscape-using-openalex-mapper",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.3 Navigating the Scholarly Landscape: Using OpenAlex Mapper",
    "text": "4.3 Navigating the Scholarly Landscape: Using OpenAlex Mapper\nInteracting with OpenAlex Mapper is designed to be an intuitive process, enabling researchers to explore interdisciplinary connections and the contextual position of specific topics or publication sets. The tool is accessible online, and users can follow a straightforward procedure to generate custom maps.\nThe process begins with a visit to the OpenAlex website. Here, users can leverage the full search capabilities of the OpenAlex platform to identify a corpus of literature relevant to their interests. This might involve searching for papers using a particular model (e.g., the Kuramoto model), publications from a specific institution within a given year (e.g., Utrecht University papers from 2019), or articles citing a seminal work (e.g., Wittgenstein’s Philosophical Investigations). Once the desired search results are obtained, the user copies the URL generated by OpenAlex for that specific query.\n\n\n\nThe OpenAlex Mapper user interface, showing input fields for OpenAlex search URLs and sample settings.\n\n\nThis URL is then pasted into the designated input field within the OpenAlex Mapper interface. Users can adjust several settings, such as sample size, particularly for large result sets, as the embedding process can be computationally intensive. Options may also exist to configure aspects of the final visualisation, for instance, colouring points by publication date or displaying citation links between mapped papers.\nUpon clicking “Run Query”, OpenAlex Mapper initiates its backend processes. It downloads the records corresponding to the OpenAlex search URL, extracts the abstracts, and then embeds them using the fine-tuned language model. Subsequently, the trained UMAP model projects these new embeddings onto the base-map. After a processing period, the interactive visualisation appears, displaying the queried papers as distinct points on the map, often highlighted for clarity. Users can then zoom, pan, and hover over points to retrieve metadata, thereby facilitating an exploratory analysis of how their topic of interest relates to the wider scholarly domain.\n\n\n\nAn example of OpenAlex Mapper processing a query for “scale free networks”, displaying the results on the interactive map.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#illuminating-hpss-applications-and-insights",
    "href": "ai-nepi_004_chapter.html#illuminating-hpss-applications-and-insights",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.4 Illuminating HPSS: Applications and Insights",
    "text": "4.4 Illuminating HPSS: Applications and Insights\nOpenAlex Mapper offers a potent instrument for addressing enduring questions within the History and Philosophy of Science and Scholarship (HPSS). It particularly aids in tackling challenges related to the generalisability of findings from small samples or specific case studies and in validating qualitative insights against broader, large-scale trends in contemporary science. The tool facilitates heuristic investigations rooted in quantitative methods, yet always allows for a return to the textual sources.\n\n\n\nA visualisation of interconnected academic fields, representative of maps generated by OpenAlex Mapper.\n\n\n\n4.4.1 Tracing Model Templates Across Disciplines\nOne significant application lies in the investigation of model templates. In philosophy of science, model templates conceptualise how models with very similar structures emerge and find utility across diverse scientific disciplines. OpenAlex Mapper can help visualise where specific model templates—for instance, the Hopfield model, originally developed in one context and later transported to others—have genuinely established a presence and continued usage. By mapping papers related to such templates, researchers can observe their distribution across the scientific landscape, identifying areas of concentration and unexpected applications, thereby understanding how these templates might structure science in ways orthogonal to traditional disciplinary boundaries.\n\n\n4.4.2 Mapping Conceptual Terrains\nThe tool also proves valuable for mapping the landscape of scientific concepts. For example, one can explore the distribution and interplay of concepts like ‘phase transition’ versus ‘emergence’. By querying OpenAlex for papers discussing these concepts and projecting them onto the base-map, their disciplinary loci and areas of overlap or distinction become apparent. This capability allows researchers to broaden conceptual analysis into interdisciplinary contexts, overcoming the difficulties often associated with obtaining and integrating disparate datasets from various fields.\n\n\n4.4.3 Analysing the Distribution of Scientific Methods\nFurthermore, OpenAlex Mapper can illuminate the distribution and adoption of specific scientific methods across disciplines. Consider, for example, ongoing debates in the philosophy of science regarding the role of machine learning techniques versus classical statistical methods. Researchers used OpenAlex Mapper to compare the prevalence of a machine learning technique, the random forest model, with a somewhat analogous classical method, logistic regression. Plotting papers that employ these methods reveals distinguishable patterns of disciplinary preference. Such visualisations prompt deeper philosophical questions: why, for instance, might neuroscientists favour random forests while researchers in thematically proximate fields like psychiatry or mental health research continue to rely more on logistic regressions? Exploring these distributions can uncover underlying epistemological commitments or practical constraints shaping methodological choices.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#considerations-and-future-prospects",
    "href": "ai-nepi_004_chapter.html#considerations-and-future-prospects",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.5 Considerations and Future Prospects",
    "text": "4.5 Considerations and Future Prospects\nSeveral considerations inform the current capabilities and future development of OpenAlex Mapper, ensuring a nuanced understanding of its outputs. These qualifications are crucial for interpreting the generated maps and for guiding further refinements of the tool.\n\n\n\nA summary of qualifications regarding OpenAlex Mapper, including data dependencies and methodological considerations.\n\n\nA primary factor is the OpenAlex database itself. Whilst OpenAlex offers a vast and commendably open collection of scholarly metadata, its completeness and the quality of its data are not perfect. The accuracy and comprehensiveness of the underlying data naturally influence the insights derivable from the maps. Data quality is, however, reasonable, especially when compared to other available large-scale bibliographic databases.\nThe current iteration of the embedding model within OpenAlex Mapper is trained exclusively on English-language texts. This inherently limits the scope of analysis, particularly for research areas with significant non-English literature or for historical periods where English was not the dominant scholarly language. However, as the tool often focuses on more recent scientific history, this limitation may be less acute for certain applications. Future developments could incorporate multilingual models to broaden coverage, although high-quality, science-trained multilingual models remain an area of active research.\nEffectiveness of the embedding step also depends on the availability of abstracts or informative titles within the source records. Papers lacking these textual elements cannot be effectively processed and mapped, potentially leading to omissions in the visualisation.\nFinally, the Uniform Manifold Approximation and Projection (UMAP) algorithm, central to the dimensionality reduction and visualisation, possesses inherent characteristics that users must acknowledge. UMAP is a stochastic algorithm, meaning that each run can produce slightly different layouts, even with identical input data. The maps generated are thus one realisation amongst many possibilities. More fundamentally, projecting extremely high-dimensional data (such as Specter 2 embeddings, which have 768 dimensions) into just two dimensions inevitably involves trade-offs. The algorithm must prioritise certain relationships and necessarily distorts others; not all high-dimensional proximities can be perfectly preserved in the lower-dimensional representation. Understanding these trade-offs is key to a sound interpretation of the visual patterns.\nFurther details on the technical aspects and ongoing research are available in a working paper titled “Philosophy at Scale: Introducing OpenAlex Mapper” (Noichl2023working?).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#conclusion",
    "href": "ai-nepi_004_chapter.html#conclusion",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nOpenAlex Mapper represents a novel approach to navigating and understanding the complex, interconnected web of scientific knowledge. By combining advanced language models with powerful dimensionality reduction techniques, it provides researchers, particularly within HPSS, with an interactive means to explore the distribution of concepts, methods, and intellectual traditions across diverse disciplinary landscapes. Its capacity to ground large-scale visualisations in specific textual sources offers a promising avenue for integrating qualitative insights with quantitative analyses, thereby enriching our understanding of how science operates and evolves. Despite certain limitations inherent in its data sources and algorithms, the tool furnishes a dynamic platform for heuristic investigation and the generation of new research questions about the structure and dynamics of scholarly communication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html",
    "href": "ai-nepi_005_chapter.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\n(inproceedings?){DanilovaAangenendt2025, author = {Danilova, Vera and Aangenendt, Gijs}, title = {Post-OCR Correction of Historical German Periodicals using LLMs}, booktitle = {Proceedings of the Third Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2025)}, publisher = {ACL}, year = {2025} }\n(inproceedings?){DanilovaSoderfeldt2025, author = {Danilova, Vera and Söderfeldt, Ylva}, title = {Classifying Textual Genre in Historical Magazines (1875-1990)}, booktitle = {Proceedings of the 9th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2025)}, publisher = {ACL}, year = {2025} }\n(book?){Petrenz2004, author = {Petrenz, P.}, year = {2004}, title = {A placeholder title for Petrenz 2004 on genre classification}, publisher = {Placeholder Publisher} }\n(book?){Kessler1997, author = {Kessler, B. and Nunberg, G. and Schütze, H.}, year = {1997}, title = {Automatic detection of text genre}, booktitle = {Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics}, pages = {32–38}, publisher = {ACL} }\n(book?){Broersma2010, author = {Broersma, M.}, year = {2010}, title = {A placeholder title for Broersma 2010 on communicative strategies}, publisher = {Placeholder Publisher} }\n(inproceedings?){Conneau2020, author = {Conneau, Alexis and Khandelwal, Kartik and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{’a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin}, title = {Unsupervised Cross-lingual Representation Learning at Scale}, booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, month = jul, year = {2020}, address = {Online}, publisher = {Association for Computational Linguistics}, url = {https://www.aclweb.org/anthology/2020.acl-main.747}, doi = {10.18653/v1/2020.acl-main.747}, pages = {8440–8451} }\n(inproceedings?){Devlin2019, author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding}, booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, month = jun, year = {2019}, address = {Minneapolis, Minnesota}, publisher = {Association for Computational Linguistics}, url = {https://www.aclweb.org/anthology/N19-1423}, doi = {10.18653/v1/N19-1423}, pages = {4171–4186} }\n(inproceedings?){Schweter2022, author = {Schweter, Stefan and H tarihi, Erion}, title = {hm{BERT}: Historical Multilingual Language Models for Named Entity Recognition}, booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference}, month = jun, year = {2022}, address = {Marseille, France}, publisher = {European Language Resources Association}, url = {https://aclanthology.org/2022.lrec-1.561}, pages = {5235–5242} }\n(inproceedings?){LepekhinSharoff2022, author = {Lepekhin, Nikita and Sharoff, Serge}, title = {Web Genre Classification with Deep Learning Models: A Comparative Study}, booktitle = {Proceedings of the 2nd Workshop on Resources for Computational Humanities and Social Sciences (ResHum 2022)}, month = jun, year = {2022}, address = {Marseille, France}, publisher = {European Language Resources Association}, url = {https://aclanthology.org/2022.reshum-1.1}, pages = {1–9} }\n(inproceedings?){KuzmanLjubesic2023, author = {Kuzman, Taja and Fi{}er, Darja and Ljube{}i{'c}, Nikola}, title = {Leveraging Pretrained Language Models for Web Genre Identification}, booktitle = {Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing}, month = sep, year = {2023}, address = {Varna, Bulgaria}, publisher = {INCOMA Ltd.}, url = {https://aclanthology.org/2023.ranlp-1.69}, pages = {620–629} }\n(inproceedings?){Laippala2023, author = {Laippala, Veronika and Luotolahti, Juhani and Ginter, Filip and Kanerva, Jenna and Salakoski, Tapio}, title = {Fin{GENRE}: A Finnish Multi-Genre Corpus with Genre Boundary Detection}, booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)}, month = may, year = {2023}, address = {Tórshavn, Faroe Islands}, publisher = {University of Tartu Library}, url = {https://aclanthology.org/2023.nodalida-1.15}, pages = {140–151} }\n(inproceedings?){Kuzman2023, author = {Kuzman, Taja and Ljube{}i{'c}, Nikola and Fi{}er, Darja}, title = {{X-GENRE}: A Cross-lingual Open-source Text Genre Classification Dataset and Models}, booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, month = may, year = {2023}, address = {Dubrovnik, Croatia}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2023.eacl-main.211}, pages = {2901–2913} }\nThis chapter delves into the ActDisease project, exploring its objectives, the dataset of historical medical periodicals it employs, and the inherent challenges encountered during dataset digitisation. Subsequently, it details a series of genre classification experiments. These experiments encompass the motivation behind genre classification, an examination of zero-shot and few-shot classification techniques, and specific trials involving few-shot prompting with the Llama-3.1 8b Instruct model. The chapter culminates in a conclusion that synthesises the findings and their implications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#about-actdisease",
    "href": "ai-nepi_005_chapter.html#about-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 About ActDisease",
    "text": "5.1 About ActDisease\nThe ActDisease project, an initiative funded by the European Research Council (ERC), investigates the histories of patient organisations across Europe. Central to this research are the periodicals published by these organisations in England, Germany, France, and Great Britain. These documents serve as the primary source material for understanding the evolution and impact of patient advocacy.\n\n\n\nTitle slide of the presentation ‘GENRE CLASSIFICATION FOR HISTORICAL MEDICAL PERIODICALS’, ActDisease Project, listing authors and affiliation.\n\n\n\n5.1.1 About the Project\nActDisease, an acronym for ‘Acting out Disease – How Patient Organizations Shaped Modern Medicine’, is an ERC-funded research endeavour. Its core purpose is to study how patient organisations in 20th-century Europe contributed to shaping disease concepts, illness experiences, and medical practices. The project focuses on ten European patient organisations from Sweden, Germany, France, and Great Britain, covering a period from approximately 1890 to 1990. The principal source materials are the periodicals, mostly magazines, produced by these patient organisations.\n\n\n\nSlide describing the ActDisease project: its funding, purpose, focus, and main source material, with an image of Heligoland, Germany.\n\n\n\n\n5.1.2 Dataset Description\nThe ActDisease dataset comprises a private, recently digitised collection of patient organisation magazines. This collection encompasses materials from Germany, Sweden, France, and the United Kingdom, covering diseases such as allergy/asthma, diabetes, multiple sclerosis, lung diseases, and rheumatism/paralysis. The accompanying image displays a table that summarises the magazines by country, disease, total page count, and year coverage, amounting to 96,186 pages in total. Initial explorations reveal a diverse array of text types within these materials, with notable similarities in content across all magazines.\n\n\n\nSlide detailing the ActDisease Dataset with a table summarising magazines by country, disease, size, and year coverage, alongside example magazine covers.\n\n\n\n\n5.1.3 Dataset Digitisation Challenges\nThe digitisation process for the ActDisease dataset primarily involved Optical Character Recognition (OCR) using ABBYY FineReader Server 14. Whilst this software performed well on most common layouts and fonts, several challenges persist. Complex layouts, slanted text, rare fonts, and varying scan or photograph quality continue to pose difficulties for OCR accuracy. Consequently, remaining issues include OCR errors, particularly in German and French texts, and disrupted reading order. Researchers conducted experiments on post-OCR correction of German texts using instruction-tuned generative models to address some of these problems (DanilovaAangenendt2025?).\nFurthermore, OCR errors appear frequently in creative texts, such as advertisements, humour pages, and poems. A significant challenge arises from the co-occurrence of different text types within a single page—for instance, an administrative report might appear alongside an advertisement and a humour section. This heterogeneity means that conventional topic models and term counts, which do not account for such juxtapositions, are likely biased towards the most frequent text type on a page.\n\n\n\nSlide outlining digitization challenges, including OCR issues with complex layouts and creative texts, and showing examples of historical periodical pages.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "href": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 Genre Classification Experiments",
    "text": "5.2 Genre Classification Experiments\nThe inherent diversity of texts within the historical medical periodicals necessitates a robust method for distinguishing between them. Genre classification emerges as a pivotal approach to address this need.\n\n5.2.1 Motivation for Genre Classification\nAn examination of the ActDisease materials reveals a wide variety of text types, which, interestingly, exhibit similarities across all magazines. Different text types, such as administrative reports, advertisements, and humour sections, often appear side-by-side on the same page. This textual diversity poses a challenge for analytical methods like yearly and decade-based topic models or term counts, as these methods typically do not account for such internal heterogeneity.\n\n\n\nSlide highlighting challenges in analysing diverse text types within historical magazines.\n\n\nGenre, therefore, presents itself as a useful concept for differentiating kinds of text. In Language Technology, genre is often defined as a class of documents sharing a communicative purpose (Petrenz2004?; Kessler1997?)—a definition that proves highly applicable here. The ability to classify genre is crucial for exploring the data from multiple perspectives to construct historical arguments. Specifically, genre classification enables the comparative study of communicative strategies across different countries, diseases, and publications over time (Broersma2010?). It also facilitates a more fine-grained analysis of term distributions and topic models within distinct genre groups.\n\n\n\nSlide explaining why genre is a useful concept for classification and its benefits for historical analysis.\n\n\nThe ActDisease data showcases a rich tapestry of genres. Examples include poetry, academic reports (such as studies on the pancreas), legal documents (like deeds of covenant), and advertisements (for instance, for chocolate aimed at diabetics). Instructive messages, including recipes or medical advice, feature prominently, alongside patient organisation reports detailing meetings and activities. Narratives about patients’ lives also constitute a significant portion of the content.\n\n\n\nSlide illustrating the variety of genres found in the ActDisease dataset, such as patient experiences, advertisements, and instructive texts.\n\n\n\n\n5.2.2 Zero-Shot and Few-Shot Classification\nGiven the scarcity of annotated data within the ActDisease project, researchers explored both zero-shot and few-shot learning approaches for genre classification (DanilovaSoderfeldt2025?). For zero-shot learning, key research questions focused on whether genre labels from publicly available datasets could be efficiently mapped to the project’s custom labels and how performance would vary across different datasets and models. For few-shot learning, the investigation centred on how performance changes with varying training set sizes across models and whether prior fine-tuning on the full dataset could substantially enhance performance.\n\n\n\nSlide outlining research questions for zero-shot and few-shot learning due to limited annotated data.\n\n\n\n5.2.2.1 Genre Definition and Annotation\nThe project team, under the supervision of the main historian, defined the genre labels. The aim was to create labels that are useful for separating content within the ActDisease materials and sufficiently general for potential application to similar datasets. The defined genres include:\n\nAcademic: Research-based reports or explanations of scientific ideas (e.g., research article, report).\nAdministrative: Documents on organisational activities (e.g., meeting minutes, reports, announcements).\nAdvertisement: Promotes products or services for commercial purposes.\nGuide: Provides step-by-step instructions (e.g., health tips, legal advice, recipes).\nFiction: Entertains and emotionally engages (e.g., stories, poems, humour, myths).\nLegal: Explains legal terms and conditions (e.g., contracts, rules, amendments).\nNews: Reports recent events and developments.\nNonfiction Prose: Narrates real events or describes cultural/historical topics (e.g., memoir, essay, documentary).\nQA (Question & Answer): Structured as questions with expert answers, typically from periodical sections.\n\n\n\n\nSlide presenting a table of genres and their definitions used in the ActDisease project.\n\n\nResearchers selected the paragraph as the annotation unit, merging paragraphs from the ABBYY FineReader output based on font patterns (type, size, bold, italic) within a page. Annotators sampled from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal”, specifically focusing on the first and mid-year issues for each year. A team of four historians and two computational linguists, all either native or proficient in Swedish and German, performed the annotation. Each paragraph received two annotations, achieving an average inter-annotator agreement of 0.95 Krippendorff’s alpha, indicating a high level of consistency. Annotators used a structured file format, assigning hard genre labels to each paragraph.\n \n\n\n5.2.2.2 Data Splits and Distribution\nFor the experiments, researchers first split the annotated data into training and held-out sets, with the held-out set comprising approximately 30% of the data. For few-shot experiments, they further divided the held-out set equally and balanced it by label. Researchers excluded the ‘Legal’ and ‘News’ genres from these few-shot experiments due to insufficient training data. Researchers utilised the entire test set for zero-shot experiments. The distribution of genres across languages (German and Swedish) in the training and held-out samples reveals some imbalances. Notably, there is a strong imbalance in ‘Advertisement’ and ‘Nonfiction Prose’ across the two languages.\n\n\n\nSlide presenting bar charts of genre distribution in the ActDisease training and held-out samples for German and Swedish languages.\n\n\n\n\n5.2.2.3 External Datasets and Genre Mapping for Zero-Shot Learning\nTo facilitate zero-shot experiments, researchers incorporated external datasets. These included modern datasets from previous work on automatic web genre classification: the Corpus of Online Registers of English (CORE) and the Functional Text Dimensions (FTD) dataset, both annotated at the document level. Additionally, they used a sample from Universal Dependencies (UDM) Treebanks, which contains sentence-level annotations in multiple languages.\nTwo annotators independently performed the genre mapping from these external datasets to the ActDisease categories. For the final mapping, researchers selected only assignments with full agreement. This process revealed that for some ActDisease genres, no directly suitable labels existed in the available external datasets. The pipeline for creating training data involved this mapping, followed by preprocessing, chunking, and sampling in several configurations based on language family and label levels (ActDisease original vs. external dataset original).\n\n\n\nSlide showing a table mapping ActDisease genre categories to those in CORE, UDM, and FTD datasets.\n\n\n\n\n5.2.2.4 Models Employed\nResearchers selected multilingual encoders for these experiments, models that have demonstrated success in previous automatic genre classification tasks. The chosen models were:\n\nXLM-RoBERTa (Conneau2020?)\nmBERT (multilingual BERT) (Devlin2019?)\nhistorical mBERT (hmBERT) (Schweter2022?)\n\nBERT-like models have seen extensive use in prior work on web register and genre classification (LepekhinSharoff2022?; KuzmanLjubesic2023?; Laippala2023?). XLM-RoBERTa is recognised as a state-of-the-art web genre classifier (Kuzman2023?). The inclusion of hmBERT was particularly pertinent as it is pretrained on a large corpus of multilingual historical newspapers, encompassing the languages in the ActDisease dataset. mBERT was included for comparison with hmBERT, as direct comparison with XLM-RoBERTa is not straightforward. Fine-tuning these models on all configurations of the training data (derived from FTD, CORE, UDM, and a merged set) yielded a total of 48 fine-tuned models. Researchers typically average subsequent metrics across these configurations.\n \n\n\n5.2.2.5 Zero-Shot Learning Evaluation\nIn evaluating zero-shot learning, the imperfect overlap between label sets necessitated an analysis of individual genres and confusion matrices to avoid potential biases. The state-of-the-art web genre classifier, X-GENRE, served as a baseline, considering only the most similar labels.\n\n\n\nIntroduction slide for Zero-Shot Learning Evaluation.\n\n\nOverall, models fine-tuned on the Functional Text Dimensions (FTD) dataset, using the established mapping, performed better. In most FTD configurations, researchers observed no systematic bias, and per-genre metrics were quite good. An interesting observation emerged: on certain datasets, some models handled specific genres much more effectively than others on average. For instance, XLM-RoBERTa demonstrated superior prediction of ‘QA’ (Question & Answer) texts compared to other models when fine-tuned on UDM. Conversely, hmBERT, when fine-tuned on UDM, showed a 16% average increase in correct ‘Administrative’ predictions over XLM-RoBERTa and mBERT. Models based on the CORE dataset proved adept at predicting the ‘Legal’ genre. However, researchers noted class-specific biases in other datasets: UDM fine-tuning tended towards ‘News’ (as the ‘News’ training data had the highest number of Germanic instances, mostly German), whilst CORE fine-tuning leaned towards ‘Guide’ (as only ‘Guide’ training data in CORE was multilingual).\n\n\n\nSlide summarising key results from zero-shot learning, highlighting performance with FTD and specific model-dataset-genre strengths.\n\n\nConfusion matrices for specific configurations illustrate this behaviour. For example, hmBERT fine-tuned on UDM (hmbert_UDM_True_True) shows strong performance for ‘Administrative’. XLM-RoBERTa fine-tuned on CORE (xlmr_CORE_True_False) effectively identifies ‘Legal’ and ‘Academic’ texts. XLM-RoBERTa fine-tuned on UDM (xlmr_UDM_False_False) excels with ‘QA’. Finally, XLM-RoBERTa fine-tuned on FTD (xlmr_FTD_False_False) accurately classifies ‘Legal’ texts.\n\n\n\nSlide displaying four confusion matrices for different model configurations in zero-shot learning, highlighting specific genre prediction strengths.\n\n\nThe table below presents detailed average F1 scores per category, averaged across data configurations. Highlighted values in the original presentation (not reproduced here as bold text) indicate performance that is not a result of systematic biases towards those categories. Notably, models fine-tuned on FTD and CORE show strong F1 scores for the ‘Legal’ genre. hmBERT (UDM) performs well for ‘Administrative’, and XLM-RoBERTa (UDM) for ‘QA’.\n\n\n\nTable of zero-shot per-category F1 scores averaged across data configurations for different models and datasets.\n\n\nAnalysis of average performance across different training configurations (balancing strategies, language family inclusion) for each external dataset (FTD, CORE, UDM) reveals nuances. For FTD, balancing by original labels alongside ActDisease labels ([B2]) or including only Germanic languages ([G+]) decreased performance compared to balancing by ActDisease labels alone ([B1]) or including all language families ([G-]). For CORE, the small number of Finnish and French instances (in the ‘Guide’ genre) slightly decreased performance. For UDM, the presence of other language families and balancing generally improved performance in terms of macro F1.\n\n\n\nSlide showing average F1 scores for different training configurations within FTD, CORE, and UDM datasets.\n\n\n\n\n5.2.2.6 Few-Shot Learning Evaluation\nThe investigation then turned to few-shot learning scenarios.\n\n\n\nIntroduction slide for Few-Shot Learning Evaluation.\n\n\nExperiments demonstrated how models performed with varying training data sizes, both with and without prior Masked Language Model (MLM) fine-tuning on the entire ActDisease dataset. This prior MLM fine-tuning (+MLM) proved clearly advantageous. F1 scores generally increased with the number of training instances, although they remained below 0.8 even with 1182 instances. Notably, hmBERT-MLM (the historical model with prior fine-tuning) outperformed other models, particularly at larger dataset sizes, boosting its performance significantly and even surpassing other models by a small margin.\n\n\n\nLine graph showing few-shot learning performance (F1 score vs. dataset size) for different models with and without MLM fine-tuning.\n\n\nA detailed examination of scores revealed that hmBERT-MLM’s superior performance is largely attributable to its sustained ability to differentiate between ‘Fiction’ and ‘Nonfiction Prose’ as dataset size increases. In contrast, other models, especially XLM-RoBERTa-MLM, exhibited a drastic drop in performance for ‘Fiction’ when using the full-sized training dataset (1182 instances), often over-predicting ‘Nonfiction Prose’ for ‘Fiction’ instances. Both these genres in the ActDisease data frequently contain narratives about patient experiences, particularly concerning diabetes. It is plausible that with a larger data size, the linguistic features of these two genres become more similar, especially as they are confined to the specific domain of patient organisation magazines focused on diabetes and often share themes and narrative structures. This suggests that more data, or perhaps more nuanced features, might be necessary to improve discrimination between these closely related genres.\n \n\n\n\n5.2.3 Few-Shot Prompting Llama-3.1 8b Instruct\nRecognising the limitations of available data for extensive instruction tuning, researchers also explored few-shot prompting with Llama-3.1 8b Instruct, a prominent multilingual generative model with open weights. The prompt structure incorporated genre definitions and two to three carefully selected examples for each genre. The instruction guided the model to label input text with one of the defined genres based on its perceived purpose and content.\n\n\n\nSlide illustrating the prompt structure used for few-shot prompting of Llama-3.1 8b Instruct, including genre definitions and example placeholders.\n\n\nThe results from few-shot prompting Llama-3.1 8b Instruct on the zero-shot test set (the entire held-out set) indicate that the model handles certain labels reasonably well. For instance, ‘Legal’ texts achieved an F1-score of 0.84, and ‘Academic’ and ‘Advertisement’ texts scored 0.72 and 0.73, respectively. However, the provision of only two or three examples proved insufficient for the model to adequately represent and distinguish more nuanced genres such as ‘Nonfiction Prose’ (F1-score 0.49), ‘Administrative’ (F1-score 0.60), and ‘News’ (F1-score 0.08). The overall macro average F1-score was 0.59. The confusion matrix reveals particular difficulties in distinguishing ‘Nonfiction Prose’ from ‘Fiction’ and ‘Administrative’ texts, and ‘Advertisement’ from ‘Administrative’ texts.\n\n\n\nSlide presenting results (F1 scores and confusion matrix) for few-shot prompting of Llama-3.1 8b Instruct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#conclusion",
    "href": "ai-nepi_005_chapter.html#conclusion",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nHistorical periodicals, particularly popular magazines, represent a promising yet challenging source for research into the history of science and medicine. Their genre-rich nature reflects diverse communicative strategies employed over time. Accurately accounting for these genres is crucial for the detailed interpretation of text mining results.\nThis exploration demonstrates that genre classification can significantly enhance the accessibility of such complex historical sources for computational analysis. When faced with no training data, researchers can successfully leverage available modern datasets, provided the genre categories are sufficiently general-purpose. Alternatively, few-shot prompting of capable open generative models, like Llama-3.1 8b Instruct, can achieve decent quality for some genres, although performance may be limited for categories requiring more nuanced understanding with minimal examples.\nHowever, if some annotated data is available, even in limited quantities, few-shot learning with multilingual encoders—such as XLM-RoBERTa or, notably, historical multilingual BERT (hmBERT)—especially when combined with prior Masked Language Model (MLM) fine-tuning on the target domain data, emerges as a superior strategy. For the ActDisease project, this approach yielded the most promising results, with hmBERT-MLM showing considerable gains in performance.\n\n\n\nSlide summarising the main conclusions regarding genre richness in popular magazines and effective strategies for genre classification.\n\n\nOngoing and future efforts aim to further refine these methodologies and apply them to specific historical hypotheses. This includes developing a new annotation scheme with more fine-grained genres, an annotation project financed by Swe-CLARIN, exploring synthetic data generation techniques, and implementing active learning strategies to improve classifier quality efficiently. These endeavours seek to enhance the utility of these methods for both the ActDisease project and the broader digital humanities community.\n\n\n\nSlide outlining future and present work, including working with historical hypotheses, new annotation schemes, and advanced machine learning techniques.\n\n\n\n\nAcknowledgements\nThe project team extends its gratitude to the annotators: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. We also thank Dr Maria Skeppstedt and the anonymous reviewers for their valuable feedback. This research received funding from the European Research Council (ERC-2021-STG, 101040999). The Centre for Digital Humanities and Social Sciences at Uppsala University provided essential support in the form of GPUs and data storage.\n\n\n\nSlide listing acknowledgements to the project team, reviewers, European Research Council, and Centre for Digital Humanities and Social Sciences.\n\n\nFor further information, please visit the project website.\n\n\n\nThank you slide with a QR code.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html",
    "href": "ai-nepi_006_chapter.html",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "",
    "text": "Overview\nThe VERITRACE web application, currently in its ‘alpha’ stage of development, represents an ambitious step towards new research methodologies. This preliminary version is not yet publicly accessible, requiring substantial further work; it serves more as a promise of future capabilities. Central to its current iteration, researchers are testing a BERT-based Large Language Model (LLM), specifically LaBSE (Language-agnostic BERT Sentence Embedding), to generate vector embeddings. These embeddings aim to represent every passage within the project’s extensive textual corpus. However, initial assessments suggest this model may not ultimately prove sufficient for the complex demands of the research. The screenshots presented herein offer a glimpse into the application’s design and potential, though they remain a very poor substitute for direct interaction with the evolving platform.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#introduction-to-the-veritrace-project",
    "href": "ai-nepi_006_chapter.html#introduction-to-the-veritrace-project",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.1 Introduction to the VERITRACE Project",
    "text": "7.1 Introduction to the VERITRACE Project\nThe VERITRACE project, a five-year initiative funded by an ERC Starting Grant, embarks on an ambitious journey to trace the intellectual currents flowing from the ‘ancient wisdom’ tradition into the burgeoning natural philosophy and science of the early modern period. This tradition manifests in a diverse collection of works, including texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for historians of chemistry, the Corpus Hermeticum.\n\n\n\nVERITRACE Project Title Slide\n\n\nResearchers have assembled a core corpus of 140 works that typify this ancient wisdom tradition, forming the basis for close textual analysis. Whilst the historical record confirms the influence of these ideas—Newton, for instance, read the Sibylline Oracles, and Kepler was familiar with the Corpus Hermeticum—VERITRACE seeks to delve deeper. The project aims to uncover a much broader and more intricate network of texts and authors who engaged with this tradition. Many of these works, often penned by lesser-known figures, constitute what one scholar has termed ‘the great unread’, having seldom been the primary focus of historical inquiry due to their sheer volume and relative obscurity. Illuminating these connections is a central objective of the project.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#computational-approaches-in-historical-research",
    "href": "ai-nepi_006_chapter.html#computational-approaches-in-historical-research",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.2 Computational Approaches in Historical Research",
    "text": "7.2 Computational Approaches in Historical Research\nTo address its research questions, the VERITRACE project pioneers large-scale, multilingual exploration within the domain of History, Philosophy, and Sociology of Science (HPSS). The team develops tools for keyword searching across its vast corpus. Furthermore, a significant component involves identifying instances of textual reuse. This encompasses both direct, lexical quotations—which may or may not be explicitly cited—and more indirect forms of influence, such as paraphrasing or subtle allusions that contemporary readers would have recognised as originating from specific sources, like the Corpus Hermeticum.\n\n\n\nVERITRACE Project Team and Goals\n\n\nEffectively, the project endeavours to construct an ‘early modern plagiarism detector’, capable of navigating a substantial multilingual collection of texts. Through this computational lens, researchers anticipate uncovering previously ignored networks of texts, passages, themes, topics, and authors. Such discoveries hold the potential to reveal new patterns and shed fresh light on the intellectual history and philosophy of science during this transformative period.\n\n\n\nComputational HPSS Aims for the VERITRACE Project",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-corpus-a-large-diverse-multilingual-data-set",
    "href": "ai-nepi_006_chapter.html#the-corpus-a-large-diverse-multilingual-data-set",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.3 The Corpus: A Large, Diverse Multilingual Data Set",
    "text": "7.3 The Corpus: A Large, Diverse Multilingual Data Set\nThe foundation of VERITRACE’s analytical work rests upon a large, diverse, and multilingual data set, focusing exclusively on printed books rather than handwritten manuscripts. This corpus draws from three primary data sources and encompasses texts in at least six different languages, with publication dates spanning nearly two centuries, from 1540 to 1728—a period concluding shortly after Newton’s death.\nThe principal data sources include:\n\nEarly English Books Online (EEBO)\nGallica, providing access to digitised materials from the French National Library\nThe Bavarian State Library, which constitutes the largest single contributor to the corpus\n\nCollectively, these sources provide approximately 430,000 texts. To analyse this extensive collection, the project employs an array of state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, amongst others.\n\n\n\nLarge, Diverse Multilingual Data Set for VERITRACE",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "href": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.4 Core Challenges and the Role of Large Language Models",
    "text": "6.4 Core Challenges and the Role of Large Language Models\nSeveral core challenges are inherent in a project of this scale and complexity. Variable Optical Character Recognition (OCR) quality presents a significant hurdle. The textual data, supplied directly by libraries in raw formats such as XML, HOCR, or even HTML files, often lacks ground truth page images. This variability in OCR accuracy inevitably affects all downstream processing and analytical tasks. Managing early modern typography and semantics across at least six languages introduces further complexities. Furthermore, the sheer volume of data—hundreds of thousands of texts printed across Europe over nearly 200 years—demands robust computational strategies.\nLarge Language Models (LLMs) play a crucial role in addressing these challenges. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, acting as ‘judges’ in this process. Whilst this application holds considerable interest, the current focus shifts towards the encoder side. Here, BERT-based LLMs generate embeddings to encode the semantic meaning of sentences and short passages (groups of sentences) within the textual corpus. This encoding is fundamental to the project’s semantic matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#a-brief-look-at-llms-as-judges-for-metadata-enrichment",
    "href": "ai-nepi_006_chapter.html#a-brief-look-at-llms-as-judges-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.5 A Brief Look at LLMs as Judges for Metadata Enrichment",
    "text": "7.5 A Brief Look at LLMs as Judges for Metadata Enrichment\nOne application of LLMs within VERITRACE involves their use as ‘judges’ to enrich bibliographic metadata. The basic motivation stems from the desire to map records from high-quality sources like the Universal Short Title Catalogue (USTC) onto the project’s own records. Successful mapping creates enriched metadata that is less likely to require manual cleaning. Whilst some mapping can be automated using external identifiers, many records lack such links, and the project’s data often requires cleaning before effective matching can occur.\n\n\n\nCase Study: LLMs as Judges to Enrich VERITRACE Metadata\n\n\nTo tackle this, researchers employ a panel, or ‘bench’, of LLMs. These models operate under extensive prompt guidelines to compare pairs of bibliographic records—a task exceedingly tedious for human team members, who previously faced reviewing tens of thousands of such pairs. The LLMs determine whether records match and provide reasoning for their decisions.\n\n\n\nAttempt 1: Matching Gallica and USTC Records using Fuzzy Matching and LLM Review\n\n\nThis endeavour remains a work in progress. A major current challenge is the propensity of the open-source models (such as Llama and its variants) to produce hallucinations in their output. Attempts to mitigate this by requesting more structured output often lead to more generic and less helpful responses, particularly in the reasoning provided. Achieving a balance between structured output and insightful reasoning is an ongoing refinement. Despite these hurdles, the potential for LLMs to save considerable time in metadata enrichment remains significant, and further development continues.\n\n\n\nPrompt Guidelines and an Example of LLM Output for Metadata Matching",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-veritrace-web-application-an-initial-glimpse",
    "href": "ai-nepi_006_chapter.html#the-veritrace-web-application-an-initial-glimpse",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.6 The VERITRACE Web Application: An Initial Glimpse",
    "text": "7.6 The VERITRACE Web Application: An Initial Glimpse\nThe VERITRACE web application, a new development shared publicly for the first time here, represents the project’s primary interface for textual exploration and analysis. It is crucial to understand that this is an ‘alpha’ version, not yet available for public use and currently residing only on a local machine. It requires substantial further work and should be viewed as an indication of the project’s aspirations rather than a finished product.\n\n\n\nVERITRACE Web Application Overview and Current Status\n\n\nCurrently, the team is testing a BERT-based LLM, LaBSE (Language-agnostic BERT Sentence Embedding), to generate the vector embeddings that underpin semantic analysis within the application. However, early indications suggest that this particular model may not be sufficiently robust for the nuanced requirements of the historical texts in the VERITRACE corpus. The screenshots presented offer only a static snapshot; the true utility of such a tool can only be appreciated through interactive use.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-intricate-data-processing-pipeline",
    "href": "ai-nepi_006_chapter.html#the-intricate-data-processing-pipeline",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.7 The Intricate Data Processing Pipeline",
    "text": "7.7 The Intricate Data Processing Pipeline\nTransforming raw textual data from its original formats (XML, HOCR, HTML files) into a structured and searchable Elasticsearch database, which serves as the backend for the VERITRACE web application, involves a considerable amount of preparatory work. This is not a simple push-button operation; rather, it entails a complex, 15-stage data processing pipeline.\n\n\n\nVERITRACE Data Processing Pipeline Dashboard and Stages\n\n\nEach stage in this pipeline—from extracting text into files, generating character position mappings, segmenting documents, to assessing OCR quality—demands careful optimisation. Numerous background processes must execute flawlessly to prepare the data for scholarly analysis. The generation of vector embeddings, crucial for semantic matching, occurs towards the end of this intricate pipeline. Each step could itself be the subject of extensive refinement.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#navigating-the-features-of-the-veritrace-web-application",
    "href": "ai-nepi_006_chapter.html#navigating-the-features-of-the-veritrace-web-application",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.8 Navigating the Features of the VERITRACE Web Application",
    "text": "7.8 Navigating the Features of the VERITRACE Web Application\nThe VERITRACE web application organises its functionalities into roughly five main sections, designed to facilitate comprehensive exploration and analysis of the corpus.\n\n7.8.1 Explore Section: Corpus Insights and Metadata\nThe ‘Explore’ section provides users with statistical overviews of the corpus. For instance, current metadata records, drawn directly from a MongoDB database, number 427,395, describing the entirety of the collected books. This area allows users to gain a broad understanding of the corpus’s composition.\n\n\n\nVERITRACE Explore Section Interface with Corpus Statistics\n\n\nWithin this section, the ‘Metadata Explorer’ enables users to browse and filter documents based on their rich metadata attributes. One notable feature is the detailed language information. Language identification algorithms run on every text, analysing segments as small as approximately 50 characters. This granularity is vital because many early modern texts are multilingual, often containing sections in Greek or other languages alongside the primary Latin, for example. The system can identify such instances, noting, for example, that a text comprises 15% Greek and 85% Latin, classifying it as substantively multilingual.\nFurthermore, the application attempts to assess OCR quality on a page-by-page basis. This is a challenging task without access to ground truth page images, relying instead on analysis of the raw text. Nevertheless, providing some level of quality assessment for each page, rather than a single score for an entire book, is a key objective.\n\n\n7.8.2 Search Section: From Keywords to Complex Queries\nMost scholars will likely gravitate towards the ‘Search’ section for initial investigations. This feature supports standard keyword searches. A prototype, operating on just 132 files from the total corpus, already demonstrates the scale: a search for “Hermes” yields 22 documents with 332 total matches, and the index for this small subset alone occupies 15 gigabytes. Extrapolating to the full corpus of over 400,000 texts suggests the final index will reach terabyte scales.\n\n\n\nVERITRACE Search Section Interface with Example Queries\n\n\nBeyond basic keywords, the underlying Elasticsearch engine empowers much more sophisticated queries. Users can perform field-specific searches, such as locating all books by Kepler that also contain the keyword “Hermes”. The system also supports complex Boolean queries with ‘AND’ and ‘OR’ operators, nested queries, and proximity searches. The latter allows for nuanced investigations, for example, finding all texts where “Hermes” and “Plato” are mentioned within ten words of each other.\n\n\n7.8.3 Analyse Section: Future Capabilities\nThe ‘Analyse’ section of the website is currently under development but will house advanced analytical tools. Planned features include:\n\nTopic modelling to discover thematic structures within the corpus or selected document sets.\nLatent Semantic Analysis (LSA) for exploring document similarity.\nDiachronic analysis to visualise linguistic and conceptual shifts over time. Insights from the wider research community actively inform the development of these analytical capabilities.\n\n\n\n\nVERITRACE Analyse Section (Future Implementation)\n\n\n\n\n7.8.4 Read Section: Accessing Digital Facsimiles\nRecognising that scholars often need to consult original textual sources, the ‘Read’ section provides access to PDF facsimiles of every text in the corpus. An integrated Mirador viewer allows users to read these documents directly within the web application, much like interacting with digital collections on library websites. This feature ensures that close reading of the source material can occur alongside computational analysis, with relevant metadata readily available.\n\n\n\nVERITRACE Read Section with Integrated Mirador Viewer",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#unveiling-textual-connections-the-match-functionality",
    "href": "ai-nepi_006_chapter.html#unveiling-textual-connections-the-match-functionality",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.9 Unveiling Textual Connections: The Match Functionality",
    "text": "7.9 Unveiling Textual Connections: The Match Functionality\nThe ‘Match’ section of the VERITRACE web application is central to the project’s aim of identifying textual reuse and influence. Users can specify a query text and a comparison text (or set of texts). The system supports comparisons between single documents, multiple documents (e.g., all of Kepler’s works in the database), and even a single text against the entire corpus. The latter, whilst a powerful goal, presents considerable computational challenges regarding processing time for the user.\n\n\n\nVERITRACE Match Interface for Comparing Documents, Illustrated with Newton’s Opticks\n\n\nA key design principle is to expose underlying parameters to the user. Text matching is not a one-size-fits-all process; numerous parameters can and must be tweaked to achieve optimal results for different research questions. Users can, if they wish, adjust settings such as the minimum similarity score to observe how results change.\n\n7.9.1 Types and Modes of Matching\nThe application offers several approaches to textual comparison:\n\nLexical Matching: This method uses keyword matching to find passages with similar vocabulary. It is effective for identifying direct textual reuse within the same language but struggles with translations or paraphrases.\nSemantic Matching: Employing vector embeddings, this approach seeks conceptually similar passages, even if they share little common vocabulary. This is particularly vital for a multilingual corpus where ideas may be expressed differently across languages.\nHybrid Matching: This combines lexical and semantic techniques, allowing for weighted contributions from each to identify a broader range of connections.\n\nUsers can also select a ‘Matching Mode’:\n\nStandard: Offers a balance between performance and accuracy.\nComprehensive: Aims for maximum recall of potential matches, though this is computationally more intensive and slower.\nSelective (Faster): Prioritises higher precision, potentially yielding fewer results but executing more quickly.\n\n\n\n7.9.2 Sanity Checks: Testing the Matching Algorithms\nTo evaluate the matching capabilities, researchers performed several ‘sanity checks’ using Isaac Newton’s Opticks, specifically comparing the Latin edition of 1719 with the English edition of 1718.\n\nLexical Match Across Languages (Latin vs. English Opticks): When performing a lexical match between these two texts in different languages, the expectation is to find no significant matches. Using the ‘Standard’ mode, the system indeed found no matches, aligning with this expectation. Interestingly, switching to ‘Comprehensive’ mode did yield three matches, all in English. This suggests that the Latin edition likely contains some English text (perhaps in a preface or notes), which the more sensitive comprehensive mode correctly identified. This outcome serves as a useful validation of the different modes’ behaviours.\n\n\n\nSanity Check 1: Results of a Lexical Match Across Languages (Newton’s Opticks)\n\n\nLexical Match of a Text to Itself (English Opticks vs. English Opticks): When a text is lexically matched against itself, a high degree of similarity is expected. The interface displays a match summary, including a quality score, average similarity of matches, and a coverage score (indicating how much of the text is involved in matches). The system also provides automatic highlighting of the query passage on the left and the comparison passage on the right, along with the similarity score for each pair. In this self-comparison, the results demonstrated the expected high similarity.\n\n\n\nSanity Check 2: Summary Statistics for a Text Lexically Matched to Itself (Newton’s Opticks)\n\n\nSemantic Match Between a Text and Its Translation (Latin vs. English Opticks): For a semantic match between a text and its translation, one would anticipate finding numerous conceptually similar passages, even though their vocabularies differ. The results of this test appeared reasonable. Despite OCR imperfections, the system identified passages discussing similar concepts, such as ‘colours’, across the two language versions. This demonstrates the potential of semantic matching to bridge linguistic divides.\n\n\n\nSanity Check 3: Setting up a Semantic Match Between Newton’s Opticks (Latin) and its English Translation\n\n\nThe detailed comparison view allows users to explore these semantic connections side-by-side.\n\n\n\nSemantic Matches Displayed: Latin Optice Query Passage on Left, English Opticks on the Right\n\n\nHowever, analysis of the summary statistics for this semantic match reveals areas for further investigation. Whilst the quality score (average similarity of matches) appears reasonable and high, the coverage score might seem low. This could indicate limitations in the current embedding model (LaBSE) or, alternatively, reflect genuine substantive differences between the Latin and English editions, as the Latin edition is reportedly longer. Other queries using the LaBSE model have suggested it may be inadequate for the specific domain of historical texts, potentially suffering from ‘out-of-domain model collapse’.\n\n\n\nSummary Statistics for the Semantic Match Between Newton’s Opticks (Latin vs. English)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#issues-on-the-horizon-and-future-directions",
    "href": "ai-nepi_006_chapter.html#issues-on-the-horizon-and-future-directions",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.10 Issues on the Horizon and Future Directions",
    "text": "7.10 Issues on the Horizon and Future Directions\nAs the VERITRACE project progresses, several critical issues and future directions require careful consideration to achieve its ambitious goals.\n\n\n\nKey Issues and Future Considerations for the VERITRACE Project\n\n\nThe choice of an appropriate vector embedding model is paramount. LaBSE, selected initially for its speed and relatively modest storage requirements, appears insufficient for the nuanced demands of the historical corpus. Researchers are exploring alternatives such as XLM-Roberta, intfloat multilingual-e5-large, or various historical mBERT models. Each of these presents its own trade-offs between accuracy, storage footprint, and inference time. A fundamental question is whether to persist with pre-trained models or to invest in fine-tuning a base model specifically on the VERITRACE historical corpus, given its distinct characteristics.\nThe phenomenon of semantic meaning changing over time poses another significant challenge. How can a single model effectively capture and compare concepts expressed in texts published centuries apart (e.g., from 1540 versus 1700) and across different languages? Ensuring that texts from disparate periods and linguistic backgrounds are meaningfully represented within the same vector space is a complex problem.\nPoor-quality OCR continues to be a pervasive issue, affecting all downstream processes. If the raw text is flawed, accurate segmentation into sentences and passages becomes difficult, undermining the foundation for reliable analysis. Re-OCRing the entire corpus of 430,000 texts is not feasible. Potential strategies include selectively re-OCRing only the texts with very poor quality or investing time in finding existing higher-quality digital versions of these works.\nFinally, scaling and performance will increasingly become critical concerns. The current prototype, operating on only 132 texts, already sees query times of around 15 seconds for certain operations. Extrapolating these performance characteristics to the full corpus of 430,000 texts highlights the immense computational and optimisation challenges that lie ahead.\nThe VERITRACE team actively seeks advice and collaboration from the wider research community to address these multifaceted issues and to realise the full potential of computational methods in exploring the rich tapestry of early modern intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html",
    "href": "ai-nepi_007_chapter.html",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThe field of explainable Artificial Intelligence (AI) garners increasing attention, yet a universally accepted definition of what constitutes an ‘explanation’, particularly within the machine learning community, remains an evolving concept. This chapter delves into the nuances of interpretability for Large Language Models (LLMs), exploring methods that offer transparency, practical applications, and the potential for novel scientific insights, especially within the humanities. We begin by establishing a foundational understanding of explanations in machine learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "ai-nepi_007_chapter.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI (XAI) 1.0: Feature Attributions",
    "text": "7.1 Explainable AI (XAI) 1.0: Feature Attributions\nMachine learning historically concentrated on visual data, primarily images. Only in the last decade or so has the field turned more substantial attention towards language, although earlier work certainly exists. The significant shifts in language-focused AI are relatively recent. To comprehend the decision-making processes of ‘black box’ machine learning models, researchers typically examined classification tasks. Given an input, such as an image containing a specific object, a model might yield a correct prediction. However, users often remain unaware of the basis for this classification.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’.\n\n\nThe domain of explainable AI has dedicated considerable research, spanning roughly a decade, to understanding and tracing the origins of model predictions. In the context of image classification, this often involves generating a heatmap. Such a heatmap highlights the pixels most responsible for a given prediction. For example, it could visually demonstrate why a model recognised a rooster in an image.\n\n\n\nDiagram illustrating a black box AI system classifying an image of a rooster. Input x (rooster image) goes into a Black Box AI System, resulting in prediction f(x) (Rooster). Citation: Samek et al. (2017).\n\n\nBeyond simply understanding individual predictions, the broader imperative for explainability serves several critical functions. Firstly, it allows for the verification of predictions, ensuring that the model operates on a reasonable basis. Secondly, explainability aids in correcting errors and understanding how models make mistakes. Thirdly, it can reveal surprising solutions or patterns, thereby contributing to learning about the underlying problem itself. Increasingly, explainability is also vital for ensuring compliance with regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-shift-towards-generative-ai",
    "href": "ai-nepi_007_chapter.html#the-shift-towards-generative-ai",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.2 The Shift Towards Generative AI",
    "text": "7.2 The Shift Towards Generative AI\nThe landscape of AI has undergone a transformation with the advent of Generative AI (GenAI). Previously, the standard scenario involved classification models. Now, models possess multifaceted capabilities: they can classify, find similar images, generate new images, and answer questions on a vast array of topics. This versatility makes it considerably more challenging to ground an LLM’s prediction or answer in specific input features.\n\n\n\nDiagram contrasting classification models with generative AI. An input image of a rooster feeds into a Black Box AI System, which can now produce varied outputs like ‘rooster’, ‘similar images’, ‘generate examples’, ‘Q&A’. Text highlights ‘beyond heatmaps’, ‘feature interactions’, ‘mechanistic view’. Today’s foundation models are described as multi-task and world models. Citation: Samek et al. (2017).\n\n\nConsequently, there is a need to move beyond heatmap representations. Future interpretability methods must consider feature interactions and adopt more mechanistic perspectives. Today’s foundation models function not merely as task-specific tools but as models of the world, capable of reflecting societal nuances and the evolution of textual features over time. This capacity makes them particularly interesting for humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#why-scrutiny-of-models-remains-essential",
    "href": "ai-nepi_007_chapter.html#why-scrutiny-of-models-remains-essential",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.3 Why Scrutiny of Models Remains Essential",
    "text": "7.3 Why Scrutiny of Models Remains Essential\nAI models, despite their advancements, can and do make surprising errors. Two well-known examples illustrate this fallibility. In one instance, a standard object classifier identified a boat based on the surrounding water—a correlated, easier-to-detect textural feature—rather than the boat itself (Lapuschkin2019?). More recently, LLMs have demonstrated mistakes in multi-step planning tasks. For example, when tasked with the Tower of Hanoi puzzle and asked to predict the next move, an LLM might incorrectly attempt to move the largest, inaccessible disk, thereby failing to grasp the problem’s physical constraints (MondalWebb2024?).\n\n\n\nSlide titled ‘Models can make mistakes’. Left: Object detection example showing a sailboat and a heatmap indicating water as a key feature (Lapuschkin et al., Nat Commun ’19). Right: Multi-step planning example showing an LLM making an incorrect move in the Tower of Hanoi game (Mondal & Webb et al., arxiv ’24).\n\n\nAlthough more recent reasoning models may perform better, these examples, observed in relatively standard models, underscore the continued need for robust interpretability methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#advancing-interpretability-xai-2.0-and-structured-approaches",
    "href": "ai-nepi_007_chapter.html#advancing-interpretability-xai-2.0-and-structured-approaches",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.4 Advancing Interpretability: XAI 2.0 and Structured Approaches",
    "text": "7.4 Advancing Interpretability: XAI 2.0 and Structured Approaches\nTo address the limitations of earlier methods, particularly in the context of complex models like LLMs, the field is progressing towards XAI 2.0, emphasising structured interpretability. This involves moving beyond simple heatmaps to explore more sophisticated relationships within the data and the model.\n\n\n\nSlide titled ‘XAI 2.0’ with subtitle ‘Structured Interpretability’.\n\n\n\n7.4.1 First-Order Explanations\nFirst-order explanations, often visualised as heatmaps, remain useful for understanding classifiers. Researchers applied these to a table classifier designed to distinguish subgroups within historical data, specifically historical tables. To verify that the classifier operated meaningfully, heatmaps helped confirm that predictions were based on relevant features. Indeed, the model correctly focused on numerical content, a reasonable proxy for identifying a numerical table.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ with a diagram of four circles (x1 highlighted) and ‘classifier predictions’ with images of historical tables.\n\n\n\n\n7.4.2 Second-Order and Higher-Order Interactions\nBeyond individual features, pairwise relationships, or second-order features, offer deeper insights. When examining the similarity between embeddings of two items (e.g., images or tables), explaining the resulting similarity score benefits from an interaction-based representation. This approach can reveal, for instance, that the perceived similarity between two historical tables stems from interactions between specific corresponding digits, confirming the model’s intended function.\nFurther advancements explore higher-order interactions, particularly within graph structures. In contexts such as citation networks or networks of books, where entities are classified, subgraphs or sets of features (feature walks) often become relevant collectively. Identifying these complex interactions can lead to more sophisticated insights into model behaviour, approaching a circuit-level understanding.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’ showing three columns: ‘first-order’ (classifier predictions), ‘second-order’ (pairwise relationships), and ‘higher-order’ (graph structure), each with a diagram and example images.\n\n\nThese evolving interpretability methods provide a richer understanding of how models arrive at their conclusions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "href": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.5 Applications in Language and the Humanities",
    "text": "7.5 Applications in Language and the Humanities\nThe principles of structured interpretability find compelling applications when analysing language models and deriving insights within humanities research.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Three columns: ‘first-order’, ‘second-order’, ‘higher-order’ with corresponding diagrams and examples. This slide serves as a visual summary before diving into specific examples.\n\n\n\n7.5.1 First-Order Attributions in LLMs: Sentiment Prediction and Bias Detection\nA standard application for first-order attributions involves sentiment prediction, often using movie reviews—a common dataset in the language processing community. By training a model on such reviews and subsequently examining the basis of its predictions, researchers can gain valuable insights. Heatmaps, generated using methods tailored for transformers, can rank sentences and highlight influential tokens.\n\n\n\nSlide titled ‘First-Order Attributions in LLMs’. Dark blue background with white text.\n\n\nSuch analyses have revealed that certain features disproportionately affect sentiment scores. For example, male Western names (e.g., Lee, Barry, Raphael, or references to the Cohen brothers) tend to correlate with positive reviews. Conversely, names perceived as foreign (e.g., Saddam, Castro, Chan) are more likely to be associated with negative scores. These findings underscore the presence of biases within models, a well-recognised issue in the AI community. Explainable AI techniques prove highly effective in detecting these fine-grained biases.\n\n\n7.5.2 First-Order Attributions for Long-Range Dependencies in LLMs\nAnother area of investigation concerns long-range dependencies in LLMs. In a typical scenario, an LLM processes a long context window—perhaps up to 8,000 tokens from Wikipedia articles—and is then prompted to generate a summary. The model begins to produce free text, and the objective is to determine the origin of this generated information within the provided context. Specifically, researchers explore whether models can effectively utilise information from distant parts of the input.\nFindings indicate that models predominantly focus on the latter portions of the context, prioritising information presented more recently. While they can access and pull information from earlier in the context, doing so is significantly less probable (note that analyses often use a log scale for counts). This tendency is important to bear in mind when using LLMs for summarisation; the output may not be a balanced representation of the entire text but rather skewed towards content nearer to the prompt (Jafari2024?).\n\n\n\nSlide titled ‘Ex 1b: First-Order Attributions for Long-Range Dependencies in LLMs’. Setup: generating summaries for long inputs. Results: example text with highlighted tokens indicating source of generated summary. Citation: Jafari et al., MambaLRP (NeurIPS ’24).\n\n\n\n\n7.5.3 Second and Higher-Order Interactions in Text\nMoving to second and higher-order interactions, consider a standard scenario involving sentence embeddings. Given a pair of sentences, such as “A cat I really like” and “It is a great cat,” a model (e.g., BERT or Sentence-BERT) produces an embedding and a similarity score. However, the reasons behind this specific similarity value often remain opaque.\n\n\n\nSlide titled ‘Second & Higher-Order Interactions in Text’. Dark blue background with white text.\n\n\nSecond-order explanations can illuminate these reasons by providing interaction scores between tokens. These scores reveal why the model considered the sentences highly similar. Even in toy examples, common patterns emerge, such as noun-matching strategies (synonyms or identical noun tokens), noun-verb interactions, and connections involving separator tokens. The model’s strategy often resembles a ‘bag of token types’. This suggests that, despite their complexity, models are forced to compress vast amounts of information and, in doing so, may rely on relatively simplistic strategies. This observation, perhaps not immediately intuitive, is relevant for anyone embedding data and subsequently ranking items based on similarity.\n\n\n7.5.4 Graph Neural Networks for Structured Predictions in Language\nGraph Neural Networks (GNNs) offer another avenue for exploring structured information, yielding attributions in terms of ‘walks’ or feature interactions. Intriguingly, GNNs, which inherently encode structural information, can be framed as LLMs. This is because the attention mechanism within transformers essentially dictates how tokens can ‘message pass’ or influence one another. This conceptual link allows for the application of GNN-based interpretability methods to language.\n\n\n\nSlide titled ‘Graph Neural Networks for Structured Predictions’. Shows input graph, interaction, and prediction stages.\n\n\nFor example, standard first-order explanations (akin to a Bag-of-Words approach) may fail to capture the complexity of language, such as negation. A sentence like “First, I didn’t like the boring pictures” might receive a high positive score simply due to the presence of “like,” overlooking the negation. In contrast, more sophisticated higher-order explanation methods can correctly identify that the initial negative phrase contributes negatively, whilst the subsequent positive part of a sentence (“but it is certainly one of the best movies I have ever seen”) is appropriately recognised, respecting the hierarchical structure of the language. This demonstrates how higher-order interactions can lead to a more accurate understanding of complex linguistic structures. Natural language’s hierarchical nature is well-suited to graph representations, and training a GNN (or an LLM framed as such) on tasks like movie review sentiment analysis allows for the extraction of these insightful walks (Schnake2022?).\n\n\n\nSlide titled ‘Ex 3: Interaction of nodes learns complex language structure’. Setup: GNN/LLM on movie review sentiment. Example sentence with standard (BoW) vs. high-order interaction explanations. Citation: Schnake et al., Higher-Order Explanations of Graph Neural Networks via Relevant Walks. (TPAMI ’22).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#ai-based-scientific-insights-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.6 AI-based Scientific Insights in the Humanities",
    "text": "7.6 AI-based Scientific Insights in the Humanities\nThe application of AI, particularly explainable AI, extends into generating novel scientific insights within the humanities, moving beyond mere technical analysis to active knowledge discovery.\n\n\n\nSlide titled ‘B. AI-based Scientific Insights in the Humanities’.\n\n\n\n7.6.1 Extracting Visual Definitions from Corpora\nAn initial foray into this area involved using heatmap-based approaches to analyse a corpus of mathematical instruments. Researchers developed a classifier to categorise images as, for example, a “machine” or a “mathematical instrument.” In collaboration with historians, including Matteo Valleriani and Jochen Büttner, they scrutinised these visual definitions to determine if AI could offer more objective categorisation criteria (Valleriani2019?). This process requires close collaboration with domain experts to ensure the meaningfulness of AI-derived definitions. For instance, the analysis revealed that fine-grained scales on mathematical instruments were highly relevant features for the model’s decisions (ElHajjEberle2023?).\n\n\n\nSlide titled ‘Ex 4: Extracting visual definitions from corpora’. Left: Examples of historical mathematical instruments and machines. Right: Class-specific heatmap explanations showing relevant visual features for ‘math. instrument’, ‘machine’, etc. Citation: El-Hajj & Eberle+ (Int J Digit Humanities ’23).\n\n\n\n\n7.6.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA more extensive project focused on numerical tables from the Sphaera corpus, comprising early modern texts from 1472 to 1650 (Valleriani2019?; Eberle2024a?). Historians sought an automated method for matching tables with similar semantics, a task previously unfeasible at scale due to the sheer volume and complexity of the data.\n\n\n\nSlide titled ‘Ex 5: Corpus-level analysis of early modern astronomical tables’, showing examples of historical astronomical tables from the Sphaera Corpus and Sacrobosco Table Corpus.\n\n\nThis collaboration led to the development of a workflow designed to aid historians in gaining insights at scale. The concept of the “XAI-Historian” emerged—an historian equipped with AI and XAI tools to discover case studies and engage in more data-driven hypothesis generation. Instead of applying large foundation models directly to this out-of-domain data (which proved ineffective), researchers trained a smaller, specialised model capable of detecting numerical bigrams (pairs of adjacent numbers) within the tables.\n\n\n\nSlide titled ‘Ex 5: Historical insights at scale: xAI-Historian’. Setup: Historical tables as carriers of knowledge; Sacrobosco Corpus; ML challenges. Diagram shows workflow: data collections -&gt; atomization-recomposition (input table, bigram map, histograms) -&gt; corpus-level analysis (embeddings, data similarity). Citation: Eberle et al. (Sci Adv ’24).\n\n\nThe reliability of this bigram model was verified by checking if it consistently detected the same bigrams in corresponding inputs (e.g., “38” and “38” in two related tables). This verification step engendered trust in the model’s decisions, allowing its effective use for larger-scale analysis (Eberle2022?; Eberle2024b?).\n\n\n\nSlide titled ‘Verifying modeling and features using XAI and Historians’. Setup: Tables represented by bag of bigrams. Results: Scatter plot of table embeddings. Bigram Model: Images of tables with corresponding bigrams highlighted. Expert Ground Truth: Tables and charts showing histogram correlation and cluster classification purity. Citations: Eberle et al. (TPAMI ’22), Eberle et al. (Sci Adv ’24).\n\n\n\n\n7.6.3 Investigating Innovation Spread with Cluster Entropy\nArmed with these tools, researchers conducted case studies. One such study employed cluster entropy to investigate the spread of innovation across Europe during the early modern period. Publishing locations (cities) each produced a certain “programme” of printed table types. Some cities exhibited diverse outputs, whilst others focused on reprinting existing materials. Analysing this at scale was previously intractable.\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’, showing a map of Europe with Sphaera publication cities. Left side details setup and results with table clusters for Lisbon, Venice, Wittenberg. Citation: Eberle et al. (Sci Adv ’24).\n\n\nA clustering approach, based on the representations derived from the bigram model, quantified the diversity of each city’s print programme using entropy. A low entropy score indicated repetitive content, whereas a high entropy score signified a more diverse output. This analysis identified Frankfurt and Wittenberg as having particularly low entropy scores. Frankfurt am Main was already known as a centre for reprinting editions. The case of Wittenberg proved more intriguing: political control by Protestant reformers actively limited the print programme, dictating the curriculum to be published. This AI-driven analysis detected this historical anomaly, which aligned with existing historical knowledge and intuition (Eberle2024b?).\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Bar chart showing H(p) - H(pmax) for various cities, with Frankfurt/Main and Wittenberg highlighted. Explanations: Frankfurt (reprinting centre), Wittenberg (political control). Citation: Eberle et al. (Sci Adv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "ai-nepi_007_chapter.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.7 Conclusion: AI-based Methods for the Humanities",
    "text": "7.7 Conclusion: AI-based Methods for the Humanities\nHumanities and Digital Humanities (DH) researchers have largely concentrated on digitising source material. However, the automated analysis of these corpora is far from trivial, often hindered by data heterogeneity and a scarcity of labels. Multimodality presents further complexities.\n\n\n\nSlide titled ‘Conclusion - AI-based methods for the Humanities’ with bullet points summarizing challenges and opportunities.\n\n\nMachine learning, coupled with XAI, offers the potential to scale humanities research and cultivate novel research directions. Foundation Models and LLMs, through prompting, can assist with intermediate tasks such as labelling, data curation, and error correction. Nevertheless, for more complex research questions, their utility remains limited, particularly when dealing with low-resource data—a significant roadblock due to scaling laws. Furthermore, out-of-domain transfer, especially for historical and small-scale datasets, requires thorough evaluation. Current LLMs are primarily trained and aligned for natural language tasks and code generation, which may not directly translate to the nuanced requirements of many humanities research endeavours. Continued development and careful application of these AI tools, in close collaboration with domain experts, will be crucial for unlocking their full potential in the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html",
    "href": "ai-nepi_008_chapter.html",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThe application of Large Language Models (LLMs) to the History, Philosophy, and Sociology of Science (HPSS) presents intriguing possibilities. Nevertheless, current LLM technologies exhibit significant limitations that curtail their utility for rigorous scholarly inquiry. These models frequently require an adversarial mechanism to counteract hallucinations, and a fundamental misunderstanding persists: embedding vectors do not equate to the meanings of expressions. For LLMs to serve as genuine tools for knowledge discovery, they must transcend the mere formulation of plausible yet false statements and avoid the uncritical repetition of information disseminated across internet media. Instead, the pursuit of well-justified conclusions and the capacity to formulate plans for scientific investigation remain critical, yet largely unaddressed, challenges. Presently, no existing model adequately performs these tasks, nor does current technological development offer immediate prospects for achieving these essential goals.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#navigating-the-deficiencies-of-contemporary-language-models",
    "href": "ai-nepi_008_chapter.html#navigating-the-deficiencies-of-contemporary-language-models",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.1 Navigating the Deficiencies of Contemporary Language Models",
    "text": "9.1 Navigating the Deficiencies of Contemporary Language Models\nLarge Language Models, in their current state, exhibit several fundamental shortcomings when considered for sophisticated scholarly tasks. A primary concern involves their propensity for “hallucination”—generating plausible yet incorrect information. Effectively, an adversarial mechanism to counter such fabrications remains absent. Furthermore, the embedding vectors that underpin LLM operations do not equate to the meanings of expressions in a human, contextual sense. Consequently, there is a risk that these models might formulate statements that sound coherent but are, in fact, false.\n\n\n\nModelling Science: LLM for HPSS\n\n\nA significant issue arises from the tendency of LLMs to repeat information disseminated across internet media without critical assessment. Such repetition does not constitute knowledge, nor does it align with the objectives for LLM contributions in academic research. Instead of merely echoing prevalent narratives, a system for scholarly use should actively seek what is best justified. Moreover, current models possess no inherent capacity to devise plans for scientific inquiry or to strategise research. These limitations are not trivial; they represent core functionalities that today’s technologies show no immediate promise of achieving. This raises a crucial question: how can we cultivate hope for more epistemically sound AI?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#championing-validation-in-scientific-inquiry",
    "href": "ai-nepi_008_chapter.html#championing-validation-in-scientific-inquiry",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.2 Championing Validation in Scientific Inquiry",
    "text": "9.2 Championing Validation in Scientific Inquiry\nAddressing the identified gaps in LLM capabilities necessitates a focus on validation. Indeed, validation emerges as a critical, perhaps paramount, requirement. Validation, in this context, involves furnishing reasons and arguments that support or contest the truth of a proposition. It extends to providing evidence for or against a proposition’s veracity and offering justifications for or against pursuing particular actions.\n\n\n\nKey elements missing from current AI models for science\n\n\n\n9.2.1 Defining Computational Epistemology\nTo systematically address the challenge of validation, a new discipline becomes essential. “Computational Epistemology” is proposed as this new subject, dedicated to developing the methods and methodologies required to integrate robust validation processes into AI systems. This endeavour aims to cultivate epistemic agency within these systems. Such agency would empower them to identify propositions that transcend mere sentences, discern arguments within diverse texts and historical sources, and recognise the intentions, plans, and actions of individuals as documented in historical records.\n\n\n\nThe central role of validation and epistemic agency",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#implementing-validated-ai-assisted-research",
    "href": "ai-nepi_008_chapter.html#implementing-validated-ai-assisted-research",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.3 Implementing Validated AI-Assisted Research",
    "text": "9.3 Implementing Validated AI-Assisted Research\nThe practical application of these principles can transform how researchers conduct historical inquiries. Consider a working environment designed for such purposes, where AI assists in navigating complex historical questions.\n\n9.3.1 A Case Study: The Sanssouci Waterworks\nAn illustrative example involves the construction of the water fountain at Sanssouci castle, a project undertaken by Frederick the Great. A long-standing debate in the history of science concerns the involvement of Leonhard Euler, one of the 18th century’s foremost mathematicians. Specifically, questions persist regarding whether his participation contributed to the project’s ultimate failure—one of the most significant construction failures of that era—or if responsibility lay elsewhere. Historians of science continue to grapple with assigning accountability.\nWithin an AI-enhanced research environment, an investigator can pose a precise query, such as: “Reconstruct which persons carried out which work on the water fountain.” The objective is to receive a validated, qualified answer, one grounded in proven evidence and demonstrably correct, rather than mere hearsay. The system, in response, can generate a detailed list of individuals involved, their specific contributions, timelines, remuneration, and outcomes, potentially uncovering information about figures previously unknown to the researcher. This interface, exemplified by a tool named ‘Cursor’ employing an AI agent (perhaps named ‘Bernoulli’), facilitates such structured inquiry. However, a primary difficulty emerges: effective research demands more than analysing a single PDF document. It requires the capability to search across all available sources, a task for which simple token-based indexing proves insufficient.\n\n\n\nAI-assisted inquiry into historical sources regarding Sanssouci",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#erecting-a-scholarly-foundation-for-ai",
    "href": "ai-nepi_008_chapter.html#erecting-a-scholarly-foundation-for-ai",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.4 Erecting a Scholarly Foundation for AI",
    "text": "9.4 Erecting a Scholarly Foundation for AI\nTo overcome these challenges and enable meaningful AI-driven research, several core components are indispensable. These components form the bedrock upon which reliable, validated inquiries can be built.\n\n9.4.1 The Role of Curated Scholarly Editions\nFirstly, a scholarly curated editorial board, which has meticulously worked on primary sources, provides an essential foundation. For instance, the Opera Omnia of Leonhard Euler, comprising 86 volumes, represents approximately 120 years of dedicated scholarly effort by numerous academics. This comprehensive collection, completed only recently, includes all of Euler’s 866 publications and his entire correspondence. Such rigorously edited collections, supplemented by the work of other scholars, are fundamental.\n\n\n\nThe necessity of curated scholarly content as a data foundation\n\n\nThese curated collections serve as a crucial substitute for the often-unreliable information landscape from which LLMs typically draw. They form the basis of a structured database of content items. This database should encompass elements such as:\n\nA chronology of actions.\nA record of expressions communicated between individuals.\nA timeline of terminological and linguistic usage by specific persons.\nA history of the tools and material objects employed by historical figures.\n\nCrucially, all entries within this database must be validated by primary sources, creating a detailed inventory of historically proven activities for which a reliable record exists.\n\n\n\nExamples of curated scholarly works like Euler’s Opera Omnia\n\n\n\n\n9.4.2 Advancing Beyond Embeddings: The Scholarium Registry\nOnce such detailed and validated records are established, they can be interrogated using advanced multimodal AI models. Current findings suggest that models like Gemini 2.5, capable of integrating information from both text and images, are particularly well-suited to meet the complex requirements of these tasks.\nThis structured, curated content forms what can be termed a ‘Scholarium’—a registry of knowledge that moves beyond the limitations of opaque embedding vectors. This registry catalogues:\n\nPersonal actions, including communication acts (letters, publications, reports).\nStatements, encompassing implications, arguments, and inquiries.\nThe use of language, terminology, concepts, and relations.\nThe application of models, methods, tools, and devices.\nThe utilisation of data, information, evidence, and sources.\n\nAccess to this rich, structured information can be facilitated through Application Programming Interfaces (APIs), such as the Model Context Protocol (MCP) API, enabling sophisticated AI-driven analysis.\n\n\n\nThe Scholarium concept: a registry of curated scholarly information and the Model Context Protocol",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#fostering-an-open-ecosystem-for-knowledge",
    "href": "ai-nepi_008_chapter.html#fostering-an-open-ecosystem-for-knowledge",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.5 Fostering an Open Ecosystem for Knowledge",
    "text": "9.5 Fostering an Open Ecosystem for Knowledge\nThe development and maintenance of such a sophisticated research infrastructure depend on robust support systems and a commitment to open principles.\n\n9.5.1 Ensuring Long-Term Access with FAIR Repositories\nA critical component is a long-term, Findable, Accessible, Interoperable, and Reusable (FAIR) repository for storing and publishing the curated data. Zenodo, hosted by CERN in Geneva, offers a suitable platform, ensuring the persistence and accessibility of these valuable datasets for many years. This commitment to FAIR principles is vital for the scholarly community.\n\n\n\nZenodo as a FAIR infrastructure for long-term data preservation\n\n\n\n\n9.5.2 Collaborative Technical Support and Open Principles\nTechnical support for this ecosystem is also paramount. A startup, OpenScienceTechnology, provides the necessary expertise to run the infrastructure. This includes managing an MCP API server, which allows artificial intelligence models worldwide to access the curated data via a standardised API. This entire endeavour rests upon a foundation of open collaboration. The core principles guiding this initiative are:\n\nOpen Source\nOpen Access\nOpen Data\n\nIncluding the MCP API Server\n\nOpen Collaboration\n\nThese principles ensure transparency, accessibility, and community involvement, fostering a sustainable and evolving research environment.\n\n\n\nPrinciples of OpenScienceTechnology supporting the research infrastructure",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#towards-a-future-of-validated-scientific-discovery",
    "href": "ai-nepi_008_chapter.html#towards-a-future-of-validated-scientific-discovery",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "9.6 Towards a Future of Validated Scientific Discovery",
    "text": "9.6 Towards a Future of Validated Scientific Discovery\nThe journey towards integrating AI meaningfully into the history, philosophy, and sociology of science requires a paradigm shift. Rather than relying on the probabilistic outputs of general-purpose LLMs, the focus must turn to building systems grounded in validated, curated knowledge. By establishing robust scholarly registries, leveraging FAIR infrastructure, and fostering open collaboration, we can develop AI tools that act as genuine partners in discovery. This approach promises not only to enhance research efficiency but also to deepen our understanding of science itself, moving towards a future where AI contributes to generating reliable, justifiable, and insightful knowledge.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#overview",
    "href": "ai-nepi_001_chapter.html#overview",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Workshop title slide featuring a blueprint-style illustration, event details including dates, location (TU Berlin, Room H3005, + online), keynote speakers (Iryna Gurevych, Pierluigi Cassotti & Nina Tahmasebi), organisers (Gerd Graßhoff, Arno Simons, Adrian Wüthrich, Michael Zichert), and logos including nepi and ERC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#network-epistemology-in-practice-a-nexus-for-llm-application",
    "href": "ai-nepi_001_chapter.html#network-epistemology-in-practice-a-nexus-for-llm-application",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.2 Network Epistemology in Practice: A Nexus for LLM Application",
    "text": "2.2 Network Epistemology in Practice: A Nexus for LLM Application\nFunding for this workshop stems from the European Research Council (ERC) grant supporting the Network Epistemology in Practice (NEPI) project. A primary objective of NEPI involves studying the internal communication of the Atlas collaboration at CERN, the renowned particle physics laboratory. Through this investigation, we seek to better understand how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual methodological approach. On one hand, network analysis allows us to map and comprehend the communication structure within this vast collaboration. On the other hand, semantic tools are utilised to trace how ideas flow and evolve within these network structures. It is precisely here that the application of LLMs becomes crucial. This particular application—using LLMs to discern patterns of conceptual development and transmission in scientific discourse—represents an area of intense personal interest, though the workshop promises to unveil a multitude of other fascinating applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#overview",
    "href": "ai-nepi_003_chapter.html#overview",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "",
    "text": "Today’s Menu slide outlining the chapter’s structure: Primer on LLMs, Applications in HPSS, and Reflections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "href": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.2 Adapting and Specialising Language Models",
    "text": "3.2 Adapting and Specialising Language Models\nThe proliferation of scientific language models, as surveyed by Ho and colleagues (Ho2024?), underscores the diverse efforts to tailor these technologies for specific research domains.\n\n\n\nA landscape diagram of Scientific Large Language Models, showing a timeline and categorisation of various models by type (Encoders, Decoders, Enc-Dec) and indicating open-source versus closed-source status.\n\n\nAdapting these powerful pre-trained models to specific scientific language or tasks involves several strategies.\n\n3.2.1 Strategies for Domain Adaptation\nInitial pre-training, where a model first encounters language, demands substantial computational resources and data. During this phase, models learn language by predicting the next token (as in GPT models) or by predicting randomly masked words within a sentence (as in BERT models). For many research groups, undertaking full pre-training from scratch is infeasible.\nA more accessible approach involves continued pre-training. Researchers can take an existing pre-trained model, such as a general BERT model, and continue its training on a specialised corpus, for instance, a collection of physics texts. This allows the model to adapt its parameters to the nuances of that specific domain.\nAlternatively, one can use pre-trained models as feature extractors. By adding a few extra layers on top of a pre-trained model, researchers can train these new layers for specific downstream tasks, such as sentiment classification or named entity recognition.\nContrastive learning offers another key method, particularly for generating sentence or document embeddings. While word embeddings capture semantic relationships between words, many applications require representations for entire sentences or documents. SentenceBERT, for example, employs contrastive learning to fine-tune BERT-like models to produce meaningful sentence embeddings, placing semantically similar sentences close together in the embedding space. This technique is vital for tasks requiring semantic similarity assessment at a level beyond individual words.\n\n\n3.2.2 Retrieval Augmented Generation (RAG)\nRetrieval Augmented Generation (RAG) has emerged as a significant technique for adapting LLMs to specific domains or tasks, often without requiring extensive model retraining. RAG systems typically involve multiple models acting in concert.\n\n\n\nDiagram illustrating the Retrieval Augmented Generation (RAG) process, showing a ‘Retrieval’ phase (querying documents, pooling) and a ‘Generation’ phase (using retrieved context with a generative model).\n\n\nIn a RAG pipeline, a user query (e.g., “What are LLMs?”) is first encoded, often by a BERT-like model, into a sentence embedding. This embedding is then used to search a database of relevant documents, retrieving passages most similar to the query. These retrieved passages provide specific context. The pipeline then integrates this retrieved information into the prompt supplied to a generative model (like GPT). The generative model uses this augmented context to produce a more informed and domain-specific answer. Many contemporary applications, including some functionalities within ChatGPT that involve searching the internet, utilise RAG principles. Reasoning models and the increasingly discussed ‘agents’ are also typically not single LLMs but rather complex systems of LLMs combined with various other tools and data sources.\n\n\n3.2.3 Key Distinctions to Remember\nTo navigate the LLM landscape effectively, several core distinctions warrant reiteration. These include the fundamental differences between encoder, decoder, and encoder-decoder architectures. Grasping various fine-tuning strategies is also essential. Furthermore, understanding the distinction between word embeddings and sentence (or document) embeddings is crucial, as they serve different analytical purposes. Finally, appreciating the different levels of abstraction at which these models operate—from token processing to document-level understanding—helps in selecting and applying them appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "href": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.3 Applications of LLMs in HPSS Research",
    "text": "3.3 Applications of LLMs in HPSS Research\nA nascent but growing body of work explores the utility of LLMs as tools within History and Philosophy of Science and Science Studies (HPSS) research. Preliminary surveys reveal several emerging categories of application.\n\n\n\nSlide listing applications of LLMs in HPSS, categorised into: Dealing with data and sources, Knowledge structures, Knowledge dynamics, and Knowledge practices.\n\n\n\n3.3.1 Categorising HPSS Applications\nResearchers are employing LLMs for a variety of tasks:\n\nDealing with data and sources: This includes parsing and extracting structured information from texts, such as publication types, acknowledgements, or citations. Interacting with sources through summarisation or RAG-type ‘chatting with your documents’ also falls into this category.\nAnalysing knowledge structures: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds. They also aid in mapping complex relationships, such as those between disciplines, interdisciplinary fields, or science-policy discourses.\nInvestigating knowledge dynamics: Conceptual histories of terms (e.g., “theory” in Digital Humanities, or “virtual” and “Planck” in physics) can be traced using LLM-derived embeddings. Identifying novelty, such as breakthrough papers or emerging technologies, represents another application.\nExamining knowledge practices: LLMs can support argument reconstruction by identifying premises and conclusions or causal relationships. Citation context analysis, an established HPSS method, can be enhanced to determine the purpose or sentiment of citations. Discourse analysis, focusing on elements like hedge sentences, jargon, or boundary work, also benefits from these tools.\n\n\n\n3.3.2 Observed Trends and Recurring Concerns\nThe application of LLMs in HPSS exhibits several notable trends and prompts recurring concerns amongst researchers.\n\n\n\nSlide summarising trends and concerns in LLM use for HPSS: accelerating interest, varying customisation, repeating concerns (resources, opaqueness, data, benchmarks, model trade-offs), and a trend toward accessibility.\n\n\nAn accelerating interest in LLMs is evident, with studies appearing not only in information science journals like Scientometrics and JASIST but also increasingly in journals traditionally less focused on computational methods. This suggests that the semantic capabilities of LLMs are attracting qualitative researchers and philosophers.\nThe degree of customisation varies widely. Some researchers develop new architectures or undertake custom pre-training, whilst others fine-tune existing models or use off-the-shelf tools like ChatGPT.\nSeveral concerns consistently surface. The substantial computational resources required for training and, in some cases, running large models pose a significant barrier. The ‘opaqueness’ or lack of interpretability of some models remains a challenge. A scarcity of suitable training data and domain-specific benchmarks for HPSS tasks is frequently noted. Researchers also grapple with trade-offs between different model types (e.g., BERT-like versus GPT-like). The potential for generative models to ‘hallucinate’ or produce plausible but incorrect information is another significant concern, although this issue is gradually improving with newer models and techniques like RAG.\nDespite these challenges, a trend towards greater accessibility is apparent. Tools like BERTopic, which simplifies topic modelling, are gaining popularity due to their ease of use and robust maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "href": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.4 Critical Reflections and Future Pathways",
    "text": "3.4 Critical Reflections and Future Pathways\nEngaging with LLMs in HPSS necessitates careful consideration of specific disciplinary challenges, a commitment to building LLM literacy, and adherence to core HPSS methodologies.\n\n\n\nSlide outlining key reflections: Acknowledging HPSS-specific challenges, Building LLM literacy, and Staying true to HPSS methodologies.\n\n\n\n3.4.1 Acknowledging HPSS-Specific Challenges\nSeveral challenges are particular to HPSS contexts. The historical evolution of concepts and language is crucial; LLMs are typically trained on modern language, which may lead to biases or misinterpretations when applied to historical texts. HPSS often adopts a reconstructive, critical, and reflective perspective, seeking to read between the lines and understand texts within their situated socio-historical contexts, including subtle discursive strategies. Current LLMs are not inherently trained for this type of nuanced reading. Furthermore, HPSS research frequently contends with sparse data, multiple languages, archaic scripts, and incompletely digitised archives.\n\n\n3.4.2 The Imperative of LLM Literacy\nTo address these challenges effectively, HPSS researchers must cultivate LLM literacy. This involves familiarising themselves with the underlying principles of LLMs, NLP, and Deep Learning—encompassing both the tools and their theoretical underpinnings. It requires learning to identify the most appropriate model architectures and training regimes for specific HPSS research questions and data. The terminology itself is in flux; the term “LLM” may become less adequate as models become increasingly multimodal, incorporating images, sound, and other data types. The definition of “large” in “Large Language Model” also shifts rapidly with technological advancements. Developing shared datasets and benchmarks tailored to HPSS problems is another vital aspect of building collective literacy and capability. For tasks involving multilinguality, understanding which models are suitable or whether custom training is feasible given available resources becomes paramount.\n\n\n3.4.3 Upholding HPSS Methodological Integrity\nWhilst embracing new tools, it is essential to remain true to established HPSS methodologies. HPSS research problems must be thoughtfully translated into NLP tasks (e.g., classification, generation, summarisation) without allowing the technical task to overshadow or distort the original research question. Simultaneously, LLMs offer new opportunities for bridging qualitative and quantitative approaches, potentially fostering richer, mixed-methods research designs.\nLLMs may offer novel ways to address core HPSS questions. For instance, contextualised word embeddings can track the evolving meanings of concepts like “Planck” across different contexts (Max Planck the person, Planck institutes, the Planck satellite, Planck length), revealing shifts in scientific discourse over time. There is potential, though requiring careful exploration, to use LLMs to investigate complex phenomena such as paradigm shifts and incommensurability.\nFinally, HPSS can reflect on its own pre-history concerning some concepts now central to LLMs. For example, co-word analysis, developed in the 1980s by science studies scholars like Michel Callon and Arie Rip, shares intellectual roots with current embedding-based approaches to mapping knowledge landscapes.\n\n\n3.4.4 The Evolving Landscape: Agents and Beyond\nThe field of language modelling is developing at a rapid pace. The rise of ‘agents’—systems where LLMs interact with other tools and data sources to perform complex tasks autonomously—signals a further evolution. Interestingly, some of the language used by computer scientists to describe these emerging agentic systems echoes concepts from Actor-Network Theory (ANT) and other STS frameworks, suggesting that HPSS theories may offer valuable conceptual tools for understanding and critically engaging with these technological advancements. The journey requires continuous learning, critical assessment, and a commitment to harnessing these powerful models responsibly in the pursuit of insightful HPSS research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "href": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence",
    "text": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence\nThe VERITRACE project, a five-year ERC Starting Grant initiative, embarks on an ambitious journey to trace the intellectual currents flowing from the early modern ‘ancient wisdom’ tradition into the burgeoning field of natural philosophy and science of that era. This tradition manifests in a diverse collection of works, including notable texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and perhaps most famously for historians of chemistry, the Corpus Hermeticum. These 140 core texts form a ‘close reading corpus’, providing a focused lens on this influential body of thought.\n\n\n\nPresentation Title Slide illustrating the VERITRACE project’s scope and context.\n\n\nHistorical records confirm the impact of these ancient wisdom texts; for instance, Newton engaged with the Sibylline Oracles, and Kepler possessed familiarity with the Corpus Hermeticum. Nevertheless, the project seeks to delve deeper, aiming to uncover a far broader network of texts and intellectual connections that interacted with this tradition. Many of these works, often penned by lesser-known authors, constitute what one scholar has termed ‘the great unread’, frequently overlooked by historians due to their sheer volume and obscurity. Consequently, VERITRACE focuses on bringing these neglected sources to light.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)",
    "text": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)\nTo address its core research questions, the VERITRACE project pioneers large-scale, multilingual exploration within the domain of computational History, Philosophy, and Sociology of Science (HPSS). The team develops tools not only for conventional keyword searching but also for the sophisticated identification of textual reuse. This encompasses both direct, lexical quotation—instances where authors use verbatim material from other works, perhaps without explicit citation—and more subtle, indirect influences. Such indirect reuse might involve paraphrase or allusions that, whilst not direct copies, would have been recognisable to contemporary readers as originating from sources like the Corpus Hermeticum.\n\n\n\nThe VERITRACE project team and its mission statement.\n\n\nEffectively, the project endeavours to construct an ‘early modern plagiarism detector’ capable of navigating a vast, multilingual corpus. Beyond identifying direct and indirect textual linkages, a primary objective is to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this comprehensive analytical approach, researchers anticipate the emergence of new patterns and insights into the intellectual history and philosophy of science.\n\n\n\nKey objectives of the VERITRACE project in computational HPSS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "href": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.3 Navigating a Vast Multilingual Corpus",
    "text": "6.3 Navigating a Vast Multilingual Corpus\nThe foundation of this investigation rests upon a large, diverse, and multilingual dataset, focusing exclusively on printed books and texts, thereby excluding handwritten materials from its current scope. This corpus draws from three primary data sources and encompasses works in at least six different languages, published over approximately two centuries. The chronological parameters span from 1540, chosen for specific historical reasons, to 1728, shortly after Newton’s death.\nKey data repositories include:\n\nEarly English Books Online (EEBO)\nGallica, the digital library of the French National Library\nThe Bavarian State Library, which constitutes the largest single source\n\nCollectively, these sources contribute to a corpus of roughly 430,000 books. State-of-the-art digital techniques are employed to analyse this extensive collection of early modern texts.\n\n\n\nOverview of the large, diverse, and multilingual dataset used by VERITRACE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "href": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.5 Employing LLMs as Judges for Metadata Enrichment",
    "text": "6.5 Employing LLMs as Judges for Metadata Enrichment\nOne specific, albeit challenging, application of LLMs within VERITRACE involves their use as ‘judges’ to enrich metadata. The basic motivation stems from the desire to map records from high-quality external sources, such as the Universal Short Title Catalogue (USTC), onto the project’s own records. Successful mapping creates enriched metadata, less likely to require extensive manual cleaning.\n\n\n\nCase study overview of using LLMs as judges to enrich VERITRACE metadata.\n\n\nWhilst some mapping can be automated using external identifiers, many records lack such straightforward connections. Compounding this, much of the project’s internal data has not yet undergone cleaning, making matching a non-trivial task. The manual comparison of bibliographic metadata—assessing pairs of records to determine if they represent the same underlying printed text—is exceedingly tedious. Team members faced the prospect of reviewing tens of thousands of such pairs, highlighting the need for an automated solution.\n\n6.5.1 Initial Attempts and Emerging Hurdles\nTo address this, researchers are exploring a panel, or ‘bench’, of LLMs. Extensive prompt guidelines direct these models to evaluate potential matches, which are initially generated via fuzzy matching algorithms. The LLMs provide yes/no decisions along with reasoning for why a pair of records may or may not represent the same underlying text.\n\n\n\nExample of metadata comparison for LLM judging.\n\n\nThis endeavour remains a work in progress. A major current challenge is the prevalence of hallucinations in the output from the open-source models (e.g., Llama-based) currently under evaluation. Attempts to mitigate this by requesting more structured output, paradoxically, often lead to more generic and less helpful responses, particularly in the reasoning provided by the models. Achieving the right balance in prompting to elicit accurate and insightful judgments is an ongoing refinement process. Despite these initial difficulties, the potential for LLMs to save considerable time in metadata enrichment remains significant, and further investigation is warranted.\n\n\n\nExample of prompt guidelines and LLM output for metadata matching.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "href": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.6 Introducing the VERITRACE Web Application",
    "text": "6.6 Introducing the VERITRACE Web Application\nThe VERITRACE web application serves as the primary interface for exploring the project’s data and analytical tools. This platform is exceptionally new; indeed, its introduction here marks its first public discussion, preceding even internal team dissemination. As an ‘alpha’ version, it is not yet publicly available and remains under active development on a local machine, with screenshots offering a preliminary view. It functions more as a demonstration of the project’s aspirations than a finalised product.\n\n\n\nOverview of the VERITRACE Web Application’s alpha version.\n\n\nCurrently, testing involves a BERT-based LLM (LaBSE) to generate vector embeddings for every passage in the corpus. However, early indications suggest this model may not possess the requisite sophistication for the project’s ultimate goals, particularly for nuanced semantic matching. The application’s development continues, with these initial explorations informing future refinements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "href": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.7 The Data Processing Backbone",
    "text": "6.7 The Data Processing Backbone\nTransforming raw textual data from library sources into a queryable format within an Elasticsearch database—the backend of the web application—involves an intricate data processing pipeline. This multi-stage process is far from a simple button-push operation. Numerous steps are essential to prepare the data for analysis.\n\n\n\nDiagram of the VERITRACE data processing pipeline dashboard and stages.\n\n\nThese steps include:\n\nExtracting text into manageable files.\nGenerating mappings of all character positions.\nSegmenting texts into meaningful units.\nAssessing OCR quality.\n\nEach of these fifteen stages requires careful optimisation. The generation of embeddings, crucial for semantic analysis, occurs near the end of this complex pipeline. Significant background work underpins the functionality accessible through the web interface.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "href": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata",
    "text": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata\nThe VERITRACE web application offers several modules for interacting with the corpus. The ‘Explore’ section, for instance, provides users with comprehensive statistics about the dataset, drawn directly from a MongoDB database. At present, this encompasses 427,305 metadata records describing the books within the collection. This area allows researchers to gain an overview of the corpus’s composition, including language distributions, data sources, publication decades, and prominent publication places.\n\n\n\nThe VERITRACE ‘Explore’ interface showing corpus statistics.\n\n\nBeyond aggregate statistics, a ‘Metadata Explorer’ enables users to browse and inspect the rich metadata associated with each text. A key feature here is detailed language information. Language identification algorithms operate on every text, down to segments of approximately 50 characters. This granularity is vital because many early modern texts are multilingual, often containing sections in Greek or other languages alongside the primary Latin, for example. The system identifies these languages and their proportions within each document—such as a text being 15% Greek and 85% Latin—classifying them as ‘substantively multilingual’.\nFurthermore, the system attempts to assess OCR quality on a page-by-page basis. This is a challenging task without access to ground truth page images, relying instead on analysis of the raw text. Nevertheless, providing page-level quality assessments, rather than a single score for an entire book, offers more nuanced information for researchers. The efficacy of this OCR assessment method continues to be evaluated.\n\n\n\nThe VERITRACE ‘Metadata Explorer’ interface displaying detailed record information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "href": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry",
    "text": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry\nFor many scholars, the ‘Search’ function will likely be the initial point of engagement. The web application supports standard keyword searches across the corpus. Even with a prototype dataset of only 132 files (rather than the full 430,000), the Elasticsearch index already occupies 15 gigabytes, hinting at the terabytes of data the full system will manage. A simple search for “Hermes” in this prototype, for example, might yield 22 documents with 332 total matches.\n\n\n\nThe VERITRACE ‘Search’ interface showing basic and fielded query examples.\n\n\nLeveraging the power of Elasticsearch, users can execute far more complex queries. Fielded queries allow searching within specific metadata, such as finding all books by Kepler that also contain the keyword “Hermes”. Advanced capabilities include Boolean operators (AND, OR), nested queries, and proximity searches—for instance, locating texts where “Hermes” and “Plato” appear within ten words of each other.\nAn ‘Analyse’ section is planned, though not yet implemented. This module will incorporate tools for:\n\nTopic modelling\nLatent Semantic Analysis (LSA)\nDiachronic analysis, to explore linguistic and conceptual shifts over time.\n\nInsights from the wider research community inform the development of these analytical features.\n\n\n\nThe VERITRACE ‘Analyse’ interface showing planned analysis tools.\n\n\nRecognising the importance of accessing original source materials, a ‘Read’ section integrates a Mirador viewer. This allows scholars to view PDF facsimiles of every text in the corpus, alongside its metadata, mirroring the experience of browsing a physical library’s digital collection.\n\n\n\nThe VERITRACE ‘Read’ interface with an integrated Mirador viewer for text facsimiles.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "href": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.10 Unveiling Textual Reuse: The Match Functionality",
    "text": "6.10 Unveiling Textual Reuse: The Match Functionality\nA cornerstone of the VERITRACE web application is its ‘Match’ section, designed to identify textual reuse between different works. This tool allows users to specify query texts and comparison texts. Comparisons can be performed between single documents, across multiple selected documents (e.g., comparing Newton’s Latin Opticks to all of Kepler’s works in the database), or, ambitiously, between one text and the entire corpus. The latter presents considerable computational challenges regarding processing time and user experience, but remains a developmental goal.\n\n\n\nThe VERITRACE ‘Match’ interface for configuring textual similarity comparisons.\n\n\n\n6.10.1 Lexical and Semantic Matching Approaches\nThe system offers two fundamental types of matching:\n\nLexical matching: This approach uses keyword-based techniques to find passages with similar vocabulary. It is effective for identifying direct textual parallels but is language-dependent.\nSemantic matching: Employing vector embeddings, this method seeks conceptually similar passages, even if they share little or no common vocabulary. This is crucial for a multilingual corpus where translations or paraphrases might obscure lexical links.\n\nHybrid approaches, combining lexical and semantic methods with adjustable weighting, are also available.\n\n\n6.10.2 Customisable Parameters for Nuanced Analysis\nRecognising that text matching is not a one-size-fits-all process, the interface exposes numerous parameters for users to tweak. Whilst default settings provide a balanced starting point, users can adjust elements such as minimum similarity scores to refine search results according to their specific research needs. Different matching modes—‘Standard’, ‘Comprehensive’ (for maximum recall, albeit slower), and ‘Faster’ (for higher precision with potentially fewer results)—offer further control over the comparison process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "href": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.11 Validating the Approach: Sanity Checks and Case Studies",
    "text": "6.11 Validating the Approach: Sanity Checks and Case Studies\nTo evaluate the efficacy of the matching tools, researchers conduct several ‘sanity checks’ using known textual relationships. One such check involves comparing Newton’s Latin version of his Opticks (1719 edition) with the English edition from 1718. These texts, being translations of each other, provide a useful test case.\n\n6.11.1 Sanity Check 1: Lexical Matching Across Languages\nWhen a lexical match is performed between the Latin and English editions of Opticks, the expectation is that no significant matches will be found, given their different languages. Using the ‘Standard’ matching mode, this holds true—no matches are reported. Interestingly, the ‘Comprehensive’ mode does identify three matches, revealing small sections of English text, likely from prefatory material, within the predominantly Latin edition. This demonstrates the sensitivity of different modes and confirms the general principle that lexical matching is language-specific.\n\n\n\nResults of a lexical match attempt between Latin and English versions of Newton’s Opticks, showing no significant matches.\n\n\n\n\n6.11.2 Sanity Check 2: Lexical Self-Matching\nAs another baseline, lexically matching a text against itself should, ideally, yield near-perfect results. When Newton’s English Opticks is compared to itself, the system reports a high degree of similarity, with extensive coverage and quality scores. The interface provides detailed statistics, including the number of passages compared and the distribution of similarity scores, offering transparency into the matching process. Automatic highlighting displays the query passage on the left and the comparison passage on the right, along with their similarity score.\n\n\n\nResults of lexically matching Newton’s Opticks to itself, showing high similarity.\n\n\n\n\n6.11.3 Sanity Check 3: Semantic Matching of Translations\nThe real test for the LLM-powered tools comes with semantic matching across languages. When comparing the Latin and English Opticks using semantic matching, the system should identify conceptual similarities despite the linguistic differences.\n\n\n\nConfiguration for a semantic match between Newton’s Latin Optice and its English translation.\n\n\nInitial results from such semantic comparisons appear reasonable. Passages discussing similar concepts, such as colours, are identified as matches, even with underlying OCR imperfections. This suggests that the vector embeddings are capturing some level of conceptual correspondence across translations.\n\n\n\nExamples of semantic matches found between Latin and English versions of Newton’s Opticks.\n\n\n\n\n6.11.4 Preliminary Findings and Model Adequacy\nHowever, the semantic matching performance is not without its issues. Whilst the quality score for identified matches can be high, indicating strong similarity for the pairs found, the coverage score—representing how much of the documents are involved in matches—can sometimes be lower than expected. This discrepancy might, in part, reflect genuine differences between editions; for instance, the Latin edition of Opticks is considerably longer than the English one.\n\n\n\nSummary statistics for the semantic match between Latin and English Opticks, highlighting areas for investigation.\n\n\nNevertheless, further queries using the current LaBSE embedding model suggest it may not be entirely adequate for the nuanced demands of historical textual analysis. The potential for ‘out-of-domain model collapse’—where a model trained on general modern text performs poorly on specialised historical corpora—is a concern.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "href": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.12 Future Directions and Outstanding Challenges",
    "text": "6.12 Future Directions and Outstanding Challenges\nAs the VERITRACE project progresses, several critical issues and areas for development lie on the horizon. The choice of vector embedding model is paramount. LaBSE, selected partly for its efficiency in terms of storage and processing speed, may prove insufficient. Alternative models, such as XLM-Roberta, intfloat multilingual-e5-large, or specialised historical mBERT variants, present other trade-offs between accuracy, storage requirements, and inference time. A fundamental question is whether to persist with pre-trained models or to invest in fine-tuning a base model specifically on the VERITRACE historical corpus.\n\n\n\nA summary of key issues and future challenges for the VERITRACE project.\n\n\nFurther challenges include:\n\nSemantic drift: The meaning of words and concepts changes over time. How effectively current LLMs handle such diachronic semantic shifts across centuries and languages within the same vector space remains an open question.\nOCR quality: Poor OCR accuracy profoundly impacts downstream tasks, from basic sentence segmentation to complex semantic analysis. Re-OCRing the entire corpus is not feasible. Strategies might involve selectively re-OCRing the poorest quality texts or investing effort in locating existing higher-quality digital versions.\nScaling and performance: The current prototype, operating on only 132 texts, already shows query times of around 15 seconds for complex operations. Scaling these capabilities to the full corpus of 430,000 texts will undoubtedly present significant performance engineering challenges.\n\nAddressing these multifaceted issues will be crucial for realising the full potential of VERITRACE to illuminate the complex intellectual heritage of early modern science. Continued research, methodological refinement, and community engagement will guide these future endeavours.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#overview",
    "href": "ai-nepi_007_chapter.html#overview",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "",
    "text": "Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities. Speaker: Oliver Eberle, Senior Researcher, Berlin Institute for Learning and Data (BIFOLD), TU Berlin. Logos of BIFOLD and TU Berlin.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#understanding-explainable-ai-xai",
    "href": "ai-nepi_007_chapter.html#understanding-explainable-ai-xai",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.1 Understanding Explainable AI (XAI)",
    "text": "7.1 Understanding Explainable AI (XAI)\nHistorically, a significant portion of machine learning development centred on visual data, primarily images. Only in the last decade or so has the field intensified its focus on language, although foundational work in this area extends further into the past. The major shifts in language-focused AI, however, are relatively recent phenomena.\n\n7.1.1 XAI 1.0: Feature Attributions\nTo comprehend the internal workings of ‘black box’ machine learning models, researchers initially concentrated on classification tasks. Typically, an input, such as an image containing a specific object, would be fed into a model, which would then, ideally, produce a correct prediction. Nevertheless, the user often remained unaware of the basis for this classification.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’. Dark blue background with white text.\n\n\nConsequently, the domain of explainable AI dedicated approximately a decade of research to understanding and tracing the origins of these predictions. A common output from such investigations was a heatmap, visually indicating which pixels were most influential in the model’s decision-making process. For instance, a heatmap might clearly show why a model recognised a rooster in an image.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’. Dark blue background with white text.\n\n\n\n\n7.1.2 The Rationale for Explainability\nThe pursuit of explainability addresses several critical needs. Primarily, it serves to verify predictions, ensuring that a model operates on a reasonable basis. Furthermore, explainability aids in correcting errors and understanding how models make mistakes. It can also illuminate the learning process itself, as models occasionally discover surprising or unconventional solutions to problems. Increasingly, explainability is vital for ensuring compliance with regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-advent-of-generative-ai-expanding-capabilities-and-challenges",
    "href": "ai-nepi_007_chapter.html#the-advent-of-generative-ai-expanding-capabilities-and-challenges",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.2 The Advent of Generative AI: Expanding Capabilities and Challenges",
    "text": "7.2 The Advent of Generative AI: Expanding Capabilities and Challenges\nThe landscape of AI, once dominated by classification models, has undergone a significant transformation with the rise of Generative AI (GenAI) over approximately the last five years. This paradigm shift marks a departure from models designed for specific tasks towards those with multifaceted capabilities.\n\n\n\nDiagram illustrating a black box AI system: input image of a rooster leads to ‘Black Box AI System’, which outputs ‘Rooster’. Citation: Samek et al. (2017).\n\n\nUnlike their predecessors, contemporary GenAI models can classify, identify similar images, generate entirely new images, and respond to queries on a vast array of topics. This versatility, however, introduces considerable complexity in grounding a model’s prediction or an LLM’s answer to specific input features. The following discussion explores avenues beyond simple heatmap representations, considering feature interactions and more mechanistic perspectives to understand these advanced systems. Today’s foundation models function not only as multi-task systems but also as models of the world, capturing insights about society and the evolution of textual features over time, which underpins much of the current interest in them.\n\n\n\nDiagram contrasting classification models with generative AI. Generative AI can produce various outputs like ‘rooster’, ‘similar images’, ‘generate examples’, ‘Q&A’. Mentions ‘beyond heatmaps’, ‘feature interactions’, ‘mechanistic view’. Citation: Samek et al. (2017).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-imperative-of-recognising-model-errors",
    "href": "ai-nepi_007_chapter.html#the-imperative-of-recognising-model-errors",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.3 The Imperative of Recognising Model Errors",
    "text": "7.3 The Imperative of Recognising Model Errors\nIt is crucial to acknowledge that even advanced models can, and do, make surprising mistakes. Two well-documented examples illustrate this point. In one instance, a standard object classifier incorrectly based its identification of a boat on the surrounding water rather than the boat itself. The water, being a correlated feature and texturally simpler to detect, became the misleading focal point for the model (Lapuschkin2019?).\nAnother, more recent, example involves multi-step planning errors in LLMs. When tasked with the Tower of Hanoi puzzle—moving disks from a starting peg to a destination peg according to specific rules—an LLM might incorrectly attempt to move the largest, inaccessible disk directly to the target. This demonstrates a failure to comprehend the fundamental physical constraints of the problem (MondalWebb2024?). While more recent reasoning models may exhibit improved performance, such errors have been observed in fairly standard models like Llama 3.something.\n\n\n\nSlide titled ‘Models can make mistakes’. Left: Object detection example (boat identified by water, Lapuschkin et al., Nat Commun ’19). Right: Multi-step planning example (Tower of Hanoi error, Mondal & Webb et al., arxiv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#xai-2.0-towards-structured-interpretability",
    "href": "ai-nepi_007_chapter.html#xai-2.0-towards-structured-interpretability",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.4 XAI 2.0: Towards Structured Interpretability",
    "text": "7.4 XAI 2.0: Towards Structured Interpretability\nTo move beyond the limitations of heatmap-based explanations, the concept of structured interpretability offers a more nuanced approach to understanding model behaviour. This progression is sometimes referred to as XAI 2.0.\n\n\n\nSlide titled ‘XAI 2.0’ with subtitle ‘Structured Interpretability’. Dark blue background with white text.\n\n\n\n7.4.1 First-Order Explanations: Identifying Key Features\nFirst-order explanations, as previously touched upon, are particularly useful for elucidating the decisions of classifiers. They allow for the generation of heatmaps that highlight influential features. For instance, in a project involving a table classifier for historical data, the goal was to distinguish subgroups within these tables. To ensure the classifier operated meaningfully, heatmaps verified that its predictions were based on relevant features. Indeed, these visualisations confirmed that the model correctly focused on numerical content to identify numerical tables—a sensible proxy.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ diagram with x₁ highlighted, and ‘classifier predictions’ with images of historical tables.\n\n\n\n\n7.4.2 Second-Order Explanations: Uncovering Pairwise Relationships\nInvestigations then extended to second-order features, where pairwise relationships between features became significant. This was particularly evident when examining similarity. For example, when calculating a similarity score (e.g., a dot product) between the embeddings of two images or, in this context, two tables, explaining this prediction reveals the importance of feature interactions. Such explanations can highlight interactions between specific digits, confirming, for instance, that two tables are identical and that the model functions as intended.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ diagram and ‘classifier predictions’. Right side is mostly empty. This slide focuses on the first-order aspect before evolving.\n\n\n\n\n7.4.3 Higher-Order Interactions: Revealing Complex Structures\nMore recent work delves into graph structures, where higher-order interactions prove more meaningful. Consider a citation network, or a network of books or other entities, upon which a classification task is trained. In such scenarios, subgraphs or feature walks—sets of features that become relevant collectively—can be identified. This approach yields more complex insights into model behaviour and moves towards a circuit-level understanding of their operations. These explorations represent ongoing efforts in the field of interpretability.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Three columns: ‘first-order’ (x₁ highlighted), ‘second-order’ (x₁-x₄ interaction), ‘higher-order’ (x₁, x₂, x₄ triangle). Below are examples: classifier predictions, pairwise relationships between tables, and graph structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#ai-driven-scientific-insights-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#ai-driven-scientific-insights-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.6 AI-driven Scientific Insights in the Humanities",
    "text": "7.6 AI-driven Scientific Insights in the Humanities\nThe application of AI, particularly explainable AI (XAI), extends into the humanities, offering novel methodologies for research and discovery. These techniques can help to analyse complex historical and cultural data at scale.\n\n\n\nSlide with diagonal split: left dark blue, right beige with text ‘B. AI-based Scientific Insights in the Humanities’.\n\n\n\n7.6.1 Extracting Visual Definitions from Corpora\nInitial explorations using heatmap-based approaches have proven fruitful in projects involving historical artefacts. For instance, when working with a corpus of mathematical instruments, a classifier was developed to categorise items (e.g., as a ‘machine’ or a ‘mathematical instrument’). In collaboration with historians (Matteo Valleriani, Jochen Büttner, and others), these visual definitions generated by the AI were scrutinised to determine if they could provide more objective classification criteria. This process underscores the necessity of close collaboration with domain experts to ensure the meaningfulness of AI-derived definitions. One finding from this work was that fine-grained scales on mathematical instruments are highly relevant features for the model’s decision-making process (ElHajjEberle2023?).\n\n\n\nSlide titled ‘Ex 4: Extracting visual definitions from corpora’. Left: Engraving of a machine and descriptions of historical instruments. Right: Class-specific heatmap explanations for ‘math. instrument’, ‘machine’, ‘scientific illustration’. Citation: El-Hajj & Eberle+, Explainability and transparency in the realm of DH. (Int J Digit Humanities ’23).\n\n\n\n\n7.6.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA more extensive collaborative project focused on numerical tables from early modern texts, specifically the Sphaera corpus (1472-1650) (Valleriani2019?). Historians approached the Berlin Institute for Learning and Data (BIFOLD) with this data, seeking an automated method to match tables with similar semantics—a task previously unfeasible at scale.\n\n\n\nSlide titled ‘Ex 5: Corpus-level analysis of early modern astronomical tables’. Images of various early modern astronomical tables. Corpora: Sphaera Corpus (Valleriani+’19), Sacrobosco Table Corpus (Eberle+’24).\n\n\n\n7.6.2.1 The XAI-Historian: Aiding Historical Research\nTogether, a workflow was developed to assist historians in gaining insights from this large-scale data. This collaboration gave rise to the concept of the ‘XAI-Historian’—an historian equipped with AI and XAI tools to discover case studies and engage in more data-driven hypothesis generation. Instead of feeding entire tables into a large foundation model (which proved ineffective due to the out-of-domain nature of the data), a smaller, custom model was trained to detect numerical bigrams (pairs of adjacent numbers). XAI methods then verified that this model functioned as intended; for example, by confirming that identical bigrams (e.g., ‘38’ and ‘38’) in two different input tables were correctly identified and matched. This validation engendered trust in the model’s decisions, allowing its use for broader analysis (Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Ex 5: Historical insights at scale: xAI-Historian’. Setup: Historical tables, Sacrobosco Corpus. Diagram of workflow: data collections -&gt; atomization-recomposition (input table, bigram map, histograms) -&gt; corpus-level analysis (embedding, data similarity). Citation: Eberle et al., Historical insights at scale. (Sci Adv ’24).\n\n\n\n\n7.6.2.2 Verifying Modelling and Features using XAI and Historians\nHistorical tables serve as carriers of scientific knowledge processes, such as the mathematisation of science. Representing these tables using a ‘bag of bigrams’ (e.g., ‘01’ or ‘21’) became a key strategy, especially given limited annotations. This involved a learned feature extractor combined with hard-coded structural information. The bigram model’s performance, when verified by expert ground truth, showed strong histogram correlations and superior cluster purity compared to unigram or standard VGG-16 approaches, confirming its efficacy (Eberle2022TPAMI?; Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Verifying modeling and features using XAI and Historians’. Setup: tables as bag of bigrams. Results: scatter plot, bigram model matching, expert ground truth (histogram correlation table, cluster classification bar chart). Citations: Eberle et al. (TPAMI ’22), Eberle et al. (Sci Adv ’24).\n\n\n\n\n7.6.2.3 Cluster Entropy Analysis for Innovation Insights\nWith a reliable model for table representation, case studies commenced. Cluster entropy was employed to investigate the spread of innovation across Europe during the early modern period. The publishing output of various cities was analysed; each city produced a ‘programme’ of printed works, some more diverse than others. Some locations focused on reprinting existing materials, whilst others fostered greater novelty. Quantifying this diversity at scale was previously challenging.\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Setup: cluster entropy. Results: clustering diagram (Lisbon, Venice, Wittenberg), map of Sphaera publication cities. Citation: Eberle et al. (Sci Adv ’24).\n\n\nThe approach involved using the model-derived representations to perform distance-based clustering of tables. The entropy of these clusters for each city then served as a measure of its print programme’s diversity. A low entropy score indicated repetitive content, whereas a higher entropy score signified a more varied output. This analysis identified Frankfurt and Wittenberg as cities with particularly low entropy. Frankfurt am Main was already known as a centre for reprinting editions. More strikingly, Wittenberg’s low diversity was linked to political control by Protestant reformers, who actively limited the print programme and dictated the curriculum to be published. This AI-driven finding corroborated existing historical intuition and uncovered a quantifiable aspect of this historical anomaly (Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Setup: cluster entropy. Results: table clusters diagram, bar chart of H(p) - H(pmax) for cities, highlighting Frankfurt/Main and Wittenberg with explanations. Citation: Eberle et al. (Sci Adv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#conclusion-the-evolving-role-of-ai-based-methods-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#conclusion-the-evolving-role-of-ai-based-methods-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.7 Conclusion: The Evolving Role of AI-based Methods in the Humanities",
    "text": "7.7 Conclusion: The Evolving Role of AI-based Methods in the Humanities\nThe integration of AI-based methods into humanities research presents both significant opportunities and distinct challenges.\n\n\n\nSlide titled ‘Conclusion - AI-based methods for the Humanities’. Bullet points summarising challenges and opportunities.\n\n\nHumanities and Digital Humanities (DH) researchers have historically focused extensively on the digitisation of source material. However, the automated analysis of these corpora is far from trivial, often complicated by data heterogeneity and a scarcity of labels. Multimodality further compounds these complexities.\nNevertheless, Machine Learning (ML) and Explainable AI (XAI) offer the potential to scale humanities research and foster novel research directions. Foundation Models, including LLMs, alongside prompting techniques, can assist with intermediate tasks such as labelling, data curation, and error correction. For more complex research questions, however, their utility currently remains limited.\nA primary roadblock is the challenge posed by low-resource data, which impacts the scaling laws fundamental to many ML models. Furthermore, out-of-domain transfer, especially for historical and small-scale datasets, requires thorough evaluation. Current LLMs are predominantly trained and aligned for natural language tasks and code generation, and their applicability to specialised humanities data cannot be assumed without careful validation. Continued interdisciplinary collaboration will be crucial in navigating these challenges and harnessing the full potential of AI in enriching our understanding of the human past and its cultural artefacts.\n\nNote: A bibliography.bib file with the following (or similar) entries would be required for the citations to render correctly: (article?){Samek2017, title={Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models}, author={Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert}, journal={ITU Journal: ICT Discoveries}, volume={1}, number={1}, pages={39–57}, year={2017} } (article?){Lapuschkin2019, author = {Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert}, title = {Unmasking Clever Hans predictors and assessing what machines really learn}, journal = {Nature Communications}, volume = {10}, number = {1}, pages = {1096}, year = {2019}, doi = {10.1038/s41467-019-08987-4} } (misc?){MondalWebb2024, author = {Mondal, ShB and Webb, N and others}, title = {Multi-step planning mistakes of LLMs}, year = {2024}, eprint = {arXiv:xxxx.xxxxx}, archiveprefix = {arXiv} } (inproceedings?){Jafari2024, title={MambaLRP: Long-Range Positive Explanations for Mamba Models}, author={Jafari, Ali and others}, booktitle={Advances in Neural Information Processing Systems (NeurIPS)}, year={2024} } (article?){Schnake2022, author = {Schnake, Thomas and Eberle, Oliver and Sch{\"u}tt, Kristof T. and M{\"u}ller, Klaus-Robert and Samek, Wojciech}, title = {Higher-Order Explanations of Graph Neural Networks via Relevant Walks}, journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, volume = {45}, number = {5}, pages = {5995–6007}, year = {2022}, doi = {10.1109/TPAMI.2022.3202015} } (book?){Valleriani2019, editor = {Valleriani, Matteo}, title = {The Sphaera Corpus: A Collection of Texts Written in the Context of the Medieval Study of the Sphere}, publisher = {Max Planck Institute for the History of Science}, year = {2019}, series = {Edition Open Access} } (article?){ElHajjEberle2023, author = {El-Hajj, Hiba and Eberle, Oliver and others}, title = {Explainability and transparency in the realm of DH}, journal = {International Journal of Digital Humanities}, year = {2023}, doi = {10.1007/s42803-023-00069-0} } (article?){Eberle2024Sacrobosco, author = {Eberle, Oliver and others}, title = {The Sacrobosco Table Corpus (1472-1650)}, year = {2024}, note = {Details to be confirmed} } (article?){Eberle2024SciAdv, author = {Eberle, Oliver and Valleriani, Matteo and B{\"u}ttner, Jochen and others}, title = {Historical insights at scale: Automated analysis of early modern astronomical tables with explainable AI}, journal = {Science Advances}, year = {2024}, doi = {10.1126/sciadv.adk1000} } (article?){Eberle2022TPAMI, author = {Eberle, Oliver and others}, title = {Learning Semantic Similarity for Numerical Tables}, journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, year = {2022}, doi = {10.1109/TPAMI.2022.xxxxxxx} }",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#overview",
    "href": "ai-nepi_008_chapter.html#overview",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Title slide: Modeling Science - LLM for HPSS.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#addressing-current-llm-deficiencies",
    "href": "ai-nepi_008_chapter.html#addressing-current-llm-deficiencies",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Addressing Current LLM Deficiencies",
    "text": "8.1 Addressing Current LLM Deficiencies\nA critical examination reveals several areas where contemporary LLMs fall short of the requirements for robust academic use in HPSS. Fundamentally, an opponent mechanism is needed to actively counter the generation of erroneous or misleading information, commonly termed hallucinations. Furthermore, it is crucial to recognise that embedding vectors, whilst useful for certain computational tasks, do not inherently capture the semantic meaning of expressions.\n\n\n\nKey deficiencies in current LLM capabilities.\n\n\nLLMs intended for scholarly applications should not merely generate text that sounds convincing but may be factually incorrect. Their function must extend beyond echoing content readily available on the internet, which often lacks rigorous verification. Instead, these systems ought to prioritise the generation of information that is best justified by available evidence. Moreover, the ability to develop and propose coherent plans for scientific inquiry represents a sophisticated cognitive function that current LLMs do not possess, even in nascent forms. Addressing these deficiencies is paramount for the responsible and effective integration of LLMs into scientific and historical research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#the-centrality-of-validation-and-computational-epistemology",
    "href": "ai-nepi_008_chapter.html#the-centrality-of-validation-and-computational-epistemology",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 The Centrality of Validation and Computational Epistemology",
    "text": "8.2 The Centrality of Validation and Computational Epistemology\nTo bridge these identified gaps, the concept of validation emerges as an indispensable component. Validation, in this context, encompasses the processes that provide reasons, arguments, and evidence both for and against the truth of a given proposition. It also extends to furnishing justifications for or against the pursuit of particular actions or lines of inquiry.\n\n\n\nThe concept of validation and epistemic agency.\n\n\nAddressing the methodological challenges of validation necessitates a new disciplinary approach, here termed Computational Epistemology. This proposed field would systematically develop methods and methodologies to instil validation capabilities within computational systems. Central to this endeavour is the cultivation of epistemic agency. This involves enabling systems to identify propositions that extend beyond mere sentential forms, to discern and analyse argumentation within texts and historical sources, and to recognise the intentions, plans, and actions of historical persons as documented or inferred from their extant traces.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#ai-assisted-historical-inquiry-in-practice",
    "href": "ai-nepi_008_chapter.html#ai-assisted-historical-inquiry-in-practice",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 AI-Assisted Historical Inquiry in Practice",
    "text": "8.3 AI-Assisted Historical Inquiry in Practice\nA practical implementation of these principles can be observed in a specialised working environment designed for AI-assisted historical investigation. Consider, for instance, a historical inquiry into the construction of the Sanssouci palace, a project involving the distinguished 18th-century mathematician Leonhard Euler. A persistent question in the history of science concerns the extent of Euler’s involvement and whether any failures in the construction were attributable to him or to others; this episode remains one of the significant construction failures of its era, prompting longstanding debates amongst historians.\n\n\n\nA working environment for AI-assisted historical inquiry.\n\n\nWithin this digital workbench, researchers can pose specific queries, such as, “Reconstruct which persons carried out which work on the water fountain.” The objective is to obtain a validated, qualified answer that relies on verifiable evidence, rather than conjecture. The system, employing an AI agent (in this instance, named ‘Bernoulli’), can then analyse historical sources to produce a detailed list of individuals, their specific contributions, timelines, remuneration, and outcomes. This interface typically includes an inquiry window and tools for interacting with the AI agent. However, a primary difficulty arises: effective inquiry demands more than processing a single PDF document. It requires the capacity to search across all available sources, a task that simple indexing and token-based concentration cannot adequately address.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#essential-foundations-for-scholarly-ai-systems",
    "href": "ai-nepi_008_chapter.html#essential-foundations-for-scholarly-ai-systems",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Essential Foundations for Scholarly AI Systems",
    "text": "8.4 Essential Foundations for Scholarly AI Systems\nDeveloping such sophisticated AI-assisted inquiry systems necessitates several core components. Firstly, a scholarly curated editorial board, which has meticulously worked on the primary sources, provides an indispensable foundation. An exemplar of such foundational work is the Opera Omnia of Euler, comprising 86 volumes compiled over approximately 120 years by numerous scholars, a project that concluded only recently with the editing of all his publications and correspondence. This corpus is further complemented by the work of other scholars.\n\n\n\nInterface displaying source material and AI-generated analysis.\n\n\nThis curated collection of content items effectively serves as a substitute for reliance on opaque embeddings. It forms a detailed database encompassing chronologies of actions, communicated expressions (such as letters or publications), the evolution of terminology and language used by historical figures, and records of tools and materials employed. Each item within this inventory is validated by source material, offering a rich, historically grounded record of activities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#leveraging-the-scholarium-with-advanced-multimodal-models",
    "href": "ai-nepi_008_chapter.html#leveraging-the-scholarium-with-advanced-multimodal-models",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Leveraging the Scholarium with Advanced Multimodal Models",
    "text": "8.5 Leveraging the Scholarium with Advanced Multimodal Models\nOnce these meticulously documented records are established within a framework—perhaps termed a ‘Scholarium’—they can be interrogated using advanced, accessible multimodal AI models. Current findings suggest that multimodal models, such as the latest iterations of Google’s Gemini, are particularly well-suited to meet the complex requirements of these tasks. These models possess the capability to synthesise information from diverse sources, including both textual and visual data, thereby enriching the analytical process.\n\n\n\nThe Scholarium concept with examples of curated scholarly editions.\n\n\nThe Scholarium would draw upon significant scholarly undertakings, such as the Opera Bernoulli Euler, Kepler’s Gesammelte Werke, and Brahe’s Opera Omnia. These comprehensive editions represent decades, even centuries, of dedicated scholarship, providing the high-quality, verified data essential for meaningful AI-driven analysis in HPSS.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#implementing-registries-and-standardised-protocols",
    "href": "ai-nepi_008_chapter.html#implementing-registries-and-standardised-protocols",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Implementing Registries and Standardised Protocols",
    "text": "8.6 Implementing Registries and Standardised Protocols\nTo manage and utilise this curated scholarly information effectively, a shift from reliance on simple embeddings towards structured registries is necessary. These registries would catalogue various types of information, including:\n\nPersonal actions: Communication acts such as letters, publications, and reports.\nStatements: Logical implications, arguments, inquiries, and the use of specific language, terminology, concepts, models, methods, tools, data, and evidence.\n\n\n\n\nThe Scholarium’s registry approach and the Model Context Protocol.\n\n\nThe accessibility and interoperability of these registries depend on robust Application Programming Interfaces (APIs). Initiatives like the Model Context Protocol (MCP), advanced by organisations such as Anthropic, offer pathways towards standardising how AI models interact with such rich datasets, ensuring that contextual information is preserved and appropriately utilised.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#ensuring-sustainability-through-fair-infrastructure-and-open-collaboration",
    "href": "ai-nepi_008_chapter.html#ensuring-sustainability-through-fair-infrastructure-and-open-collaboration",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.7 Ensuring Sustainability through FAIR Infrastructure and Open Collaboration",
    "text": "8.7 Ensuring Sustainability through FAIR Infrastructure and Open Collaboration\nThe long-term viability of such scholarly resources hinges upon a robust and Findable, Accessible, Interoperable, and Reusable (FAIR) data infrastructure. Platforms like Zenodo, hosted by CERN in Geneva, offer a reliable solution for storing and publishing these curated datasets, ensuring their availability for many years to come.\n\n\n\nThe Zenodo platform for FAIR data infrastructure.\n\n\nTechnical support for maintaining and developing this infrastructure is also crucial. A startup, OpenScienceTechnology, provides services for running the necessary systems, including an MCP API server. This server facilitates worldwide access to the curated data, enabling artificial intelligence models to conduct inquiries via a standardised API.\n\n\n\nPrinciples of technical support through OpenScienceTechnology.\n\n\nThis entire endeavour thrives on a spirit of open collaboration. The principles guiding this work include commitments to:\n\nOpen Source software\nOpen Access to publications and resources\nOpen Data, supported by mechanisms like the MCP API Server\nOpen Collaboration amongst researchers, institutions, and technical providers\n\n\n\n\nCore tenets of OpenScienceTechnology’s support model.\n\n\nThrough these combined efforts—rigorous scholarship, advanced AI, robust infrastructure, and open practices—we can aspire to create powerful new tools for exploring the history, philosophy, and sociology of science.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-genesis-of-a-collaborative-endeavour",
    "href": "ai-nepi_001_chapter.html#the-genesis-of-a-collaborative-endeavour",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 The Genesis of a Collaborative Endeavour",
    "text": "2.1 The Genesis of a Collaborative Endeavour\nThe workshop emerged from two distinct yet complementary initiatives. One impetus originated within the “Network Epistemology in Practice” (NEPI) project. Arno Simons, a key member of this project, pioneered the training of one of the initial large language models specifically on physics texts, a domain of primary interest to the NEPI team. He proposed a broader discussion of such work, a suggestion that readily convinced Michael Zichert, also integral to the project, who had himself employed LLMs to analyse conceptual issues in physics.\nA second stream of inspiration came from Gerd Graßhoff, a cooperation partner of the NEPI project with a long-standing connection to its members. Professor Graßhoff has consistently championed the application of artificial intelligence in the history and philosophy of science, particularly for analysing processes of scientific discovery. He independently conceived of a workshop focused on novel AI-assisted methods for HPSS. Recognising the shared vision, these initiatives merged, culminating in the present workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#network-epistemology-in-practice-illuminating-knowledge-creation",
    "href": "ai-nepi_001_chapter.html#network-epistemology-in-practice-illuminating-knowledge-creation",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Network Epistemology in Practice: Illuminating Knowledge Creation",
    "text": "2.2 Network Epistemology in Practice: Illuminating Knowledge Creation\nFunding for this workshop stems from the European Research Council (ERC) grant supporting the “Network Epistemology in Practice” (NEPI) project. Within NEPI, researchers investigate the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The objective is to understand more deeply how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual approach. Firstly, network analysis helps to delineate the communication structures within the collaboration. Secondly, semantic tools, crucially involving the use of LLMs, are utilised to trace the flow of ideas through these established network structures. This application of LLMs to understand idea propagation within complex scientific communities represents a significant area of current research interest.\n\n2.2.1 Broader Applications and Anticipation\nBeyond the specific focus of the NEPI project, this workshop aims to showcase a wide array of other applications for LLMs in HPSS. A rich programme of presentations promises to unveil diverse ideas and methodologies, fostering a vibrant exchange of knowledge and stimulating further innovation in the field.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#essential-contributions-and-support",
    "href": "ai-nepi_001_chapter.html#essential-contributions-and-support",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Essential Contributions and Support",
    "text": "2.3 Essential Contributions and Support\nThe realisation of this event owes much to the dedicated efforts of several individuals. Svenja Goetz, Lea Stengel, and Julia Kim provided invaluable assistance in conceptualising aspects of the workshop and managing numerous administrative and organisational tasks. Their contributions were vital to navigating the complexities of planning.\nFurthermore, the technical execution of the workshop, including the recording of keynotes and other sessions, and the provision of a seamless online experience via Zoom, benefits from the expertise of Oliver Ziegler and his Unicam team. Their support ensures that the proceedings are accessible and well-documented.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#navigating-the-workshop-participation-and-interaction",
    "href": "ai-nepi_001_chapter.html#navigating-the-workshop-participation-and-interaction",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Navigating the Workshop: Participation and Interaction",
    "text": "2.4 Navigating the Workshop: Participation and Interaction\nTo ensure a productive and inclusive environment for all attendees, several modalities for participation and interaction have been established.\n\n2.4.1 Recording and Consent\nParticipants should be aware that all sessions are being recorded. This information was provided during registration, at which point consent was requested. A camera is directed towards the speaker, complemented by four roving microphones and an iPhone serving as a backup audio recorder. Subject to presenter approval, videos of the talks, including the audio of the discussion segments (featuring video of the presenter only, not the audience), will be uploaded to the workshop’s YouTube channel. Attendees with concerns or requiring further information regarding the recording are encouraged to approach the organisers.\n\n\n2.4.2 Engagement and Discussion Protocols\nGiven the large number of participants and the relatively condensed schedule for presentations, a structured approach to questions and comments will facilitate efficient discussions. Following each presentation, approximately four questions or comments will be collected. The presenter will then have the opportunity to address these collectively. This method aims to maximise the exchange of ideas within the available time.\nTo supplement in-session discussions, an Etherpad (or Cryptpad) has been established. A QR code provides access, allowing attendees to post comments or questions after sessions. This platform offers presenters an additional avenue to review and respond to feedback. During sessions, both online participants and those present in person can utilise the Zoom chat function to post questions or comments at any time.\n\n\n2.4.3 Networking Opportunities\nThe workshop design incorporates ample opportunities for informal networking. Lunch breaks, coffee breaks, a modest reception, and a workshop dinner (for which participation was confirmed due to limited seating) are intended to facilitate interaction amongst researchers and fellows. Coffee breaks and refreshments will be available in the main workshop area. Lunch and the reception will take place in room H2051, located down the hall towards the far end of the building, then one floor down. Guidance will be available for navigating to this location.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.1 Introduction to OpenAlex Mapper",
    "text": "4.1 Introduction to OpenAlex Mapper\nResearchers at Utrecht University and the University of Vienna developed OpenAlex Mapper, a tool designed to navigate and analyse the vast expanse of scholarly communication. Maximilian Noichl, in collaboration with Andrea Loettgers and Taya Knuuttila, spearheaded this initiative, which received support from an ERC grant focused on ‘Possible Life’. The tool offers an interactive platform for investigating interdisciplinary connections and the topical distribution of research. Users can access the slides accompanying this work and interact with the tool itself via the developer’s website (maxnoichl.eu/talk), allowing for a direct engagement with its capabilities. This chapter elucidates the technical underpinnings of OpenAlex Mapper, demonstrates its practical application, and explores its potential contributions to research in the History, Philosophy, and Sociology of Science (HPSS).\n\n\n\nA visual representation of interconnected academic fields, illustrating the concept behind OpenAlex Mapper.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.2 Technical Architecture of OpenAlex Mapper",
    "text": "4.2 Technical Architecture of OpenAlex Mapper\nThe operational framework of OpenAlex Mapper integrates several sophisticated computational techniques to generate its insightful visualisations. Its construction involved a multi-step process, beginning with model refinement and culminating in a system for dynamic query projection.\n\n\n\nDiagram illustrating the three-stage workflow of OpenAlex Mapper: Finetuning the embedding model, Base-map preparation, and Individual user-query processing.\n\n\n\n4.2.1 Fine-tuning the Embedding Model\nThe initial phase concentrated on refining a language model to better capture the nuances of disciplinary distinctions. Investigators selected Specter 2 (Singh2022?), a language model adept at generating embeddings for scientific documents. They fine-tuned this model using a dataset of articles from closely related disciplinary backgrounds. This procedure trained the model to distinguish more effectively between these proximate fields, thereby improving its sensitivity to disciplinary boundaries. Visualisations produced through UMAP dimensionality reduction during this training process confirmed the enhanced separation of disciplines. These adjustments to the language model were incremental, rather than a complete retraining, yet crucial for the subsequent mapping accuracy.\n\n\n4.2.2 Base-map Preparation\nFollowing the model refinement, attention turned towards constructing the foundational map of scientific literature. For this, researchers utilised the OpenAlex database, a comprehensive and openly accessible repository of scholarly material that surpasses Web of Science and Scopus in its inclusiveness and ease of batch querying. From OpenAlex, they randomly sampled 300,000 article abstracts. These abstracts were then processed using the fine-tuned Specter 2 model to generate high-dimensional embeddings.\nSubsequently, Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?) served to reduce these embeddings from their high-dimensional space to a two-dimensional representation. This projection formed the ‘base-map’, a visual landscape of scientific research. Crucially, the UMAP model trained during this stage was preserved for later use.\n\n\n4.2.3 Processing Individual User Queries\nOpenAlex Mapper empowers users to explore this base-map with their own research questions. An individual can submit an arbitrary OpenAlex search query, typically as a URL. The tool then downloads the corresponding records from OpenAlex, often employing PyAlex for this task. It proceeds to embed the abstracts of these retrieved documents using the same fine-tuned Specter 2 model employed for the base-map.\nThe core of the dynamic mapping lies in the next step: the newly generated embeddings are projected into the two-dimensional space using the previously trained UMAP model. This ensures that the queried articles are positioned on the map as if they had been part of the original layout process. UMAP’s architecture facilitates this projection of new data onto an existing embedding. The final output is an interactive map, available online and for download via data-mapplot, where the user’s query results appear highlighted against the backdrop of the broader scientific landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "href": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.3 Utilising OpenAlex Mapper: A Practical Guide",
    "text": "4.3 Utilising OpenAlex Mapper: A Practical Guide\nEngaging with OpenAlex Mapper is a relatively straightforward process, designed to be accessible for researchers seeking to explore interdisciplinary connections. The tool is available online at https://m7n-openalex-mapper.hf.space.\n\n\n\nThe user interface of OpenAlex Mapper, showing input fields for OpenAlex search URLs and settings for sample size and plot customisation.\n\n\nThe primary workflow involves these steps:\n\nNavigate to the OpenAlex website (openalex.org).\nConduct a search for a topic, author, institution, or any other entity of interest, utilising the full search capabilities of OpenAlex. For instance, one might search for papers discussing “scale-free network models”, articles published by a specific university in a given year, or publications citing a particular seminal work.\nOnce the search results appear, copy the URL from the browser’s address bar. This URL encapsulates the precise query.\nReturn to the OpenAlex Mapper interface and paste this URL into the designated “OpenAlex-search URL” input field.\nAdjust sample settings if necessary. Given that embedding abstracts can be computationally intensive, particularly for large result sets, users can opt to reduce the sample size. Options include selecting the first ‘n’ samples or other sampling methods.\nConfigure plot settings, such as choosing to colour points by publication date or displaying the citation graph over the map.\nClick the “Run Query” button.\n\nBehind the scenes, OpenAlex Mapper then downloads the specified records from OpenAlex. It embeds the abstracts of these documents and projects them onto the base-map. After a processing period, the interactive visualisation appears, allowing users to explore where their query results cluster and how they relate to different regions of the scientific map.\n\n\n\nThe OpenAlex website displaying search results for “scale free networks”, from which a URL can be copied for use in OpenAlex Mapper.\n\n\nThe tool provides immediate visual feedback on the distribution of the queried literature. For example, a search for “coriander” might reveal its presence not only in expected fields like botany or food science but also in unexpected areas such as epidemiology or public health, prompting further investigation into these connections. Similarly, mapping publications on “scale-free network models” can illustrate their prevalence and application across diverse scientific domains.\n\n\n\nThe OpenAlex Mapper interface processing a query for “scale free networks”, with a progress bar indicating the embedding stage.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper offers a valuable methodological addition to the toolkit of HPSS researchers. It aims to help bridge the gap between detailed, qualitative case studies and the broader, large-scale dynamics of contemporary science. Many insights in HPSS derive from close-up views of specific scientific episodes, often based on close reading of texts, interaction with scientists, or ethnographic methods. Whilst these studies provide rich understanding, generalising their findings or validating them in the context of global, rapidly evolving “big science” presents a considerable challenge.\nThe tool assists in addressing questions about the reach, influence, and contextual embedding of scientific ideas, models, and methods. For instance, one might ask: Where did the Hopfield Model, developed in a specific context, truly gain traction and find lasting application across the sciences? OpenAlex Mapper allows for such heuristic, qualitative investigations to be supported and guided by quantitative, large-scale data analysis, whilst always enabling a return to the underlying textual sources.\n\n4.4.1 Investigating Model Templates\nResearchers initially developed OpenAlex Mapper with the study of ‘model templates’ in mind. In the philosophy of science, model templates refer to abstract structural forms of models that recur across diverse scientific disciplines, potentially structuring scientific inquiry in ways orthogonal to traditional disciplinary boundaries. Using the tool, investigators can map the occurrence of specific model templates—such as percolation models, network models, or agent-based models—revealing their distinct, sometimes non-continuous, footprints across the scientific landscape. This visualisation can illuminate how similar formalisms are adopted and adapted in varied epistemic contexts.\n\n\n4.4.2 Mapping Conceptual Landscapes\nThe tool also proves useful for exploring the distribution and interrelation of scientific concepts. For example, one can map the concept of ‘phase transition’ and contrast its disciplinary spread with that of ‘emergence’. Whilst conceptual mapping is an established practice, OpenAlex Mapper extends this capability to broad interdisciplinary contexts, overcoming common difficulties associated with acquiring and harmonising diverse datasets. It allows researchers to visualise how concepts travel, transform, and are contested across different fields.\n\n\n4.4.3 Analysing Methodological Distributions\nA further application lies in examining the distribution of specific research methods. Consider the ongoing debate in philosophy of science regarding the role of machine learning techniques versus classical statistical methods in scientific discovery. OpenAlex Mapper can map the usage of a machine learning technique like the random forest model against a more traditional method such as logistic regression. Observing distinct patterns in their disciplinary uptake—for instance, why neuroscientists might favour random forests whilst psychiatric researchers predominantly use logistic regressions for thematically similar problems—can generate new philosophical questions about methodological choice, epistemic justification, and disciplinary cultures.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "href": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.5 Methodological Considerations and Limitations",
    "text": "4.5 Methodological Considerations and Limitations\nWhilst OpenAlex Mapper offers powerful analytical capabilities, several qualifications warrant attention.\n\n\n\nA slide summarising key qualifications and limitations of the OpenAlex Mapper tool and its underlying data.\n\n\nA list of these considerations includes:\n\nData Source Imperfections: The utility of the tool is intrinsically linked to the comprehensiveness and accuracy of the OpenAlex database. Although data quality is generally reasonable, and comparable to other major bibliographic databases, it is not flawless. Users must remain mindful of potential biases or gaps in the underlying data.\nLanguage Limitations: Currently, the fine-tuned Specter 2 model employed by OpenAlex Mapper processes only English-language texts. This inherently limits the scope of analysis, particularly for research published in other languages or for historical periods where English was less dominant in scientific communication. Future developments could incorporate multilingual models to address this, although high-quality, science-trained multilingual models are not yet widely available.\nDependency on Abstracts and Titles: The embedding process relies on the availability of abstracts or, at a minimum, informative titles. Sources lacking such textual information cannot be effectively processed or mapped, potentially excluding certain types of publications or older literature.\nUMAP Algorithm Characteristics: The UMAP algorithm, central to the dimensionality reduction and visualisation, possesses certain characteristics that influence the output.\n\nStochasticity: UMAP is a stochastic algorithm, meaning that each run can produce slightly different layouts. The generated map represents one realisation amongst many possibilities.\nDimensionality Trade-offs: Projecting high-dimensional data (such as the 768 dimensions of Specter embeddings) into a mere two dimensions inevitably involves trade-offs and potential distortions. The algorithm must prioritise certain relationships, which can lead to some misalignments or compressions of the true semantic distances between documents.\n\n\nA working paper, “Philosophy at Scale: Introducing OpenAlex Mapper,” provides a more detailed exposition of the technical aspects and further discusses these methodological points.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#large-language-models-evolution-and-limitations",
    "href": "chapter_ai-nepi_008.html#large-language-models-evolution-and-limitations",
    "title": "8  Modeling Science",
    "section": "8.1 Large Language Models: Evolution and Limitations",
    "text": "8.1 Large Language Models: Evolution and Limitations\n\n\n\nSlide 01\n\n\nLarge Language Models (LLMs) have undergone a rapid and notable evolution. Their development commenced with the influential “Attention is all you need” concept, which powered early conversational agents like GPT. Subsequently, the focus shifted towards “Context is all you need,” a phase emphasising the importance of larger contextual inputs, often managed through techniques such as Retrieval Augmented Generation (RAG). More recently, the paradigm has advanced to “Thinking is all you need,” aiming to imbue models with reasoning abilities, sometimes involving explicit planning.\nDespite these advancements, significant deficiencies persist in current LLM technologies. A critical missing element is an effective counter-mechanism to mitigate or prevent the generation of hallucinatory content. Moreover, a fundamental misunderstanding prevails: embedding vectors, whilst useful for pattern recognition, do not inherently capture the nuanced meanings of expressions. This leads to a concerning tendency for LLMs to formulate statements that sound convincing yet are demonstrably false.\nThese models also exhibit a propensity to repeat unverified information circulating on internet media, treating it as knowledge without proper validation. Crucially, current models lack the ability to systematically seek out what is best justified or to formulate strategic plans for scientific inquiry. Present technological trajectories offer little immediate hope for overcoming these profound limitations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#validation-and-computational-epistemology-for-llms",
    "href": "chapter_ai-nepi_008.html#validation-and-computational-epistemology-for-llms",
    "title": "8  Modeling Science",
    "section": "8.2 Validation and Computational Epistemology for LLMs",
    "text": "8.2 Validation and Computational Epistemology for LLMs\n\n\n\nSlide 03\n\n\nTo address the identified shortcomings in Large Language Models, a new imperative emerges: “Validation is all you need,” or, at the very least, a component of paramount importance. Such a validation-centric approach would equip systems with several core capabilities. These include the capacity to furnish reasons both for and against the veracity of a proposition, to articulate arguments supporting or contesting its truth, and to present evidence that either corroborates or contradicts it. Furthermore, this framework must provide reasoned justifications for or against the pursuit of particular actions.\nThe systematic development of these validation mechanisms falls under the purview of a newly proposed academic discipline: “Computational Epistemology.” This field aims to create the sophisticated methods and rigorous methodologies required to fill the critical validation gap in contemporary AI. Central to this endeavour is the cultivation of epistemic agency. This entails enabling systems to identify propositions with precision, moving beyond mere sentence-level processing. It also requires the ability to recognise complex arguments within varied textual sources and historical inquiries, and, crucially, to discern the intentions, plans, and actions of individuals as preserved in historical documents and the traces they leave.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#ai-assisted-historical-research-a-working-environment",
    "href": "chapter_ai-nepi_008.html#ai-assisted-historical-research-a-working-environment",
    "title": "8  Modeling Science",
    "section": "8.3 AI-Assisted Historical Research: A Working Environment",
    "text": "8.3 AI-Assisted Historical Research: A Working Environment\n\n\n\nSlide 04\n\n\nResearchers have developed a practical working environment, named Cursor, to facilitate AI-assisted historical inquiry. A visual demonstration showcased this environment, displaying a historical source document on one side of the screen and an interactive AI interface on the other. The chosen case study involved the construction of Frederick the Great’s Sanssouci castle, specifically investigating the contentious involvement of the distinguished 18th-century mathematician Leonhard Euler and his potential role in the project’s notable failure. This particular construction project remains a subject of intense debate amongst historians of science, who continue to explore the precise locus of responsibility.\nWithin this Cursor environment, users can pose complex queries. For instance, a German prompt, “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten,” directs the system to reconstruct the personnel and tasks associated with the castle’s water fountain. The system aims to deliver validated and qualified answers, substantiated by proven evidence, detailing individuals such as Nahl (sculptor), Benkert, Heymüller, and Giese, their specific contributions, timelines, remuneration, and outcomes. An AI agent, designated Bernoulli, operates within Cursor to process these inquiries. However, a significant challenge lies in moving beyond the analysis of single documents; effective historical research necessitates searching across all available sources, a task for which current token-based indexing methods are inadequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-scholarium-evidence-curated-scholarly-sources",
    "href": "chapter_ai-nepi_008.html#the-scholarium-evidence-curated-scholarly-sources",
    "title": "8  Modeling Science",
    "section": "8.4 The Scholarium (Evidence): Curated Scholarly Sources",
    "text": "8.4 The Scholarium (Evidence): Curated Scholarly Sources\n\n\n\nSlide 07\n\n\nThe envisioned research infrastructure rests upon several interconnected components, with the first being a meticulously curated collection of scholarly sources, termed the “Scholarium (Evidence).” Oversight for this collection resides with a scholarly editorial board, ensuring the validation and integrity of the included materials. A paramount example of such a resource is the Opera Omnia of Leonhard Euler. This monumental work spans 86 volumes, documenting 120 years of Euler’s scientific output. The colossal task of scholarly editing, involving generations of academics over approximately 120 years, reached completion in 2022. It now encompasses all 866 of Euler’s publications and his entire body of correspondence. This foundational evidence base is further complemented by other critical scholarly editions, such as the Kepler Gesammelte Werke and Brahe’s Opera Omnia, forming a rich repository for historical scientific inquiry.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-scholarium-registry-a-structured-alternative-to-embeddings",
    "href": "chapter_ai-nepi_008.html#the-scholarium-registry-a-structured-alternative-to-embeddings",
    "title": "8  Modeling Science",
    "section": "8.5 The Scholarium (Registry): A Structured Alternative to Embeddings",
    "text": "8.5 The Scholarium (Registry): A Structured Alternative to Embeddings\n\n\n\nSlide 06\n\n\nComplementing the curated sources, researchers have developed the “Scholarium (Registry instead embeddings),” a system designed as a robust alternative to reliance on embedding vectors for representing historical content. This registry functions as a meticulously curated database, cataloguing a diverse array of validated content items. It systematically organises information such as chronologies of personal actions and interpersonal communications.\nFurthermore, the registry documents the evolution of an individual’s language use, including specific terminology and conceptual frameworks. It also tracks the application of particular models, methods, tools, and material devices, always ensuring that such information is validated against primary historical sources. This detailed inventory of historically proven activities, each linked to its evidentiary basis, provides a structured and reliable foundation for inquiry. The system design anticipates interaction via APIs, including a general AI API and a Model Context Protocol (MCP) API, potentially aligning with initiatives like Anthropic’s Model Context Protocol.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#multimodal-llms-within-the-cursor-environment",
    "href": "chapter_ai-nepi_008.html#multimodal-llms-within-the-cursor-environment",
    "title": "8  Modeling Science",
    "section": "8.6 Multimodal LLMs within the Cursor Environment",
    "text": "8.6 Multimodal LLMs within the Cursor Environment\n\n\n\nSlide 07\n\n\nTo interact with the rich data held within the Scholarium’s registry and curated sources, the system leverages accessible multimodal Large Language Models. Researchers have found that multimodal models, capable of processing diverse data types, currently offer the best approach for meeting the multifaceted requirements of historical inquiry. Specifically, the latest models of Gemini 2.5 are employed, valued for their ability to synthesise information from both textual content and images.\nBeyond Gemini, the infrastructure also integrates other prominent LLMs, including Claude and Llama. These models operate within the LettreAI on Cursor platform, where the AI agent Bernoulli executes complex queries against the historical data. This technical configuration allows for sophisticated, AI-driven exploration of the validated scholarly materials.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-data-repository-for-scholarly-outputs",
    "href": "chapter_ai-nepi_008.html#fair-data-repository-for-scholarly-outputs",
    "title": "8  Modeling Science",
    "section": "8.7 FAIR Data Repository for Scholarly Outputs",
    "text": "8.7 FAIR Data Repository for Scholarly Outputs\n\n\n\nSlide 08\n\n\nRecognising the critical need for sustainable data management, the project incorporates a long-term FAIR (Findable, Accessible, Interoperable, and Reusable) repository. For this purpose, project leaders selected Zenodo. Hosted by CERN in Geneva, Zenodo provides a robust platform for storing and publishing research data, thereby ensuring its accessibility and utility for the scholarly community over extended periods. This commitment to FAIR principles underpins the project’s dedication to enduring scholarly value and verifiability.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-support-and-open-collaboration",
    "href": "chapter_ai-nepi_008.html#technical-support-and-open-collaboration",
    "title": "8  Modeling Science",
    "section": "8.8 Technical Support and Open Collaboration",
    "text": "8.8 Technical Support and Open Collaboration\n\n\n\nSlide 09\n\n\nTechnical sustenance for this ambitious infrastructure comes from OpenScienceTechnology, a startup dedicated to supporting such scholarly endeavours. This organisation provides crucial technical support, including the operation of an MCP (Model Context Protocol) API server. This server enables artificial intelligence models from across the globe to access and interrogate the curated data holdings via a standardised interface, promoting widespread utility. The entire initiative operates under a strong commitment to open principles: Open Source, Open Access, Open Data, and Open Collaboration. These tenets aim to foster a transparent, accessible, and collaborative research ecosystem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Workshop Overview\nThe organisers orchestrated a three-day workshop dedicated to exploring the applications of Large Language Models (LLMs) within the history, philosophy, and sociology of science. This event, held at TU Berlin and accessible online from 2-4 April 2025, garnered considerable interest. Over 50 paper submissions were received, and approximately 220 participants registered, demonstrating the broad appeal of the topic. Adrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert meticulously curated the programme, ultimately selecting 16 papers for presentation. The European Research Council’s (ERC) Network Epistemology in Practice (NEPI) grant (number 10104932) secured the necessary funding for the workshop.\nThe workshop’s conceptualisation stemmed from two distinct yet complementary initiatives. Firstly, the NEPI project provided a foundational impetus. Within this framework, Arno Simons pioneered the training of early LLMs on physics texts, whilst Michael Zichert applied these models to analyse conceptual issues in physics.  Secondly, Gerd Graßhoff, a long-standing collaborator, consistently advocated for integrating Artificial Intelligence (AI) into the history and philosophy of science, particularly for scrutinising scientific discovery processes. These converging interests culminated in a unified workshop, fostering broader discussion on AI-assisted methodologies.\nThe NEPI project itself investigates the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. Researchers employ network analysis to map communication structures and utilise semantic tools, including LLMs, to trace the flow of ideas within these complex networks. This endeavour aims to elucidate how large research collaborations collectively generate new knowledge.\nKeynote speakers for the workshop included Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. They presented on large-scale text analysis for cultural and societal change, focusing on semantic change detection and data science for the humanities. Iryna Gurevych, head of the Ubiquitous Knowledge Processing (UKP) Lab at Technical University Darmstadt, delivered a keynote on elevating Natural Language Processing (NLP) to the cross-document level, covering information extraction, semantic text processing, machine learning, and NLP applications in social sciences and humanities.\nLogistical arrangements for the workshop included a comprehensive recording policy. Sessions were captured by a camera focused on the presenter, four microphones, and an iPhone backup. The organisers intend to upload videos of talks, including discussions, to the NEPI YouTube channel, subject to presenter consent. A structured Q&A protocol facilitated engagement, limiting questions to four per session to ensure efficiency. Furthermore, an Etherpad/Cryptpad provided an asynchronous platform for comments and questions, whilst the Zoom chat enabled real-time interaction. Ample networking opportunities were provided through scheduled lunch and coffee breaks, a modest reception, and a limited-seat workshop dinner.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-genesis-organisation-and-acknowledgements",
    "href": "chapter_ai-nepi_001.html#workshop-genesis-organisation-and-acknowledgements",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Genesis, Organisation, and Acknowledgements",
    "text": "2.1 Workshop Genesis, Organisation, and Acknowledgements\n\n\n\nSlide 01\n\n\nThe organisers launched the workshop, “Large Language Models for the History, Philosophy and Sociology of Science,” which commenced from 2nd to 4th April 2025. This significant event unfolded at TU Berlin, specifically in Room H2005, with comprehensive provisions for online participation, accessible via a registration QR code. The organising committee comprised Gerd Graßhoff, Arno Simons, Adrian Wüthrich, and Michael Zichert. Comprehensive details reside on the NEPI project website: https://www.tu.berlin/hps-mod-sci/workshop-llms-for-hpss.\nThe call for papers elicited an exceptional response, yielding over 50 submissions. From these, the organisers meticulously selected 16 for presentation during the workshop’s programme. Consequently, on-site attendance rapidly reached full capacity, complemented by a substantial online audience, culminating in approximately 220 registered participants.\nThe workshop’s inception traces back to two distinct, yet convergent, intellectual foundations. Within the Network Epistemology in Practice (NEPI) project, Arno Simons pioneered the training of one of the initial Large Language Models specifically on physics texts. Concurrently, Michael Zichert, also a member of the NEPI team, applied Large Language Models to scrutinise conceptual issues within physics. Their work prompted a call for broader discussion on this type of research. Independently, Professor Gerd Graßhoff, a cooperation partner of the NEPI project and a long-standing proponent of employing AI in the history and philosophy of science—particularly for analysing scientific discovery processes—envisioned a similar workshop focused on novel AI-assisted methodologies for HPS. Recognising their shared objectives, these parties coalesced, thereby realising the workshop.\nAn European Research Council (ERC) grant, awarded to the Network Epistemology in Practice (NEPI) project, provided the financial backing for this endeavour. Svenja Goetz, Lea Stengel, and Julia Kim ensured the smooth execution of administrative and organisational tasks. Furthermore, Oliver Ziegler and his Unicam team provided crucial technical support, managing the recording of keynotes and other sessions, and facilitating a seamless Zoom meeting experience for all participants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#the-nepi-project-research-focus-and-llm-application",
    "href": "chapter_ai-nepi_001.html#the-nepi-project-research-focus-and-llm-application",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 The NEPI Project: Research Focus and LLM Application",
    "text": "2.2 The NEPI Project: Research Focus and LLM Application\n\n\n\nSlide 01\n\n\nThe workshop derives significant impetus from the Network Epistemology in Practice (NEPI) project, an initiative supported by an European Research Council (ERC) grant. Researchers within NEPI investigate the intricate internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The project primarily aims to cultivate a profound understanding of how such large-scale, prominent research collaborations collectively generate new scientific knowledge.\nTo achieve this, the project employs a dual methodological approach. On one hand, network analysis techniques facilitate a detailed study of the communication structure inherent in the collaboration. On the other hand, investigators utilise semantic tools, with Large Language Models playing a pivotal role, to trace and analyse the propagation of ideas within these complex network structures. Indeed, the application of LLMs for discerning these ideational flows constitutes a central component of the NEPI research strategy and remains an area of particular interest to the organisers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-protocols-interaction-and-logistics",
    "href": "chapter_ai-nepi_001.html#workshop-protocols-interaction-and-logistics",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Workshop Protocols, Interaction, and Logistics",
    "text": "2.3 Workshop Protocols, Interaction, and Logistics\n\n\n\nSlide 04\n\n\nThe organisers established clear protocols for the workshop, commencing with the recording of all sessions. Participants provided consent for this during registration. A single camera focuses on the presenter, whilst audio capture employs four circulating microphones and an iPhone as a backup. Subject to presenter approval, the NEPI project will make these recordings, encompassing both the talk and the subsequent discussion (featuring audience audio and presenter video), available on its YouTube channel after the workshop. Attendees requiring further information or wishing to discuss their consent may approach the organising team.\nTo ensure efficient discussions, the workshop will adopt a structured Q&A format. Participants should keep their questions and comments brief and pertinent. After each presentation, the chair will gather approximately four questions or comments, to which the presenter will respond collectively, thereby minimising time lost in back-and-forth exchanges. Beyond direct Q&A, an Etherpad or Cryptpad, accessible via a QR code, offers a platform for continued interaction; here, attendees can post comments or questions after sessions, and presenters have the opportunity to review and reply. Additionally, the Zoom chat remains open for questions and comments at any time throughout the sessions for all participants.\nThe workshop design also incorporates ample opportunities for informal networking. Scheduled lunch and coffee breaks, along with a modest reception and a workshop dinner (for which seating is limited and pre-confirmed), aim to foster connections amongst researchers. The organisers will conveniently provide coffee and refreshments within the main workshop room, H2005. Lunch and the reception, however, will take place in Room H2051, located down the hall, via the last staircase on the opposite side of the building, and one floor below.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-speakers-and-their-contributions",
    "href": "chapter_ai-nepi_001.html#keynote-speakers-and-their-contributions",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Keynote Speakers and Their Contributions",
    "text": "2.4 Keynote Speakers and Their Contributions\nThe workshop features two distinguished keynote addresses. Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg will deliver the first, titled “Large-scale text analysis for the study of cultural and societal change.” Professor Tahmasebi serves as the Principal Investigator of the “Change is Key!” research programme at Gothenburg, where Dr Cassotti is a researcher. Their collective expertise centres on semantic change detection, encompassing the development of critical benchmarks and the innovative application of data science methods to address complex questions within the humanities. Their contributions are thus highly pertinent to the workshop’s themes.\nSubsequently, Professor Iryna Gurevych will deliver the second keynote address, “How to InterText? Elevating NLP to the cross-document level,” scheduled for the late afternoon of the following day. Professor Gurevych leads the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University Darmstadt. Her extensive research portfolio covers information extraction, semantic text processing, and machine learning. Crucially, her work also involves the practical application of Natural Language Processing techniques to enrich research in the social sciences and humanities, aligning perfectly with the workshop’s interdisciplinary focus.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture-foundation-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture-foundation-of-large-language-models",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.1 The Transformer Architecture: Foundation of Large Language Models",
    "text": "3.1 The Transformer Architecture: Foundation of Large Language Models\n\n\n\nSlide 02\n\n\nResearchers first introduced the foundational Transformer architecture in 2017, originally conceiving the model for language translation tasks, such as converting German text to English. This architecture now underpins virtually all contemporary Large Language Models. Structurally, the Transformer comprises two interconnected streams: an encoder and a decoder, linked centrally.\nThe encoder, typically visualised on the left, ingests source language words—for instance, a German sentence—and transforms them into numerical representations. These numbers undergo processing through multiple layers before transmission to the decoder. Crucially, the encoder processes the entire input sentence simultaneously, allowing every word to attend to all other words within that sentence. This mechanism enables the model to construct a comprehensive representation of the sentence’s meaning.\nConversely, the decoder, situated on the right, receives these numerical encodings and generates words in the target language, such as English. Its operational mode differs significantly; output words can only reference preceding words in the sequence, not subsequent ones, as the model’s task involves predicting the next word. Each word generated by the decoder then feeds back into the system, continuing this loop until the complete target sentence materialises. Within these processing streams, layers progressively refine contextualised word embeddings, enhancing their representational richness. Vaswani and colleagues detailed this architecture in their seminal 2017 paper, “Attention is all you need”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-to-pre-trained-models-bert-and-gpt",
    "href": "chapter_ai-nepi_003.html#evolution-to-pre-trained-models-bert-and-gpt",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.2 Evolution to Pre-trained Models: BERT and GPT",
    "text": "3.2 Evolution to Pre-trained Models: BERT and GPT\n\n\n\nSlide 03\n\n\nSubsequent to the Transformer’s introduction, developers began re-engineering its encoder and decoder streams independently, leading to the creation of pre-trained language models. These models aim to achieve a profound understanding or generation capability in language, allowing for subsequent adaptation to specific Natural Language Processing (NLP) tasks with relatively minor additional training.\nFrom the encoder stream emerged the BERT (Bidirectional Encoder Representations from Transformers) family of models, which remain highly influential. Devlin and colleagues introduced BERT in 2018. Its defining characteristic is bidirectionality: each word in an input sequence can consider all other words, both preceding and succeeding it. This allows BERT to develop a comprehensive, full-context understanding of the entire input at once.\nConversely, the decoder stream gave rise to GPT (Generative Pre-trained Transformers) models, famously powering applications such as ChatGPT. Radford and colleagues developed these models, also in 2018. GPT models operate unidirectionally, meaning they process text sequentially and can only consider preceding words when generating new content. This structural difference endows them with the ability to produce novel text, a capability generally absent in BERT-like models. Thus, a core distinction arises: generative models excel at language production, whilst full-context models like BERT offer more coherent sentence understanding. Beyond these two primary types, other architectures exist, including combined encoder-decoder models and sophisticated decoder applications like XLM (which builds upon XLNet) that can emulate encoder-like functionalities.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-landscape-of-scientific-llms-and-adaptation-techniques",
    "href": "chapter_ai-nepi_003.html#the-landscape-of-scientific-llms-and-adaptation-techniques",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.3 The Landscape of Scientific LLMs and Adaptation Techniques",
    "text": "3.3 The Landscape of Scientific LLMs and Adaptation Techniques\n\n\n\nSlide 06\n\n\nThe domain of scientific Large Language Models has expanded considerably, as Ho and colleagues documented in their 2024 survey on pre-trained language models for scientific text. Their work illustrates the evolution of these models from 2018 to 2024, categorising them into encoder-decoder, decoder-only, and encoder-only types, and distinguishing between open-source and closed-source variants. This diverse landscape includes numerous specialised models such as BioBERT, ChemBERT, SciBERT, ClinicalBERT, MathBERT, and PhysicsBERT, alongside foundational models like BERT and GPT.\nAdapting these general models to specific scientific language involves several techniques. Initial pre-training, where the model first encounters language, occurs through next-token prediction (for GPT-like models) or masked-word prediction (for BERT-like models). However, full pre-training demands computational resources and data volumes often prohibitive for individual research teams. A more feasible approach involves continued pre-training, where an existing pre-trained model undergoes further training on a specialised corpus, such as a BERT model refined with physics-specific texts.\nAlternatively, developers can add extra layers to pre-trained models, training these new components as classifiers for tasks like sentiment analysis or named entity recognition. Prompt-based adaptation offers another avenue, though the presentation did not elaborate upon it in detail. Furthermore, contrastive learning serves as a crucial technique for deriving sentence or document embeddings from word-level embeddings; SentenceBERT represents a prominent implementation of this method, potentially a topic for discussion by Irina Gurevich.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-and-key-model-distinctions",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-and-key-model-distinctions",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.4 Retrieval Augmented Generation (RAG) and Key Model Distinctions",
    "text": "3.4 Retrieval Augmented Generation (RAG) and Key Model Distinctions\n\n\n\nSlide 10\n\n\nEngineers employ Retrieval Augmented Generation (RAG) as a sophisticated pipeline to adapt language models to specific scientific domains, often without necessitating complete model retraining. This approach, increasingly common in applications like ChatGPT for tasks such as internet searching, orchestrates multiple models—typically at least two—working in synergy.\nThe RAG process commences when a user poses a query, for instance, “What are LLMs?”. A BERT-type model then encodes this query into a sentence embedding. Subsequently, this embedding facilitates a search within a curated database of relevant documents, such as expert interviews or technical papers, to identify the most semantically similar passages. These retrieved passages are then integrated into the input prompt supplied to a generative language model. Furnished with this enriched context, the generative model formulates and delivers an answer.\nIt is important to recognise that advanced systems, including reasoning models and agents, seldom consist of a single LLM; rather, they are complex assemblages of multiple LLMs and various other computational tools. To navigate this complexity, one must grasp several key distinctions:\n\nThe fundamental architectural differences (encoder-based, decoder-based, or encoder-decoder hybrids) and their pre-training objectives.\nThe diverse strategies available for fine-tuning.\nThe critical difference between word embeddings and sentence embeddings.\nThe varying levels of abstraction, from individual LLMs to integrated pipelines and autonomous agents.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#applications-and-trends-of-llms-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#applications-and-trends-of-llms-in-hpss-research",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.5 Applications and Trends of LLMs in HPSS Research",
    "text": "3.5 Applications and Trends of LLMs in HPSS Research\n\n\n\nSlide 13\n\n\nResearchers are currently conducting a survey to map the applications of Large Language Models as tools within History, Philosophy, and Sociology of Science (HPSS) research. Preliminary findings group these applications into four main categories.\nFirstly, LLMs assist in dealing with data and sources, facilitating tasks such as:\n\nParsing and extracting information like publication types, acknowledgements, and citations.\nEnabling interactive engagement with sources through summarisation or RAG-based conversational interfaces.\n\nSecondly, these models aid in analysing knowledge structures by:\n\nPerforming entity extraction—identifying scientific instruments, celestial bodies, or chemicals.\nCreating mappings of disciplines, interdisciplinary fields, or science-policy discourses, which are classical subjects in HPSS.\n\nThirdly, LLMs contribute to understanding knowledge dynamics. This includes:\n\nTracing conceptual histories of terms like “theory” in Digital Humanities or “virtual” and “Planck” in physics.\nIdentifying novelty such as breakthrough papers or emerging technologies.\n\nFourthly, they support the study of knowledge practices, for example, in:\n\nArgument reconstruction by identifying premises and conclusions.\nCitation context analysis to discern purpose and sentiment—revitalising an older HPSS tradition.\nDiscourse analysis, including the identification of hedge sentences, jargon, and boundary work.\n\nSeveral trends emerge from this survey. An accelerating interest in LLMs is evident, with publications appearing not only in information science journals like Scientometrics and JASIST but also increasingly in traditionally non-computational HPSS journals. This indicates their growing appeal to qualitative researchers and philosophers due to enhanced semantic capabilities. The degree of customisation varies widely, from using off-the-shelf tools like ChatGPT to developing entirely new model architectures.\nPersistent concerns include the substantial computational resources required, model opaqueness, shortages of suitable training data and benchmarks, and the inherent trade-offs between different model types (e.g., BERT-like versus GPT-like). These reinforce that no single model suits all purposes. Concurrently, a trend towards greater accessibility is evident, with user-friendly tools like BERTopic for topic modelling becoming popular due to robust maintenance and ease of use, potentially supplanting older tools such as pyLDAvis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#reflections-challenges-and-methodological-considerations-for-llms-in-hpss",
    "href": "chapter_ai-nepi_003.html#reflections-challenges-and-methodological-considerations-for-llms-in-hpss",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.6 Reflections: Challenges and Methodological Considerations for LLMs in HPSS",
    "text": "3.6 Reflections: Challenges and Methodological Considerations for LLMs in HPSS\n\n\n\nSlide 12\n\n\nEffectively integrating Large Language Models into History, Philosophy, and Sociology of Science (HPSS) necessitates careful reflection on several fronts. Firstly, researchers must acknowledge the unique challenges inherent to HPSS. The historical evolution of concepts and language is paramount; since LLMs are predominantly trained on contemporary language, their application to historical texts carries a risk of bias. HPSS also adopts a reconstructive and critical perspective, seeking to read “between the lines” by considering authorial intent, socio-historical context, and subtle discursive strategies like boundary work—nuances that current models are not typically trained to discern. Furthermore, practical data limitations, including sparse datasets, multiple languages, archaic scripts, and incomplete digitalisation, pose significant hurdles.\nSecondly, building robust LLM literacy within the HPSS community is crucial. This involves familiarising scholars with the underlying principles of LLMs, Natural Language Processing (NLP), and Deep Learning (DL), encompassing both the practical tools and their theoretical underpinnings. Developing the skill to select appropriate model architectures and training regimens for specific HPSS problems is essential, which may, for some, involve acquiring coding skills, even as natural language interfaces for these tools become more sophisticated. The overarching goal is to move beyond superficial engagement with tools that might produce attractive visualisations but lack deep interpretative grounding. A collective effort towards developing shared datasets and benchmarks tailored to HPSS needs would also be beneficial.\nFinally, it is vital to remain true to established HPSS methodologies. This involves thoughtfully translating HPSS research questions into tractable NLP tasks—such as classification, generation, or summarisation—without sacrificing the original scholarly focus or allowing the technical task to overshadow the intellectual inquiry. Simultaneously, these new tools present opportunities for bridging qualitative and quantitative research paradigms. HPSS scholars might also reflect on their field’s own contributions to the conceptual pre-history of these models, citing, for example, co-word analysis techniques developed by figures like Callon and Rip in the 1980s, which were informed by theoretical frameworks such as Actor-Network Theory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#introduction-and-project-context",
    "href": "chapter_ai-nepi_004.html#introduction-and-project-context",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 Introduction and Project Context",
    "text": "4.1 Introduction and Project Context\n\n\n\nSlide 01\n\n\nMaximilian Noichl, a PhD candidate in Theoretical Philosophy at Utrecht University, alongside Andrea Loettgers and Taya Knuuttila from the Philosophy Department in Vienna, unveiled OpenAlex Mapper. This research initiative garnered support from a European Research Council grant, specifically linked to the “Possible Life” project. The presentation sought to elucidate the OpenAlex Mapper tool, meticulously detailing its functions and underlying technical architecture at a high level.\nBeyond this, the talk featured a live demonstration of the tool, followed by a comprehensive discussion of its intended purpose and practical applications, particularly its relevance to the History and Philosophy of Science and Scholarship (HPSS). To facilitate interactive engagement, the presentation slides and direct access to the tool were made available via the website maxnoichl.eu/talk.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-workflow-and-technical-architecture",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-workflow-and-technical-architecture",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 OpenAlex Mapper: Workflow and Technical Architecture",
    "text": "4.2 OpenAlex Mapper: Workflow and Technical Architecture\n\n\n\nSlide 04\n\n\nDevelopers engineered OpenAlex Mapper to project user-defined search queries from the OpenAlex database onto a pre-generated background map, meticulously constructed from randomly sampled scholarly papers. The workflow commences with the meticulous fine-tuning of an embedding model. Specifically, researchers adapted the Specter 2 language model, as detailed by Singh et al. (2023), to more effectively recognise and enforce clearer distinctions between scientific disciplines. This fine-tuning involved training the model on a dataset of articles from closely related disciplinary backgrounds, thereby enhancing its capacity for differentiation. The training process itself was visualised using UMAP dimensionality reduction. Crucially, these adjustments constituted minor modifications rather than a comprehensive retraining of the underlying language model.\nSubsequent to model refinement, the team meticulously prepared a foundational base-map. This process necessitated accessing the OpenAlex database—a vast, open-data repository of scholarly material renowned for its extensive coverage compared to Web of Science or Scopus. From OpenAlex, they randomly sampled 300,000 article abstracts. These abstracts were then embedded utilising the fine-tuned Specter 2 model. Subsequently, employing Uniform Manifold Approximation and Projection (UMAP), a technique pioneered by McInnes, Healy, and Melville (2018), these high-dimensional embeddings were elegantly reduced to a two-dimensional representation. This crucial step yielded both the visual base-map and a persistently trained UMAP model, preserved for subsequent operations.\nWhen a user initiates a query, they provide an arbitrary OpenAlex search URL. The OpenAlex Mapper tool then leverages PyAlex (De Bruin, 2023) to download the records corresponding to this search. The abstracts of these retrieved records undergo an embedding process identical to that employed for the base-map, utilising the fine-tuned Specter 2 model. These novel embeddings are subsequently projected into the two-dimensional space of the base-map by applying the previously trained UMAP model. Consequently, the user’s query results are assigned precise positions on the map, appearing as if they had been intrinsically included in the original layout—a capability inherent to the UMAP algorithm. The final output manifests as an interactive map, readily available online and for download through data mappers, enabling dynamic exploration of the underlying data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#tool-demonstration-and-features",
    "href": "chapter_ai-nepi_004.html#tool-demonstration-and-features",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Tool Demonstration and Features",
    "text": "4.3 Tool Demonstration and Features\n\n\n\nSlide 09\n\n\nThe OpenAlex Mapper tool provides an intuitive interface for scholarly exploration, accessible online at https://m7n-openalex-mapper.hf.space and via a link at maxnoichl.eu/talk. Users commence by executing a desired search on the OpenAlex website—for instance, for “scale-free network models”. They then copy the URL generated by this OpenAlex search and paste it into the designated input field within OpenAlex Mapper before initiating the query.\nUpon execution, the tool’s backend swiftly processes the request. It downloads an initial subset of records from the OpenAlex search—typically the first 1,000—a limitation implemented to manage processing time efficiently. Subsequently, it embeds the abstracts of these retrieved records. Should the user enable the relevant option, the system additionally processes the citation graph associated with these records. Users can adjust various settings, including:\n\nVisualising temporal distributions\nOverlaying citation graphs\nReducing sample sizes\nChoosing sample selection methods\nColouring data points by publication date\nExporting data as CSV or PNG files\n\nThe result manifests as a compelling visualisation where the search results are projected onto a grey base-map, enabling users to discern the disciplinary clusters where their queried terms, authors, or papers appear. Crucially, this map offers full interactivity. This interactivity permits deeper investigation into individual data points, facilitating an understanding of contextual nuances—for instance, why a paper discussing “coriander” might surface within fields such as epidemiology or public health. An illustrative example, employing “scale-free network models”, effectively demonstrated this functionality. For more demanding analyses, an alternative version of the tool, leveraging a higher-latency GPU setup for larger queries, was also noted as being temporarily available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#utility-in-hpss-and-research-applications",
    "href": "chapter_ai-nepi_004.html#utility-in-hpss-and-research-applications",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Utility in HPSS and Research Applications",
    "text": "4.4 Utility in HPSS and Research Applications\n\n\n\nSlide 13\n\n\nOpenAlex Mapper offers profound utility for the History and Philosophy of Science and Scholarship (HPSS), particularly in addressing pervasive methodological challenges. It endeavours to mitigate problems associated with small sample sizes and the inherent difficulty of generalising findings from specific case studies. The tool seamlessly facilitates the connection between detailed, “close-up” understandings of scientific processes—often derived from methods such as close reading or direct interaction with scientists—and the broader, large-scale dynamics of contemporary global science. Researchers can employ it to investigate questions such as the actual disciplinary impact and spread of a specific model, for instance, the Hopfield Model, by tracing its origins, its journey beyond its initial context, and its eventual adoption and usage across diverse scientific fields.\nThe fundamental methodological aim involves leveraging robust quantitative methods, operating discreetly in the background, to underpin and profoundly enhance what are ultimately qualitative, heuristic investigations. A pivotal feature supporting this aim is the tool’s capacity to allow users to trace any visualised data point directly back to its original textual source, thereby ensuring a constant, verifiable link to the primary evidence.\nSeveral ongoing research applications compellingly demonstrate the tool’s expansive potential. Firstly, within the study of “model templates”, OpenAlex Mapper assists in conceptualising how models possessing remarkably similar underlying structures arise and are utilised across diverse scientific domains. This exploration considers how such templates might structure scientific knowledge in ways orthogonal to conventional disciplinary boundaries; an example visualisation depicted three distinct model templates appearing in specific, non-contiguous areas of the scientific map. Secondly, the tool proves invaluable for mapping the interdisciplinary presence of scientific concepts. For instance, researchers have mapped the concept of “phase transition” against that of “emergence”, vividly illustrating their respective distributions. This capability significantly broadens the investigation of concepts into interdisciplinary territories where obtaining comprehensive datasets often proves problematic.\nFinally, OpenAlex Mapper proves exceptionally valuable for analysing the distribution of scientific methods. It facilitates inquiry into where specific methods find application within diverse interdisciplinary contexts. An illustrative study examined the ongoing discussion concerning the roles of machine learning techniques versus more classical statistical methods in scientific discovery. By comparing the distribution of the Random Forest model—a prominent machine learning technique—with that of Logistic Regression—a classical statistical method—researchers observed quite distinguishable patterns of usage across different disciplines. Such findings prompt deeper philosophical questions; for example, they invite exploration into why neuroscientists might frequently employ random forests, whilst researchers in thematically proximate fields such as psychiatry or mental health research might predominantly rely on logistic regressions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#qualifications-and-limitations",
    "href": "chapter_ai-nepi_004.html#qualifications-and-limitations",
    "title": "4  Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper",
    "section": "4.4 Qualifications and Limitations",
    "text": "4.4 Qualifications and Limitations\n\n\n\nSlide 21\n\n\nResearchers acknowledge several inherent qualifications and limitations within the OpenAlex Mapper. Firstly, the tool’s performance is intrinsically linked to the OpenAlex database itself. Whilst OpenAlex constitutes a valuable resource, its data is not entirely flawless; users must bear in mind that data quality, though generally deemed reasonable and comparable to other major databases, is not absolute.\nSecondly, the current implementation relies exclusively on an English-only language model. This inherently restricts the scope of analyses, although for studies concentrating on the more recent history of science, this limitation may prove less critical. In principle, employing multilingual models could address this constraint, yet the availability of high-quality, science-trained multilingual models currently presents a significant challenge.\nThirdly, the embedding process necessitates that sources possess either abstracts or sufficiently descriptive titles. Consequently, records lacking such information cannot be effectively processed by the current methodology.\nFurthermore, the methodology heavily depends upon the Uniform Manifold Approximation and Projection (UMAP) algorithm, which itself presents certain issues. UMAP operates as a stochastic algorithm, implying that any single output map represents merely one of many potential configurations. More significantly, the algorithm must make inherent trade-offs when reducing the high-dimensional data from the Specter language model—which comprises several hundred and sixty-eight dimensions—into just two dimensions for visualisation. This dimensional reduction inevitably introduces some degree of distortion, or ‘pushing and pulling and misaligning’, to fit the complex data into a lower-dimensional space.\nFor individuals seeking more in-depth technical explanations, the working paper Philosophy at Scale: Introducing OpenAlex Mapper is readily available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-core-dataset",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-core-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 The ActDisease Project: Core Dataset",
    "text": "5.1 The ActDisease Project: Core Dataset\n\n\n\nSlide 01\n\n\nResearchers at Uppsala University spearhead the ActDisease project, an ERC-funded initiative (ERC-2021-STG, 10104099) titled “Acting out Disease - How Patient Organizations Shaped Modern Medicine.” This project meticulously investigates the influence of patient organisations on the evolution of disease concepts, illness experiences, and medical practices across 20th-century Europe. Its focus encompasses ten patient organisations from Sweden, Germany, France, and Great Britain, examining their activities and impact from approximately 1890 to 1990. The primary source materials for this historical inquiry are the periodicals, chiefly magazines, issued by these organisations; an early example of such an entity is the Hay Fever Association of Heligoland, established in Germany in 1897.\nThis research centres on the ActDisease dataset, a private, recently digitised collection of these patient organisation magazines, amounting to a substantial 96,186 pages. This corpus spans materials from Germany, Sweden, France, and the United Kingdom, addressing a range of conditions including Allergy/Asthma, Diabetes, Multiple Sclerosis, Lung Diseases, and Rheumatism/Paralysis. The temporal coverage varies among the individual magazines—such as the “BRA Review” and “Allergia”—but generally extends from the early 1900s to 1990 or 1991. For instance, German materials on diabetes comprise 19,324 pages from 1931-1990, whilst Swedish texts on lung diseases cover 16,790 pages from 1938-1991.\nResearchers digitised this extensive collection using ABBYY FineReader Server 14 for Optical Character Recognition (OCR). Whilst this tool demonstrated good performance on common layouts and fonts, significant challenges emerged with more complex page designs, slanted text, rare typefaces, and inconsistent scan or photograph quality. Consequently, OCR errors persist, especially within German and French texts, alongside issues of disrupted reading order. To address these imperfections, investigators have explored post-OCR correction techniques for historical German periodicals using instruction-tuned generative models, a method detailed by Danilova and Aangenendt (RESOURCEFUL-2025, ACL). Notably, OCR inaccuracies are particularly prevalent in creatively formatted texts like advertisements, humour sections, and poetry.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#textual-genres-rationale-definition-and-annotation",
    "href": "chapter_ai-nepi_005.html#textual-genres-rationale-definition-and-annotation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 Textual Genres: Rationale, Definition, and Annotation",
    "text": "5.2 Textual Genres: Rationale, Definition, and Annotation\n\n\n\nSlide 11\n\n\nA pressing need for genre classification emerged from the inherent nature of the ActDisease dataset. Initial examinations revealed that whilst the patient organisation magazines contained a diverse array of text types, these types were remarkably consistent across different publications. A significant challenge arises from the common co-occurrence of multiple genres—such as administrative reports, advertisements, and humour sections—within the confines of a single page. Conventional analytical techniques, like yearly or decade-based topic modelling and term counts, fail to account for this granular diversity, leading to a likely bias in results towards the most prevalent text types. Consequently, the concept of genre, defined in language technology as a class of documents sharing a communicative purpose (Petrenz, 2004; Kessler, 1997), proved invaluable. Genre classification directly supports the project’s central aim: to explore the source material from multifaceted perspectives to construct robust historical arguments. Specifically, it enables the comparative study of communicative strategies over time and across various countries, diseases, and publications (Broersma, 2010), and facilitates more precise, fine-grained analyses of term distributions and topic models within distinct genre categories. Examples of genres found include poetry, academic reports (such as studies on the pancreas), legal documents like deeds of covenant, advertisements (for instance, for diabetic chocolate), instructive content like recipes or medical advice, patient organisation reports detailing meetings and activities, and narratives recounting patient experiences.\nTo structure this classification, researchers defined a set of genre labels under the close supervision of the project’s lead historian, an expert on patient organisations. These labels aim to segregate content effectively for subsequent historical analysis and were designed to be as general-purpose as possible to allow for potential application to similar datasets. The established genres include:\n\nAcademic: research-based reports, conveying scientific information to the magazine’s audience\nAdministrative: documents on organisational activities, reporting on events\nAdvertisement: commercial promotions\nGuide: instructional content\nFiction: entertaining texts\nLegal: documents explaining legal terms\nNews: reports on recent events\nNonfiction Prose: narratives of real events or cultural topics\nQA: question-and-answer sections\n\nThe annotation process focused on paragraphs, defined as units from the ABBYY OCR output merged according to font patterns (type, size, italic) on a per-page basis. Annotators sampled content from two specific periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” concentrating on the first and mid-year issues of each year. A team comprising four historians and two computational linguists, all possessing native or proficient skills in Swedish and German, undertook the annotation. Each paragraph received two independent annotations, resulting in a high average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha. Annotators utilised tools like a .numbers spreadsheet, where they made direct, or ‘hard’, assignments of genres to the text segments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-genre-classification-experiments-and-evaluation",
    "href": "chapter_ai-nepi_005.html#zero-shot-genre-classification-experiments-and-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Zero-Shot Genre Classification: Experiments and Evaluation",
    "text": "5.3 Zero-Shot Genre Classification: Experiments and Evaluation\n\n\n\nSlide 04\n\n\nGiven the limited availability of annotated data within the ActDisease project, researchers strategically explored zero-shot and few-shot learning techniques for genre classification. For the zero-shot approach, researchers focused on key questions: the feasibility of mapping genre labels from existing public datasets to the project’s custom labels, and how classification performance would vary across these datasets and different computational models. Danilova and Söderfeldt (LaTeCH-CLFL 2025, ACL) document the detailed methodology and findings of these experiments. The project’s annotated data, consisting of 1182 training paragraphs and 552 held-out paragraphs (approximately 30% of the total, stratified by label), provided the evaluative foundation; the entire held-out set served as the test set for zero-shot experiments. An initial analysis of this data revealed a strong imbalance in the distribution of ‘Advertisement’ and ‘Non-fictional prose’ genres across the German and Swedish language samples.\nTo facilitate zero-shot learning, investigators leveraged several publicly available datasets:\n\nThe Corpus of Online Registers of English (CORE) (Egbert et al., 2015): This dataset offers document-level annotations in English with some categories in Swedish, Finnish, and French.\nThe Functional Text Dimensions (FTD) dataset (Sharoff, 2018): Providing balanced English and Russian web genres, this dataset is also annotated at the document level and was previously employed by Kuzman et al. (2023).\nUD-MULTIGENRE (UDM): A subset of Universal Dependencies (de Marneffe et al., 2021), this dataset features sentence-level genre annotations across 38 languages, as recovered by Danilova and Stymne (2023).\n\nCrucially, a preparatory step involved mapping genres between these public datasets and the ActDisease schema. Two annotators independently conducted this mapping, with only fully agreed-upon assignments adopted for the final schema. Nevertheless, some ActDisease genres did not find direct equivalents in the external datasets. For instance, the ActDisease ‘Academic’ genre mapped to ‘research article’ in CORE, ‘academic’ in UDM, and ‘academic (A14)’ in FTD.\nCreating training data for zero-shot experiments involved a systematic pipeline encompassing genre mapping, preprocessing (which included the removal of web addresses, emails, XML tags, and emojis), text chunking, and strategic sampling. Sampling configurations varied by language focus ([G+] for Germanic languages only, [G-] for all language families) and balancing strategy ([B1] balancing by ActDisease labels, [B2] balancing by both ActDisease and original dataset labels). This process yielded four distinct training samples from each source (FTD, CORE, UDM, and a merged set). Researchers selected three multilingual encoder models for these experiments: XLM-Roberta (Conneau et al., 2020), noted as a state-of-the-art web genre classifier by Kuzman et al. (2023); mBERT (Devlin et al., 2019), included for comparative purposes; and historical mBERT (hmBERT) (Schweter et al., 2022), which is pretrained on a substantial corpus of multilingual historical newspapers, including German and Swedish. These BERT-like models have a strong precedent in web register and genre classification (Lepekhin and Sharoff, 2022; Kuzman and Ljubešić, 2023; Laippala et al., 2023). Fine-tuning these three models on all dataset configurations resulted in a total of 48 distinct fine-tuned models, with reported metrics typically averaged across these configurations.\nEvaluating zero-shot predictions acknowledged the challenge of imperfect label set overlap between datasets, precluding direct comparison of overall performance metrics. Instead, analyses focused on per-genre performance and the examination of confusion matrices. The X-GENRE web genre classifier (Kuzman et al., 2023) served as a baseline, with evaluations considering predictions on the most similar, directly mappable labels. These experiments inherently involved cross-lingual scenarios, fully for FTD and X-GENRE (which lack German or Swedish data) and partially for UDM and CORE.\nOverall, models fine-tuned on the FTD dataset, using the ActDisease genre mapping, consistently demonstrated superior performance, generally avoiding systematic biases and achieving commendable per-genre metrics. In contrast, models trained on UDM and CORE exhibited certain class-specific biases: UDM-trained models tended to favour ‘news’ predictions, likely because its ‘news’ training data contained the largest proportion of Germanic instances (primarily German), whilst CORE-trained models showed a bias towards ‘guide’, as ‘guide’ was the only multilingual training category in CORE. Interestingly, specific model-dataset combinations revealed particular strengths: XLM-Roberta fine-tuned on UDM achieved, on average, 32% more correct predictions for the QA genre compared to mBERT and hmBERT. Conversely, hmBERT fine-tuned on UDM yielded 16% more correct predictions for the Administrative genre than its counterparts. Furthermore, models based on the CORE dataset performed well in identifying the Legal genre. Analysis of F1 scores and the impact of training configurations indicated that for FTD, balancing by original labels (B2) or restricting to Germanic languages (G+) diminished performance, whereas for UDM, including diverse language families and applying balancing generally enhanced macro F1 scores.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-with-multilingual-encoders",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-with-multilingual-encoders",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Few-Shot Learning with Multilingual Encoders",
    "text": "5.4 Few-Shot Learning with Multilingual Encoders\n\n\n\nSlide 13\n\n\nFurther investigation explored genre classification capabilities using few-shot learning techniques with the ActDisease dataset. This phase addressed central questions: how classification performance would scale with varying training set sizes across different models, and whether prior fine-tuning of these models on a Masked Language Modelling (MLM) task using the full dataset would yield substantial performance enhancements. For these experiments, researchers created six distinct training set sizes: 100, 200, 300, 400, 500, and the full 1182 paragraphs. All were randomly sampled from the main training pool and balanced by genre label. The initial 552-paragraph held-out set was then equally divided into validation and test sets, also balanced by label. Notably, the ‘legal’ and ‘news’ genres were excluded from these particular few-shot training runs due to an insufficient number of instances in the smaller sampled sets.\nResults unequivocally indicated that additional training on the ActDisease dataset, particularly when preceded by MLM fine-tuning (indicated by the “-MLM” suffix), offered a distinct advantage. As anticipated, F1 scores generally improved with increasing numbers of training instances, although even with the largest training set of 1182 paragraphs, these scores typically remained below the 0.8 threshold. Amongst the models tested, hmBERT-MLM emerged as the top performer, albeit by a small margin.\nCloser examination of per-category F1 scores and overall metrics revealed that hmBERT-MLM’s slight superiority stemmed largely from its consistent ability to differentiate between ‘fiction’ and ‘nonfiction prose’, even when trained on the full dataset. In contrast, other models, especially XLM-Roberta, exhibited a significant decline in performance on the ‘fiction’ category when utilising the complete training data. Analysis of the XLM-Roberta-MLM confusion matrix for the full-sized training dataset showed a frequent misclassification of ‘fiction’ instances as ‘nonfiction prose’. This confusion, investigators hypothesise, arises because both genres, within the specific context of patient organisation magazines focused on diabetes, often feature narratives about patient experiences. Such texts are likely to share thematic elements and narrative structures, and this similarity might become more pronounced with larger amounts of training data, thereby complicating distinction. This suggests a need for more data or more sophisticated methods to better separate these closely related genres. The benefits of MLM pre-fine-tuning were significant across models, with hmBERT-MLM showing a 24% improvement, compared to 14.5% for mBERT-MLM and 16.9% for XLM-RoBERTa-MLM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-llama-3.1-8b-instruct-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-llama-3.1-8b-instruct-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Few-Shot Prompting: Llama-3.1 8b Instruct Evaluation",
    "text": "5.5 Few-Shot Prompting: Llama-3.1 8b Instruct Evaluation\n\n\n\nSlide 39\n\n\nRecognising the current paucity of annotated data for comprehensive instruction tuning, investigators explored few-shot prompting with a prominent open-weights multilingual generative model, Llama-3.1 8b Instruct. This approach involved crafting prompts that furnished the model with genre definitions alongside two to three carefully selected examples for each defined genre. The model’s predictive performance then underwent assessment using the entire held-out portion of the ActDisease dataset, which also served as the test set for zero-shot experiments.\nThe Llama-3.1 8b Instruct model demonstrated a notable capacity to handle certain genre labels reasonably well under this few-shot prompting regime. For instance, it achieved an F1-score of 0.84 for ‘legal’ texts, 0.73 for ‘advertisement’, and 0.72 for ‘academic’ content. Other F1-scores included 0.64 for ‘fiction’, 0.62 for ‘QA’, 0.61 for ‘guide’, and 0.60 for ‘administrative’. Performance was notably lower for ‘nonfictional prose’ (0.49) and ‘news’ (0.08). The overall accuracy reached 0.62, with a macro average F1 of 0.59 and a weighted average F1 of 0.63. These results suggest that whilst few-shot prompting can yield decent quality, the provision of only two or three examples per genre is not adequate for the model to fully capture the characteristics of more diverse or subtly defined categories like ‘nonfictional prose’, ‘advertisement’, and ‘administrative’.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusions-and-future-directions",
    "href": "chapter_ai-nepi_005.html#conclusions-and-future-directions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Conclusions and Future Directions",
    "text": "5.6 Conclusions and Future Directions\n\n\n\nSlide 39\n\n\nThis investigation into genre classification for historical medical periodicals yields several salient conclusions. Popular magazines, rich historical sources for understanding the history of science, present unique text mining challenges due to their inherent multiplicity of genres. These genres are not arbitrary; rather, they reflect deliberate communicative strategies employed by their creators. Consequently, accounting for these varied genres is paramount for achieving an accurate and detailed interpretation of any text mining results derived from such materials. Genre classification emerges as a powerful tool to render these complex sources more accessible and amenable to computational analysis.\nResearchers confronting a scarcity of domain-specific training data have several viable pathways. In scenarios with no training data, one can successfully leverage existing modern datasets, provided the target genre categories are sufficiently general-purpose. Alternatively, employing few-shot instruction prompting with a competent open generative model can also yield decent classification quality. However, when some annotated data is available, few-shot learning with multilingual encoders—such as XLM-Roberta or, notably, historical multilingual BERT (hmBERT)—particularly when augmented with prior Masked Language Model (MLM) fine-tuning, offers a superior approach. Indeed, this latter strategy proved to be the most effective for the ActDisease project. The historical multilingual BERT model, in particular, benefited substantially from MLM fine-tuning, exhibiting a 24% performance gain, which surpassed the improvements seen with mBERT-MLM (14.5%) and XLM-RoBERTa-MLM (16.9%).\nLooking ahead, the research team actively pursues several avenues to enhance and expand upon this work. Current efforts involve applying these methods to address specific historical hypotheses, developing a new, more fine-grained annotation scheme for genres, and undertaking a further annotation project funded by Swe-CLARIN. Investigators are also exploring the potential of synthetic data generation and implementing active learning strategies. These ongoing activities aim to continually improve the quality and utility of genre classification methodologies, both for the ActDisease project’s objectives and for the broader research community working with similar historical textual data.\nThe project acknowledges the contributions of:\n\nits annotation team (Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, Gijs Aangenendt)\nthe European Research Council for funding\nthe Centre for Digital Humanities and Social Sciences for resources\nits reviewers",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#overview",
    "href": "chapter_ai-nepi_006.html#overview",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "The VERITRACE project, an ambitious five-year (2023-2028) European Research Council (ERC) Starting Grant initiative, operates from Vrije Universiteit Brussel under the leadership of Professor Cornelis J. Schilt. A team of five researchers, including historians and a classicist, investigates the profound influence of an early modern 'ancient wisdom' tradition on the evolution of natural philosophy and science. This tradition encompasses seminal texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Employing advanced computational humanities methodologies, the project analyses an extensive, multilingual corpus comprising approximately 430,000 printed texts. These texts, published between 1540 and 1728, originate from significant digital repositories including Early English Books Online (EEBO), Gallica, and the Bavarian State Library. Core objectives involve large-scale multilingual exploration, the identification of both direct (lexical) and indirect (semantic) textual reuse, and the meticulous uncovering of intricate networks connecting texts, authors, and prevalent themes. The project confronts several challenges, notably the variable quality of Optical Character Recognition (OCR) in source materials, the complexities of early modern typography and semantics across at least six languages, and the sheer volume of data. To address these, researchers utilise Large Language Models (LLMs): GPT-based models serve as 'LLMs-as-Judges' for metadata enrichment, whilst BERT-based models, such as LaBSE, generate vector embeddings for semantic encoding. An alpha version of the VERITRACE web application, built upon an Elasticsearch backend, offers tools for corpus exploration, metadata searching, text reading via a Mirador viewer, and a sophisticated text-matching component designed for lexical, semantic, and hybrid analyses. The research details a comprehensive data processing pipeline, presents sanity checks for the matching tool using Newton's *Opticks* (comparing Latin and English editions), and candidly discusses future hurdles. These include optimal model selection (considering alternatives like XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT), the necessity of fine-tuning models, strategies for handling semantic drift over time, persistent OCR quality issues, and the critical need for scalable solutions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-introduction-and-team-composition",
    "href": "chapter_ai-nepi_006.html#project-introduction-and-team-composition",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 Project Introduction and Team Composition",
    "text": "6.1 Project Introduction and Team Composition\n\n\n\nSlide 02\n\n\n    Dr. Jeffrey Wolf introduced the VERITRACE project, formally titled \"Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy.\" A team of five, including the Principal Investigator Prof. Dr. Cornelis J. Schilt, a classicist, and historians, spearheads this research from its base in Brussels at Vrije Universiteit Brussel. Dr. Wolf, a historian of science and medicine whose own research specialisation in the 18th century begins where the project's timeframe concludes, manages the digital humanities aspects of VERITRACE. The European Research Council supports this endeavour through Starting Grant number 101076836, and further information is available via the project website, HTTPS://VERITRACE.EU. The core team comprises Prof. Dr. Cornelis J. Schilt, Dr. Eszter Kovács, Dr. Jeffrey Wolf, Niccolò Cantoni, and Demetrios Paraschos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-project-scope-and-guiding-objectives",
    "href": "chapter_ai-nepi_006.html#veritrace-project-scope-and-guiding-objectives",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 VERITRACE Project Scope and Guiding Objectives",
    "text": "6.2 VERITRACE Project Scope and Guiding Objectives\n\n\n\nSlide 01\n\n\n    Researchers at Vrije Universiteit Brussel embark on a five-year (2023-2028) ERC Starting Grant project, \"Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,\" led by Research Professor Cornelis J. Schilt. The project's central aim involves tracing the pervasive influence of an early modern 'ancient wisdom' tradition on the trajectory of natural science. This tradition manifests in key texts including the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for historians of chemistry, the Corpus Hermeticum. A core corpus of 140 works, selected for their representation of this tradition, forms the basis for close reading. Beyond established connections, such as Newton's reading of the Sibylline Oracles and Kepler's knowledge of the Corpus Hermeticum, the project strives to unearth a much broader network of texts and authors. These often overlooked works, termed the 'great Unread' by one scholar, represent a significant, yet under-explored, dimension of early modern intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-methodologies-in-historical-philosophy-and-science-of-science-hpss",
    "href": "chapter_ai-nepi_006.html#computational-methodologies-in-historical-philosophy-and-science-of-science-hpss",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Methodologies in Historical Philosophy and Science of Science (HPSS)",
    "text": "6.3 Computational Methodologies in Historical Philosophy and Science of Science (HPSS)\n\n\n\nSlide 04\n\n\n    To investigate its research questions, the VERITRACE team employs large-scale multilingual exploration techniques within the framework of computational Historical Philosophy and Science of Science (HPSS). Keyword search tools facilitate initial investigations. A primary objective focuses on identifying textual reuse across the vast, multilingual corpus; this includes both direct lexical quotations, which may be uncited, and more subtle indirect semantic borrowings, such as paraphrases recognisable to contemporary readers. In essence, the project aims to construct what one might term an \"Early Modern Plagiarism Detector.\" Furthermore, researchers seek to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through these computational approaches, the project anticipates revealing new patterns in the intellectual history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#characteristics-of-the-data-set-and-analytical-approaches",
    "href": "chapter_ai-nepi_006.html#characteristics-of-the-data-set-and-analytical-approaches",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Characteristics of the Data Set and Analytical Approaches",
    "text": "6.4 Characteristics of the Data Set and Analytical Approaches\n\n\n\nSlide 04\n\n\n    The project's empirical foundation rests upon a large, diverse, and multilingual data set composed entirely of digital texts from printed works; handwritten materials fall outside its scope. Researchers have aggregated content from three primary multilingual data sources, covering at least six languages and a period of roughly 200 years, from 1540 to 1728—a timeframe concluding just after Newton's death. These sources include Early English Books Online (EEBO), Gallica (from the French National Library), and, most substantially, the Bavarian State Library. In total, the corpus comprises approximately 430,000 texts. To analyse this extensive collection, the team will utilise a range of state-of-the-art digital techniques, such as Keyword Search, Text Matching, Topic Modelling, and Sentiment Analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-project-challenges-and-the-strategic-use-of-large-language-models",
    "href": "chapter_ai-nepi_006.html#core-project-challenges-and-the-strategic-use-of-large-language-models",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Core Project Challenges and the Strategic Use of Large Language Models",
    "text": "6.5 Core Project Challenges and the Strategic Use of Large Language Models\n\n\n\nSlide 06\n\n\n    Several core challenges confront the VERITRACE project. Firstly, the team contends with variable Optical Character Recognition (OCR) quality in texts provided directly by libraries in raw formats like XML, HOCR, and HTML, crucially without access to ground truth page images, which complicates downstream processing. Secondly, the inherent complexities of early modern typography and semantics, present across at least six different languages, demand sophisticated handling. Thirdly, the sheer volume of data—hundreds of thousands of texts printed across Europe over two centuries—poses significant logistical and computational challenges. To navigate these, researchers strategically employ Large Language Models (LLMs). On the decoder-side, GPT-based LLMs function as \"LLMs-as-Judges\" to aid in enriching and cleaning metadata. The presentation, however, focuses on encoder-side applications, where BERT-based LLMs generate embeddings to capture the semantic meaning of sentences and passages within the extensive textual corpus.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-an-overview",
    "href": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-an-overview",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 Metadata Enrichment via LLMs-as-Judges: An Overview",
    "text": "6.6 Metadata Enrichment via LLMs-as-Judges: An Overview\n\n\n\nSlide 08\n\n\n    Researchers are exploring the use of LLMs-as-Judges to enrich VERITRACE metadata, a task detailed as a case study but not fully elaborated upon in this segment. The basic motivation involves using the Universal Short Title Catalogue (USTC), a source of high-quality metadata, to enhance the project's own records. By mapping USTC entries to VERITRACE records, the team aims to produce 'enriched' metadata, thereby reducing the need for subsequent cleaning. However, automating this mapping proves difficult for most records, particularly as the VERITRACE data has not yet undergone comprehensive cleaning and often lacks common external identifiers. Consequently, LLMs are tasked with the laborious process of comparing numerous pairs of bibliographic metadata records—a task that involved team members manually reviewing 10,000 pairs each—to ascertain if they correspond to the same printed text.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llm-prompting-strategies-and-output-formats-for-metadata-matching-an-overview",
    "href": "chapter_ai-nepi_006.html#llm-prompting-strategies-and-output-formats-for-metadata-matching-an-overview",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 LLM Prompting Strategies and Output Formats for Metadata Matching: An Overview",
    "text": "6.7 LLM Prompting Strategies and Output Formats for Metadata Matching: An Overview\n\n\n\nSlide 07\n\n\n    To manage the LLM-based metadata matching, the project employs a panel, or \"bench,\" of LLMs acting as judges. These models receive extensive prompt guidelines, which detail \"FIELD PRIORITIES,\" \"MATCH CRITERIA,\" and \"NON-MATCH INDICATORS\" to steer their decision-making. The desired output from the LLMs is structured to include not only their decisions on whether records match but also the key factors influencing these decisions and, importantly, their reasoning. This reasoning aims to articulate why a given pair of records is, or is not, deemed to represent the same underlying printed text.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#ongoing-challenges-in-llm-driven-metadata-matching-an-overview",
    "href": "chapter_ai-nepi_006.html#ongoing-challenges-in-llm-driven-metadata-matching-an-overview",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Ongoing Challenges in LLM-driven Metadata Matching: An Overview",
    "text": "6.8 Ongoing Challenges in LLM-driven Metadata Matching: An Overview\n\n\n\nSlide 08\n\n\n    The development of LLMs-as-Judges for metadata enrichment remains a work in progress, with significant challenges. A major current obstacle is the occurrence of hallucinations in the output from the open-source LLMs (such as Llama), where models invent records not part of the input. Furthermore, efforts to obtain more structured output, aimed at curbing overly verbose or unhelpful responses, can paradoxically lead to more 'generic' and less useful feedback, especially diminishing the quality of the reasoning provided. Achieving the right balance in this process appears to be more an art than a science. Nevertheless, the team believes this approach has considerable potential to save substantial time, and they actively seek advice on overcoming these hurdles.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#introducing-the-veritrace-web-application-alpha-version",
    "href": "chapter_ai-nepi_006.html#introducing-the-veritrace-web-application-alpha-version",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Introducing the VERITRACE Web Application: Alpha Version",
    "text": "6.9 Introducing the VERITRACE Web Application: Alpha Version\n\n\n\nSlide 13\n\n\n    An alpha version of the VERITRACE web application has been developed, though it is not yet publicly accessible and currently operates on the presenter's local machine. This marks its inaugural public reveal, even preceding internal team dissemination, and it serves as a \"promise\" of the project's envisioned functionalities. Currently, researchers are testing a BERT-based LLM, LaBSE (Language-Agnostic BERT Sentence Embedding), for its capacity to generate vector embeddings for all textual passages. However, a preliminary assessment suggests LaBSE may not prove adequate for the task's demands. The application's features were demonstrated via screenshots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#a-glimpse-into-the-data-processing-pipeline",
    "href": "chapter_ai-nepi_006.html#a-glimpse-into-the-data-processing-pipeline",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 A Glimpse into the Data Processing Pipeline",
    "text": "6.10 A Glimpse into the Data Processing Pipeline\n\n\n\nSlide 14\n\n\n    Transforming raw textual data from formats like XML, HOCR, and HTML into the Elasticsearch database that underpins the web application requires a substantial data processing effort. This involves a complex 15-stage pipeline, with each stage demanding optimisation. Although not detailed exhaustively, key stages include extracting text into files, generating positional mappings, segmenting text, and assessing OCR quality. The generation of vector embeddings occurs near the conclusion of this pipeline. A \"VERITRACE Pipeline Dashboard\" allows for monitoring overall progress and the status of individual stages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-corpus-exploration-interface",
    "href": "chapter_ai-nepi_006.html#web-application-feature-corpus-exploration-interface",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Web Application Feature: Corpus Exploration Interface",
    "text": "6.11 Web Application Feature: Corpus Exploration Interface\n\n\n\nSlide 15\n\n\n    The VERITRACE web application, organised into roughly five main sections, includes an \"Explore\" feature designed to provide users with statistical overviews of the corpus. These statistics, derived from a MongoDB database, currently reflect 427,395 metadata records. Users can visualise various metrics, including the total document count, language distribution, document distribution by data source, document counts per decade, and the geographical spread of publication places, presented through charts and graphs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-elasticsearch-metadata-explorer",
    "href": "chapter_ai-nepi_006.html#web-application-feature-elasticsearch-metadata-explorer",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Web Application Feature: Elasticsearch Metadata Explorer",
    "text": "6.12 Web Application Feature: Elasticsearch Metadata Explorer\n\n\n\nSlide 16\n\n\n    Within the web application, an \"Elasticsearch Metadata Explorer\" allows users to delve into the rich metadata compiled for each text. This interface presents sample records and, upon selection, displays detailed information categorised into Bibliographic Metadata, Language Information, and OCR Information. A notable feature is the language identification process, which analyses every text down to segments of about 50 characters. This granularity enables the detection of multiple languages within a single work—one example showed a text comprising 84.7% Latin and 15.22% Ancient Greek—allowing for texts to be tagged as \"substantively multilingual.\" The system also attempts to assess OCR quality on a page-by-page basis using the raw text, a challenging task without ground truth images; one illustrated record was categorised as \"Non-satisfactory.\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-advanced-search-capabilities",
    "href": "chapter_ai-nepi_006.html#web-application-feature-advanced-search-capabilities",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.13 Web Application Feature: Advanced Search Capabilities",
    "text": "6.13 Web Application Feature: Advanced Search Capabilities\n\n\n\nSlide 17\n\n\n    The \"Search\" section of the web application, powered by an Elasticsearch backend, provides robust keyword search functionalities and is expected to be a primary tool for scholars. Currently, testing occurs on a prototype corpus of 132 files; even this small subset generates a 15-gigabyte index, indicating that the full 430,000-text corpus will necessitate terabytes of storage. A simple keyword search for \"hermes\" in the prototype located 22 documents with 332 total matches. Beyond basic searches, Elasticsearch enables more sophisticated queries. Users can perform field-specific searches (e.g., for an author and a keyword, such as \"author:kepler 'hermes'\"), construct complex queries using Boolean operators and nesting, and execute proximity searches (e.g., finding terms like \"Hermes\" and \"Plato\" within a specified word distance). The interface displays corpus results, segment counts, index size, and a summary of results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-planned-analytical-tools-future-implementation",
    "href": "chapter_ai-nepi_006.html#web-application-feature-planned-analytical-tools-future-implementation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.14 Web Application Feature: Planned Analytical Tools (Future Implementation)",
    "text": "6.14 Web Application Feature: Planned Analytical Tools (Future Implementation)\n\n\n\nSlide 14\n\n\n    An \"Analyse\" section is planned for the VERITRACE web application, although its functionalities are yet to be implemented. This section intends to offer users advanced analytical tools. Specifically, the project aims to incorporate Topic Modeling, Latent Semantic Analysis (LSA), and various forms of Diachronic Analysis, enabling scholars to explore textual data in more nuanced ways. The interface design for this section includes distinct selection options for these future capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-integrated-text-reading-environment",
    "href": "chapter_ai-nepi_006.html#web-application-feature-integrated-text-reading-environment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.15 Web Application Feature: Integrated Text Reading Environment",
    "text": "6.15 Web Application Feature: Integrated Text Reading Environment\n\n\n\nSlide 19\n\n\n    To facilitate direct engagement with the source materials, the web application includes a \"Read\" section. Here, scholars can access and read the texts, typically presented as PDFs, using an integrated Mirador viewer. This feature aims to provide a user experience comparable to that of online library interfaces. Alongside the document viewer, the system displays pertinent metadata, including source information, citation details, preferred titles, and creator information, enhancing the contextual understanding of each text.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-feature-the-veritrace-textual-reuse-matching-tool",
    "href": "chapter_ai-nepi_006.html#web-application-feature-the-veritrace-textual-reuse-matching-tool",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.16 Web Application Feature: The VERITRACE Textual Reuse Matching Tool",
    "text": "6.16 Web Application Feature: The VERITRACE Textual Reuse Matching Tool\n\n\n\nSlide 20\n\n\n    A central component of the web application is the \"Match\" section, which houses the VERITRACE Match Tool, engineered to help researchers find textual reuse. This tool evaluates both lexical (direct word usage) and semantic (conceptual similarity) matches between texts. Users can compare a single document to another, analyse multiple documents (for instance, all of Kepler's works against each other), or, ambitiously, compare one text against the entire corpus—a feature acknowledged as computationally intensive. Crucially, the interface allows users to adjust various parameters, such as the minimum similarity score, to refine their searches. The \"Match Options Panel\" provides distinct controls for lexical, semantic, hybrid, and corpus-wide matching, exemplified by a case study comparing Latin and English versions of Newton's *Opticks*.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-sanity-check-cross-language-lexical-matching",
    "href": "chapter_ai-nepi_006.html#text-matching-sanity-check-cross-language-lexical-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.17 Text Matching Sanity Check: Cross-Language Lexical Matching",
    "text": "6.17 Text Matching Sanity Check: Cross-Language Lexical Matching\n\n\n\nSlide 16\n\n\n    The VERITRACE Match Tool offers several approaches to textual comparison, including lexical matching (keyword-based), semantic matching (vector embedding-based for conceptual similarity), and hybrid methods. Users can also select different matching modes, such as Standard, Comprehensive (more resource-intensive), or Faster. A sanity check involved performing a lexical match between Newton's Latin *Opticks* (1719) and the English edition (1718). As anticipated, due to the different languages, the standard mode found no significant matches. Interestingly, the comprehensive mode did identify three matches, indicating that the Latin edition likely contains some English text, possibly in its prefatory material. This test confirms that the lexical matching functions logically for cross-language comparisons.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-sanity-check-lexical-self-comparison-and-summary-metrics",
    "href": "chapter_ai-nepi_006.html#text-matching-sanity-check-lexical-self-comparison-and-summary-metrics",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.18 Text Matching Sanity Check: Lexical Self-Comparison and Summary Metrics",
    "text": "6.18 Text Matching Sanity Check: Lexical Self-Comparison and Summary Metrics\n\n\n\nSlide 19\n\n\n    To further validate the lexical matching, a text—Newton's 1718 English *Opticks*—was compared against itself. The \"Match Summary\" provides key metrics such as a Quality Score (average similarity of matches), a Coverage Score (proportion of text matched), and the total number of comparisons performed (nearly 1.3 million in this instance). For this self-comparison, the system reported a Normalized Match Score of 100%, a Coverage Score of 99.7%, and a Quality Score of 100.0%, indicating accurate self-identification. These results are displayed with summary statistics and a similarity score distribution chart.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-feature-visualisation-of-lexical-matches",
    "href": "chapter_ai-nepi_006.html#text-matching-feature-visualisation-of-lexical-matches",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.19 Text Matching Feature: Visualisation of Lexical Matches",
    "text": "6.19 Text Matching Feature: Visualisation of Lexical Matches\n\n\n\nSlide 19\n\n\n    The \"Match Details\" view within the VERITRACE application enhances the analysis of lexical matches by automatically highlighting shared terms. It presents passages side-by-side—the \"Source Passage\" from the query text and the \"Comparison Passage\" from the target text—along with a similarity score for each identified pair, facilitating direct visual assessment of the overlaps.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-sanity-check-semantic-matching-of-a-text-and-its-translation",
    "href": "chapter_ai-nepi_006.html#text-matching-sanity-check-semantic-matching-of-a-text-and-its-translation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.20 Text Matching Sanity Check: Semantic Matching of a Text and Its Translation",
    "text": "6.20 Text Matching Sanity Check: Semantic Matching of a Text and Its Translation\n\n\n\nSlide 25\n\n\n    A crucial test for the semantic matching capabilities involved comparing Newton's Latin *Opticks* with its English translation. The expectation here is that, despite minimal lexical overlap due to the language difference, the semantic content should align closely, resulting in numerous strong matches. The system interface allows selection of \"Semantic\" as the match type before initiating the comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-results-semantic-matches-between-a-text-and-its-translation",
    "href": "chapter_ai-nepi_006.html#text-matching-results-semantic-matches-between-a-text-and-its-translation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.21 Text Matching Results: Semantic Matches Between a Text and Its Translation",
    "text": "6.21 Text Matching Results: Semantic Matches Between a Text and Its Translation\n\n\n\nSlide 20\n\n\n    Upon performing the semantic match between the Latin *Optice* and the English *Opticks*, the results appear reasonable. Despite OCR issues, the system successfully identified conceptually related passages. For instance, segments discussing \"colors\" in the Latin text were matched with corresponding discussions of \"colors\" in the English version, demonstrating the model's ability to capture semantic similarity across languages. The \"Match Details\" view presents these aligned passages with their similarity scores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-analysis-summary-and-issues-in-semantic-matching-of-translations",
    "href": "chapter_ai-nepi_006.html#text-matching-analysis-summary-and-issues-in-semantic-matching-of-translations",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.22 Text Matching Analysis: Summary and Issues in Semantic Matching of Translations",
    "text": "6.22 Text Matching Analysis: Summary and Issues in Semantic Matching of Translations\n\n\n\nSlide 20\n\n\n    The summary statistics for the semantic match between the Latin *Optice* and English *Opticks* revealed a high Quality Score (91.2%), indicating that the identified matches were indeed of high conceptual similarity. However, the Coverage Score was relatively low at 36.9%. This lower coverage might reflect actual textual differences between the editions—the Latin version is reportedly longer and potentially varies in content—rather than a deficiency in the matching algorithm itself. Nevertheless, broader testing with other queries suggests that the LaBSE embedding model may not be fully adequate for the task, potentially suffering from \"out-of-domain model collapse\" when confronted with the unique characteristics of the historical corpus, including archaic language, typography, and OCR inaccuracies.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#anticipated-future-challenges-and-key-considerations-for-text-matching",
    "href": "chapter_ai-nepi_006.html#anticipated-future-challenges-and-key-considerations-for-text-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.23 Anticipated Future Challenges and Key Considerations for Text Matching",
    "text": "6.23 Anticipated Future Challenges and Key Considerations for Text Matching\n\n\n\nSlide 21\n\n\n    Looking ahead, the VERITRACE project must address several significant issues, particularly concerning its text matching capabilities. The current embedding model, LaBSE, chosen for its efficiency, may not be robust enough. Researchers are considering alternatives like XLM-Roberta, intfloat multilingual-e5-large, or historical mBERT, each with its own balance of accuracy and resource demands. A fundamental question is whether to fine-tune a base model on the unique historical corpus, rather than relying on general pre-trained models. Furthermore, handling semantic change—how word meanings evolve across centuries (1540-1700) and languages within a single vector space—presents a complex challenge. Poor OCR quality continues to affect all downstream tasks, including text segmentation; while re-OCRing the entire corpus is infeasible, targeted re-OCR or sourcing better existing versions are potential remedies. Finally, scaling and performance are critical: operations taking seconds on the current 132-text prototype will become prohibitively slow on the full 430,000-text corpus. The team welcomes advice on these pressing matters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis chapter explores the critical domains of Explainable AI (XAI) and the application of AI-based scientific insights within the humanities. Initially, the discourse establishes the foundational principles of XAI, particularly its evolution from feature attribution in classification models to addressing the complexities of generative AI. We underscore the necessity of understanding model predictions, identifying biases, and ensuring regulatory compliance. Furthermore, we meticulously detail various orders of interpretability, progressing from first-order attributions, such as heatmaps, to more intricate second and higher-order interactions, including those within graph structures.\nSubsequently, the chapter transitions to practical applications, showcasing how these advanced AI and XAI methodologies facilitate novel research in the humanities. Specific case studies illustrate the extraction of visual definitions from historical corpora and the large-scale analysis of early modern astronomical tables. A significant workflow, termed XAI-Historian, empowers historians to generate data-driven hypotheses and discover new insights. This system employs specialised statistical models to derive bigram representations from challenging, out-of-domain historical data, enabling robust analysis. Crucially, applying cluster entropy analysis reveals patterns of innovation spread across historical European publishing centres, identifying anomalies such as the politically controlled print programme in Wittenberg. The chapter concludes by acknowledging the inherent challenges in applying AI to heterogeneous, low-resource humanities data whilst underscoring the transformative potential of multimodal approaches and explainable machine learning for scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#foundations-of-explainable-ai-xai-1.0",
    "href": "chapter_ai-nepi_007.html#foundations-of-explainable-ai-xai-1.0",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.1 Foundations of Explainable AI (XAI 1.0)",
    "text": "7.1 Foundations of Explainable AI (XAI 1.0)\n\n\n\nSlide 03\n\n\nResearchers in machine learning pioneered the field of explainable AI (XAI) to demystify the decision-making processes of complex algorithms. Historically, machine learning applications centred heavily on visual data, whilst substantial engagement with language data developed more recently, over the past decade. XAI addresses a core challenge: the “black box” nature of many AI systems, particularly in classification, where users receive predictions without insight into their underlying rationale. For instance, an AI might correctly identify a rooster in an image, yet the specific visual features driving this classification often remain hidden (Samek et al., 2017).\nConsequently, the domain of post-hoc explainability emerged, prompting extensive research over the last decade into methods that trace and illuminate these predictions. One prevalent technique involves generating heatmaps, which visually highlight the input pixels most influential in an AI system’s decision. Beyond mere technical curiosity, the drive for explainability serves several critical functions. It allows for the verification of predictions, ensuring models operate logically. It also facilitates the identification of flaws and biases, enabling error correction and a deeper understanding of model fallibility. Furthermore, XAI can reveal novel insights about the problem domain through surprising solutions uncovered by models and, increasingly, helps ensure compliance with regulatory frameworks like the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#generative-ai-and-explainability-challenges",
    "href": "chapter_ai-nepi_007.html#generative-ai-and-explainability-challenges",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.2 Generative AI and Explainability Challenges",
    "text": "7.2 Generative AI and Explainability Challenges\n\n\n\nSlide 03\n\n\nThe advent of Generative AI (GenAI) within the last five years has profoundly reshaped the AI landscape, presenting novel challenges for explainability. Unlike earlier classification-focused systems, contemporary foundation models exhibit remarkable multi-task capabilities. They can classify, retrieve similar images, generate novel content, and engage in question-answering across a vast range of subjects. This inherent versatility, however, significantly complicates the process of grounding an LLM’s output to specific input features.\nConsequently, current XAI research strives to advance beyond traditional heatmap representations. Investigators are exploring methods to understand intricate feature interactions and to develop more mechanistic views of how these complex models operate. Often described as “world models” due to their broad knowledge, such foundation models offer considerable potential for uncovering insights about societal trends, the evolution of language, and various textual characteristics. Nevertheless, these powerful models are not infallible and can make quite surprising errors. For instance, an object classifier erroneously based its identification of a boat on the surrounding water—a correlated but distinct feature—rather than the boat itself (Lapuschkin et al., Nat Commun ’19). In another example, an LLM tasked with the Tower of Hanoi puzzle failed to grasp the game’s physical constraints, incorrectly attempting to move the largest, inaccessible disk (Mondal Webb et al., arxiv ’24), although newer reasoning models might exhibit improved performance in such tasks.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.3 XAI 2.0: Structured Interpretability",
    "text": "7.3 XAI 2.0: Structured Interpretability\n\n\n\nSlide 05\n\n\nResearchers are now advancing towards what they term “XAI 2.0,” which emphasises structured interpretability, moving beyond the limitations of simple heatmaps. This progression involves analysing different orders of feature interactions. First-order explanations, for example, remain valuable for understanding classifiers. In one project involving a table classifier for historical data, heatmaps successfully verified that the model correctly focused on numerical content to identify numerical tables, thereby confirming its meaningful operation.\nMoving to a greater level of complexity, second-order explanations scrutinise pairwise relationships between features. These become particularly important when explaining similarity metrics, such as the dot product between the embeddings of two images. Such methods can reveal specific interactions—for instance, between corresponding digits in two tables—that underpin the model’s assessment of their identity, thus validating its intended function.\nMore recently, investigations into graph structures have highlighted the significance of higher-order interactions. When analysing complex networks like citation systems or relationships between books and entities, researchers find that entire subgraphs or “feature walks”—sets of co-dependent features—collectively contribute to a model’s decision. This line of inquiry aims to furnish more intricate insights into model mechanisms, potentially leading to a circuit-level comprehension of their internal workings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.4 First-Order Attributions in LLMs",
    "text": "7.4 First-Order Attributions in LLMs\n\n\n\nSlide 12\n\n\nApplying first-order attribution methods to Large Language Models (LLMs) reveals crucial characteristics regarding bias and information processing, particularly within language and humanities contexts. In one study focusing on sentiment prediction in Transformer LLMs using movie reviews (Ali et al., ICML ’22), researchers utilised a specialised heatmap technique. Their analysis uncovered biases: positive sentiment predictions frequently associated with male, Western names such as “Lee” or “Coen brothers,” whilst names like “Saddam,” “Castro,” or “Chan,” and even “Martha Stewart,” tended to correlate with negative sentiment scores. This demonstrates XAI’s utility in pinpointing subtle, fine-grained biases embedded within these models.\nAnother investigation explored long-range dependencies in LLMs during text summarisation tasks involving extensive inputs like Wikipedia articles (up to an 8,000-token context window) (Jafari et al., NeurIPS ’24). Scientists examined which parts of the input text most influenced the generated summaries. They discovered that LLMs exhibit a strong tendency to focus on the latter portions of the context, prioritising information presented more recently. Although these models possess the capability to draw upon long-range information from the beginning of the input, a logarithmic scale of attention counts indicates significantly less frequent occurrences. Consequently, users should be aware that LLM-generated summaries might not offer a balanced overview of the entire source text, potentially favouring content appearing closer to the end of the input or the prompt itself.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-in-text-and-graphs",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-in-text-and-graphs",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.5 Higher-Order Interactions in Text and Graphs",
    "text": "7.5 Higher-Order Interactions in Text and Graphs\n\n\n\nSlide 11\n\n\nBeyond first-order attributions, researchers investigate second and higher-order interactions to gain deeper insights into how models process text. When models like BERT or Sentence-BERT compute a similarity score between two sentences (e.g., “a cat I really like” and “it is a great cat”), the precise reasons for the score often elude clear explanation. Second-order explanation techniques address this by revealing interaction scores between individual tokens. These analyses frequently uncover that models rely on strategies such as noun matching (identifying synonyms or identical nouns), along with noun-verb pairings and interactions involving separator tokens. It appears that in compressing vast amounts of information, models may resort to comparatively simplistic heuristics, a crucial consideration for those employing embeddings for data ranking.\nFurthermore, Graph Neural Networks (GNNs), which inherently encode structural information, offer another avenue for structured interpretability. Researchers conceptualise attributions in GNNs as “walks” or paths of feature interactions. Intriguingly, as the attention mechanism defines information flow between tokens, researchers can view GNNs as a form of LLM, enabling the application of GNN-based XAI techniques to language. For instance, in analysing sentiment in movie reviews (Schnake et al., TPAMI ’22), higher-order explanations derived from GNNs demonstrate superior capability in capturing complex linguistic structures, such as negation. Whilst a first-order method might misinterpret a phrase like “First I didn’t like the boring pictures,” a higher-order approach can correctly identify the negative sentiment of this clause whilst accurately assessing the sentiment of subsequent, contrasting statements within the same review.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-for-humanities-visual-and-textual-insights",
    "href": "chapter_ai-nepi_007.html#ai-for-humanities-visual-and-textual-insights",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.6 AI for Humanities: Visual and Textual Insights",
    "text": "7.6 AI for Humanities: Visual and Textual Insights\n\n\n\nSlide 20\n\n\nThe application of AI methods extends compellingly into the humanities, promising novel scientific insights from historical data. One such endeavour involved extracting visual definitions from a corpus of historical mathematical instruments (El-Hajj Eberle+, Int J Digit Humanities ’23). Researchers, in collaboration with historians Matteo Mariani and Jochen Büttner, analysed images of items such as a 16th-century time measurement device by Cortés and a 17th-century machine for striking gold medals by Branca. They employed heatmap-based approaches alongside a classifier designed to categorise these instruments (e.g., as “mathematical instrument” or “machine”).\nA primary objective sought to determine whether AI-derived visual definitions could offer more objective classification criteria. This process underscored the necessity of close partnership with domain experts to ensure the interpretations were historically meaningful. Crucially, the AI models identified fine-grained scales on the mathematical instruments as highly salient features for their decisions, also highlighting specific components such as ‘scaffolding’ or ‘footstands’ in other illustrations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#large-scale-analysis-the-xai-historian",
    "href": "chapter_ai-nepi_007.html#large-scale-analysis-the-xai-historian",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.7 Large-Scale Analysis: The XAI-Historian",
    "text": "7.7 Large-Scale Analysis: The XAI-Historian\n\n\n\nSlide 22\n\n\nA significant collaborative project focused on the corpus-level analysis of numerical tables from early modern astronomical texts, primarily drawing from the Sphaera Corpus (1472-1650) and the Sacrobosco Table Corpus, which comprises 76,000 pages of university textbooks from the same period (Valleriani+’19; Eberle+’24). Historians Matteo Mariani and Jochen Büttner from Bifold initiated this work, seeking automated methods to match tables and discern semantic similarities—tasks previously unfeasible at such a scale. Despite initial reservations about the data’s complexity, the collaboration yielded the “XAI-Historian” workflow, where historians leverage AI and XAI for discovering case studies and generating data-driven hypotheses (Eberle et al., Sci Adv ’24).\nThe machine learning aspect faced considerable challenges, including extreme data heterogeneity, scarce annotations, and the inadequacy of standard OCR tools and foundation models for this specialised historical material. Researchers devised an “atomization-recomposition” pipeline, transforming input tables into bigram maps and histograms, which then fed into corpus-level analyses using embeddings and similarity measures. Instead of relying on general-purpose foundation models, which struggled with this out-of-domain data, the team engineered a smaller, custom model specifically for detecting numerical bigrams (e.g., ‘01’, ‘21’). Crucially, the team employed XAI techniques to verify this bespoke model; for instance, confirming that it correctly identified identical bigrams (like ‘38’) in corresponding input tables established confidence in its outputs (Eberle et al., TPAMI ’22; Eberle et al., Sci Adv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#cluster-entropy-innovation-in-early-modern-printing",
    "href": "chapter_ai-nepi_007.html#cluster-entropy-innovation-in-early-modern-printing",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.8 Cluster Entropy: Innovation in Early Modern Printing",
    "text": "7.8 Cluster Entropy: Innovation in Early Modern Printing\n\n\n\nSlide 17\n\n\nBuilding upon the validated table representations, researchers conducted case studies using cluster entropy analysis to investigate the dissemination of innovation in early modern European printing (Sphaera publication EPISCI-626). This method involved examining the diversity of printed astronomical tables produced by various publishing centres. The types of tables printed characterised each city’s output; some locations fostered diverse programmes, whilst others concentrated on reprinting established works—a pattern previously challenging to analyse systematically across the continent.\nThe team developed a clustering approach using the learned bigram-based representations to group similar tables. They then applied entropy as a measure of diversity: a low entropy score for a city signified a tendency to reproduce the same types of content, whereas a score approaching the maximum possible entropy indicated a more varied publishing output. This analysis (Eberle et al., Sci Adv ’24) pinpointed cities with particularly low entropy. For example, analysis confirmed Frankfurt am Main as a known hub for reprinting editions. More notably, Wittenberg emerged as a significant case; the analysis suggested that political control exerted by Protestant reformers actively constrained the diversity of the print programme, as they dictated the specific curriculum to be published. This data-driven finding identified a historical anomaly that resonated with and supported existing historical understanding.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#concluding-perspectives",
    "href": "chapter_ai-nepi_007.html#concluding-perspectives",
    "title": "7  Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities",
    "section": "7.9 Concluding Perspectives",
    "text": "7.9 Concluding Perspectives\n\n\n\nSlide 26\n\n\nHumanities and Digital Humanities (DH) researchers have made substantial strides in digitising source materials; however, the automated analysis of these diverse and often sparsely labelled corpora presents considerable difficulties. Researchers increasingly recognise the integration of multimodality as an important dimension. Machine learning, augmented by explainable AI (ML+XAI), offers the potential to scale humanities research significantly and to foster entirely new avenues of inquiry. Whilst foundation models and Large Language Models, accessed via prompting, can prove helpful for intermediate tasks like data labelling, curation, and error correction, their applicability to complex, nuanced research questions within the humanities currently faces limitations.\nSeveral challenges persist. The prevalence of low-resource datasets in many humanities domains acts as a roadblock, especially when considering the scaling laws that govern the performance of many machine learning models. Furthermore, the effective transfer of models to out-of-domain data—a common scenario with historical and small-scale datasets—necessitates rigorous evaluation. Contemporary LLMs, predominantly trained and aligned for general natural language processing and code generation, make their direct application to specialised historical data non-trivial. This body of research benefits from extensive collaboration involving individuals such as Klaus-Robert Müller, Matteo Valleriani, and Jochen Büttner, and institutions including TU Berlin, BIFOLD, and the Max Planck Institute for the History of Science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs; AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-sdg-classification-in-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-sdg-classification-in-bibliometric-databases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.1 Background: SDG Classification in Bibliometric Databases",
    "text": "9.1 Background: SDG Classification in Bibliometric Databases\n\n\n\nSlide 01\n\n\nResearchers initiated an investigation to employ Large Language Models (LLMs) as a technology for assessing biases within publications classified by three principal bibliometric databases. This work acknowledges that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, function as critical digital infrastructures, enabling bibliometric analyses and impact assessments throughout the scientific community. Nevertheless, these databases possess a performative nature, shaped by particular understandings of the science system and specific value attributions, as Whitley (2000) and Winkler (1988) have noted.\nMajor bibliometric databases have recently implemented classifications that align scholarly publications with the United Nations Sustainable Development Goals (SDGs). However, prior research, notably by Armitage et al. (2020), discovered significant discrepancies in SDG labelling across various providers like Elsevier, Bergen, and Aurora, revealing minimal overlap in the resulting datasets. Such differences in classification carry substantial implications; they can foster varying perceptions of research priorities, which, in turn, may influence resource allocation and policy decisions.\nFurthermore, these databases exert considerable influence over academics, researchers, funding bodies, and policymakers, whilst also responding to diverse political and commercial interests. The current study specifically considered Web of Science, Scopus, and OpenAlex, building upon earlier findings that highlighted the limited overlap in publications when different SDG search queries were applied.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-objectives-and-framework",
    "href": "chapter_ai-nepi_009.html#case-study-objectives-and-framework",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.2 Case Study Objectives and Framework",
    "text": "9.2 Case Study Objectives and Framework\n\n\n\nSlide 02\n\n\nInvestigators conducted a case study, detailed by Ottaviani & StahlSchmidt (2024), focusing on the representation of UN Sustainable Development Goals within bibliometric data. A primary motivation involved assessing the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly if LLM-based tools were to be introduced. To achieve this, researchers employed DistilGPT2, a minimally pre-trained Large Language Model. They separately trained this model on distinct subsets of publication abstracts, each corresponding to the SDG classifications provided by the diverse bibliometric databases.\nThe LLM technology served a dual purpose in this study. Firstly, it functioned as a detector of biases present in the data. Secondly, it acted as a proof-of-concept exercise, demonstrating the potential of LLMs in automating information extraction to inform research-related decision-making. Researchers aimed to understand the aggregate effects stemming from metadata processing by bibliometric databases and how these subsequently influence various stakeholders, including researchers, policymakers, and consultants. Ultimately, the project sought to develop a generalisable exercise for assessing the potential impact of such technologies on research policy.\nA conceptualised chain of dependencies illustrates this process: SDG classification initially defines what constitutes “Research” on SDGs. Various actors, including researchers, small and medium-sized enterprises (SMEs), governments, and other intermediaries, then process this research. Subsequently, this processed research informs “Decision-making to align with SDGs,” which in turn affects “Socioeconomic inequalities.” Parallel to this, the LLM, acting as a “detector of ‘biases’,” influences the “Introduction of LLM in Research Policy,” which also has repercussions for “Socioeconomic inequalities.” Therefore, alterations in the metadata that define “research on SDGs” can significantly impact advice, choices, indicators, and implemented measures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.3 Methodology: Actors, Data, and SDG Selection",
    "text": "9.3 Methodology: Actors, Data, and SDG Selection\n\n\n\nSlide 04\n\n\nThe research design incorporated three principal bibliometric databases as key actors. These included two proprietary systems, Web of Science (operated by Clarivate, US) and Scopus (managed by Elsevier, UK), alongside the open-access database OpenAlex (formerly a Microsoft entity, US). Investigators focused their analysis on five specific UN Sustainable Development Goals, chosen for their direct relevance to socioeconomic inequalities. They categorised these into two dimensions: the socio dimension, encompassing SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities); and the economic dimension, which included SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).\nFor data processing, researchers utilised a jointly indexed subset comprising 15,471,336 publications. They compiled this dataset by collecting publications shared across all three bibliometric databases, identifying them through exact DOI matching, and covering the period from January 2015 to July 2023. Subsequently, the team undertook an analysis of the performance of the three distinct classification standards for the five selected SDGs. This approach led to the creation of three separate subsets of publications for each SDG—one corresponding to each bibliometric database—forming the basis for the comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "href": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.4 Comparative Analysis of SDG-Classified Papers",
    "text": "9.4 Comparative Analysis of SDG-Classified Papers\n\n\n\nSlide 05\n\n\nA comparative analysis, detailed in Ottaviani & StahlSchmidt (2024), examined the overlap of papers classified under specific Sustainable Development Goals (SDGs) across Web of Science, OpenAlex, and Scopus, particularly for the socio-dimension SDGs. For SDG4 (Quality Education), Scopus classified the largest share of publications (339,063; 52.2%), followed by OpenAlex (218,907; 33.6%), and Web of Science (124,359; 19.1%). The intersection of all three databases for SDG4 contained 46,711 publications (7.2%).\nConcerning SDG5 (Gender Equality), Web of Science accounted for the majority of classifications (373,224; 57.4%), with Scopus classifying 82,277 (26.2%) and OpenAlex 38,066 (12.1%). Notably, all three databases commonly classified only 21,770 publications (6.9%) under SDG5. Investigators observed that Scopus did not designate some publications present in its database as SDG5. Furthermore, Web of Science’s SDG5 classifications included approximately 10% of publications from mathematics, such as those on geometrical differential equations, indicating potential discrepancies in classification criteria.\nFor SDG10 (Reduce Inequalities), Scopus again led in volume (236,665; 36.2%), with OpenAlex (213,419; 32.7%) and Web of Science (99,460; 15.2%) following. The common overlap for SDG10 was the smallest, at 13,319 publications (2.0%). These findings generally align with Armitage (2020), underscoring the consistently small overlap in SDG labelling across different bibliometric providers. This limited congruence highlights the varying interpretations and applications of SDG classifications.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "href": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.5 LLM Selection and Fine-Tuning Strategy",
    "text": "9.5 LLM Selection and Fine-Tuning Strategy\n\n\n\nSlide 07\n\n\nResearchers conceptualised the development of Large Language Models (LLMs) possessing knowledge derived exclusively from publications classified under a specific Sustainable Development Goal (SDG) by a particular bibliometric database. The initial ambition to train such LLMs from scratch solely on these publications proved a substantial undertaking due to its resource-intensive nature. Consequently, a compromise involved fine-tuning an existing, pre-trained LLM that possessed minimal prior knowledge, using the abstracts of the selected publications.\nLeading commercial and open-source pre-trained LLMs, such as GPT-4 (with 1.76 trillion parameters), were deemed ineligible for this work. Their unsuitability stemmed from their extensive pre-training datasets (which include sources like Wikipedia and Reddit conversations), meaning they already embed considerable knowledge about SDGs and possess strong, pre-existing semantic associations. To circumvent this, investigators selected DistilGPT2. This model, a “very light” English-speaking variant of the open-source GPT-2, employs a “distillation” technique (Sanh, 2019) and has significantly fewer parameters (82 million).\nIts advantages included feasibility for use with proprietary data and its minimally instructed nature, which ensures its behaviour is more strongly influenced by the fine-tuning data. Researchers operated under the premise that DistilGPT2 had no significant prior semantic knowledge relevant to the publications or the prompts used. This approach facilitated the fine-tuning of 15 distinct DistilGPT2 models, each tailored to a specific combination of one of the three bibliometric databases and one of the five chosen SDGs (DistilGPT2{bibDB, SDG}).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompt-engineering-and-llm-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompt-engineering-and-llm-benchmarking",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.6 Prompt Engineering and LLM Benchmarking",
    "text": "9.6 Prompt Engineering and LLM Benchmarking\n\n\n\nSlide 10\n\n\nThe United Nations’ Sustainable Development Goals (SDGs) are structured with specific targets; for instance, SDG4 (Quality Education) includes targets such as ensuring universal completion of primary and secondary education (Target 4.1) and equal access to tertiary education (Target 4.3), amongst others (typically 8-12 targets for the SDGs analysed, as per the 2030 Agenda for SDGs, UN). To benchmark the fine-tuned LLMs, researchers developed a systematic approach to prompt engineering. For each individual target within an SDG, they crafted ten diverse questions, or prompts, each designed to probe different facets of that target.\nThis methodology yielded a unique set of 80 to 120 prompts for every SDG under investigation. These prompts established a benchmark, enabling the assessment of the LLMs’ compliance with SDG objectives and the identification of potential “biases” in their responses. For example, for Target 4.1 of SDG4, prompts included questions like, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” and others addressing strategies, outcome improvements, and challenges.\nThe research design followed a structured workflow for each bibliometric database (DB) and SDG combination. Initially, a set of publication abstracts, classified under a specific SDG by a particular database, served as the input for fine-tuning a DistilGPT-2 model. The resultant fine-tuned model (Fine-tuned DistilGPT-2 SDG# DB#) then processed the set of prompts specifically designed for that SDG. This processing employed three distinct decoding strategies—top-k, nucleus, and contrastive search—generating three sets of responses. Finally, a “prompts’ words filter” was applied to these responses, leading to the extraction of noun phrases (Noun phrases SDG# DB#), which formed the basis for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-4-results-unaddressed-targets-and-biases",
    "href": "chapter_ai-nepi_009.html#sdg-4-results-unaddressed-targets-and-biases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.7 SDG 4 Results: Unaddressed Targets and Biases",
    "text": "9.7 SDG 4 Results: Unaddressed Targets and Biases\n\n\n\nSlide 13\n\n\nResearchers analysed the responses generated by the fine-tuned Large Language Models (LLMs) by matching extracted noun phrases with the official SDG targets. This analysis spanned four key dimensions: Locations, Actors, Data/Metrics, and Focuses. For each SDG, the assessment aimed to determine, firstly, the LLM’s compliance with its targets and, secondly, any discernible biases. Notably, the team also observed differences in the outputs corresponding to the different source bibliometric databases.\nTaking SDG4 (Quality Education) as an illustrative example, the LLMs, despite receiving target-specific questions, failed to address several critical areas. In terms of Locations, whilst countries like South Africa, the U.S., Australia, China, and Hong Kong appeared (reflecting the content of unique database subsets), a significant omission of African countries (beyond South Africa), Developing Countries more broadly, Least Developed Countries, Other Developing Countries, and Small Island Developing States was evident—all explicitly or implicitly relevant to SDG4 targets.\nRegarding Actors, terms like “Classroom” and “Family” appeared, and general categories such as “All Women and Men,” “Children,” “Teachers,” and “Youth” were addressed. However, the LLM responses did not adequately cover crucial vulnerable groups specified in SDG4 targets, including “The Vulnerable,” “Persons With Disabilities,” “Indigenous Peoples,” “Children In Vulnerable Situations,” and “All Learners.”\nThe Data/Metrics dimension saw mentions of “Survey,” “PISA,” “Evaluation,” “Self-Efficacy,” and “Thematic Analysis,” indicating a focus on certain research methodologies and assessment tools. For Focuses, whilst aspects like “Quality Primary and Secondary Education” and “Access” were addressed, numerous target areas remained unmentioned. These unaddressed focuses included:\n\n“Affordable And Quality Technical, Vocational And Tertiary Education”\n“Relevant Skills” for employment\n“Vocational Training”\n“Scholarships”\n“Safe, non-violent, inclusive learning environments”\n“Sustainable Lifestyles”\n“Human Rights”\n“Global Citizenship”\n“Appreciation Of Cultural Diversity”\n“Free primary and secondary education”\n“Tertiary education”\n\nThis pattern of overlooking specific target elements, particularly concerning sensitive locations, vulnerable actors, and key educational focuses, emerged as a recurrent finding.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-analysis-systematic-oversights",
    "href": "chapter_ai-nepi_009.html#cross-sdg-analysis-systematic-oversights",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.8 Cross-SDG Analysis: Systematic Oversights",
    "text": "9.8 Cross-SDG Analysis: Systematic Oversights\n\n\n\nSlide 14\n\n\nAcross the five Sustainable Development Goals (SDGs) analysed, several consistent patterns and systematic oversights emerged from the LLM responses. Regarding Locations, least developed countries received scant attention; for instance, Sub-Saharan Africa appeared notably only in the context of SDG8. The United States featured with such prominence that it suggested an “undoubted monopoly” in the data’s geographical focus. Following the U.S., South Africa and China were the most frequently cited locations, with the United Kingdom and Australia also appearing.\nConcerning Actors, a particularly troubling finding was the systematic overlooking of discriminated and vulnerable categories of people. This oversight persisted across all five SDGs examined, indicating a significant gap in the LLM-generated content relative to the inclusive aims of the SDGs themselves.\nIn the Metrics dimension, the LLMs frequently referenced various data sources, such as Demographic and Health Surveys (DHS) and World Values Surveys (WVS), alongside numerous other metrics, indicators, and benchmarks. A range of research methodologies also appeared, including theoretical and empirical approaches, thematic analysis, market dynamics, and macroeconomic studies. An interesting distinction arose from the source databases: for three of the SDGs, LLMs trained on Web of Science data tended to reflect a more theoretical research approach, whereas those trained on Scopus and OpenAlex data exhibited a more empirical orientation.\nFinally, whilst Focuses were generally SDG-specific, the LLMs often failed to address the most sensitive topics embedded within the SDG targets. Examples of such overlooked critical issues include human trafficking, human exploitation, and migration, all of which are pertinent to the broader aims of socioeconomic equality and development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#summary-and-limitations",
    "href": "chapter_ai-nepi_009.html#summary-and-limitations",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.9 Summary and Limitations",
    "text": "9.9 Summary and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s principal finding highlights that introducing Large Language Models (LLMs) as an analytical AI tool, positioned between the initial SDG classification of scientific literature and its subsequent use by policymakers, uncovers a systematic oversight within the data. Specifically, scientific publications, when classified by SDGs and processed by these LLMs, tend to neglect the most disadvantaged categories of individuals, the poorest countries, and various underrepresented topics that the SDG targets explicitly aim to address. Conversely, the analysis indicates that substantial attention is directed towards economic superpowers and highly developing nations. This outcome underscores how an ostensibly objective, science-informed practice, such as the bibliometric classification of SDGs, can have decisive and potentially skewed impacts on the perceived landscape of research.\nResearchers acknowledge several inherent limitations in this study. LLMs exhibit high sensitivity to various factors, including model architecture; although the choice of DistilGPT2 aimed to mitigate some aspects of this, more developed architectures could yield different outcomes. Sensitivity to training data presents another critical factor, which this work partially addressed by utilising three distinct bibliometric databases. Furthermore, (hyper-)parameters and the chosen decoding strategy significantly influence LLM outputs; the use of three different decoding strategies (top-k, nucleus, and contrastive search) attempted to account for this variability. The study presents a general framework, and whilst its application to very specific, applied cases could potentially produce different results, the researchers express some reservation about this likelihood.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-imperative-for-citation-graphs-and-existing-data-deficiencies",
    "href": "chapter_ai-nepi_010.html#the-imperative-for-citation-graphs-and-existing-data-deficiencies",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.1 The Imperative for Citation Graphs and Existing Data Deficiencies",
    "text": "10.1 The Imperative for Citation Graphs and Existing Data Deficiencies\n\n\n\nSlide 01\n\n\nResearchers embark upon the complex task of parsing footnotes within law and humanities scholarship, exploring the capabilities of Large Language Models (LLMs) and other algorithmic approaches. A primary objective involves generating high-quality data, specifically citation graphs, which prove essential for addressing nuanced research questions in intellectual history. Such graphs offer powerful means to uncover patterns and relationships within the production of knowledge, particularly in the history of science. Moreover, they enable scholars to trace influences and quantify the reception of published ideas. For instance, one can analyse shifts in the most-cited authors over specific periods, exemplified by data from the Journal of Law and Society (1994-2003), accessible via an interactive web application linked in the original presentation slides. Conceptually, these citation networks connect entities such as institutions, authors, works, and publication venues, illustrating how institutions produce authors, authors create works, works cite other works, and works find publication in various venues.\nA significant impediment to this research arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) material by established bibliometric datasources. Widely known platforms like Web of Science™, Scopus®, and OpenAlex prove largely inadequate for the specific domain under investigation. Investigators find that these databases simply do not contain the requisite data for their analytical needs.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-current-bibliometric-databases-and-the-challenge-of-footnotes",
    "href": "chapter_ai-nepi_010.html#limitations-of-current-bibliometric-databases-and-the-challenge-of-footnotes",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.2 Limitations of Current Bibliometric Databases and the Challenge of Footnotes",
    "text": "10.2 Limitations of Current Bibliometric Databases and the Challenge of Footnotes\n\n\n\nSlide 02\n\n\nFurther examination of current bibliometric databases reveals profound limitations. Web of Science and Scopus, for example, impose extremely high costs and operate under very restrictive licences, creating an undesirable dependency for researchers. Crucially, these platforms are not open and often lack the specific data required for SSH inquiries. Whilst OpenAlex presents a more favourable open-access model, its coverage for the specialised materials needed in this research remains insufficient. A general failing across all such databases is their typical lack of coverage for journals not deemed “A-journals,” publications from the pre-digital era, and content in languages other than English. The German journal, Zeitschrift für Rechtssoziologie, serves as a stark illustration: available citation data improves somewhat after the 2000s, but data from earlier periods, such as the 1980s and 1990s, is practically non-existent according to metrics from dimensions and OpenAlex, with WoS showing no coverage at all for this publication across the decades analysed.\nSeveral factors contribute to this poor SSH coverage. A primary reason is the perceived lack of commercial interest in humanities scholarship when contrasted with STEM fields, medicine, and economics, which receive the bulk of attention from bibliometric database providers. These databases also tend to focus on “impact factor” metrics for science evaluation, a concern largely irrelevant to the aims of intellectual history research. Compounding these issues is the very nature of SSH literature, which frequently features extensive and complex footnotes.\nThese “footnotes from hell,” as termed by the researchers, constitute a second major problem. They often feature embedded commentary, messy and unstructured information, and a high degree of “noise” in the form of non-referential text. Consequently, creating suitable training data for automated extraction (Problem 3) becomes an arduous and laborious process, involving detailed manual annotation of elements like author, title, and publication details within these dense textual segments. Even with such efforts, existing tools, often relying on traditional machine learning techniques like conditional random forests, struggle to handle footnotes effectively (Problem 4). Performance data from the ExCite tool, for instance, indicates low extraction and segmentation accuracy when applied to footnoted material, highlighting the inadequacy of current solutions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#exploring-large-language-models-and-the-imperative-for-robust-evaluation",
    "href": "chapter_ai-nepi_010.html#exploring-large-language-models-and-the-imperative-for-robust-evaluation",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.3 Exploring Large Language Models and the Imperative for Robust Evaluation",
    "text": "10.3 Exploring Large Language Models and the Imperative for Robust Evaluation\n\n\n\nSlide 10\n\n\nGiven the limitations of existing methods, researchers explore whether Large Language Models (LLMs) can offer a viable solution. Early anecdotal experiments conducted in 2022 with models such as text-davinci-003 demonstrated a promising capacity for LLMs to extract references from complex and messy textual data. The advent of newer, more powerful models, including Vision Language Models (VLMs) capable of processing PDF documents directly, suggests even greater potential. Various methods, such as prompt engineering, Retrieval Augmented Generation (RAG), and finetuning, are being investigated to harness these capabilities effectively.\nHowever, a critical concern overshadows this optimism: the trustworthiness of LLM-generated results. The well-publicised instance of a lawyer submitting a court filing with ChatGPT-invented case citations serves as a stark reminder of LLM fallibility. This underscores a fundamental principle for the researchers: one should not attempt to solve problems without adequate validation data to verify the accuracy of the solutions. Consequently, developing a robust testing and evaluation solution becomes paramount.\nThis solution necessitates three key components:\n\nA high-quality Gold Standard dataset.\nA flexible framework that can readily adapt to the fast-moving technological landscape of LLMs.\nSolid testing and evaluation algorithms capable of producing comparable metrics across different approaches.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-for-evaluation",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-for-evaluation",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.4 Developing a TEI-Annotated Gold Standard for Evaluation",
    "text": "10.4 Developing a TEI-Annotated Gold Standard for Evaluation\n\n\n\nSlide 13\n\n\nTo address the need for robust evaluation, Andreas Wagner details the development of a TEI-annotated Gold Standard dataset. Researchers opted for TEI XML encoding to compile training data that simultaneously serves as high-quality evaluation data. This choice stems from TEI XML’s status as a well-established, thoroughly specified, and comprehensive standard for text interchange, particularly prominent within the humanities, digital editorics, and text-based scholarly communities. Unlike more narrowly focused bibliographical standards such as CSL or BibTeX, TEI XML accommodates a broader range of phenomena, extending beyond simple reference management to include citations, cross-references, and other forms of contextual markup. This capability allows for the encoding of contextual information, which can prove valuable for tasks like classifying citation intention. Furthermore, adopting TEI XML facilitates access to a wealth of existing digital editions, text collections, and corpora already published in this format by numerous digital editorics projects, offering valuable resources for testing the generalisation and robustness of new extraction mechanisms. Despite its strengths, the TEI standard presents certain conceptual (e.g., distinguishing pointers from references) and technical (e.g., handling constrained elements versus elliptic material) challenges, though these were not elaborated upon in detail.\nThe creation of the TEI dataset involves several encoding stages. Beginning with a PDF image of a document, such as a German text containing a footnote (e.g., “Siehe hierzu etwa die Debatte zwischen ALAN RODGER…”), the process first involves segmenting the actual reference string from any surrounding non-referential text (e.g., “See also the debate between…”). Subsequently, this string is parsed into a structured TEI biblStruct element, populating fields such as analytic title, monographic title, author, imprint details, date, journal, volume, and page numbers. The dataset aims to encompass PDFs (with their corresponding text), the extracted reference strings, and these fully parsed structures.\nCurrently a work in progress, the dataset comprises approximately 1,100 foot- and endnotes sourced from 25 articles across 10 Directory of Open Access Journals (DOAJ). It maintains a humanities focus, specifically on legal and historical scholarship, and includes texts in French, German, Spanish, Italian, and Portuguese, spanning the years 1958 to 2018. This collection is estimated to contain over 1,600 individual references; notably, multiple citations to the same work are encoded separately to capture the context of each occurrence. The team revised their strategy part-way through development to incorporate PDFs, enabling the use of VLM technologies, and to ensure the entire dataset—from the original PDFs to the parsed reference structures—could be openly published by concentrating on Open Access journals.\nA further compelling reason for selecting TEI XML is the availability of associated tooling. Grobid, a widely used tool for reference and information extraction, employs TEI XML for its own training and evaluation processes. Utilising the same data format allows for direct performance comparisons between new tools and Grobid, facilitates the use of Grobid’s existing training data, and enables the contribution of this new, meticulously curated dataset back to the Grobid team and the wider research community. This collaborative potential, however, leads to the fundamental question of how “performance” itself should be precisely defined and measured.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-based-reference-extraction-and-evaluation",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-based-reference-extraction-and-evaluation",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.5 Llamore: A Python Package for LLM-based Reference Extraction and Evaluation",
    "text": "10.5 Llamore: A Python Package for LLM-based Reference Extraction and Evaluation\n\n\n\nSlide 14\n\n\nDavid Carreto Fidalgo introduces Llamore, an acronym for Large Language Models for Reference Extraction. Engineers developed this Python package to perform two primary functions: firstly, it extracts citation data from either raw input text or PDF documents by interfacing with various LLMs, including multimodal variants, and outputs these references in TEI XML format. Secondly, when provided with a set of gold standard references, Llamore can evaluate the extraction performance, typically reported as an F1 score. Key design objectives for Llamore included maintaining a lightweight codebase (under 2000 lines of code) and ensuring broad compatibility with both open-source and proprietary LLMs and VLMs; rather than embedding models itself, Llamore acts as a versatile interface.\nUsing Llamore involves a straightforward workflow. After installation via pip (pip install llamore), users can perform extraction by importing an appropriate extractor class, such as GeminiExtractor or OpenaiExtractor. The OpenaiExtractor notably facilitates compatibility with many open models that offer OpenAI-compatible API endpoints, such as those served by Olama or VLM. An extractor instance is then initialised (e.g., with an API key) and called with either a PDF file path or a raw text string. The resulting extracted references can then be exported as TEI biblStructs to an XML file. For evaluation, users import the F1 class, instantiate it (optionally configuring parameters like Levenshtein distance for matching), and then compute a macro-average F1 score by comparing the extracted references against a set of gold references.\nThe evaluation process meticulously compares extracted references to gold standard versions. Standard metrics of Precision (the ratio of correct matches to predicted elements) and Recall (the ratio of correct matches to actual gold elements) are calculated. These are then combined into the F1-score, the harmonic mean of precision and recall, which ranges from 0 (indicating no matches) to 1 (signifying perfect extraction). An example comparison might involve checking fields such as analytic title, monographic title, author names, and publication date between an extracted and a gold reference, where even minor discrepancies (like an extra period in an initial) would register as a mismatch under exact matching criteria.\nA further challenge in evaluation involves aligning the set of extracted references with the set of gold references, especially when a document contains multiple citations. Llamore tackles this by formulating it as an Unbalanced Assignment Problem. It first computes F1 scores for every possible pairing of an extracted reference with a gold reference, constructing a cost matrix. A solver, internally leveraging SciPy’s implementation, then identifies the assignment that maximises the total F1 score across all pairs, ensuring each reference is uniquely assigned. The F1 scores from these optimal pairings are then macro-averaged. Crucially, this method penalises both missing references (those present in the gold standard but not extracted) and hallucinated references (those extracted by the LLM but not present in the gold standard) by assigning them an F1 score of zero. This evaluation approach bears similarity to methods described by Packet et al. in a recent publication.\nInitial performance assessments yield insightful results. On the PLOS 1000 dataset, comprising 1000 biomedical PDF articles, Llamore (using Gemini 1.0 Flash) achieves an F1 score of approximately 0.62 with exact matching, comparable to Grobid’s score of around 0.61. This is noteworthy as Grobid was partly trained on similar biomedical literature. However, Grobid remains vastly more efficient in terms of computational resources, with the Gemini model requiring orders of magnitude more compute. In contrast, when applied to the researchers’ custom dataset of challenging, footnoted humanities literature, Llamore (Gemini 1.0 Flash) demonstrates significantly superior performance, achieving an F1 score of approximately 0.45, roughly three times better than Grobid’s score of about 0.14. This indicates Grobid’s difficulty with such out-of-distribution material, highlighting Llamore’s potential for these specific, complex use cases.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#conclusion-takeaways-and-future-directions",
    "href": "chapter_ai-nepi_010.html#conclusion-takeaways-and-future-directions",
    "title": "10  The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models",
    "section": "10.6 Conclusion, Takeaways, and Future Directions",
    "text": "10.6 Conclusion, Takeaways, and Future Directions\n\n\n\nSlide 28\n\n\nChristian Boulanger concludes by summarising key takeaways and outlining future directions. A primary takeaway is that Grobid continues to be the preferable tool for literature similar to its training data, owing to its significantly greater speed and lower resource demands. However, for the specific challenge of footnoted literature common in the humanities, experiments utilising Llamore with Gemini models demonstrate a performance approximately three times better than Grobid. It is important to note that these current performance metrics focus solely on pure reference extraction and do not yet encompass more nuanced aspects such as citation context or the resolution of cross-references.\nLooking ahead, the research team plans to expand their efforts by producing more comprehensive training data and further refining their test metrics. Future development aims to incorporate support for analysing citations in their context (e.g., determining if a citation is approving or contracting), resolving abbreviations like “op. cit.”, extracting the specific pages cited within a work, and accurately counting multiple citations to the same work.\nThe presentation also highlights persistent challenges in citation parsing. These include the wide variation in citation styles, which can lead to ambiguities such as distinguishing volume numbers from page numbers, or determining whether a page reference indicates the start of an article or the specific page being cited. Multilingual terminology presents another hurdle, with different languages using distinct abbreviations for contributor roles (e.g., “eds” versus “hrsg. v.”) and employing special terms like “passim”, “ibid”, or “n.d.” (no date). Canonical citation formats, common in fields like Bible studies, Roman law, and classical literature, add further complexity, as do ellipses, various abbreviations, and intricate cross-referencing systems within texts. Examples illustrate these difficulties, such as interpreting numbers in legal citations or recognising that terms like “DERS.” (derselbe) in German refer back to a previously mentioned author.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Challenges of Footnote Parsing in Law and Humanities Scholarship and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#introduction-addressing-information-overload",
    "href": "chapter_ai-nepi_011.html#introduction-addressing-information-overload",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.1 Introduction: Addressing Information Overload",
    "text": "11.1 Introduction: Addressing Information Overload\n\n\n\nSlide 01\n\n\nResearchers from DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, an archive also engaged in research, recently unveiled a collaborative project. This endeavour originated from extensive experimentation at DANS, particularly under the guidance of senior research engineer Slava Tykhonov, who has proven instrumental in constructing intricate data pipelines. A central impetus for this initiative stems from the escalating challenge of managing the “flood of information” pervading contemporary science. Indeed, continuous growth and increasing differentiation across disciplines present significant hurdles for researchers striving to review, evaluate, and select pertinent information.\nSuccessfully discovering and comprehending existing knowledge forms the bedrock of all new knowledge creation, benefiting both individual researchers and the broader academic sphere. Consequently, a pivotal question emerges: can modern AI transcend its role in contributing to this information proliferation? Can it actively support the intricate process of knowledge production itself? This inquiry firmly positions the project within the domain of Information Retrieval.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-question-and-system-architecture",
    "href": "chapter_ai-nepi_011.html#research-question-and-system-architecture",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.2 Research Question and System Architecture",
    "text": "11.2 Research Question and System Architecture\n\n\n\nSlide 02\n\n\nThis investigation centres upon a precise research question: “Can we build an AI solution to chat with papers from a specific selection?” To explore this query, developers judiciously selected articles from the method-data-analysis (mda) journal as a practical use case. The proposed solution integrates two principal components, internally designated as Ghostwriter and EverythingData. Ghostwriter serves as the interactive front-end, whilst EverythingData orchestrates the complex array of processes within the backend. This work draws extensively upon principles from information retrieval, human-machine interaction, and, crucially, Retrieval-Augmented Generation (RAG) techniques within generative AI.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#conceptual-framework-ghostwriter-and-rag",
    "href": "chapter_ai-nepi_011.html#conceptual-framework-ghostwriter-and-rag",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.3 Conceptual Framework: Ghostwriter and RAG",
    "text": "11.3 Conceptual Framework: Ghostwriter and RAG\n\n\n\nSlide 03\n\n\nEngineers conceptualise the Ghostwriter approach through a series of metaphors, each illustrating evolving interaction complexities. These range from a user engaging with a single database to interacting with a “librarian” who represents structured data, knowledge organisation systems, and established classifications. Further interaction involves a “library” or a “round of experts,” where these experts signify natural language processing capabilities. Ultimately, the system aspires to a state where the user converses “with experts and librarians at the same time.” This sophisticated interaction relies upon a locally deployed Large Language Model (LLM), a precisely targeted data collection, and a network of supplementary data interpretation sources accessible via APIs. The Ghostwriter interface, therefore, aims to facilitate simultaneous dialogue with both structured data and insights derived from natural language.\nThis work firmly situates itself within the broader scientific discourse on Retrieval Augmented Generation (RAG). For a deeper understanding of RAG, the presentation highlighted Philip Rattliff’s (Neo4j) paper, “GenAI Knowledge Graph: The GraphRAG Manifesto”. Core RAG components encompass a vector space, which engineers construct from data file content by encoding it into embeddings using various machine learning algorithms and LLMs, complete with properties and attributes. Complementing this, a graph layer furnishes metadata, seamlessly integrated with ontologies and controlled vocabularies. This integration adheres to the Croissant ML standard and incorporates principles of responsible AI. The overarching vision, termed GraphRAG, endeavours to merge graph and vector data into a singular model. Such an implementation gravitates towards a ‘local’, distributed AI paradigm, where the LLM functions as both an interface and a reasoning engine, connecting to a ‘RAG library’ (the graph) to navigate datasets and utilise embeddings for contextual understanding.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-workflow-ingestion-and-query",
    "href": "chapter_ai-nepi_011.html#system-workflow-ingestion-and-query",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.4 System Workflow: Ingestion and Query",
    "text": "11.4 System Workflow: Ingestion and Query\n\n\n\nSlide 05\n\n\nThe system’s workflow commences with a collection of articles, for instance, full-text documents meticulously scraped from the mda journal. These articles enter the EverythingData backend, where a series of operations transform the raw information. Initially, the system processes content into a vector store, utilising the Quadrant tool. This stage involves crucial enrichment processes, including term extraction and the generation of embeddings. Furthermore, the system represents selected terms as structured data within a graph, augmenting them through linkage with external knowledge bases such as Wikidata. This connection to knowledge graphs significantly enhances the contextual value of words, phrases, and their corresponding embeddings.\nThe processed and enriched data subsequently forms a comprehensive vector space, or RAG-Graph. User interaction occurs via the Ghostwriter interface. Here, an individual can pose a question in natural language, for example, “Explain male breadwinner model to me?”. This query then interacts directly with the RAG-Graph. In response, the system furnishes two primary outputs: a list of documents identified as relevant to the query, and an explanatory text synthesising a summary from the retrieved information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-implementation-factual-grounding",
    "href": "chapter_ai-nepi_011.html#ghostwriter-implementation-factual-grounding",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.5 Ghostwriter Implementation: Factual Grounding",
    "text": "11.5 Ghostwriter Implementation: Factual Grounding\n\n\n\nSlide 07\n\n\nThe development of Ghostwriter, particularly from engineer Slava Tykhonov’s perspective, involved meticulously deconstructing complex LLM training processes into practical components. This versatile approach extends beyond academic papers to encompass web content and even spreadsheets, empowering users to query specific values. A paramount design principle dictates the system’s unwavering commitment to factual accuracy: the system derives responses exclusively from the ingested source material, thereby rigorously mitigating the risk of LLM hallucination. Should the required information prove absent from the provided documents, the system explicitly states its inability to answer. This fidelity is achieved through a streamlined 1-billion parameter LLM, powerfully augmented by knowledge graphs.\nFor the method-data-analysis (mda) journal use case, developers ingested papers into Ghostwriter as a discrete collection. This ensures the system draws solely upon the content of these specific documents, rather than any general knowledge from the LLM’s pre-training. The test corpus comprised 100 articles, meticulously scraped from the mda website and accessible via a Ghostwriter instance at https://gesis.now.museum. This collection generated 37,637 vectors. The “Ask Questions” interface empowers users to manage collections and even add new content via URLs or RSS feeds. An example query, “explain male breadwinner model to me,” yielded not only a comprehensive explanation but also precise references to source articles, complete with URLs and relevance scores—a feature significantly enhancing verifiability. Internally, the system segments each paper into small, uniquely identified blocks. Subsequently, LLM-based techniques connect and retrieve these blocks, whilst knowledge graphs play a crucial role in predicting the most relevant text segments for any given query.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-refinement-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#iterative-refinement-and-multilinguality",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.6 Iterative Refinement and Multilinguality",
    "text": "11.6 Iterative Refinement and Multilinguality\n\n\n\nSlide 11\n\n\nThe system robustly supports an iterative approach to information seeking. Should direct information prove unavailable for a query, Ghostwriter explicitly states this. Users can also contribute by adding relevant papers they discover, thereby enriching the system’s knowledge base for subsequent interactions. For instance, a refined query, “explain how data was collected on male breadwinner model,” might elicit a response indicating no direct information, yet referencing related studies, such as those by Hahmmueller et al. (2015) or Haase et al. (2016).\nBehind this intuitive interface, several sophisticated backend mechanisms operate. An entity extraction pipeline annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby connecting the vector space to the knowledge graph. The system further links these entities to extensive knowledge graph representations like Wikidata. Crucially, the system offers immediate multilinguality, enabling it to process an English query against, for example, German or Chinese documents. The LLM then synthesises an ‘explanatory text’ from the retrieved information segments. Fact extraction involves decomposing queries into fundamental pieces; a JSON output meticulously details these “facts” and their relationships within a graph structure, including importance scores. Integration with a Knowledge Organisation System (KOS) facilitates iterative refinement and the exploration of multiple semantic layers for any given term. This KOS linkage, particularly through Wikidata, proves pivotal for multilinguality. The system maps entities to Wikidata identifiers, which in turn connect to translations in numerous languages and their associated properties. A term like “Male,” for instance, might link to a specific Wikidata entity, with similarity scores provided by LLM embeddings. This mechanism allows an initial query to expand effectively into multiple languages, significantly broadening the LLM’s search capability across diverse linguistic sources.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#vision-kos-and-live-demonstration",
    "href": "chapter_ai-nepi_011.html#vision-kos-and-live-demonstration",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.7 Vision: KOS and Live Demonstration",
    "text": "11.7 Vision: KOS and Live Demonstration\n\n\n\nSlide 14\n\n\nA pivotal aspect of the system’s design involves transforming user queries into lists of Wikidata identifiers. This effectively decouples essential knowledge—represented by these identifiers—from the specific wording of questions and the original textual content of the papers. Such an approach enables the independent storage of knowledge, irrespective of any particular LLM. This decoupling offers profound advantages: different or future LLMs can seamlessly interface with the same robust knowledge base, and researchers can benchmark various models by comparing their identifier outputs for consistent queries. Consequently, Knowledge Organisation Systems (KOS) are envisioned as foundational tools for future scientific endeavours, with ongoing collaborations with industry partners like Google and Meta actively fostering a sustainable ecosystem for these technologies.\nA live demonstration compellingly showcased the GESIS “Ask Questions” interface, utilising the mda collection. A query for “Rational Choice Theory” prompted the system to retrieve relevant information, presenting a concise summary from various papers alongside precise source references. A more specific query, “explain utility in Rational Choice Theory,” yielded highly targeted results, again with clear links to source articles such as “The measurement of utility and subjective probability…” by Best. The system also features an API for automated interactions, thereby facilitating the development of sophisticated agentic architectures. Furthermore, the demonstration underscored the system’s inherent multilinguality: English queries successfully extracted information from a German-language paper by Henning Best, highlighting the practical efficacy of the Wikidata integration.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#concluding-remarks-a-new-research-paradigm",
    "href": "chapter_ai-nepi_011.html#concluding-remarks-a-new-research-paradigm",
    "title": "11  Chatting with Papers: the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond",
    "section": "11.8 Concluding Remarks: A New Research Paradigm",
    "text": "11.8 Concluding Remarks: A New Research Paradigm\nConcluding the presentation, the team emphasised several distinct advantages of this AI-driven approach. Notably, the capacity for local processing significantly diminishes reliance on large, external computational resources, thereby granting users enhanced control and potentially lowering operational costs. The interaction itself frames as akin to “chatting with an invisible college,” suggesting a profoundly collaborative and exploratory engagement with information.\nThe presenters advocated a specific philosophy for utilising such tools. Rather than seeking definitive facts or ultimate answers, users should leverage the system to provoke and refine their own thinking processes. The AI serves as a powerful assistant, empowering humans to comprehend complex questions more deeply and to formulate their own incisive research inquiries. Ultimately, AI in this context functions as a robust support for human intellect, not as its substitute.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers: the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics - and beyond</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-rag-systems",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.1 Addressing LLM Limitations in Philosophical Inquiry with RAG Systems",
    "text": "12.1 Addressing LLM Limitations in Philosophical Inquiry with RAG Systems\n\n\n\nSlide 01\n\n\nPhilosophical inquiry, with its profound emphasis on linguistic precision and semantic accuracy, poses unique challenges for artificial intelligence. Researchers often grapple with questions such as “What is Aristotle’s theory of matter in the Physics?” or “Does Einstein’s idea of locality evolve from his earlier works on relativity to his 1948 paper concerning quantum mechanics and Wirklichkeit?” Whilst standard Large Language Models (LLMs) like ChatGPT can generate superficially adequate and differentiated responses to such high-level queries, their utility for deep scholarly work remains constrained by several inherent problems.\nA primary concern centres on the problem of access. Despite their extensive training data, LLMs frequently lack direct, verifiable access to the full texts of crucial scholarly works. For instance, although Einstein’s papers might have formed part of an LLM’s training dataset, the model typically cannot quote specific chapters or passages accurately, often resorting to hallucination or admitting its inability. Even when equipped with online search capabilities, copyright restrictions can prevent the reproduction of necessary materials. Furthermore, LLMs’ fundamental training mechanisms are engineered to learn generalisable statistical patterns of language rather than to memorise texts verbatim; indeed, safeguards exist to prevent mere parroting. This contrasts sharply with the needs of philosophical research, which demands rigorous engagement with original textual sources and their fine-grained formulations.\nBeyond access, the limited context window of LLMs—even extending to 128,000 tokens in models like ChatGPT-4—can prove insufficient when scholars work with extensive corpora. Another significant issue arises from the attribution problem: rigorous academic work necessitates precise citation for all claims, a feature often lacking in standard LLM outputs but desired in a manner similar to the referencing seen in tools like Perplexity.\nTo overcome these limitations, engineers have developed Retrieval Augmented Generation (RAG) systems. This architecture involves a user submitting a query to an application, which then performs a retrieval query against specified data sources like vector databases or APIs. These sources return relevant text chunks, which the application subsequently uses to augment the original user query. An LLM then generates an answer based on this enriched prompt, finally delivering it to the user. Typically, this retrieval employs semantic search, although hybrid or classic search methods also represent viable options. Consequently, RAG systems address the access problem by explicitly providing texts, manage context limitations by focusing on the most pertinent segments, and solve the attribution problem by enabling the citation of sources for the information supplied.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.2 Potential Applications of RAG Systems in Philosophy",
    "text": "12.2 Potential Applications of RAG Systems in Philosophy\n\n\n\nSlide 07\n\n\nIntegrating RAG systems into philosophical workflows opens several promising avenues for both teaching and research. A central idea involves enabling scholars and students to interact conversationally with entire philosophical corpora—for instance, the complete oeuvre of John Locke. Such a system would emulate the user-friendly style of ChatGPT but would distinguish itself through significantly deeper domain knowledge and a reliance on a verbatim textual basis.\nIn didactics, RAG systems offer considerable potential. The ability for users to pose repeated questions and explore topics iteratively can prove highly instructive, fostering a more profound understanding of complex philosophical concepts and texts.\nFor research purposes, these systems can serve multiple functions. They assist scholars in efficiently looking up facts within handbooks, providing quick access to orienting information, specific remarks, or even obscure footnotes. Moreover, RAG systems empower researchers to explore unexamined or particularly large corpora, potentially uncovering new insights or connections. They can also streamline the process of identifying specific passages pertinent to close reading exercises. Ultimately, such systems aim to furnish direct, detailed, and textually grounded answers to intricate research questions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-the-stanford-encyclopedia-of-philosophy",
    "href": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-the-stanford-encyclopedia-of-philosophy",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.3 Developing and Refining a RAG System for the Stanford Encyclopedia of Philosophy",
    "text": "12.3 Developing and Refining a RAG System for the Stanford Encyclopedia of Philosophy\n\n\n\nSlide 10\n\n\nResearchers undertook the development of an example RAG system, selecting the Stanford Encyclopedia of Philosophy (SEP) as its core data source. They scraped the content from this well-known online handbook and converted it into Markdown format. Initially, the primary aim was to craft a useful tool for the philosophical community. However, the project evolved into a qualitative study focused on determining the optimal configuration of RAG systems tailored to the specific demands of philosophy.\nThis qualitative investigation centred on several key areas. Model selection involved choosing appropriate generative LLMs and embedding models. Hyperparameter tuning proved critical, encompassing adjustments to the number of documents retrieved (top-k), maximum input/output token lengths, generation parameters like temperature or top-p, and strategies for chunk size and overlap. Furthermore, the study addressed methodological challenges, such as retrieval semantic mismatch, exploring solutions like reranking.\nEarly experiences revealed that a straightforward, textbook implementation of the RAG architecture—comprising a basic retrieval component and a generation component—produced unsatisfactory answers, often inferior to those obtained from a standalone model like GPT. Consequently, enhancing performance required an iterative process of tweaking models and hyperparameters, and increasing the sophistication of the underlying algorithms, for instance, by adding a reranking step. This improvement strategy relied on what developers termed “theoretically grounded trial and error,” constantly asking by which measures the answers were improving.\nA significant hurdle emerged in evaluation. Establishing sound evaluation standards proved crucial, particularly because philosophical answers are typically free-form, unstructured texts rather than simple atomic facts (unlike a query such as “What was Wittgenstein’s last place of living?”, which expects a straightforward factual response). Assessing whether the complex propositions within these narrative answers accurately reflect philosophical truths is an inherently challenging task. The implemented system comprised a frontend for user interaction and a backend built with a few thousand lines of Python code.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#frontend-interface-and-comparative-evaluation-framework",
    "href": "chapter_ai-nepi_012.html#frontend-interface-and-comparative-evaluation-framework",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.4 Frontend Interface and Comparative Evaluation Framework",
    "text": "12.4 Frontend Interface and Comparative Evaluation Framework\n\n\n\nSlide 13\n\n\nDevelopers designed a frontend interface, titled “SEP RAG Overview”, to facilitate user interaction and system configuration. This interface includes panels for “Configuration” and “Options,” allowing users to adjust several hyperparameters. These options include:\n\nSelecting the “Generative Model”\nSetting the “Prompt Token Limit” (both for the model itself and a user-defined limit for the RAG system)\nDefining a “Persona” to influence the LLM’s response style\nSpecifying the number of texts to retrieve (top-k)\n\nA dedicated field allows users to input their “Philosophical Question,” exemplified by “What is priority monism?”.\nCrucially, the interface incorporates a comparative setup for qualitative evaluation. It presents two text areas side-by-side: one displaying the “Answer with LLM alone,” serving as a benchmark, and the other showing the “Answer with RAG.” This parallel presentation allows for a more effective comparison of the outputs. Answers generated by the RAG system feature inline citations, such as “[Text 0],” which link to the specific source texts used.\nBelow the answer fields, a “Retrieved Texts Overview” section details the documents fetched by the retrieval component. This information appears in a table with columns for file names, section headings, text length in tokens, total token count, and an “included” status. This table clarifies which texts were incorporated into the final prompt for the LLM and which were omitted due to token limits, sometimes using visual highlighting for clarity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunking-strategy-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-chunking-strategy-for-philosophical-corpora",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.5 Optimising Chunking Strategy for Philosophical Corpora",
    "text": "12.5 Optimising Chunking Strategy for Philosophical Corpora\n\n\n\nSlide 15\n\n\nThe choice of chunk size emerged as a critical hyperparameter requiring careful optimisation. Researchers initially considered several a priori options for segmenting the corpus: using a fixed number of words (for example, 500 words or tokens), a common and straightforward criterion in computer science; chunking by paragraphs; or chunking by entire sections.\nExperiments revealed that, for the Stanford Encyclopedia of Philosophy, the most effective strategy involved chunking the content into its main sections, complete with their headings. This finding suggests that philosophical facts and arguments, which are rarely brief or isolated, benefit from the more extensive contextual space provided by longer semantic units. Sections, therefore, allowed for a more coherent presentation of complex ideas.\nA particularly noteworthy observation was that the average length of these sections—approximately 3,000 words—far exceeded the 512-word cutoff limit of the embedding model employed at the time. Nevertheless, this approach produced the best results. Researchers hypothesised that this success stemmed from the highly systematic and well-ordered nature of the SEP, where the initial portion (around 500 words) of a section often effectively summarises its main theme, thereby providing a strong signal for the embedding model. It is important to recognise, however, that this particular chunking strategy might not generalise well to corpora that are more heterogeneous or less rigorously structured.\nLooking ahead, the team plans to experiment with embedding models capable of handling longer context windows, such as Cohere Embed 3. The overarching lesson from this optimisation process is that effective chunking is not a one-size-fits-all solution; rather, it depends profoundly on the specific characteristics of the corpus and the types of questions under investigation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#enhancing-retrieval-relevance-with-reranking",
    "href": "chapter_ai-nepi_012.html#enhancing-retrieval-relevance-with-reranking",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.6 Enhancing Retrieval Relevance with Reranking",
    "text": "12.6 Enhancing Retrieval Relevance with Reranking\n\n\n\nSlide 18\n\n\nTo further refine the quality of information fed to the generative model, engineers incorporated a reranking step subsequent to the initial document retrieval phase. This addition addresses the common problem of false positives, where the initial retrieval process might return documents not genuinely relevant to the user’s query. The primary aim of reranking, therefore, is to reorder these retrieved documents according to a more sophisticated measure of their actual relevance.\nThe implemented solution leverages the generative LLM (gLLM) itself to perform this evaluation. This choice rests on the premise that gLLMs exhibit more advanced semantic differentiation capabilities than standard embedding models. The gLLM assesses the retrieved texts based on criteria such as their informativeness and the length of the relevant passage they contain, subsequently calculating a “Total Score” to reflect overall relevance.\nEvaluation of this reranking mechanism demonstrated its efficacy, yielding very good results in terms of improving the pertinence of documents used for answer generation. However, this enhancement comes at a cost: the additional calls to the gLLM for evaluation purposes multiply the overall operational expenses of the system.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#conclusions-on-rag-systems-in-philosophy-advantages-cautions-and-challenges",
    "href": "chapter_ai-nepi_012.html#conclusions-on-rag-systems-in-philosophy-advantages-cautions-and-challenges",
    "title": "12  Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research",
    "section": "12.7 Conclusions on RAG Systems in Philosophy: Advantages, Cautions, and Challenges",
    "text": "12.7 Conclusions on RAG Systems in Philosophy: Advantages, Cautions, and Challenges\n\n\n\nSlide 18\n\n\nThe application of RAG systems to philosophy offers several distinct advantages. These systems capably integrate verbatim corpora with specialised domain knowledge, leading to answers that are more detailed and less prone to hallucination. Furthermore, they support the crucial academic practice of citing relevant source documents. Consequently, the RAG setup proves itself exceptionally well suited for assisting in numerous scientific and scholarly tasks.\nNevertheless, researchers must exercise caution when implementing RAG systems. These systems inherently require meticulous tweaking; optimal settings for hyperparameters and model choices are contingent upon the specific corpus and the nature of the questions posed. Evaluation emerges as a critical component, demanding a representative set of questions and expected answers. A particular challenge lies in evaluating performance on unexplored corpora where established benchmarks may be absent, underscoring that domain experts—philosophers, in this context—are indispensable for this process.\nCertain challenges also persist. Should the retrieval mechanism fail to find relevant documents, the quality of the generated answer often diminishes; adjusting the prompt in such instances can offer a partial remedy. Interestingly, for widely discussed overview questions, such as “What are the central arguments against scientific realism?”, RAG systems sometimes yield results inferior to those of standalone LLMs, again suggesting the need for prompt adjustments in these cases.\nLooking towards the future, the development of more flexible and adaptive architectures, such as agentic RAG systems, appears to be a promising direction for addressing these complexities and further enhancing the utility of RAG in scholarly research.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval Augmented Generation (RAG) Systems to Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptual-framework-quantum-gravity-and-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#conceptual-framework-quantum-gravity-and-plural-pursuit",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.1 Conceptual Framework: Quantum Gravity and Plural Pursuit",
    "text": "13.1 Conceptual Framework: Quantum Gravity and Plural Pursuit\n\n\n\nSlide 01\n\n\nThis investigation delves into general philosophy of science questions, employing a combination of computational linguistic analysis and social network analysis to examine the structure of scientific research. Quantum gravity serves as the primary case study. This work, a collaborative effort with Mike Schneider of the University of Missouri, proceeds by first constructing a bottom-up model of the quantum gravity research landscape, then comparing this model to physicists’ own perceptions of their field’s organisation.\nFundamental physics confronts a long-standing challenge: formulating a quantum theory of gravity that harmonises our understanding of phenomena at the smallest and largest scales. Numerous theoretical avenues address this problem. String theory stands as the most prominent amongst a diverse array, which also includes supergravity, loop quantum gravity (and its spin foam variant), causal set theory, and asymptotic safety. To account for this multiplicity of efforts, researchers introduce the notion of “plural pursuit.” Mike Schneider defines this as a situation featuring distinct yet concurrent instances of normal science, all directed towards a shared problem-solving objective—in this case, the unification of quantum mechanics and gravitation.\nCrucially, each instance of normal science articulates through a specific social community linked to an intellectual disciplinary matrix. This conceptualisation draws upon established ideas within the philosophy of science, such as Thomas Kuhn’s paradigms, Larry Laudan’s research traditions, and Imre Lakatos’ research programmes. From this framework, a central empirical question emerges: Does quantum gravity research indeed represent an instance of plural pursuit? More precisely, does the field comprise independent communities, each pursuing different scientific paradigms in parallel?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#methodology-for-bottom-up-landscape-reconstruction",
    "href": "chapter_ai-nepi_015.html#methodology-for-bottom-up-landscape-reconstruction",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.2 Methodology for Bottom-Up Landscape Reconstruction",
    "text": "13.2 Methodology for Bottom-Up Landscape Reconstruction\n\n\n\nSlide 04\n\n\nTo answer the empirical question about plural pursuit, researchers first undertake a bottom-up reconstruction of the quantum gravity research landscape. This reconstruction aims to map not only the intellectual structure, as revealed through linguistic patterns, but also the social fabric of the field. The foundation for this analysis is a substantial corpus comprising 228,748 abstracts and titles from theoretical physics publications, gathered from the Inspire HEP database.\nLinguistic analysis reconstructs the intellectual structure, for which the Bertopic pipeline serves as the primary tool. Initially, documents transform into representations within an embedding space (L.1). Subsequently, unsupervised clustering techniques (L.2) partition this space, yielding a highly granular set of 611 distinct topics. Such fine-graining proves necessary because some approaches within quantum gravity constitute niche areas of research, perhaps encompassing as few as one hundred papers. Finally, based on this topical classification, each physicist receives an assigned specialty (σ)—their most prevalent topic across their body of work (L.3). This step effectively organises authors based on the identified linguistic structure of the field.\nIn parallel, a social network analysis illuminates the community structure. Researchers construct a co-authorship graph where physicists are nodes and co-authorship relationships form the edges. This network, encompassing approximately 30,000 physicists, then undergoes community detection methods. This process (S.1) reveals around 819 distinct communities operating within the field.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-challenge-of-scale-in-defining-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#the-challenge-of-scale-in-defining-plural-pursuit",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.3 The Challenge of Scale in Defining Plural Pursuit",
    "text": "13.3 The Challenge of Scale in Defining Plural Pursuit\n\n\n\nSlide 06\n\n\nWithin this computational framework, plural pursuit translates to an idealised one-to-one mapping between social communities and intellectual topics. If such a clear division of labour existed, a correlation matrix plotting communities against topics would exhibit a distinct block-diagonal structure, with each community clearly specialising in one topic. However, when researchers construct this matrix using their initial fine-grained partitions of 819 communities and 611 topics, the result is a complex, seemingly disordered pattern. The correlation itself is quantified using a normalised pointwise mutual information metric, npmi(c, k), with the specific formula (log P(c_a = c) P(sigma_a = k)) / (log P(c_a = c, sigma_a = k)) - 1.\nSeveral factors contribute to this observed complexity. Firstly, the extreme fine-graining of the topic partition, whilst intentional, is somewhat arbitrary; a coherent research programme like string theory might consequently appear scattered across multiple small topics. Secondly, large research programmes can be undertaken by several distinct communities simultaneously, a phenomenon shaped by diverse micro-social dynamics. More fundamentally, the very computational notions of “topic” and “community” are scale-dependent—meaning that both literature and social networks can be analysed and partitioned at various levels of resolution.\nThis technical challenge mirrors a conceptual reality: research programmes themselves are often nested. String theory, for example, encompasses various sub-families such as Superstring Theory, which in turn includes Type II and Heterotic string theories, each with further subdivisions. Consequently, identifying genuine instances of plural pursuit demands a strategy to navigate and resolve the ambiguities introduced by these multiple, interacting scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-modelling-and-adaptive-coarse-graining-of-topics",
    "href": "chapter_ai-nepi_015.html#hierarchical-modelling-and-adaptive-coarse-graining-of-topics",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.4 Hierarchical Modelling and Adaptive Coarse-Graining of Topics",
    "text": "13.4 Hierarchical Modelling and Adaptive Coarse-Graining of Topics\n\n\n\nSlide 09\n\n\nTo manage the scale-dependency, researchers propose a hierarchical reconstruction of the quantum gravity landscape. For intellectual structure, they implement Ward agglomerative clustering, starting with the 611 fine-grained topics and iteratively merging them to build a topic hierarchy. For social structure, they employ hierarchical stochastic block modelling, a technique (following Peixoto, 2014) that inherently learns a multi-level partition of the network into increasingly coarse-grained communities. These hierarchical frameworks permit observation of the system at various scales; for instance, physicists in the co-authorship network can be coloured by their specialty, where “specialty” can be defined at different levels of topical aggregation.\nNevertheless, selecting specific scales for topics and communities remains a challenge, as different choices yield markedly different correlation matrices and, consequently, interpretations. To address this, an adaptive topic coarse-graining strategy is introduced. The core idea is to simplify the detailed topic structure by merging topics, provided such mergers do not discard information crucial for understanding the social organisation of the field. Essentially, linguistic distinctions that lack social consequence are consolidated. This process is governed by the Minimum Description Length (MDL) criterion, which seeks a topic partition (σ) that minimises the sum of a model fit term (- log P(G|σ), where G is the social graph) and a model complexity term (log P(σ)). This information-theoretic approach balances the imperative for the topic structure to explain the social network against the preference for a simpler, less granular partition.\nApplying this MDL-guided coarse-graining reduces the initial 600 topics to a more manageable set of 50. Significantly, this procedure preserves certain small-scale linguistic topics, indicating their importance in reflecting the social structure, whilst other topics are legitimately amalgamated into broader categories. This result underscores the value of initially generating a highly fine-grained topic classification, as some of those detailed distinctions prove indispensable for a nuanced understanding of the field’s social dynamics.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#results-coarse-grained-topics-and-community-correlations",
    "href": "chapter_ai-nepi_015.html#results-coarse-grained-topics-and-community-correlations",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.5 Results: Coarse-Grained Topics and Community Correlations",
    "text": "13.5 Results: Coarse-Grained Topics and Community Correlations\n\n\n\nSlide 13\n\n\nThe adaptive coarse-graining procedure yields a final set of 50 topics. Researchers assign descriptive labels to these topics by extracting representative N-grams, resulting in identifiers such as “cosmic energy model,” “black hole entropy,” “string theory,” “quantum field theory,” and “holographic entanglement entropy.” With this refined topic structure, the investigation returns to the community-topic correlation matrix. Each column in this matrix now represents one of the 50 coarse-grained topics, and the analysis seeks to identify, for each topic, the “best-fit” community structure from the various levels of the social hierarchy.\nThis refined correlation analysis reveals several important patterns. Some topics, such as a particularly large one (depicted in purple on the visualisations, possibly related to general quantum field theory), do not strongly associate with any single community; rather, they appear to be of interest across multiple communities, suggesting a foundational or broadly applicable nature. In contrast, the “string theory” topic aligns quite well with a community structure found at the third level of the social hierarchy. Conversely, other distinct research programmes within quantum gravity, like loop quantum gravity, seem to correspond to communities identified at much more fine-grained levels of the hierarchy.\nCrucially, the analysis uncovers evidence of nested structures that complicate a straightforward interpretation of plural pursuit. For instance, a community apparently dedicated to the intellectual topic of “holography” is observed as a sub-structure within the broader “string theory” community. Such findings indicate that the research landscape in quantum gravity is not characterised by a neat division of intellectual labour among distinct social groups. Instead, there is an entanglement of different scales, with intricate, nested relationships between topics and communities.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-validation-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#top-down-validation-physicists-intuitions",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.6 Top-Down Validation: Physicists’ Intuitions",
    "text": "13.6 Top-Down Validation: Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nTo complement the bottom-up reconstruction, researchers initiated a top-down approach by directly soliciting the intuitions of experts. They surveyed the founding members of the International Society for Quantum Gravity, asking them to list the quantum gravity approaches they perceive as structuring the overall research landscape. Although responses varied, a composite list of influential approaches was compiled from their feedback. This list includes asymptotic safety, causal sets, dynamical triangulations, group field theory, loop quantum gravity (LQG), spin foams, noncommutative geometry, the swampland programme, modified dispersion relations, doubly special relativity (DSR), quantum modified black holes, shape dynamics, tensor models, string theory, supergravity, and holography.\nFor a more focused comparison, the analysis zooms in on three particular approaches: string theory, supergravity, and holography. This selection stems from an interesting divergence in physicists’ opinions; whilst some view them as distinct, others contend that supergravity and holography are, at a fundamental level, integral parts of string theory, notwithstanding their different historical origins and conceptual nuances.\nTo map papers onto these physicist-defined categories, an SVM classifier was developed. This classifier, trained on hand-coded labels for a training set of papers, learns to predict which of the designated approaches a given paper belongs to, using text embeddings (generated by the all-MiniLM-L6-v2 model from titles and abstracts) as input features.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#juxtaposition-bottom-up-and-top-down-views",
    "href": "chapter_ai-nepi_015.html#juxtaposition-bottom-up-and-top-down-views",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.7 Juxtaposition: Bottom-Up and Top-Down Views",
    "text": "13.7 Juxtaposition: Bottom-Up and Top-Down Views\n\n\n\nSlide 16\n\n\nThe culmination of this dual approach involves confronting the top-down, supervised classification of papers (based on physicists’ intuitions) with the bottom-up, unsupervised discovery of coarse-grained topics. A heatmap effectively illustrates these correlations, yielding several insights. For certain physicist-defined approaches, particularly those that are well-established and conceptually autonomous, a strong correspondence with the computationally derived topics emerges. Conversely, approaches that are more phenomenological in nature, or perhaps less developed as comprehensive conceptual frameworks, tend not to align clearly with the bottom-up topics.\nA particularly interesting finding concerns string theory and supergravity. The bottom-up analysis reveals a substantial “string theory” cluster that seems to absorb papers classified under both “string theory” and “supergravity” in the top-down scheme. This computational result finds an echo in the qualitative feedback from the physicist survey. One respondent noted, “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on ‘supergravity’ and ‘string theory’ is so large that I’m not sure the communities can be separated in a meaningful way.”\nThus, the socio-epistemic picture painted by the bottom-up analysis—where string theory and supergravity are effectively merged once fine-grained linguistic distinctions without strong social correlates are filtered out—converges with this expert intuition. This convergence is notable because the initial, highly granular linguistic clustering did acknowledge the conceptual and historical distinctions between these areas, highlighting the power of the scale-aware coarse-graining to reveal deeper structural alignments.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-outlook",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-outlook",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.8 Conclusions and Philosophical Outlook",
    "text": "13.8 Conclusions and Philosophical Outlook\n\n\n\nSlide 21\n\n\nSeveral key conclusions emerge from this multifaceted investigation. Firstly, socio-epistemic systems, exemplified by scientific research fields, demonstrably operate and can be observed at multiple scales. This inherent multi-scalarity implies that core analytical concepts such as “communities” and “disciplinary matrices” are themselves scale-dependent constructs. Secondly, identifying configurations of plural pursuit—which the researchers operationalise as a one-to-one mapping between distinct communities and their specific intellectual foundations—necessitates sophisticated methods. These methods must be capable of aligning social and intellectual structures appropriately across these varying scales.\nThirdly, focusing on the quantum gravity case study, the bottom-up reconstruction of its research landscape proves to be a potent analytical tool. Such reconstructions can either corroborate the existing intuitions of physicists about their field or, by uncovering less obvious structural patterns, prompt a critical re-assessment of those intuitions.\nMore broadly, this work suggests a significant philosophical implication: the continuing development of powerful computational techniques offers novel means to revisit, refine, and potentially challenge philosophical insights that have long been based on intuition or smaller-scale qualitative analysis. This is particularly true for understanding the structure of scientific paradigms or the nature of research communities, as illustrated in the complex domain of quantum gravity. In a final, evocative remark, the presenter suggests, by paraphrasing Clausewitz, that computation may be seen as the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#optimal-text-levels-for-topic-modelling",
    "href": "chapter_ai-nepi_016.html#optimal-text-levels-for-topic-modelling",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Optimal Text Levels for Topic Modelling",
    "text": "14.1 Optimal Text Levels for Topic Modelling\n\n\n\nSlide 01\n\n\nFrancis Lareau, representing a collaborative effort with Christophe Malaterre, initiated the discourse by outlining a comparative investigation. This study explored the performance of two prominent topic modelling techniques—Latent Dirichlet Allocation (LDA) and BERTopic—when applied to distinct levels of textual data: titles, abstracts, and full texts of scientific articles. Topic modelling, defined as the computational extraction of underlying themes from a corpus, serves as a critical analytical instrument for navigating substantial volumes of scientific literature. Its utility proves particularly pronounced in disciplines such as the History, Philosophy, and Sociology of Science (HPSS).\nWithin HPSS, researchers employ topic modelling for a variety of analytical tasks. These encompass the identification of emergent research trends and significant paradigm shifts, the discernment of thematic substructures and their interrelations, and the meticulous analysis of evolving scientific vocabulary. A key observation underpinning this study highlights that existing scholarly works frequently apply topic modelling methodologies to these varied textual structures—titles, abstracts, or complete full texts. This prevalent practice directly informed the central research inquiry: does the application of topic modelling to concise textual elements like titles or abstracts yield sufficiently robust results, or does comprehensive full-text analysis remain a necessity for rigorous thematic exploration?\nTo address this question systematically, the investigators outlined a multi-stage methodology. Initially, they assembled a dedicated corpus of scientific articles. For each article within this corpus, they carefully delineated and extracted the title, abstract, and full-text components. Subsequently, the team applied both LDA and BERTopic algorithms independently to each of these three textual datasets. This process generated a total of six distinct topic models. Finally, the researchers subjected these six models to an in-depth comparative analysis, employing both qualitative and quantitative evaluation methods to ascertain their respective strengths and weaknesses. A visual flowchart further clarified this process, depicting the scientific corpus branching into titles, abstracts, and full texts, each subsequently processed by both LDA and BERTopic, leading to final qualitative and quantitative analyses.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparative-overview-of-lda-and-bertopic-methodologies",
    "href": "chapter_ai-nepi_016.html#comparative-overview-of-lda-and-bertopic-methodologies",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Comparative Overview of LDA and BERTopic Methodologies",
    "text": "14.2 Comparative Overview of LDA and BERTopic Methodologies\n\n\n\nSlide 06\n\n\nInvestigators compared two distinct topic modelling approaches: Latent Dirichlet Allocation (LDA) and BERTopic. Both methodologies operate on shared fundamental postulates: they represent documents as numerical vectors; they identify topics by discerning patterns of repetition that underscore linguistic regularities; and they leverage machine learning algorithms to automate the detection of these regularities.\nLDA, a classical statistical technique, constructs simple vector representations derived from word counts within documents. In this framework, topics manifest as latent variables, with Dirichlet’s law governing their distribution. A significant advantage of LDA is its inherent capability to process long texts, rendering it suitable for analysing titles, abstracts, or entire full-text documents.\nConversely, BERTopic represents a more recent, modular approach to topic modelling. It employs sophisticated vector representations based on Large Language Models (LLMs)—initially BERT, from which it derives its name. Within BERTopic, topics emerge as clusters of semantically similar documents. Whilst earlier iterations of BERTopic faced limitations in processing long texts, recent advancements in embedding techniques have overcome this constraint.\nFor this study, researchers specifically incorporated a new embedding method capable of handling extensive texts, up to approximately 131,000 tokens. The researchers chose Stella EN 1.5B version 5 as the embedding model, selecting it for its leading performance on the Massive Text Embedding Benchmark (MTEB) hosted on Hugging Face, its substantial token capacity, and its strong average scores across a range of diverse tasks. The BERTopic framework, developed by Martin Grootendorst, offers considerable modularity, permitting users to select different technical components for each stage of the modelling pipeline. In this investigation, the team utilised standard BERTopic configurations, with the crucial exception of the advanced embedding model employed for the initial text representation phase.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#astrobiology-corpus-material-and-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#astrobiology-corpus-material-and-qualitative-analysis",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Astrobiology Corpus: Material and Qualitative Analysis",
    "text": "14.3 Astrobiology Corpus: Material and Qualitative Analysis\n\n\n\nSlide 07\n\n\nThe empirical basis for this comparative study rested upon an Astrobiology corpus, which had previously undergone a thorough topic analysis. From this prior work, and after a careful evaluation process, investigators selected a full-text LDA model comprising 25 distinct topics to serve as a reference. They had analysed these 25 topics in considerable detail, examining the most representative words and documents associated with each one. Subsequently, they assigned descriptive names to these topics based on the predominant keywords.\nFurther analysis involved comparing the topics by assessing their mutual correlation, a measure calculated from the co-occurrence patterns of topics within the corpus documents. Applying a community detection algorithm to this correlation data, the researchers successfully identified four overarching thematic clusters. They designated these clusters using letters (A, B, C, and D) and visually distinguished them by distinct colours (red, green, yellow, and blue, respectively).\nThe researchers presented a graphical network that visualised these inter-topic relationships. In this graph, each of the 25 topics appeared with its assigned name and the colour corresponding to its cluster. The thickness of the lines connecting any two topics directly represented the strength of their mutual correlation. Additionally, the size of the circles (nodes) denoting each topic signified its overall prevalence across the entire collection of documents. This comprehensive existing analysis of the Astrobiology corpus provided a robust qualitative framework, enabling a nuanced comparison of the six new topic models developed in the current investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics-for-model-comparison",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics-for-model-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Quantitative Analysis Metrics for Model Comparison",
    "text": "14.4 Quantitative Analysis Metrics for Model Comparison\n\n\n\nSlide 08\n\n\nFor the quantitative dimension of their comparative analysis, the researchers employed a suite of four distinct metrics to evaluate the performance and characteristics of the topic models. The first metric, the Adjusted Rand Index (ARI), serves to quantify the similarity between any two models by examining the extent to which they group the same documents together. An ARI score of zero signifies that the clustering agreement between two models is no better than random chance.\nThe second metric, Topic Diversity, assesses the uniqueness of the salient words that define different topics; higher diversity suggests less overlap in the vocabularies of distinct themes. Thirdly, Joint Recall evaluates how comprehensively the top-ranking words of a given topic collectively represent all the documents assigned to that topic. Finally, Coherence CV measures the semantic relatedness and interpretability of the top words within individual topics, providing an indication of topic quality. The presentation slides included the specific mathematical formulae for each of these evaluative measures.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#adjusted-rand-index-model-similarity-analysis",
    "href": "chapter_ai-nepi_016.html#adjusted-rand-index-model-similarity-analysis",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Adjusted Rand Index: Model Similarity Analysis",
    "text": "14.5 Adjusted Rand Index: Model Similarity Analysis\n\n\n\nSlide 10\n\n\nInvestigators presented the results of the Adjusted Rand Index (ARI) calculations, which aimed to illuminate the similarities and divergences amongst the six generated topic models. As a crucial reminder, an ARI score of zero indicates that the agreement in document clustering between two models is merely random. The analysis, visualised as a heat map, revealed several key patterns.\nNotably, the LDA model applied to document titles (LDA Title) demonstrated the most significant dissimilarity when compared to the other five models. This particular model yielded ARI values consistently below 0.2, highlighted by yellow colouration in the heat map, signifying poor concordance. In contrast, the remaining models exhibited a generally better degree of mutual agreement, with their pairwise ARI scores typically exceeding 0.2.\nFurthermore, the BERTopic models, when compared amongst themselves (BERTopic Full-text, BERTopic Abstract, BERTopic Title), showed a notably stronger internal coherence, achieving ARI values greater than 0.35. Within this group, the BERTopic model derived from abstracts (BERTopic Abstract) emerged as a somewhat central point of reference. It displayed good correspondence with nearly all other models, the main exception being its low agreement with the divergent LDA Title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#lda-model-comparisons-full-text-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#lda-model-comparisons-full-text-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 LDA Model Comparisons: Full-text, Abstracts, and Titles",
    "text": "14.6 LDA Model Comparisons: Full-text, Abstracts, and Titles\n\n\n\nSlide 09\n\n\nA more granular analysis focused on the relationships between the LDA models derived from different text levels. When comparing the LDA Full-text model with the LDA Abstract model (Table A), investigators found a generally good fit. Many topics from the full-text model showed a strong tendency to align with a corresponding topic in the abstract model, evidenced by a high proportion of shared documents; this alignment formed a noticeable reddish diagonal pattern in the presented table (after data reorganisation for clarity).\nNevertheless, some structural changes occurred: three topics present in the LDA Full-text model did not appear in the LDA Abstract model (visualised as three long, horizontal dark grey lines). Conversely, three full-text topics appeared to split into multiple, more granular topics within the abstract model (indicated by three short, horizontal dark grey lines). Additionally, three entirely new topics emerged in the LDA Abstract model that lacked direct counterparts in the full-text version (shown as three long, vertical dark grey lines). The analysis also identified three abstract topics that seemed to result from the merging of full-text topics (again, described as three short, horizontal dark grey lines). The researchers also noted a minor issue: the presence of one small class in the abstract topics, containing fewer than 50 documents, as revealed by the total document counts per topic.\nIn stark contrast, the comparison between the LDA Full-text model and the LDA Title model (Table B) revealed a poor fit. This lack of correspondence manifested as extensive reorganisations of topics and document assignments, visually represented by a proliferation of dark vertical and horizontal lines throughout Table B. Such patterns indicated substantial divergence in how these two models structured the thematic content of the corpus.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#bertopic-model-comparisons-against-lda-full-text",
    "href": "chapter_ai-nepi_016.html#bertopic-model-comparisons-against-lda-full-text",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 BERTopic Model Comparisons: Against LDA Full-text",
    "text": "14.7 BERTopic Model Comparisons: Against LDA Full-text\n\n\n\nSlide 11\n\n\nThe investigation continued by comparing the baseline LDA Full-text model against the three BERTopic models. When contrasting LDA Full-text with BERTopic Full-text (Table C), the researchers assessed the overall fit as average. From the perspective of the LDA Full-text topics (horizontal axis), eight topics effectively disappeared, whilst six others underwent splitting when mapped to the BERTopic Full-text model. Conversely, examining the BERTopic Full-text topics (vertical axis), five new topics emerged, and one topic appeared to be the result of merged LDA Full-text topics. Analysis of document distribution revealed class size imbalances in the BERTopic Full-text model, specifically four small classes and one notably very large class.\nNext, the comparison between LDA Full-text and BERTopic Abstract (Table D) indicated a relatively good overall fit. In this scenario, four LDA Full-text topics disappeared, and six were split. From the BERTopic Abstract model’s perspective, two new topics materialised, and four topics formed through mergers.\nFinally, contrasting LDA Full-text with BERTopic Title (Table E) yielded an average fit. Seven LDA Full-text topics disappeared, and one was split. The BERTopic Title model introduced seven new topics and one topic resulting from a merger. Similar to the BERTopic Full-text model, the BERTopic Title model also exhibited class size issues, with three small classes and one large class identified from the document counts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#qualitative-top-word-analysis-lda-models",
    "href": "chapter_ai-nepi_016.html#qualitative-top-word-analysis-lda-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Qualitative Top-Word Analysis: LDA Models",
    "text": "14.8 Qualitative Top-Word Analysis: LDA Models\n\n\n\nSlide 13\n\n\nDelving into a qualitative assessment, researchers examined the top words defining topics across the different LDA models. The researchers generally observed that the LDA Full-text, LDA Abstract, and LDA Title models generated topics that were, on the whole, relatively well-formed and interpretable.\nCertain topics demonstrated remarkable robustness. For instance, the topic labelled “A-Radiation spore” exhibited very good correspondence in its defining top words across all three LDA models, indicating stability regardless of the text level used. Other topics from the full-text model, however, underwent structural changes. The researchers observed the topic “A-Life civilization,” for example, splitting into several more specific topics in both the abstract and title models; this particular split appeared logical, likely reflecting a broad theme concerning astrobiology research fragmenting into more focused sub-themes. In contrast, the topic “B-Chemistry” also split, but the rationale behind this fragmentation proved more challenging to discern without deeper analytical investigation.\nThe analysis also identified instances of topic merging. Specifically, the full-text topics “B-Amino acid” and “B-Protein gene RNA” coalesced into a single, more encompassing topic in the other LDA models. The researchers deemed this merger sensible, as it consolidated related concepts into a broader thematic category.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#qualitative-top-word-analysis-bertopic-models",
    "href": "chapter_ai-nepi_016.html#qualitative-top-word-analysis-bertopic-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Qualitative Top-Word Analysis: BERTopic Models",
    "text": "14.9 Qualitative Top-Word Analysis: BERTopic Models\n\n\n\nSlide 15\n\n\nThe qualitative assessment of top words extended to the three BERTopic models, comparing them against the baseline LDA Full-text model and amongst themselves. Similar to the LDA comparisons, the BERTopic Full-text, BERTopic Abstract, and BERTopic Title models generally generated topics that appeared relatively well-formed and coherent.\nThe previously identified robust topic, “A-Radiation spore,” continued to demonstrate strong stability, maintaining its core characteristics across all BERTopic model variants. Another significant topic, “A-Life civilization,” also showed considerable stability when analysed with BERTopic. However, it did exhibit some instances of splitting across the different BERTopic models, resulting in the formation of more narrowly defined topics specifically focused on aspects of extraterrestrial life.\nFurthermore, the theme “B-Chemistry,” which had shown complex splitting behaviour in the LDA comparisons, also tended to fragment into narrower, more specialised topics when processed by the various BERTopic models. This consistent pattern of splitting for certain broad topics across different modelling approaches and text levels suggests inherent complexities in their thematic structure.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-coherence-cv",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-coherence-cv",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Quantitative Analysis: Coherence (CV)",
    "text": "14.10 Quantitative Analysis: Coherence (CV)\n\n\n\nSlide 15\n\n\nInvestigators then presented performance metrics for all six models, evaluated across a range of topic numbers from 5 to 50. The researchers first discussed Coherence CV, a metric that assesses the semantic meaningfulness and interpretability of the top words defining each topic.\nSeveral distinct patterns emerged from this analysis. Models based on document titles (both LDA Title and BERTopic Title) consistently yielded the poorest coherence scores. In contrast, models derived from abstracts (LDA Abstract and BERTopic Abstract) demonstrated superior coherence compared to their full-text counterparts (LDA Full-text and BERTopic Full-text). When comparing the two algorithmic approaches, BERTopic generally achieved better coherence scores than LDA, particularly for models built on abstracts and titles. However, this advantage for BERTopic tended to lessen as the specified number of topics for the models increased.\nConsidering all factors, the BERTopic Abstract model clearly emerged as the top performer in terms of topic coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-topic-diversity",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-topic-diversity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Quantitative Analysis: Topic Diversity",
    "text": "14.11 Quantitative Analysis: Topic Diversity\n\n\n\nSlide 16\n\n\nThe analysis then shifted to Topic Diversity, a metric that quantifies the uniqueness of the top words characterising each topic; higher diversity indicates less lexical overlap between topics. The researchers also conducted this evaluation for models with topic numbers ranging from 5 to 50.\nThe researchers observed a general trend: topic diversity tended to decrease as the total number of topics in a model increased. Regarding the impact of text level, models constructed from titles (LDA Title, BERTopic Title) generally offered better diversity scores than those based on abstracts or full texts. Comparing the algorithmic approaches, BERTopic models consistently demonstrated superior diversity compared to LDA models.\nThe leading model for this metric was BERTopic Title, with the BERTopic Full-text model performing almost as well, securing a close second place.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-joint-recall",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Quantitative Analysis: Joint Recall",
    "text": "14.12 Quantitative Analysis: Joint Recall\n\n\n\nSlide 17\n\n\nThe researchers examined Joint Recall as the third quantitative metric. This measure evaluates how effectively the set of top words associated with a topic collectively represents every document classified under that specific topic. The analysis again encompassed models with 5 to 50 topics.\nThe results indicated that models based on titles yielded the poorest joint recall scores. Conversely, full-text models (LDA Full-text, BERTopic Full-text) demonstrated superior performance in joint recall when compared to their counterparts derived from abstracts and titles.\nThe top-performing models for joint recall were LDA Full-text and BERTopic Full-text. The BERTopic Abstract model also performed commendably, achieving scores very close to those of the leading full-text models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#model-performance-a-consolidated-overview",
    "href": "chapter_ai-nepi_016.html#model-performance-a-consolidated-overview",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Model Performance: A Consolidated Overview",
    "text": "14.13 Model Performance: A Consolidated Overview\n\n\n\nSlide 16\n\n\nInvestigators consolidated the various analytical results into a summary table, offering an overall perspective on the performance of the six topic models. This table used a system of circles (filled for highest score, white for lesser score, and ‘X’ for significant issues) to denote performance across five key assessment criteria: overall fit (based on ARI and qualitative comparisons), top-word quality, coherence, diversity, and joint recall.\nCrucially, the researchers emphasised that no single model emerges as universally superior; the optimal choice invariably depends on specific research objectives. For instance, if the primary goal is the discovery of major thematic trends, without a stringent requirement for precise classification of every document, then metrics like poor recall or the presence of imbalanced class sizes might be less critical. In such a scenario, the BERTopic Full-text model demonstrated strong performance in uncovering topics, albeit with some noted issues of class imbalance. The BERTopic Title model, whilst generally suboptimal, did succeed in identifying some robust topics that were consistently found across other, more comprehensive models.\nHowever, the researchers clearly did not recommend the LDA Title model, as it performed poorly across nearly all evaluation criteria. Based on these comprehensive findings, the researchers generally advised conducting topic modelling on either abstracts or full texts. Both LDA and BERTopic are viable algorithmic choices for these text levels, with the caveat that the chosen combination should not result in significant misclassification of documents pertinent to the topics of interest.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-conclusions-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-conclusions-and-future-directions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.14 Discussion, Conclusions, and Future Directions",
    "text": "14.14 Discussion, Conclusions, and Future Directions\n\n\n\nSlide 20\n\n\nIn concluding the presentation, Francis Lareau summarised several key findings and their implications. Firstly, the research highlighted the generally poor performance of topic models derived solely from titles. A plausible explanation for this lies in the inherent brevity of titles, which often lack sufficient informational content, thereby increasing the risk of erroneous document classifications. Nevertheless, the BERTopic Title model notably generated a considerable number of meaningful topics. This suggests a potential need to strike a balance between achieving well-defined, coherent topics and ensuring adequate document coverage for each topic.\nSecondly, full-text models, whilst information-rich, sometimes encounter challenges in processing the sheer volume of data. When applied to full texts, LDA can occasionally yield topics that are somewhat loosely defined and overly broad in scope. Furthermore, full-text analysis might identify some topics of secondary interest, such as those pertaining to methodology rather than core content. BERTopic, when applied to full texts, can sometimes produce topics that are overly narrow or specific, which may lead to issues of poor document coverage and imbalanced class sizes.\nThirdly, abstract-based models appeared to strike an effective balance, performing well by leveraging the summary information contained within abstracts. The LDA Abstract and BERTopic Abstract models yielded results demonstrating good consistency with the LDA Full-text model’s findings, suggesting that abstracts can serve as a reliable proxy for thematic content in many instances.\nA fourth significant finding concerned the robustness of numerous topics across different models and text levels. Investigators observed the recurrent emergence of very similar thematic structures irrespective of the specific algorithm (LDA or BERTopic) or text segment (title, abstract, or full text) employed. This consistency opens avenues for employing meta-analytic techniques to identify the most stable and consistently represented topics across various analytical approaches. Moreover, the relative “distance” or dissimilarity between models could potentially serve as a heuristic for identifying an optimal modelling configuration; in this particular study, the BERTopic Abstract model emerged as such an optimum, aligning well with its strong performance across other quantitative metrics.\nFinally, the research posed the question of whether it is time to develop new types of topic models. The presenter suggested an affirmative answer, proposing that future models could more effectively integrate structural information from documents—leveraging titles, abstracts, and full texts in a combined or hierarchical manner—to extract even more meaningful and nuanced sets of topics or descriptive top words.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-temporal-challenge-for-large-language-models",
    "href": "chapter_ai-nepi_017.html#the-temporal-challenge-for-large-language-models",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.1 The Temporal Challenge for Large Language Models",
    "text": "15.1 The Temporal Challenge for Large Language Models\n\n\n\nSlide 01\n\n\nExisting Large Language Models (LLMs) infer temporal understanding implicitly, through statistical patterns encountered in vast textual corpora. This inherent limitation precludes explicit temporal awareness, a capability researchers argue would substantially enhance their utility in historical analysis and extend to broader applications. Consider, for instance, two statements: “The primary architectures for processing text through NNs are LSTMs (2017)” and “The primary architectures for processing text through NNs are Transformers (2025).” Humans, however, readily discern the temporal context that resolves such apparent contradictions.\nWithin an LLM’s training data, sentences differing only by a few words and their implicit timestamps directly compete. Consequently, the model, striving to minimise error, must favour one statement over the other, inevitably misrepresenting the less favoured assertion. During inference, if prompted with “The primary architectures for processing text through NNs are…”, the model will likely predict ‘Transformers’, reflecting a common recency bias. Retrieving the historically accurate ‘LSTMs’ often necessitates meticulous prompt engineering, such as specifying “In 2017” or altering verb tenses. This process, however, offers little certainty and resembles an imprecise art. A core objective, therefore, involves empowering LLMs to explicitly learn and reproduce evolving patterns within training data as a direct function of time, particularly for generative models designed to replicate observed textual patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#a-probabilistic-framework-for-time-dependent-language-modelling",
    "href": "chapter_ai-nepi_017.html#a-probabilistic-framework-for-time-dependent-language-modelling",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.2 A Probabilistic Framework for Time-Dependent Language Modelling",
    "text": "15.2 A Probabilistic Framework for Time-Dependent Language Modelling\n\n\n\nSlide 05\n\n\nResearchers conceptualise standard LLMs as estimating the probability distribution over a vocabulary for a subsequent token (x_n), given a sequence of preceding tokens (x_1, …, x_n-1). This relationship can be expressed as p(x_n | x_1, …, x_n-1). Crucially, however, this formulation overlooks a fundamental aspect of real-world language: the probability of a token appearing in a specific context often changes over time. A more accurate representation, therefore, explicitly incorporates time (t): p(x_n | x_1, …, x_n-1, t).\nExtending this to an entire sequence of tokens (x_1, x_2, …, x_n) uttered at a particular time t, the joint probability becomes the product of these time-conditioned probabilities: Π_{k=1 to n} p(x_k | x_1, …, x_{k-1}, t). Existing LLMs primarily attempt to capture such temporal drift through in-context learning during inference, relying on cues within the prompt to guide their output towards a specific temporal context, rather than possessing an intrinsic, learned representation of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-time-transformer-pioneering-temporal-awareness",
    "href": "chapter_ai-nepi_017.html#the-time-transformer-pioneering-temporal-awareness",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.3 The Time Transformer: Pioneering Temporal Awareness",
    "text": "15.3 The Time Transformer: Pioneering Temporal Awareness\n\n\n\nSlide 07\n\n\nTo effectively model the time-dependent probability p(x_n | x_1, …, x_n-1, t), engineers considered various approaches. One alternative, time slicing—training distinct models for separate temporal segments—suffers from extreme data inefficiency. Instead, they developed the Time Transformer architecture. This innovative design directly introduces an explicit temporal dimension into the latent semantic features of each token.\nSpecifically, the embedding for a token x at a given time t, denoted E(x, t), is constructed as a vector {e_1(x), e_2(x), …, e_{d-1}(x), φ(t)}. Here, {e_1(x), …, e_{d-1}(x)} are the standard semantic features, and φ(t) is the newly introduced time feature, with d representing the total embedding dimensionality. The Transformer model then receives a sequence of these time-augmented embeddings, [E(x_1, t), E(x_2, t), …, E(x_{n-1}, t)], to predict the time-conditioned probability p_θ(x_n | x_1, …, x_n-1, t). The model parameters (θ) are optimised by minimising the negative log-likelihood across the training data. Through this mechanism, time is systematically injected into each token’s representation, allowing the model to discern and learn how strongly or weakly this temporal dimension influences every token within its context.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-and-preprocessing-for-empirical-validation",
    "href": "chapter_ai-nepi_017.html#dataset-and-preprocessing-for-empirical-validation",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.4 Dataset and Preprocessing for Empirical Validation",
    "text": "15.4 Dataset and Preprocessing for Empirical Validation\n\n\n\nSlide 16\n\n\nFor empirical validation, researchers utilised a corpus of Met Office weather reports, sourced from the UK’s national meteorological service via their digital archive at https://digital.nmla.metoffice.gov.uk/. This dataset, characterised by a limited vocabulary and simple, repetitive linguistic structures, comprises daily reports spanning 2018 to 2024. In total, this amounts to approximately 2,500 reports, each containing between 150 and 200 words.\nThe text preprocessing pipeline eschewed sub-word tokenisation, disregarding case and interpunctuation, which simplified the text and resulted in a vocabulary of 3,395 unique words. A vectorize_layer converted text to numerical representations, though specific details of this layer were not elaborated. The selection of such a dataset, and indeed the model scale, finds parallels in works such as TinyStories, which explore the capabilities of smaller language models on constrained language.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-baseline-a-vanilla-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#the-baseline-a-vanilla-transformer-architecture",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.5 The Baseline: A Vanilla Transformer Architecture",
    "text": "15.5 The Baseline: A Vanilla Transformer Architecture\n\n\n\nSlide 19\n\n\nA modest-scale vanilla Transformer model served as the initial baseline. Its architecture comprised standard components: an embedding layer, positional encoding, dropout mechanisms, multi-head attention, add & norm layers, feed-forward networks, and four multi-head attention decoder blocks, culminating in a final dense layer for output generation. This model contained 39 million parameters, occupying approximately 150 MB of storage—a stark contrast to models such as GPT-4, which features around 1.8 trillion parameters distributed across 120 layers.\nTraining occurred on a system equipped with two A100 GPUs, achieving a rate of 11 seconds per epoch. The associated code for this implementation is publicly available on GitHub (https://github.com/j-buettner/time_transformer). Performance evaluations, including training and validation accuracy curves over 50 epochs, indicated the model successfully learned to reproduce the characteristic language of the weather reports. For instance, when provided with the seed sequence ‘During the night, a band …’, the model generated coherent and contextually appropriate text, such as ‘… of rain moved into scotland northern ireland and northern england…’.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-architectural-refinements-and-time-embedding",
    "href": "chapter_ai-nepi_017.html#time-transformer-architectural-refinements-and-time-embedding",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.6 Time Transformer: Architectural Refinements and Time Embedding",
    "text": "15.6 Time Transformer: Architectural Refinements and Time Embedding\n\n\n\nSlide 15\n\n\nEngineers implemented the Time Transformer with minimal architectural adjustments to the baseline vanilla model. The key modification involves processing time data separately and integrating it with textual information at the embedding stage. Text input proceeds through a standard embedding layer, whilst contemporaneous time data undergoes processing by a dedicated time embedding layer. Subsequently, the outputs from these two embedding layers are concatenated. This combined embedding, now enriched with temporal information, then flows through the familiar sequence of Transformer components: positional encoding, dropout, multi-head attention, add & norm layers, feed-forward networks, and decoder blocks.\nCrucially, the representation of the time dimension remains non-trainable. It is derived from the day of the year, min-max normalised to a value between 0 and 1 using the formula: time_embedding = (day_of_year - 1) / (365 - 1). This ensures a consistent numerical representation of time that the model can associate with linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-of-temporal-drift-learning",
    "href": "chapter_ai-nepi_017.html#empirical-validation-of-temporal-drift-learning",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.7 Empirical Validation of Temporal Drift Learning",
    "text": "15.7 Empirical Validation of Temporal Drift Learning\n\n\n\nSlide 16\n\n\nResearchers conducted several experiments to ascertain whether the Time Transformer could efficiently learn and reproduce temporal drifts within the underlying data distribution. The first experiment, termed ‘synonymic succession’, involved injecting a synthetic temporal drift into the training data. Specifically, the word ‘rain’ was gradually replaced by ‘liquid sunshine’ over the course of a year, with the replacement probability following a sigmoid curve. Subsequent analysis of token sequences predicted for each day of the year confirmed the model successfully reproduced this time-dependent shift, as illustrated by a chart showing increasing ‘liquid sunshine’ occurrences in later months.\nBeyond synthetic changes, the model also demonstrated an ability to capture naturally occurring seasonal patterns inherent in the original weather data. A comparison of monthly occurrences for terms like ‘snow’, ‘sleet’, and ‘wintry’ versus ‘hot’ and ‘warm’ revealed the model learned these expected seasonal distributions.\nA second experiment focused on altering a weather pattern, simulating the linguistic phenomenon known as ‘fixation of a collocation’. Here, instances of ‘rain’ followed by any word other than ‘and’ were progressively modified to become ‘rain and snow’ throughout the year. The results, displayed in a monthly comparison chart, showed the model effectively learned this evolving co-occurrence. For example, outputs for later days in the year were more likely to feature ‘rain and snow’. Further investigation using attention analysis, including heatmaps and bar charts focusing on attention from ‘snow’ to ‘rain’, provided insights into how the model learned the direct relationship within this newly fixed collocation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#conclusions-applications-and-future-trajectories",
    "href": "chapter_ai-nepi_017.html#conclusions-applications-and-future-trajectories",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.8 Conclusions, Applications, and Future Trajectories",
    "text": "15.8 Conclusions, Applications, and Future Trajectories\n\n\n\nSlide 21\n\n\nThe experimental results provide a compelling proof of concept: Transformer-based LLMs can indeed be rendered efficiently time-aware through the straightforward addition of a temporal dimension to their token embeddings. This development opens several promising avenues for application. Firstly, a ‘foundation time transformer’ could provide a robust basis for diverse downstream tasks operating on historical datasets. Secondly, an instruction-tuned version might empower users to interact with the model as if ‘talking to a specific time’, potentially leading to improved performance even when querying contemporary information. Moreover, this architectural principle could be extended to model the influence of other metadata dimensions, such as country of origin or text genre, on token sequence distributions.\nLooking ahead, researchers identify several potential next steps. These include benchmarking the Time Transformer against alternative methods, such as explicit time-token approaches, and rigorously testing for any gains in training efficiency. However, translating this concept into widespread application presents certain challenges. The feasibility and efficiency of fine-tuning models with this altered architecture remain uncertain. Furthermore, the approach diverges from purely metadata-free self-supervised learning, reintroducing the complexities of data curation to ensure accurate temporal tagging. Ascertaining the intended temporal context of any given generated sequence also poses a difficulty. As a potential mitigation for some of these issues, the development of more modest, targeted encoder models for specific time-related functionalities warrants exploration.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-project-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-project-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction: Project Objectives",
    "text": "16.1 Introduction: Project Objectives\n\n\n\nSlide 01\n\n\nResearchers at Saarland University embarked upon a project titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” Diego Alves and Sergei Bagdasarov presented this work, acknowledging the invaluable contributions of their colleague Badr Abdullah, an LLM expert. Originating from the Department of Language Science and Technology, the research was presented at a workshop dedicated to LLMs in the History, Philosophy, and Sociology of Science.\nThe project articulates two primary objectives. Firstly, investigators aim to harness Large Language Models to enhance metadata for historical scientific texts. This encompasses categorising articles into their respective scientific disciplines, generating pertinent semantic tags or topics, and crafting concise abstractive summaries. Secondly, the research seeks to analyse the evolving chemical knowledge space over time, particularly across diverse scientific disciplines. Within this latter objective, the team focuses on identifying periods marked by heightened interdisciplinarity and significant knowledge transfer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#data-source-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#data-source-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 Data Source: The Royal Society Corpus",
    "text": "16.2 Data Source: The Royal Society Corpus\n\n\n\nSlide 01\n\n\nInvestigators ground their research in the evolution of scientific English over centuries, observing its optimisation as a medium for expert-to-expert communication. Beyond this linguistic focus, they also analyse phenomena such as knowledge transfer and the identification of influential papers and authors.\nThe Philosophical Transactions of the Royal Society of London serves as the foundational dataset for these inquiries. First published in 1665, this esteemed journal stands as the oldest scientific periodical in continuous publication, maintaining an unparalleled reputation. It played a pivotal role in developing scientific communication, notably by establishing the practice of peer-reviewed paper publication for disseminating scientific knowledge.\nThe corpus contains numerous influential contributions. For instance, in the 17th century, Isaac Newton published his “New Theory about Light and Colours” (1672). The 18th century documented Benjamin Franklin’s electrical kite experiment. Subsequently, in the 19th century, James Clerk Maxwell detailed his dynamical theory of the electromagnetic field (1865). Alongside such seminal works, the journal also published more speculative pieces, exemplified by Monsieur Autour’s “Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants.” Crucially, the current project does not concern itself with fact-checking or assessing the scientific validity of these historical papers.\nResearchers utilise the RSC 6.0 Full version of the Royal Society Corpus. This comprehensive collection spans over 300 years of scientific writing, from 1665 to 1996, encompassing almost 48,000 texts and nearly 300 million tokens. Whilst some metadata—such as author, century, year, and volume—are already encoded, previous attempts to infer research categories using LDA topic modelling yielded mixed results, often conflating disciplines, subdisciplines, and even text types like “Observation” or “Reporting.” This inherent limitation motivated the turn towards Large Language Models for more refined metadata enrichment. The RSC has evolved through several versions, with RSC 6.0 Full providing the most extensive coverage to date.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-metadata-enrichment-methodology",
    "href": "chapter_ai-nepi_018.html#llm-metadata-enrichment-methodology",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 LLM Metadata Enrichment: Methodology",
    "text": "16.3 LLM Metadata Enrichment: Methodology\n\n\n\nSlide 05\n\n\nTo enhance information management and knowledge organisation for historical texts, researchers leveraged Large Language Models. Their aims encompassed text clean-up, summarisation, improved access and retrieval, refined information extraction, robust categorisation, and the generation of data suitable for populating knowledge graphs. For instance, given an article titled “A Spot in one of the Belts of Jupiter,” the LLM was tasked with providing a hierarchical categorisation (e.g., Astronomy, sub-category Planetary Science), relevant index terms, and a concise “Too Long; Didn’t Read” (TL;DR) summary.\nThe team selected Llama 3 for these tasks, specifically employing the Llama Hermes-2-Pro-Llama-3 8B model. This particular variant, accessible via Hugging Face, underwent fine-tuning for instruction following and excels at producing structured output, such as JSON or YAML formats; it reportedly outperforms models like Mistral and Llama 2. Developers are also progressing with a 400 billion parameter Llama 3 model.\nResearchers engineered a detailed system prompt, casting the LLM in the role of a “librarian.” The LLM’s objective was to read, analyse, and organise the vast collection of articles from the Royal Society of London (1665-1996), thereby creating a structured database to facilitate research. The input comprised OCR-extracted text snippets from articles, alongside existing metadata such as title, author, and publication date, with an acknowledgement of potential OCR inaccuracies.\nThe LLM received four distinct tasks:\n\nSuggest an alternative, more reflective title for each article.\nCompose a 3-4 sentence TL;DR summary suitable for a high-school student.\nIdentify exactly five main topics, akin to Wikipedia keywords.\nDetermine the primary scientific discipline and a suitable second-level subdiscipline.\n\nThe primary discipline had to be chosen from a predefined list of nine: Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities. The subdiscipline was to be a relevant branch of the primary one and could not itself be one of the primary disciplines. An example based on Newton’s work on light and colours illustrated the desired input and YAML output format, which included fields for article_id, revised_title, topics, tldr, scientific_discipline, and scientific_subdiscipline. Crucially, the LLM was instructed to ensure its output was a valid YAML file, devoid of extraneous text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-metadata-enrichment-results",
    "href": "chapter_ai-nepi_018.html#llm-metadata-enrichment-results",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 LLM Metadata Enrichment: Results",
    "text": "16.4 LLM Metadata Enrichment: Results\n\n\n\nSlide 09\n\n\nInitial sanity checks on the LLM’s output revealed promising results. A remarkable 99.81% of the generated outputs (17,486 out of 17,520) conformed to the valid YAML format. Furthermore, 94% of the predicted primary scientific disciplines aligned with the predefined set of nine categories.\nNevertheless, some hallucinations and errors emerged in discipline categorisation. These included minor deviations, such as the LLM outputting “Earth Sciences” instead of the more complete “Environmental & Earth Sciences,” and the invention of novel categories like “Music.” In some instances, the numerical index of a discipline from the prompt was mistakenly incorporated into the discipline’s name. The LLM also occasionally classified what should be subdisciplines, such as “Neurology” or “Zoology,” as primary disciplines. Despite these anomalies, the researchers concluded that the LLM correctly assigned the majority of papers. A visual breakdown of file counts per discipline, including these hallucinated categories, illustrated this distribution.\nAn analysis of the LLM-assigned disciplines over time, from the 1670s to the 1990s, unveiled significant trends in the Royal Society’s publications. Until the close of the 18th century, a relatively homogeneous distribution of disciplines characterised the journal. A distinct peak in chemical articles emerged in the late 18th century, coinciding with the Chemical Revolution, after which chemistry became a central pillar. Progressing from the 19th century into the 20th, Biology, Physics, and Chemistry solidified their positions as the three dominant disciplines.\nTo further explore the semantic content, investigators applied t-SNE projection to the embeddings of the LLM-generated TL;DR summaries. This visualisation technique revealed distinct clusters corresponding to the different scientific disciplines. Notably, an overlap appeared between Chemistry, Physics, and Biology, with Chemistry often situated as an intermediary. In contrast, disciplines such as Humanities, Astronomy, and Mathematics formed more isolated clusters. This static snapshot of the entire corpus suggests the potential for diachronic application, enabling researchers to trace the evolving relationships and distinctions between disciplines over the centuries.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-chemical-analysis-methodology",
    "href": "chapter_ai-nepi_018.html#diachronic-chemical-analysis-methodology",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Diachronic Chemical Analysis: Methodology",
    "text": "16.5 Diachronic Chemical Analysis: Methodology\n\n\n\nSlide 19\n\n\nThe second part of the research, presented by Sergei Bagdasarov, focused on a diachronic analysis of the chemical knowledge space, leveraging the disciplinary classifications established by the LLM in Part I. This methodology involved a distinct data processing pipeline. Initially, the LLM classified texts from the Royal Society Corpus. Subsequently, ChemDataExtractor, a tool designed to extract chemical entities and related data, processed texts identified as belonging to relevant disciplines. Finally, researchers applied Kullback-Leibler Divergence (KLD) calculations to conduct diachronic analyses, both within individual disciplines and by comparing across different disciplines.\nKullback-Leibler Divergence (KLD), a measure of relative entropy, serves to detect linguistic or conceptual change when applied to language models derived from different situational contexts. A higher KLD value between two contexts—for example, text corpora from different time periods—signifies greater dissimilarity, whereas a lower value indicates more similarity. Essentially, KLD measures the extra information, in bits, needed to encode data from one set (A) using a model built from another set (B). In this study, researchers employed KLD in two main ways: firstly, to track intra-disciplinary evolution by comparing language models from future periods (A) against past periods (B) within Chemistry, Physics, and Biology papers. Secondly, it facilitated inter-disciplinary comparisons, for instance, by treating Biology texts as set A and Chemistry texts as set B, analysed across successive 50-year intervals of the RSC.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-chemical-analysis-results",
    "href": "chapter_ai-nepi_018.html#diachronic-chemical-analysis-results",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Chemical Analysis: Results",
    "text": "16.6 Diachronic Chemical Analysis: Results\n\n\n\nSlide 21\n\n\nApplying Kullback-Leibler Divergence to analyse the chemical space within Chemistry, Physics, and Biology papers from 1666 to 1986 revealed distinct periods of accelerated change. Line graphs plotting KLD values—comparing language models of successive periods within each discipline—showed peaks corresponding to known historical developments. For instance, the period roughly between 1776 and 1816, coinciding with the Chemical Revolution, exhibited heightened KLD, particularly in Chemistry. Detailed examination of specific chemical substance mentions (such as oxygen, hydrogen, and nitric acid) during these years illustrated the terminological shifts. A subsequent period of significant change, highlighted between approximately 1851 and 1896, aligns with the discovery of noble gases and radioactive elements, again showing fluctuations in the prominence of relevant chemical terms like helium, argon, and radium within Chemistry, and related terms in Physics and Biology.\nResearchers visualised interdisciplinary comparisons using KLD through word clouds. These representations highlighted terms significantly more characteristic of one discipline compared to another during specific eras. When contrasting Chemistry with Physics, terms like “hydrogen,” “oxygen,” and “benzene” showed high KLD. Similarly, “hydrogen,” “nitrogen,” and “carbon” distinguished Chemistry from Biology. Conversely, terms such as “calcium,” “glucose,” and “amino acids” proved more distinctive for Biology relative to Chemistry.\nFurthermore, researchers tracked potential knowledge transfer by plotting KLD values for specific chemical substances across disciplines over 50-year intervals. This analysis, for example, compared the linguistic environment of terms like “calcium” or “sodium” in chemistry papers versus biology papers, and terms like “nitrogen” or “uranium” in chemistry versus physics papers. Such comparisons offer insights into how chemical concepts and terminology permeated or diverged across disciplinary boundaries over extended periods, with a notable dynamic observed, for instance, between Chemistry and Physics in the latter half of the 20th century.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.1 Limitations in Understanding Science Funding through Public Data",
    "text": "17.1 Limitations in Understanding Science Funding through Public Data\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. Researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "17.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI). NHGRI functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics:\n\nA significant proportion of NHGRI-funded publications rank amongst the top 5% most cited.\nIts research demonstrates high citation impact within a decade.\nIt generates numerous patents leading to clinical applications.\nIts funded projects often exhibit high “disruption” scores.\n\nDespite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "17.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\n\n\n\nSlide 06\n\n\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters. These clusters stand separate from the more homogenous categories of RFAs and publications. Crucially, these internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "17.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\n\n\n\nSlide 10\n\n\nAnalysing the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling its extensive handwritten material. Researchers acknowledge the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "17.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount.\nThe developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, researchers reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "17.6 Analysing Leadership Structures: The International HapMap Project Case Study\n\n\n\nSlide 15\n\n\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. Investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, researchers identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, the analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins played significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "17.7 Modelling Funding Decisions for Organism Sequencing\n\n\n\nSlide 13\n\n\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, researchers developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features:\n\nBiological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05).\nProject-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04).\nReputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04).\nLinguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\n\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "17.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\n\n\n\nSlide 16\n\n\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages:\n\nInitial data and metadata ingestion.\nSophisticated knowledge creation processes, such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling.\n\nThe ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#introduction-to-openalex-mapper-technical-workflow",
    "href": "chapter_ai-nepi_004.html#introduction-to-openalex-mapper-technical-workflow",
    "title": "4  Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper",
    "section": "4.1 Introduction to OpenAlex Mapper: Technical Workflow",
    "text": "4.1 Introduction to OpenAlex Mapper: Technical Workflow\n\n\n\nSlide 03\n\n\nResearchers have developed the OpenAlex Mapper, a novel tool designed to explore the transdisciplinary application of language models. Maximilian Neuchel, collaborating with Andrea Loettgers and Taya Knuuttila, spearheaded this initiative. Their work, supported by an ERC grant focused on possible life, draws upon affiliations with Utrecht University’s theoretical philosophy department and Vienna’s philosophy department. The tool adeptly facilitates investigations into interdisciplinary connections by projecting search queries from the OpenAlex database onto a background map derived from randomly sampled papers. Interested parties may access the presentation slides, complete with interactive elements, at maxnoichl.eu/talk.\nThe operational workflow of the OpenAlex Mapper commences with the meticulous fine-tuning of an embedding model. Specifically, the team enhanced the Specter 2 language model, aiming to improve its capacity for recognising and respecting disciplinary boundaries. They trained this model using a dataset of articles from closely related disciplinary backgrounds, thereby enabling it to distinguish them with greater precision. This fine-tuning process, visually represented via UMAP dimensionality reduction, yielded a ‘discipline-improved Specter 2’, achieved through minor adjustments rather than extensive retraining.\nSubsequently, for base-map preparation, the team leveraged the OpenAlex database—a vast, open-access repository of scholarly material renowned for its inclusivity and ease of querying. From this extensive resource, they sampled 300,000 random articles, stipulating only that these articles possessed reasonably well-formed abstracts and were in English. These abstracts then underwent embedding using the fine-tuned Specter 2 model. Following this, Uniform Manifold Approximation and Projection (UMAP) reduced the high-dimensional embeddings to a two-dimensional representation, and the resultant UMAP model was meticulously preserved for subsequent use.\nFinally, the OpenAlex Mapper processes individual user queries. Users can submit arbitrary queries to the OpenAlex database via the tool. The system then downloads the relevant articles, embeds their abstracts using the fine-tuned Specter 2 model, and projects these new embeddings onto the existing two-dimensional map using the previously trained UMAP model. This crucial UMAP feature ensures that new query results are positioned on the map as if they had been an integral part of the original layout process. The workflow diagram references contributions from Singh et al. (2023), McInnes, Healy, and Melville (2018), and De Bruin (2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#tool-demonstration-and-interactive-features",
    "href": "chapter_ai-nepi_004.html#tool-demonstration-and-interactive-features",
    "title": "4  Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper",
    "section": "4.2 Tool Demonstration and Interactive Features",
    "text": "4.2 Tool Demonstration and Interactive Features\n\n\n\nSlide 05\n\n\nThe OpenAlex Mapper, readily accessible at https://m7n-openalex-mapper.hf.space, provides an interactive platform for scholarly exploration. During a live demonstration, an initial loading difficulty, manifesting as a red overlay, quickly resolved, allowing the presentation to proceed seamlessly. Users commence by navigating to the OpenAlex website and performing a search for their topic of interest; for example, a query for ‘scale-free networks’ recently yielded 6,281 results. They then copy the resultant OpenAlex search URL and paste it into the designated field within the OpenAlex Mapper interface.\nPrior to execution, users can adjust various settings, such as reducing the sample size—for instance, to 1,000 records for demonstration purposes. Upon running the query, the backend system downloads the specified number of records and embeds their abstracts. If selected, it also processes the citation graph. The tool then projects these search results onto a grey base map, visually indicating the precise location of items related to the query, such as those employing a specific term or citing a particular author. An example using the search term ‘coriander’, a standard OpenAlex illustration, effectively demonstrated this, followed by a live generation for ‘scale-free network models’.\nCrucially, the output offers full interactivity. Researchers can delve into specific points on the map to discern, for example, why ‘coriander’ might feature in epidemiology or public health literature. Clicking on individual papers directly navigates the user to their respective websites, ensuring a constant link back to the original textual sources. Additional settings permit viewing temporal distributions of results, observing the geographical appearance of items on the map, and examining the citation graph overlaid upon the map. For more demanding tasks, an alternative version of the tool, equipped with a higher-latency GPU setup, is available for executing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#rationale-applications-and-utility-in-hpss",
    "href": "chapter_ai-nepi_004.html#rationale-applications-and-utility-in-hpss",
    "title": "4  Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper",
    "section": "4.3 Rationale, Applications, and Utility in HPSS",
    "text": "4.3 Rationale, Applications, and Utility in HPSS\n\n\n\nSlide 14\n\n\nThe OpenAlex Mapper directly addresses persistent challenges within the History and Philosophy of Science and Scholarship (HPSS), particularly those concerning small samples and detailed case studies. Whilst such studies offer rich, close-up perspectives through methods like close reading or direct interaction with scientists, they often encounter difficulties when attempting generalisation or validation within the broader, rapidly evolving landscape of contemporary global science. The OpenAlex Mapper aims to bridge this critical gap. It provides a robust means to investigate questions such as the actual prevalence and contextual application of specific scientific models, exemplified by the Hopfield model: where did it truly establish itself, where is it actively employed, and where do researchers continue to reference it? The underlying philosophy involves leveraging robust quantitative methods to support, rather than supplant, qualitative and heuristic investigations. Consequently, the tool facilitates a dynamic interplay between a macroscopic, map-based overview and a microscopic examination of specific results.\nResearchers have explored several compelling applications to demonstrate the tool’s profound utility:\n\nInvestigation of Model Templates: Drawing on seminal work by Humphreys (2004) and Knuuttila and Loettgers (2023), this application examines how models with analogous structures emerge across diverse scientific fields. It reveals how these templates might impose a structure on science that operates orthogonally to traditional disciplinary boundaries. Mapping examples such as the Ising model (7,819 instances), the Hopfield model (5,895 instances), and the Sherrington-Kirkpatrick model (1,437 instances) revealed their appearance in distinct, non-contiguous locations across the base map.\nMapping the Distribution of Concepts: The tool adeptly aids in mapping the distribution of concepts across large, interdisciplinary samples, referencing studies by Malaterre, Chartier, and Lareau (2020) and Zichert and Wüthrich (2024). An illustrative map contrasted the concept of ‘phase transition’ with ‘emergence’, powerfully highlighting the tool’s capacity to extend such conceptual analyses into interdisciplinary domains where acquiring comprehensive datasets typically proves problematic.\nScrutiny of Scientific Methods: This represents a pertinent application for ongoing debates concerning the role of machine learning versus classical statistics in science, and the notion of theory-free science, with citations to Breiman (2001), Bzdok, Altman, and Krzywinski (2018), and Andrews (2023). A comparative mapping of the Random Forest model (2,000 instances) against Logistic Regression (1,997 instances) showed quite distinguishable patterns of usage across disciplines. This observation, for instance, prompts further philosophical inquiry into why neuroscience might favour random forests whilst thematically related fields like psychiatry or mental health research predominantly employ logistic regressions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating Transdisciplinary Applications of Language Models: The OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-unlocking-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-unlocking-unstructured-biographical-knowledge",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.1 Introduction: Unlocking Unstructured Biographical Knowledge",
    "text": "18.1 Introduction: Unlocking Unstructured Biographical Knowledge\n\n\n\nSlide 01\n\n\nResearchers consistently confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its inherent lack of digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation centres on biographical sources replete with detailed personal data, crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this fundamental limitation, investigators propose employing Large Language Models (LLMs). The core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline designed to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j.\nCrucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This pragmatic approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "18.2 Conceptual Framework: From Text to Knowledge Graph\n\n\n\nSlide 04\n\n\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which are then assembled into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (“Włochy”) or Egypt (“Egipt”). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process operates within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality suffices to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\n\n\n\nSlide 12\n\n\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which are manually refined based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, SHACL (Shapes Constraint Language) shapes are created to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\n\n\n\nSlide 15\n\n\nResearchers illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying the knowledge-graph approach to this corpus, investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists; the presentation displays sample entries for Gustav Hertz and Robert Havemann. An analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR. Further comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.6 Conclusion and Future Trajectories",
    "text": "18.6 Conclusion and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project successfully demonstrates a method for progressing from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to rigorously assess performance.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the team intends to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. Investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, plans are underway to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-project-objectives-and-team",
    "href": "chapter_ai-nepi_006.html#the-veritrace-project-objectives-and-team",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.1 The VERITRACE Project: Objectives and Team",
    "text": "6.1 The VERITRACE Project: Objectives and Team\n\n\n\nSlide 02\n\n\nFormally titled “Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project,” this five-year ERC Starting Grant initiative meticulously traces the early modern ‘ancient wisdom’ or Prisca Sapientia tradition’s profound influence upon the burgeoning fields of natural philosophy and science.\nA dedicated team of five Brussels-based scholars drives this research endeavour. Professor Cornelis J. Schilt serves as the Principal Investigator, leading a diverse group comprising Dr Eszter Kovács, Niccolò Cantoni, and Demetrios Paraschos. Dr Jeffrey Wolf, a historian of science and medicine specialising in the 18th century, focuses particularly on the project’s digital humanities aspects.\nCentral to the project’s inquiry lies a close-reading corpus of 140 works, which embody the ancient wisdom tradition. Amongst these are seminal texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, notably, the Corpus Hermeticum—a work particularly relevant to the history of chemistry. Whilst historical evidence confirms the tradition’s impact—Newton, for instance, studied the Sibylline Oracles, and Kepler possessed knowledge of the Corpus Hermeticum—the VERITRACE project endeavours to delve deeper. Researchers seek to uncover a far broader array of networks and texts that engaged with this often-overlooked tradition, frequently authored by lesser-known figures collectively termed ‘the great unread’.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-and-data-curation",
    "href": "chapter_ai-nepi_006.html#computational-approaches-and-data-curation",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.2 Computational Approaches and Data Curation",
    "text": "6.2 Computational Approaches and Data Curation\n\n\n\nSlide 04\n\n\nTo achieve its ambitious objectives, the VERITRACE project adopts a computational history and philosophy of science (HPSS) framework. This approach facilitates a large-scale, multilingual exploration of the historical corpus. Researchers specifically aim to identify textual re-use, distinguishing direct lexical quotations (even when uncited) from more indirect forms such as paraphrases or subtle allusions that contemporary readers would have recognised. Ultimately, the project aims to construct an “early modern plagiarism detector,” uncovering previously ignored networks of texts, passages, themes, topics, and authors, thus revealing novel patterns within intellectual history and the philosophy of science.\nA substantial and diverse multilingual dataset underpins this computational endeavour. The corpus exclusively comprises printed books and texts, deliberately omitting handwritten materials for manageability. Sourced from three distinct multilingual repositories—namely, Early English Books Online (EEBO), Gallica from the French National Library, and, most significantly, the Bavarian State Library—the collection spans over 200 years, from 1540 until 1728, shortly after Isaac Newton’s demise. Collectively, these sources yield approximately 430,000 texts. To navigate and analyse this extensive dataset, the project employs an array of state-of-the-art digital techniques, including keyword search, sophisticated text matching, topic modelling, and sentiment analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#addressing-data-complexities-with-large-language-models",
    "href": "chapter_ai-nepi_006.html#addressing-data-complexities-with-large-language-models",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.3 Addressing Data Complexities with Large Language Models",
    "text": "6.3 Addressing Data Complexities with Large Language Models\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several formidable challenges inherent in its historical textual analysis. Foremost amongst these lies the variable quality of Optical Character Recognition (OCR) data, since libraries provide raw text directly in diverse formats such as XML, HOCR, and HTML files. This foundational data quality profoundly impacts all subsequent processing stages. Moreover, early modern typography and the evolving semantics across at least six distinct languages introduce considerable complexity. Compounding these issues is the sheer volume of data, encompassing hundreds of thousands of texts printed across Europe over two centuries.\nTo address these complexities, the project strategically employs Large Language Models (LLMs). On the decoder side, researchers utilise GPT-based LLMs to enrich and clean metadata. A notable application involves deploying LLMs as “judges” to automate the laborious task of matching bibliographic records. Drawing upon high-quality metadata from the Universal Short Title Catalogue (USTC), the system aims to compare pairs of records, determining whether they represent the same underlying printed text—a task that previously required human team members to review 10,000 pairs each. This “LLM bench,” a series of models, operates with extensive prompt guidelines, generating structured output that includes reasoning for its match or non-match decisions. Nevertheless, this approach remains a work in progress; open-source models like Llama frequently produce hallucinations. Whilst structured output mitigates these, it often results in generic and less informative reasoning, necessitating a careful balance between the “art” and “science” of prompt engineering. Despite these hurdles, the potential for substantial time savings remains evident.\nConversely, on the encoder side, the project leverages BERT-based LLMs to generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, facilitating conceptual comparisons. The Language-agnostic BERT Sentence Embedding (LaBSE) model has undergone testing for this purpose, though initial assessments suggest it is “probably not good enough” for the project’s specific requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-features",
    "href": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-features",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.4 The VERITRACE Web Application: Architecture and Features",
    "text": "6.4 The VERITRACE Web Application: Architecture and Features\n\n\n\nSlide 15\n\n\nA robust data processing pipeline meticulously transforms raw textual data, sourced in formats such as XML, HOCR, and HTML, into a structured representation suitable for the VERITRACE web application. This intricate, 15-stage pipeline culminates in an Elasticsearch database, which serves as the application’s primary backend. Key stages within this complex workflow include: processing batches, generating character positions, extracting pages, executing language analysis, mapping languages, assessing OCR quality, segmenting documents, filtering segments, and tracking inter-segment relationships. Subsequently, the pipeline unifies data into JSON format, enriches it within a MongoDB instance, and further refines sequences. Crucially, each stage demands careful optimisation, with the generation of vector embeddings occurring towards the pipeline’s conclusion.\nThe VERITRACE web application, currently an alpha version under active development and not yet publicly accessible, promises scholars dynamic access to the project’s extensive corpus and analytical capabilities. Its backend leverages a MongoDB database to present corpus statistics. The ‘Explore’ section, one of the application’s five main functionalities, currently displays comprehensive corpus statistics derived from 427,305 metadata records. Users can visualise language distribution, data sources, document publication trends by decade, and geographical publication places through interactive charts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#metadata-exploration-and-advanced-search",
    "href": "chapter_ai-nepi_006.html#metadata-exploration-and-advanced-search",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.5 Metadata Exploration and Advanced Search",
    "text": "6.5 Metadata Exploration and Advanced Search\n\n\n\nSlide 17\n\n\nBeyond general corpus statistics, the VERITRACE web application provides a sophisticated Metadata Explorer, enabling users to delve into the rich metadata associated with each text. This feature offers granular language information, derived from automated identification processes that analyse content down to approximately 50-character segments. This precision addresses the common occurrence of multilingual texts—such as a Latin volume containing significant Greek sections—accurately categorising them as “substantively multilingual.” Furthermore, the system attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images. The interface presents detailed OCR information, including quality categories, weighted scores, and language analysis specifics.\nFor direct scholarly inquiry, the ‘Search’ section offers robust keyword search functionality. Whilst the current prototype operates on a subset of 132 files, generating a 15-gigabyte index, the full corpus is projected to scale to many terabytes. A simple query for “Hermes,” for instance, retrieves 22 documents with 332 matches. Leveraging Elasticsearch, the system supports highly complex queries, including field-specific searches (e.g., identifying books by Kepler containing “Hermes”), nested queries, and proximity searches, which allow users to locate terms like “Hermes” and “Plato” appearing within a specified word distance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-analytical-tools-and-textual-access",
    "href": "chapter_ai-nepi_006.html#future-analytical-tools-and-textual-access",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.6 Future Analytical Tools and Textual Access",
    "text": "6.6 Future Analytical Tools and Textual Access\n\n\n\nSlide 19\n\n\nAnticipating future scholarly needs, the VERITRACE web application plans to integrate an ‘Analyse’ section. This forthcoming feature will offer advanced analytical tools, including topic modelling, latent semantic analysis (LSA), and diachronic analysis, enabling researchers to uncover evolving patterns within the historical discourse.\nComplementing these analytical capabilities, a ‘Read’ section provides scholars with direct access to the textual content. Utilising a Mirador viewer, the application presents high-fidelity PDF facsimiles of every text within the corpus. This ensures users can engage with the original formatting and layout, whilst simultaneously accessing comprehensive associated metadata.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-textual-matching-for-re-use",
    "href": "chapter_ai-nepi_006.html#advanced-textual-matching-for-re-use",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.7 Advanced Textual Matching for Re-use",
    "text": "6.7 Advanced Textual Matching for Re-use\n\n\n\nSlide 20\n\n\nThe ‘Match’ section forms the core of the VERITRACE web application, specifically engineered to identify instances of textual re-use across the vast corpus. Users can perform comparisons between single documents, conduct multi-document analyses—pitting, for example, Newton’s Latin Opticks against all of Kepler’s works within the database—or even attempt to compare a single text against the entire corpus. This latter functionality, whilst highly desirable, presents significant computational challenges, potentially impacting user wait times. Crucially, the system empowers users to fine-tune numerous parameters, such as a minimum similarity score, thereby tailoring the matching process to their specific research questions.\nThe application offers two primary match types: lexical and semantic, alongside a hybrid option. Lexical matching, which relies on keyword identification, effectively uncovers vocabulary similarities but proves ineffective across different languages. Conversely, semantic matching leverages vector embeddings to identify conceptually similar passages, irrespective of linguistic differences, thereby addressing the corpus’s multilingual nature. A hybrid approach combines both methods, allowing for adjustable weighting. Furthermore, users can select from various matching modes, including a ‘Standard’ mode, a ‘Comprehensive’ mode that demands greater computational power for exhaustive results, and a ‘Faster’ mode for quicker, albeit less thorough, analyses.\nInitial sanity checks illustrate these functionalities. A lexical match between Newton’s Latin Opticks (1719 edition) and its English counterpart (1718 edition) yields no significant results in standard mode, as anticipated due to the linguistic divergence. However, employing the comprehensive mode reveals three matches, likely corresponding to English text embedded within the Latin edition’s preface. Conversely, a semantic match between these translated texts produces reasonable conceptual alignments, even amidst OCR imperfections. Nevertheless, an analysis of the match scores, particularly the coverage score, suggests the current LaBSE embedding model remains insufficient for optimal performance, despite high quality scores. The system also provides a detailed match summary, displaying metrics such as quality score, average similarity, and coverage score, alongside the total number of comparisons performed—often exceeding 1.3 million. For granular inspection, the interface offers automatic highlighting of matching terms within side-by-side views of source and comparison passages, each accompanied by its similarity score.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-directions-and-persistent-challenges",
    "href": "chapter_ai-nepi_006.html#future-directions-and-persistent-challenges",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.8 Future Directions and Persistent Challenges",
    "text": "6.8 Future Directions and Persistent Challenges\n\n\n\nSlide 25\n\n\nAs the VERITRACE project progresses, several critical issues remain on the horizon, demanding strategic solutions. The current LaBSE embedding model, whilst a starting point, appears insufficient for the nuanced requirements of historical textual analysis. Researchers are evaluating alternative models, including XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting distinct trade-offs between accuracy, storage demands, and inference time. A fundamental question arises: does the historical corpus’s unique nature necessitate fine-tuning a base model to achieve optimal results?\nFurthermore, the temporal evolution of semantic meaning poses a significant challenge. How can a model effectively compare texts published centuries apart, in different languages, and accurately place them within a coherent vector space, given the shifts in linguistic usage? The pervasive issue of poor OCR quality exacerbates these difficulties, fundamentally impairing downstream processes such as accurate sentence and passage segmentation. Re-OCR of the entire 430,000-text corpus remains unfeasible, prompting consideration of re-OCR for only the lowest quality texts or the proactive integration of high-quality external datasets.\nScaling and performance also represent substantial hurdles. The current prototype, operating on a mere 132 texts, already generates a 15-gigabyte index and requires 15 seconds for queries. Expanding this to the full 430,000-text corpus, anticipated to occupy many terabytes, necessitates robust solutions to maintain acceptable query response times. Currently, a desktop computer supports development, but the project has secured access to the VUB Flemish supercomputing cluster’s Tier One Cloud Service via a grant. Nevertheless, the long-term viability of hosting a continuous service on this academic cluster remains to be fully determined.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-engagement",
    "href": "chapter_ai-nepi_001.html#workshop-engagement",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Engagement",
    "text": "2.1 Workshop Engagement\n\n\n\nSlide 01: A Quarto slide titled “Workshop Engagement” with bullet points summarising the number of submissions, participants, and the goal of ensuring active participation.\n\n\nThe organisers convened a workshop titled “Large Language Models for the History, Philosophy and Sociology of Science,” held from 2-4 April 2025. This event, co-organised by Adrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert, welcomed participants both in person at TU Berlin’s Room H2005 and via an online platform.\nThe call for papers generated significant interest, attracting over 50 submissions. From this competitive pool, the organisers meticulously selected 16 papers for presentation during the workshop. Participation levels proved robust: in-person attendance quickly reached capacity, whilst a substantial online audience also registered. Overall, approximately 220 individuals enrolled for the workshop, with additional registrations continuing to arrive. Crucially, the organisers aimed to ensure that all individuals interested in these topics could actively participate in the discussions throughout the two-and-a-half-day programme.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#genesis-and-nepi-objectives",
    "href": "chapter_ai-nepi_001.html#genesis-and-nepi-objectives",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Genesis and NEPI Objectives",
    "text": "2.2 Genesis and NEPI Objectives\n\n\n\nSlide 02: A Quarto slide titled “Genesis and NEPI Objectives” outlining the two initiatives that led to the workshop and the NEPI project’s focus on the Atlas collaboration at CERN.\n\n\nThe workshop’s inception stemmed from two distinct yet complementary initiatives. Firstly, the Network Epistemology in Practice (NEPI) project provided a foundational impetus. Within this project, Arno Simons pioneered the training of one of the earliest Large Language Models (LLMs) specifically on physics texts, aligning with the project’s core interests.  Concurrently, Michael Zichert, also a member of the NEPI team, applied LLMs to scrutinise conceptual issues prevalent in physics.\nSecondly, Gerd Graßhoff, a long-standing collaborator of Adrian Wüthrich, significantly contributed to the workshop’s genesis. Graßhoff has consistently championed the integration of Artificial Intelligence (AI) into the history and philosophy of science, particularly for analysing the intricate processes of scientific discovery. He independently conceived a workshop focused on novel AI-assisted methodologies for these disciplines. Recognising their shared objectives, the organisers subsequently decided to combine their efforts, culminating in the present workshop.\nThe European Research Council (ERC) grant, Network Epistemology in Practice (NEPI), bearing grant number 10104932, provides the funding for this endeavour. Within the NEPI project, researchers meticulously study the internal communication of the Atlas collaboration at CERN, the prominent particle physics laboratory. This investigation aims to elucidate how one of the largest and most distinguished research collaborations collectively generates new knowledge. Researchers employ network analysis to discern the communication structures within this collaboration. Furthermore, they utilise semantic tools, including LLMs, to trace the flow of ideas throughout these complex network structures. Indeed, the application of LLMs represents a central interest for the project, with numerous other applications anticipated throughout the workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols",
    "href": "chapter_ai-nepi_001.html#recording-protocols",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Recording Protocols",
    "text": "2.3 Recording Protocols\n\n\n\nSlide 05: A Quarto slide titled “Recording Protocols” detailing the equipment used for recording sessions, consent requirements, and the plan for uploading videos to the NEPI YouTube Channel.\n\n\nThe workshop sessions are currently being recorded. Participants received prior notification of this during the registration process, thereby implying their consent. A single camera captures the presenter, ensuring focus remains on the speaker. For audio capture, four microphones are deployed, supplemented by an iPhone serving as a crucial backup recorder.\nFollowing the workshop, the organisers intend to upload videos of the talks, encompassing the accompanying discussions, to the NEPI YouTube Channel. This process, however, necessitates the explicit consent of each presenter. Crucially, whilst discussions are recorded, the audio and video capture solely focuses on the presenter, deliberately excluding the audience. Should any participant require additional information or wish to withdraw their consent, they are encouraged to approach the organisers. Ultimately, these recording efforts aim to establish a comprehensive record of the valuable discussions and presentations from this meeting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#interaction-protocols",
    "href": "chapter_ai-nepi_001.html#interaction-protocols",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Interaction Protocols",
    "text": "2.4 Interaction Protocols\n\n\n\nSlide 07: A Quarto slide titled “Interaction Protocols” outlining the Q&A format, the use of Etherpad/Cryptpad for asynchronous comments, and Zoom chat for real-time interaction.\n\n\nGiven the substantial number of participants and the constrained time allocated for presentations, the organisers have implemented a specific protocol for questions and comments. Participants are kindly requested to formulate their questions and comments concisely and directly. Following each presentation, the organisers will collect approximately four questions or comments, enabling the presenter to address them collectively, thereby optimising time and avoiding protracted back-and-forth exchanges. The organisers acknowledge that, despite the value of all inquiries, time limitations may preclude addressing every question in person.\nTo facilitate broader engagement beyond the live sessions, the organisers provide an Etherpad or Cryptpad. This platform allows participants to post comments or questions after sessions, offering presenters an opportunity to read and respond at their convenience. Consequently, this channel ensures continuous interaction even when sessions are not actively running. Furthermore, during live sessions, both online attendees and the in-person audience can utilise the Zoom chat feature to submit questions or comments at any time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#networking-and-social-programme",
    "href": "chapter_ai-nepi_001.html#networking-and-social-programme",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Networking and Social Programme",
    "text": "2.5 Networking and Social Programme\nBeyond the formal presentations, the workshop actively fosters informal networking amongst researchers and fellows. The programme incorporates ample lunch and coffee breaks, providing dedicated time for casual interaction. Additionally, a modest reception and a workshop dinner are scheduled, though seats for the dinner are strictly limited to confirmed participants.\nCoffee and refreshments are available on-site. For lunch and the reception, attendees will proceed to Room H2051, located down the hall and one floor below. The organisers will provide guidance to these locations, for instance, leading participants there after today’s final talk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-text-analysis-for-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-text-analysis-for-societal-change",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.6 Keynote 1: Text Analysis for Societal Change",
    "text": "2.6 Keynote 1: Text Analysis for Societal Change\nThe first keynote address, titled “Large-scale text analysis for the study of cultural and societal change,” featured Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi serves as the Principal Investigator for the “Change is Key” research programme in Gothenburg, whilst Pierluigi Cassotti contributes as a researcher within this project.\nThese scholars have garnered considerable recognition for their pioneering work in semantic change detection.  Their contributions encompass not only technical advancements, such as the development of crucial benchmarks, but also broader methodological considerations concerning the application of data science methods to humanities questions. This dual expertise renders their work exceptionally pertinent to the workshop’s themes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-cross-document-nlp",
    "href": "chapter_ai-nepi_001.html#keynote-2-cross-document-nlp",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.7 Keynote 2: Cross-Document NLP",
    "text": "2.7 Keynote 2: Cross-Document NLP\nIryna Gurevych delivered the second keynote address, scheduled for the late afternoon of the following day, under the title “How to InterText? Elevating Natural Language Processing (NLP) to the cross-document level.” Gurevych heads the Ubiquitous Knowledge Processing (UKP) Lab at Technical University Darmstadt.\nHer extensive research primarily focuses on information extraction, semantic text processing, and machine learning. Crucially, her work also explores the practical applications of NLP within the social sciences and humanities. This specific area of expertise positions her contributions as an ideal complement to the workshop’s overarching objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.1 The Foundational Transformer Architecture",
    "text": "3.1 The Foundational Transformer Architecture\n\n\n\nSlide 01\n\n\nAt the core of all contemporary large language models (LLMs) lies the seminal Transformer architecture, pioneered by Vaswani et al. in 2017 (Vaswani2017?). Originally designed for language translation, such as German to English, this architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.\nThe encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence’s meaning. This allows the model to understand the full context of the input.\nConversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness and enabling more nuanced language processing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "href": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.2 Differentiating BERT and GPT Models",
    "text": "3.2 Differentiating BERT and GPT Models\n\n\n\nSlide 03\n\n\nImmediately following the Transformer’s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (NLP) tasks with minimal additional training.\nFrom the encoder side, the BERT family of models emerged, standing for Bidirectional Encoder Representations from Transformers. BERT operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. This enables BERT to excel at tasks requiring deep contextual comprehension, such as sentiment analysis or question answering.\nConversely, the decoder side gave rise to the GPT models, or Generative Pre-trained Transformers, which now power applications like ChatGPT. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text. This generative function is not inherently present in BERT-like models.\nConsequently, a fundamental distinction arises: generative models, exemplified by GPT, primarily produce language, whereas full-context models, such as BERT, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like XLM and XLNet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "href": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.3 Scientific LLM Evolution and Adaptation Strategies",
    "text": "3.3 Scientific LLM Evolution and Adaptation Strategies\n\n\n\nSlide 06\n\n\nA comprehensive overview reveals the rapid evolution of large language models (LLMs), particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to BERT, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included BioBERT, Specter, and SciBERT. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.\nResearchers employ several methods to adapt these models to specific scientific language. The initial phase is pre-training, where a model learns language by predicting the next token (a word or sub-word unit), as in GPT models, or by predicting randomly masked words, characteristic of BERT models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many.\nConsequently, continued pre-training offers a viable alternative. Researchers utilise an already pre-trained model and subsequently train it on domain-specific language. For instance, a general BERT model can be adapted for physics texts by further training it on a large corpus of physics literature.\nBeyond this, engineers can add extra layers atop pre-trained models, effectively fine-tuning them for specific classification tasks like sentiment analysis or named entity recognition. Crucially, contrastive learning emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with Sentence BERT serving as a widely adopted example for creating semantically meaningful sentence representations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.4 Retrieval Augmented Generation and Key Distinctions",
    "text": "3.4 Retrieval Augmented Generation and Key Distinctions\n\n\n\nSlide 10\n\n\nRetrieval Augmented Generation (RAG) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins ChatGPT’s internet search capabilities, allowing it to access and synthesise information beyond its initial training data.\nThe RAG process commences with a user query, such as “What are LLMs?”. Subsequently, a BERT-type model encodes this query into a sentence embedding, a numerical representation of its meaning. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the RAG pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context. This approach significantly reduces the likelihood of “hallucinations” often associated with standalone LLMs.\nBeyond RAG, advanced reasoning models and agents are emerging. These are not isolated LLMs but rather intricate systems that combine LLMs with a diverse array of other tools, enabling more complex problem-solving and decision-making.\nConsequently, a clear understanding of key distinctions proves crucial for navigating the LLM landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone LLMs, complex pipelines like RAG, and sophisticated agents becomes paramount for effective application and responsible deployment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.5 Applications and Trends in HPSS Research",
    "text": "3.5 Applications and Trends in HPSS Research\n\n\n\nSlide 13\n\n\nA current survey explores the burgeoning uses of large language models (LLMs) as tools within history, philosophy, and sociology of science (HPSS) research. This investigation has identified four primary categories for these applications:\n\nLLMs assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations. For example, they can efficiently identify and categorise funding sources in large corpora of scientific papers.\nThey contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields. This could involve identifying key concepts and their relationships within historical scientific texts.\nLLMs illuminate knowledge dynamics, particularly through the study of conceptual histories of words. By tracking semantic shifts over time, researchers can trace the evolution of scientific concepts.\nThey support the analysis of knowledge practices, including citation context analysis—an older HPSS tradition now also employed for evaluatory purposes. This allows for a deeper understanding of how scientific ideas are built upon and referenced.\n\nA notable trend indicates an accelerating interest in LLMs, with findings predominantly appearing in information science journals like Scientometrics and the Journal of the Association for Information Science and Technology (JASIST). Increasingly, however, papers featuring LLM applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers, bridging disciplinary divides. Furthermore, the degree of customisation in LLM deployment varies widely, spanning from straightforward off-the-shelf use of ChatGPT to the development of entirely new model architectures tailored for specific HPSS questions.\nDespite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models (the “black box” problem), and persistent shortages of training data and benchmarks specific to HPSS domains. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by BERTopic, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "href": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.6 Critical Reflections and Future Directions for HPSS",
    "text": "3.6 Critical Reflections and Future Directions for HPSS\n\n\n\nSlide 12\n\n\nCrucially, scholars must acknowledge the specific challenges inherent to HPSS when engaging with large language models (LLMs). Foremost amongst these is the historical evolution of concepts and language. Models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models on historical corpora or the judicious use of existing ones with a keen awareness of their limitations.\nFurthermore, HPSS adopts a reconstructive and critically reflective perspective, often reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive HPSS “reading.” Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts, which pose significant hurdles for LLM application.\nConsequently, building robust LLM literacy becomes imperative for HPSS researchers. They must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing (NLP) may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.\nUltimately, HPSS researchers must remain true to their established methodologies. This involves carefully translating complex HPSS problems into specific NLP tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon HPSS’s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s (Callon1986?; Rip1986?), offers valuable theoretical grounding, particularly given the resonance of Actor-Network Theory (ANT) concepts with contemporary LLM developments. ```",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-technical-foundations",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-technical-foundations",
    "title": "4  ```",
    "section": "",
    "text": "Slide 01",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-challenge",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-challenge",
    "title": "4  ```",
    "section": "4.2 Interactive Demonstration and Core Challenge",
    "text": "4.2 Interactive Demonstration and Core Challenge\n\n\n\nSlide 08\n\n\nThe OpenAlex Mapper, accessible online at https://m7n-openalex-mapper.hf.space, offers an interactive platform for scholarly exploration. An alternative version, featuring a higher latency GPU setup, also accommodates larger, more demanding queries. Users initiate their investigations by inputting search terms, such as “scale-free network models” or “coriander,” leveraging the comprehensive capabilities of the OpenAlex search interface.\nOnce a query is submitted, the system efficiently downloads the initial 1,000 records corresponding to the user’s input, subsequently embedding their abstracts. If enabled by the user, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as an interactive projection of these search results onto a grey base map, visually indicating where specific terms, authors, or concepts appear across various disciplines. This interactive functionality empowers users to delve deeper; for instance, one can investigate the presence of “coriander” in epidemiology or public health literature. Moreover, the tool offers visualisations of temporal distributions and the overlay of citation graphs, providing multifaceted insights.\nFundamentally, OpenAlex Mapper addresses a critical challenge in the History, Philosophy, and Sociology of Science: the generalisation and validation of findings derived from small samples and case studies. It aims to answer nuanced questions regarding the adoption and prevalence of specific models, such as “Where did the Hopfield model truly establish itself?” By employing rigorous quantitative methods, the tool provides a robust foundation for qualitative, heuristic investigations. Crucially, it maintains a direct link to the actual textual sources, ensuring that all findings remain traceable and verifiable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "href": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "title": "4  ```",
    "section": "4.3 Applications in Scholarly Analysis",
    "text": "4.3 Applications in Scholarly Analysis\n\n\n\nSlide 17\n\n\nThe development of OpenAlex Mapper represents an ongoing research endeavour, with several compelling applications already emerging. Initially, researchers conceived the tool specifically for investigating model templates. This approach explores how models possessing similar structural properties manifest across disparate scientific disciplines, potentially imposing a structure on science that operates orthogonally to traditional disciplinary boundaries. For instance, the system has mapped the distribution of the Ising model (7,819 instances), the Hopfield model (589 instances), and the Sherrington-Kirkpatrick model (1,437 instances). These analyses reveal their presence in distinct, non-continuous regions across the base map. This work draws upon foundational ideas from Humphreys (2004) and Knuuttila and Loettgers (2023).\nBeyond model templates, the tool effectively visualises the distribution of key concepts. A notable example contrasts the concept of “phase transition” with “emergence,” depicted in orange on the map. This capability proves particularly advantageous for broadening conceptual analysis into interdisciplinary contexts, circumventing the common difficulties associated with acquiring specific, cross-disciplinary datasets. Relevant scholarship in this area includes contributions from Malaterre, Chartier, and Lareau (2020), and Zichert and Wüthrich (2024).\nFurthermore, OpenAlex Mapper facilitates the analysis of method distribution across scientific fields. Examining the usage patterns of “Random Forest” (2,000 instances) versus “Logistic Regression” (1,997 instances) reveals clearly distinguishable patterns of adoption within interdisciplinary research. This observation prompts profound philosophical questions, such as why neuroscientists might favour Random Forest algorithms whilst researchers in psychiatry or mental health predominantly employ Logistic Regressions. This application engages with discussions from Breiman (2001), Bzdok, Altman, and Krzywinski (2018), and Andrews (2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "title": "4  ```",
    "section": "4.4 Limitations and Future Directions",
    "text": "4.4 Limitations and Future Directions\n\n\n\nSlide 21\n\n\nDespite its considerable utility, OpenAlex Mapper operates with several acknowledged limitations. Foremost amongst these is the inherent quality of the OpenAlex database itself. Whilst not flawless, its data quality remains commendably reasonable when compared to alternative scholarly data sources.\nA significant constraint currently stems from the language model’s scope: it processes English-language content exclusively. This naturally limits the tool’s overall reach; nevertheless, for investigations focusing on the more recent history of science, this presents a less severe impediment. Developers recognise the potential for future enhancement through the integration of multilingual models, though they note the current scarcity of robust, science-trained multilingual models.\nFurthermore, the embedding process necessitates source data that includes either comprehensive abstracts or sufficiently descriptive titles. This requirement inherently restricts the range of documents the tool can effectively analyse.\nFinally, the Uniform Manifold Approximation and Projection (UMAP) algorithm, central to the tool’s visualisation, introduces its own set of challenges. As a stochastic algorithm, UMAP generates one specific output from a multitude of possibilities, implying that alternative visualisations could emerge from repeated runs. Moreover, the algorithm must inevitably make trade-offs during dimensionality reduction. Compressing the Specter language model’s 768 dimensions into a mere two introduces unavoidable distortions, manifesting as “pushing, pulling, and misaligning” of data points within the two-dimensional space.\nFor those seeking further information, the presentation slides remain accessible online at maxnoichl.eu/talk. Additionally, a comprehensive working paper, titled “Philosophy at Scale: Introducing OpenAlex Mapper,” provides more detailed technical explanations of the system’s architecture and methodology. ```",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry-into-patient-organisations",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry-into-patient-organisations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 The ActDisease Project: Historical Inquiry into Patient Organisations",
    "text": "5.1 The ActDisease Project: Historical Inquiry into Patient Organisations\n\n\n\nSlide 01\n\n\nResearchers have embarked upon the ActDisease project, formally titled “Acting out Disease: How Patient Organizations Shaped Modern Medicine.” This ERC-funded initiative meticulously investigates the historical trajectory of patient organisations across Europe during the 20th century. Its central purpose involves scrutinising how these organisations fundamentally influenced the evolution of disease concepts, the lived experience of illness, and prevailing medical practices.\nThe project’s scope encompasses ten distinct European patient organisations. It draws its primary source material from their periodicals—predominantly magazines—published in England, Germany, France, and Great Britain between approximately 1890 and 1990. Notably, the Hay Fever Association of Heligoland, established in 1897, exemplifies the type of historical entity under examination, with Heligoland, Germany, serving as its foundational site.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-dataset-periodical-collection-and-scope",
    "href": "chapter_ai-nepi_005.html#the-actdisease-dataset-periodical-collection-and-scope",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Dataset: Periodical Collection and Scope",
    "text": "5.2 The ActDisease Dataset: Periodical Collection and Scope\n\n\n\nSlide 06\n\n\nThe ActDisease project has assembled a private, recently digitised collection of patient organisation magazines, constituting a substantial dataset of 96,186 pages. This extensive archive spans various countries, diseases, and publication periods.\nSpecifically, the dataset includes: * German collection: 10,926 pages on Allergy/Asthma from two magazines (1901-1985); 19,324 pages on Diabetes from one magazine (1931-1990); and 5,646 pages on Multiple Sclerosis from one magazine (1954-1990). * Swedish materials: 4,054 pages on Allergy/Asthma (1957-1990); 7,150 pages on Diabetes (1949-1990); and 16,790 pages on Lung Diseases (1938-1991), each from a single magazine. * French contributions: 6,206 pages on Diabetes (1947-1990); and 9,317 pages on Rheumatism/Paralysis from three magazines (1935-1990). * UK segment: 11,127 pages on Diabetes (1935-1990); and 5,646 pages on Rheumatism (1950-1990), each sourced from one magazine.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-processes-and-persistent-challenges",
    "href": "chapter_ai-nepi_005.html#digitisation-processes-and-persistent-challenges",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Digitisation Processes and Persistent Challenges",
    "text": "5.3 Digitisation Processes and Persistent Challenges\n\n\n\nSlide 07\n\n\nThe digitisation process for the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition (OCR). This tool generally performed well, accurately recognising most common layouts and fonts present in the historical periodicals.\nNevertheless, significant challenges persisted. Complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality frequently hindered optimal OCR performance. Consequently, researchers observed persistent OCR errors, particularly prevalent in German and French texts, alongside instances of disrupted reading order. Notably, creative text segments, including advertisements, humour pages, and poems, exhibited a higher frequency of OCR inaccuracies. To mitigate these issues, the team conducted specific experiments, focusing on post-OCR correction of German texts through the application of instruction-tuned generative models. This work has been documented in a publication by Danilova and Aangenendt, presented at the RESOURCEFUL-2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-for-genre-classification-in-historical-periodicals",
    "href": "chapter_ai-nepi_005.html#rationale-for-genre-classification-in-historical-periodicals",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Rationale for Genre Classification in Historical Periodicals",
    "text": "5.4 Rationale for Genre Classification in Historical Periodicals\n\n\n\nSlide 11\n\n\nResearchers observed a profound diversity in the textual content of the historical periodicals, yet these varied text types consistently appeared across all magazines. Crucially, distinct text types frequently co-occurred on a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This inherent textual complexity posed a significant challenge for conventional analytical methods.\nTraditional yearly and decade-based topic models and term counts, the team realised, failed to account for this side-by-side textual variation. Consequently, these methods likely introduced a bias towards the most frequently occurring text type, potentially distorting analytical outcomes. To overcome this limitation, genre emerged as a highly pertinent concept for distinguishing between text types. Genres inherently align with the communicative purposes of authors, as defined in language technology by Petrenz (2004) and Kessler (1997).\nImplementing genre classification directly supports the project’s core objective: exploring the dataset from multiple perspectives to formulate robust historical arguments. Specifically, this approach enables a nuanced study of communicative strategies as they evolved over time, allowing for comparisons across different countries, diseases, and publications, as highlighted by Broersma (2010). Furthermore, it facilitates a more granular analysis of term distributions and topic models, enabling insights within specific genre groups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Illustrative Genre Examples within the ActDisease Dataset",
    "text": "5.5 Illustrative Genre Examples within the ActDisease Dataset\n\n\n\nSlide 19\n\n\nThe ActDisease dataset encompasses a rich array of genres, each serving distinct communicative functions. Researchers identified examples such as poetry, which often provided emotional engagement. Academic reports, exemplified by studies on the pancreas, conveyed scientific and medical information. Legal documents, including deeds of covenant, established formal agreements and regulations. Advertisements, such as those promoting chocolate for diabetics, aimed to market products or services.\nFurthermore, the collection featured instructive or guidance messages, offering practical advice like recipes, doctor’s recommendations, or dietary guidelines. Patient organisation reports documented internal affairs, detailing meetings and activities. Finally, narratives about patient lives provided first-hand accounts of illness experiences, offering a unique perspective on the human dimension of disease.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#experimental-design-for-genre-classification-with-limited-data",
    "href": "chapter_ai-nepi_005.html#experimental-design-for-genre-classification-with-limited-data",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Experimental Design for Genre Classification with Limited Data",
    "text": "5.6 Experimental Design for Genre Classification with Limited Data\n\n\n\nSlide 04\n\n\nConfronted with a scarcity of annotated data, researchers systematically explored two distinct methodological paradigms: zero-shot learning and few-shot learning. For zero-shot learning, the investigation posed two primary research questions: firstly, whether an efficient mapping of genre labels from existing public datasets to custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might fluctuate across different datasets and models.\nConversely, the few-shot learning inquiry focused on two further questions: how performance changes in relation to varying training set sizes across different models; and whether a prior fine-tuning process on the entire dataset could significantly boost classification performance. This comprehensive experimental design forms the basis of a forthcoming publication by Danilova and Söderfeldt, scheduled for presentation at the LaTeCH-CLFL 2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels-for-historical-periodical-content",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels-for-historical-periodical-content",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Defining Genre Labels for Historical Periodical Content",
    "text": "5.7 Defining Genre Labels for Historical Periodical Content\n\n\n\nSlide 32\n\n\nResearchers meticulously defined the genre labels under the direct supervision of the project’s lead historian, an expert in patient organisations. This collaborative process aimed to create categories that would effectively segment the content within the historical periodicals, thereby facilitating deeper historical analysis. Crucially, the team endeavoured to formulate these labels with sufficient generality to enable the classifier’s application to comparable datasets in the future.\nNine distinct genres emerged from this process, each with a precise definition: * Academic: Encompasses research-based reports or scientific explanations, designed to bridge the gap between the scientific medical community and the magazine’s readership. * Administrative: Documents organisational activities, reporting on patient organisation events and internal affairs. * Advertisement: Specifically promotes commercial products or services. * Guide: Provides step-by-step instructions, ranging from health tips to recipes. * Fiction: Aims to entertain and emotionally engage through stories, poems, or humour. * Legal: Explains terms, conditions, or contracts. * News: Reports on recent events. * Nonfiction Prose: Narrates real events or describes cultural and historical topics, including memoirs and essays. * QA (Question and Answer): Designates sections structured as questions with expert responses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-protocol-and-inter-annotator-agreement",
    "href": "chapter_ai-nepi_005.html#annotation-protocol-and-inter-annotator-agreement",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Annotation Protocol and Inter-Annotator Agreement",
    "text": "5.8 Annotation Protocol and Inter-Annotator Agreement\n\n\n\nSlide 37\n\n\nResearchers employed paragraphs as the fundamental annotation unit, extracting them from the ABBYY OCR output. The team subsequently merged these paragraphs based on consistent font patterns—including type, size, italicisation, and the absence of bolding—within each page. Researchers sampled content from two specific periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” focusing on their first and mid-year issues across all publication years.\nSix dedicated project members undertook the annotation task, comprising four historians and two computational linguists. All annotators possessed either native fluency or high proficiency in both Swedish and German. For each paragraph, two independent annotations were meticulously collected. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha, signifying a remarkably high level of consistency amongst the annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Dataset Splitting and Experimental Configurations",
    "text": "5.9 Dataset Splitting and Experimental Configurations\n\n\n\nSlide 39\n\n\nFor the experimental phase, researchers initially partitioned the annotated data into a primary training set of 1182 paragraphs and a held-out set comprising 552 paragraphs, which represented approximately 30% of the total annotated material. Researchers stratified both these sets by genre label to ensure representative distributions.\nWithin the few-shot experiments, the team systematically varied the training set size, employing six distinct configurations: 100, 200, 300, 400, 500, and the full 1182 paragraphs. Each of these smaller training sets was randomly sampled from the main training pool, whilst maintaining a balance across genre labels. The team subsequently divided the held-out set equally into validation and test portions, similarly balanced by label. Notably, researchers excluded the Legal and News genres from these few-shot experiments, as their limited data volume precluded sufficient training. Conversely, the zero-shot experiments leveraged the entirety of the held-out test set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease-training-and-held-out-sets",
    "href": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease-training-and-held-out-sets",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Genre Distribution within ActDisease Training and Held-Out Sets",
    "text": "5.10 Genre Distribution within ActDisease Training and Held-Out Sets\n\n\n\nSlide 39\n\n\nAnalysis of the genre distribution across both the training and held-out samples of the ActDisease dataset revealed a pronounced imbalance. Specifically, the Advertisement and Non-fictional Prose genres exhibited significant disparities in their representation across different languages. This imbalance necessitates careful consideration during model training and evaluation to prevent potential biases.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#leveraging-external-datasets-for-zero-shot-classification",
    "href": "chapter_ai-nepi_005.html#leveraging-external-datasets-for-zero-shot-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Leveraging External Datasets for Zero-Shot Classification",
    "text": "5.11 Leveraging External Datasets for Zero-Shot Classification\n\n\n\nSlide 39\n\n\nFor the zero-shot experiments, researchers incorporated external, modern datasets previously utilised in automatic web genre classification. The Corpus of Online Registers of English (CORE), developed by Egbert et al. (2015), provides document-level annotations. It encompasses English content, with main categories also available in Swedish, Finnish, and French.\nSimilarly, Sharoff’s (2018) Functional Text Dimensions (FTD) dataset, also annotated at the document level, offers balanced content in English and Russian. Kuzman et al. (2023) had previously leveraged this dataset for web genre classification. Additionally, the team employed UD-MULTIGENRE (UDM), a subset of Universal Dependencies (de Marneffe et al., 2021), which features genre annotations at the sentence level across 38 languages, as detailed by Danilova and Stymne (2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping-and-alignment",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping-and-alignment",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Cross-Dataset Genre Mapping and Alignment",
    "text": "5.12 Cross-Dataset Genre Mapping and Alignment\n\n\n\nSlide 39\n\n\nResearchers meticulously performed genre mapping across datasets, with two independent annotators undertaking the task. The team only included assignments achieving full agreement in the final mapping, ensuring robust alignment.\nThis process established correspondences between ActDisease genres and their equivalents in CORE, UDM, and FTD. For instance, “Academic” in ActDisease mapped to “research article” (RA) in CORE, “academic” in UDM, and “academic (A14)” in FTD. “Advertisement” aligned with “advertisement (AD)” in CORE, “description with intent to sell (DS)” in UDM, and “commercial (A12)” in FTD. Similarly, “Fiction” found its counterparts in “poem” (PO) and “short story” (SS) in CORE, “fiction” in UDM, and “fictive (A4)” and “poetic (A19)” in FTD. However, the team encountered a limitation: some ActDisease genres lacked suitable corresponding labels within the available external datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-generation-and-multilingual-encoder-models",
    "href": "chapter_ai-nepi_005.html#training-data-generation-and-multilingual-encoder-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Training Data Generation and Multilingual Encoder Models",
    "text": "5.13 Training Data Generation and Multilingual Encoder Models\n\n\n\nSlide 39\n\n\nResearchers generated training data through a meticulous pipeline encompassing mapping, preprocessing, chunking, and systematic sampling. They configured training sets in four distinct ways for each dataset: one configuration ([G+]) focused exclusively on Germanic languages; another ([B1]) balanced data according to ActDisease labels; a third ([G-]) incorporated all language families; and the final configuration ([B2]) balanced data by both ActDisease and original labels. This yielded four FTD, four CORE, four UDM*, and four merged training samples, all subjected to fine-tuning.\nFor classification, the team employed several multilingual encoder models, which are transformer-based neural networks capable of processing text in multiple languages. XLM-Roberta, developed by Conneau et al. (2020), served as a state-of-the-art web genre classifier, as noted by Kuzman et al. (2023). mBERT (Devlin et al., 2019) provided a baseline for comparison with historical mBERT (Schweter et al., 2022). Crucially, historical mBERT, pretrained on an extensive corpus of multilingual historical newspapers, proved particularly relevant given its inclusion of the target languages. These BERT-like models have consistently demonstrated efficacy in prior web register and genre classification studies, as evidenced by Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). Ultimately, the fine-tuning process generated 48 distinct models, and the team subsequently averaged performance metrics across all configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-and-performance-analysis",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-and-performance-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Zero-Shot Learning Evaluation and Performance Analysis",
    "text": "5.14 Zero-Shot Learning Evaluation and Performance Analysis\n\n\n\nSlide 11\n\n\nEvaluating the zero-shot predictions presented a unique challenge: the imperfect overlap of label sets prevented direct comparison of overall performance metrics. Consequently, researchers meticulously assessed the performance of each genre individually, complementing this analysis with a thorough examination of confusion matrices to mitigate potential biases. The X-GENRE web genre classifier, as detailed by Kuzman et al. (2023), served as a robust baseline, with predictions focusing exclusively on the most similar labels directly mappable to the ActDisease genres.\nThe experimental setup often involved a cross-lingual context. FTD and X-GENRE, for instance, operated without German or Swedish data, whilst UDM and CORE datasets exhibited partially cross-lingual characteristics. Overall, models fine-tuned on FTD consistently demonstrated superior performance when integrated with the ActDisease mapping. Conversely, other datasets revealed distinct class-specific biases.\nUDM, for example, exhibited a bias towards news, primarily because its news training data contained the highest proportion of Germanic instances, overwhelmingly German. Similarly, CORE displayed a bias towards the guide genre, as its training data for this category was uniquely multilingual.\nIntriguingly, certain models excelled in specific genre predictions. XLM-Roberta, when fine-tuned on UDM, achieved an average of 32% more correct predictions in the QA genre compared to mBERT and hmBERT. Conversely, hmBERT, also on UDM, produced an average of 16% more accurate predictions in the Administrative genre than XLM-Roberta and mBERT. Furthermore, CORE-based models consistently proved proficient at predicting the legal genre. Confusion matrices visually underscored these observed behavioural patterns, whilst detailed average per-category F1 scores provided a comprehensive quantitative assessment across all data configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-model-advantages",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-model-advantages",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 Few-Shot Learning Performance and Model Advantages",
    "text": "5.15 Few-Shot Learning Performance and Model Advantages\n\n\n\nSlide 39\n\n\nFew-shot learning experiments unequivocally demonstrated the advantage of further training models on the ActDisease dataset, particularly when incorporating Masked Language Model (MLM) fine-tuning. The F1 score consistently improved as the number of training instances increased, though it remained below 0.8 even with the full training set of 1182 instances.\nAmongst the models tested, hmBERT-MLM consistently outperformed its counterparts. A detailed examination of its performance revealed that, unlike other models, hmBERT-MLM retained its capacity to differentiate between fiction and nonfiction genres even when exposed to the full dataset. Conversely, other models, notably XLM-Roberta, exhibited a drastic decline in their ability to distinguish these two categories.\nAnalysis of XLM-Roberta’s confusion matrix, when fine-tuned with MLM on the full dataset, indicated a frequent overprediction of nonfiction prose for fiction. This phenomenon likely stems from the shared thematic content within the ActDisease data, where both fictional and autobiographical narratives often revolve around patient experiences, leading to similar themes and narrative structures. Consequently, researchers propose that an increased volume of data is essential to enhance performance in distinguishing these increasingly similar genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Few-Shot Prompting with Llama-3.1 8b Instruct",
    "text": "5.16 Few-Shot Prompting with Llama-3.1 8b Instruct\n\n\n\nSlide 39\n\n\nGiven the current insufficiency of data for comprehensive instruction tuning, researchers opted to evaluate few-shot prompting using Llama 3.1 8b Instruct, a widely recognised multilingual generative model with open weights. The prompt structure provided clear genre definitions, complemented by two or three illustrative examples for each category.\nThe results indicated that the model handled certain genre labels with reasonable efficacy. For instance, it achieved an F1-score of 0.84 for Legal content and 0.72 for Academic texts. However, the limited number of examples proved insufficient for robust representation across all genres. Notably, nonfictional prose yielded a lower F1-score of 0.49, whilst advertisement and administrative content also demonstrated suboptimal performance, with F1-scores of 0.73 and 0.60 respectively. The overall accuracy stood at 0.62, with a macro average F1-score of 0.59 and a weighted average of 0.63.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#key-findings-and-strategic-recommendations",
    "href": "chapter_ai-nepi_005.html#key-findings-and-strategic-recommendations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Key Findings and Strategic Recommendations",
    "text": "5.17 Key Findings and Strategic Recommendations\n\n\n\nSlide 39\n\n\nPopular magazines, unlike more specialised scientific journals or books, frequently encompass a multitude of genres. This characteristic significantly complicates text mining efforts. Researchers have concluded that genres inherently reflect deliberate choices in communicative strategies; consequently, accounting for these distinctions, whilst challenging, proves crucial for achieving accurate and detailed interpretations of text mining outcomes. Fundamentally, genre classification renders these rich historical sources accessible for advanced text mining.\nFor scenarios lacking dedicated training data, two viable strategies emerge. Firstly, one can successfully leverage existing modern datasets, provided the target categories maintain a general purpose. Alternatively, few-shot instruction of a proficient generative model offers another effective pathway.\nHowever, when some training data is available, few-shot learning of multilingual encoders, particularly those with prior Masked Language Model (MLM) fine-tuning—such as XLM-Roberta or historical multilingual BERT—demonstrates superior performance. Indeed, this latter approach emerged as the optimal solution for the project. Notably, historical multilingual BERT exhibited particularly strong gains, achieving a 24% improvement, which significantly surpassed the 14.5% gain for mBERT-MLM and 16.9% for XLM-RoBERTa. These findings underscore the value of domain-specific pre-training for historical text analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Ongoing and Future Research Directions",
    "text": "5.18 Ongoing and Future Research Directions\n\n\n\nSlide 39\n\n\nThe research team is actively pursuing several avenues to enhance the quality and scope of this work for both the project and the wider academic community. Currently, researchers are engaging with specific historical hypotheses, leveraging the insights gained from genre classification. Furthermore, they are developing a new, more fine-grained annotation scheme for genres, a project notably financed by Swe-CLARIN. Methodologically, the team is exploring advanced techniques, including synthetic data generation and active learning, to further refine their classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#acknowledgements",
    "href": "chapter_ai-nepi_005.html#acknowledgements",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Acknowledgements",
    "text": "5.19 Acknowledgements\n\n\n\nSlide 19\n\n\nThe project gratefully acknowledges the invaluable contributions of its annotators and core team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. Funding for this research came generously from the European Research Council under grant ERC-2021-STG 10104099. The Centre for Digital Humanities and Social Sciences offered crucial institutional support, supplying essential GPUs and data storage facilities. Finally, the team extends its gratitude to the diligent reviewers, Dr Maria Skeppstedt and other anonymous contributors, whose feedback significantly enhanced the work. Further details are available on the project website. ```",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-team-and-scope",
    "href": "chapter_ai-nepi_006.html#project-team-and-scope",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 Project Team and Scope",
    "text": "6.1 Project Team and Scope\n\n\n\nSlide 02\n\n\nThe VERITRACE project, formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,” constitutes a five-year ERC Starting Grant initiative. Active from 2023 to 2028, this significant undertaking, bearing the grant number 101076836, operates from the Vrije Universiteit Brussel (VUB).\nProfessor Cornelis J. Schilt serves as the project’s Principal Investigator, leading a dedicated team of five scholars. The core team comprises Dr. Eszter Kovács, Niccolò Cantoni, and Demetrios Paraschos, alongside Dr. Jeffrey Wolf. Dr. Wolf, a historian specialising in science and medicine with an eighteenth-century focus, specifically contributes his expertise in digital humanities to the project. Whilst the team bases its operations in Brussels, individual members maintain residences in various locations, including Berlin. Further information regarding the project’s scope and progress is accessible on its official website, veritrace.eu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#ancient-wisdom-in-early-modern-thought",
    "href": "chapter_ai-nepi_006.html#ancient-wisdom-in-early-modern-thought",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 Ancient Wisdom in Early Modern Thought",
    "text": "6.2 Ancient Wisdom in Early Modern Thought\n\n\n\nSlide 01\n\n\nAt its core, the VERITRACE project endeavours to trace the profound influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia. This tradition significantly impacted the evolution of natural philosophy and science during the early modern period. It manifests in various foundational works, including the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for scholars of chemistry’s history, the Corpus Hermeticum.\nResearchers assembled a close-reading corpus of 140 works, each embodying this ancient wisdom tradition. Historical evidence already confirms its significant impact; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum [Citation Needed for Newton/Kepler claims]. Beyond these established connections, the project seeks to delve deeper, aiming to uncover a far broader array of networks and texts that interacted with this tradition. One scholar aptly termed this extensive, often neglected body of work the ‘great unread’ [Citation Needed for “great unread” coinage], as it frequently comprises numerous texts by lesser-known authors, rarely forming the primary focus of historical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-historical-inquiry",
    "href": "chapter_ai-nepi_006.html#computational-historical-inquiry",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Historical Inquiry",
    "text": "6.3 Computational Historical Inquiry\n\n\n\nSlide 04\n\n\nTo address its central research questions, the VERITRACE project employs a robust computational framework, enabling large-scale multilingual exploration. A primary objective involves identifying textual reuse across a vast, multilingual corpus. This encompasses both direct lexical reuse, where authors incorporate direct, potentially uncited, quotations, and more indirect semantic reuse, involving paraphrases or subtle allusions that contemporary readers would have recognised as originating from specific works, such as the Corpus Hermeticum.\nEssentially, the system functions as an ‘early modern plagiarism detector’, designed to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this systematic analysis, researchers anticipate identifying novel patterns within the intellectual history and philosophy of science. The project leverages specific tools, including advanced keyword search capabilities and sophisticated text matching algorithms, to facilitate these investigations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-construction-and-sources",
    "href": "chapter_ai-nepi_006.html#corpus-construction-and-sources",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Corpus Construction and Sources",
    "text": "6.4 Corpus Construction and Sources\n\n\n\nSlide 04\n\n\nThe project meticulously assembled a large and diverse multilingual dataset, focusing exclusively on printed works to ensure manageability, thereby excluding handwritten materials. This extensive corpus spans approximately two centuries, from 1540 to 1728. The year 1540 was chosen as a starting point for various historical reasons, including the increasing prevalence of printing and the burgeoning intellectual movements of the Renaissance, whilst 1728 falls shortly after Isaac Newton’s death, marking a significant shift in scientific thought. The dataset incorporates texts in at least six different languages.\nThree primary digital repositories constitute the main data sources: Early English Books Online (EEBO), Gallica (which provides access to materials from the French National Library), and the Bavarian State Library. Collectively, these sources yield approximately 430,000 books for analysis. Researchers intend to apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to explore this rich historical collection.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-and-llm-applications",
    "href": "chapter_ai-nepi_006.html#challenges-and-llm-applications",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Challenges and LLM Applications",
    "text": "6.5 Challenges and LLM Applications\n\n\n\nSlide 06\n\n\nThe VERITRACE project navigates several core challenges inherent in processing historical texts at scale. A primary concern stems from the variable quality of Optical Character Recognition (OCR) text. Libraries provide this raw text directly in formats such as XML, HOCR, and HTML, frequently without accompanying ground truth page images. This raw input significantly affects all subsequent data processing stages.\nFurthermore, early modern typography and the evolving semantics across at least six distinct languages introduce considerable complexity. The sheer volume of data—hundreds of thousands of texts published across Europe over two centuries—also presents a substantial logistical and computational challenge.\nThe project strategically employs Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as ‘judges’, assisting in the enrichment and cleaning of metadata. This application, whilst promising for automating tedious tasks, currently faces challenges such as output hallucinations (fabricating information not present in the source data) and a tendency towards generic responses when structured output is requested. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, a crucial step for facilitating the project’s advanced text matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-a-case-study",
    "href": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-a-case-study",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 LLMs for Metadata Enrichment: A Case Study",
    "text": "6.6 LLMs for Metadata Enrichment: A Case Study\n\n\n\nSlide 08\n\n\nA significant internal case study explores the application of LLMs to enrich VERITRACE metadata, aiming to automate the laborious process of bibliographic record comparison. The core motivation involves mapping VERITRACE records to the Universal Short Title Catalogue (USTC), a high-quality metadata source, to create enriched records that require less manual cleaning. Whilst external identifiers facilitate some automated mapping, the majority of records necessitate manual review due to the uncleaned state of the VERITRACE data. This manual process proved exceptionally tedious, with each team member assigned 10,000 pairs of bibliographic records for comparison, determining if they represented the same underlying printed text.\nTo alleviate this burden, researchers devised an LLM-based solution, conceptualised as a ‘bench’ or ‘panel of judges’. This system employs a chain of LLMs—including Primary, Secondary, and Tiebreaker models, with an Expert LLM handling edge cases—to evaluate pairs of bibliographic records. The LLMs are tasked with judging whether records from a low-quality source and a high-quality source represent the identical underlying text. Crucially, the models must provide both their decision (match or no match) and accompanying reasoning with confidence levels. The team then compares these LLM decisions against ground truth data, followed by a final review by VERITRACE scholars.\nCurrently, the project utilises open-source LLM models, such as Llama, for this task. However, significant challenges persist. The models frequently exhibit hallucinations in their output, fabricating records not present in the input. Furthermore, whilst requesting more structured output aims to reduce unhelpful responses, it often results in more generic and less insightful reasoning. Achieving the optimal balance between structured output and helpfulness remains an ongoing challenge, which scholars describe as more ‘art’ than science, given the iterative refinement required. Despite these hurdles, the approach holds substantial theoretical potential for significant time savings, though it has not yet achieved full operational efficacy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-pipeline",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-pipeline",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 VERITRACE Web Application: Alpha Version and Pipeline",
    "text": "6.7 VERITRACE Web Application: Alpha Version and Pipeline\n\n\n\nSlide 13\n\n\nThe VERITRACE project developed an alpha version of its web application, currently in its nascent stages and remaining internally accessible. This internal prototype serves as a tangible demonstration of the project’s ambitious future capabilities. Engineers are presently testing a BERT-based Large Language Model, specifically LaBSE, to generate vector embeddings for every passage within the textual corpus. Whilst this model demonstrates functionality in certain scenarios, preliminary assessments suggest it may not ultimately suffice for the project’s comprehensive requirements, particularly concerning historical linguistic nuances.\nUnderpinning this application lies a complex 15-stage data processing pipeline. This pipeline meticulously transforms raw text, which libraries supply in various formats including XML, HOCR, and HTML, into a structured Elasticsearch database, serving as the web application’s backend. The pipeline encompasses numerous critical stages, such as extracting text into standardised files, generating precise mappings of textual positions, segmenting the content, and rigorously assessing the OCR quality of each input. The generation of vector embeddings occurs towards the latter stages of this intricate process. Crucially, each of these 15 stages demands individual optimisation to ensure the integrity and efficiency of the entire workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-exploration-and-metadata",
    "href": "chapter_ai-nepi_006.html#web-application-features-exploration-and-metadata",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Web Application Features: Exploration and Metadata",
    "text": "6.8 Web Application Features: Exploration and Metadata\n\n\n\nSlide 15\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Metadata Explorer, Search, Analyse, Read, and Match. The Explore section serves as an initial entry point, offering comprehensive statistics about the corpus. This data, drawn directly from a Mongo database, provides an overview of the collection, including the total count of 427,305 metadata records. Visualisations such as pie charts and bar charts illustrate language distribution, documents by data source, documents by decade, and publication places, enabling users to gain immediate insights into the corpus’s composition.\nBeyond this statistical overview, the Metadata Explorer section allows users to delve into the rich metadata associated with each individual text. This includes detailed fields such as Document ID, Filename, Bibliographic Title, Author, Publication Place, Printer, Format, Language, and Subject. A crucial feature involves granular language identification, performed on every text down to approximately 50 characters. This addresses the prevalent multilingual nature of early modern works, ensuring accurate language representation beyond simple metadata declarations. For instance, a text might reveal a substantive multilingual composition, such as 15% Greek and 85% Latin. Furthermore, the system attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-search-and-analysis",
    "href": "chapter_ai-nepi_006.html#web-application-features-search-and-analysis",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Web Application Features: Search and Analysis",
    "text": "6.9 Web Application Features: Search and Analysis\n\n\n\nSlide 17\n\n\nFor most scholarly users, the Search section will likely serve as the primary interface, offering robust keyword search capabilities. For example, a simple query for “Hermes” within the current prototype corpus, which comprises 132 files, yields 22 documents containing 332 matches. Notably, even this limited prototype generates a 15 GB index, indicating that the full 400,000-text corpus will necessitate terabytes of storage.\nLeveraging Elasticsearch, the system supports highly sophisticated queries beyond basic keywords. Users can perform field-specific searches, such as retrieving all books by Kepler that contain “Hermes.” Furthermore, the platform accommodates complex boolean logic (ANDs, ORs), nested queries, and proximity searches, allowing users to specify, for instance, texts where “Hermes” and “Plato” appear within ten words of each other.\nWhilst not yet implemented, the Analyse section of the website is poised to offer advanced analytical tools. These will include Topic Modelling, Latent Semantic Analysis (LSA), and Diachronic Analysis, enabling scholars to explore thematic shifts and conceptual relationships over time.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-reading-and-matching",
    "href": "chapter_ai-nepi_006.html#web-application-features-reading-and-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 Web Application Features: Reading and Matching",
    "text": "6.10 Web Application Features: Reading and Matching\n\n\n\nSlide 19\n\n\nThe web application incorporates a dedicated Read section, providing scholars with access to high-quality digital facsimiles of the texts. Utilising a Mirador viewer, users can engage with PDF versions of each document, complemented by the display of associated metadata.\nCrucially, the Match section facilitates the identification of textual reuse across the corpus. This powerful tool supports various comparison modes: users can compare a single document against another, perform multi-document comparisons (e.g., analysing textual overlap across all of Kepler’s works within the database), or even compare a single text against the entire corpus. The latter, whilst highly desirable, presents significant computational challenges regarding user wait times. The interface exposes numerous parameters for user customisation, such as minimum similarity scores, allowing scholars to fine-tune their searches.\nThe system offers two fundamental match types. Lexical matching employs keyword analysis to identify vocabulary similarities, proving ineffective for cross-language comparisons due to differing word forms. Conversely, semantic matching leverages vector embeddings to discover conceptually similar passages, irrespective of shared vocabulary. This approach relies on a BERT-based multilingual embeddings model, trained on 109 languages, which encodes passages into a unified vector space, thereby enabling seamless cross-language comparisons. A hybrid matching option also exists, combining both lexical and semantic approaches with adjustable weights. Furthermore, users can select from different matching modes: a standard mode, a comprehensive mode that demands more computational power for exhaustive results, and a faster mode for quicker, though potentially less complete, outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-newtons-opticks-case-study",
    "href": "chapter_ai-nepi_006.html#text-matching-newtons-opticks-case-study",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Text Matching: Newton’s Opticks Case Study",
    "text": "6.11 Text Matching: Newton’s Opticks Case Study\n\n\n\nSlide 16\n\n\nA compelling case study involves comparing Isaac Newton’s Latin edition of Optice (1719) with its English counterpart, Opticks (1718). When performing a lexical match between these two texts, the standard mode yields no significant results, precisely as anticipated given their differing languages. However, employing the comprehensive mode reveals three matches, likely corresponding to English text embedded within the Latin edition, such as a preface or annotations.\nConversely, a semantic match between these translated works produces reasonable results, despite existing OCR issues. Passages demonstrate clear conceptual similarity, for instance, parallel discussions on colours. The system provides detailed match summary metrics, including a high quality score of 91.2%. Nevertheless, the coverage score registers at a comparatively low 36.9%. This low coverage, whilst initially appearing problematic, actually provides valuable insight: the Latin edition is considerably longer and exhibits notable divergences from the English version, suggesting the metric accurately reflects the textual relationship rather than an error in the system. The interface for lexical matches further enhances usability by automatically highlighting matching terms within both the source and comparison passages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-horizons",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-horizons",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Future Challenges and Horizons",
    "text": "6.12 Future Challenges and Horizons\n\n\n\nSlide 21\n\n\nThe VERITRACE project faces several critical challenges as it progresses towards full implementation. The current BERT-based embedding model, LaBSE, whilst a valuable starting point, is likely insufficient for the project’s comprehensive needs, primarily due to inherent trade-offs between accuracy, storage requirements, and inference time. Researchers are actively exploring alternative models such as XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting its own set of compromises regarding performance and suitability for historical data. A fundamental question arises: given the distinct nature of the historical corpus, is fine-tuning a base model on this specific dataset essential to achieve adequate results, or can off-the-shelf models suffice?\nA further complexity involves the evolution of semantic meaning over time. The project must address how to accurately handle semantic shifts across centuries, particularly when comparing texts published in 1540 with those from 1700, often in different languages. This raises a crucial query: do texts from disparate historical periods truly reside within the same vector space when processed by modern models, or do historical linguistic changes necessitate specialised approaches?\nThe pervasive issue of poor OCR quality also impacts every downstream process, fundamentally hindering accurate segmentation into sentences and passages. Re-OCR of the entire corpus is not feasible due to its immense size; therefore, the team must consider re-OCR for only the very poorest quality texts or investing time to locate existing high-quality versions. Finally, scaling and performance present a significant hurdle. Current queries on a mere 132 texts require approximately 15 seconds. Scaling this to the full corpus of 430,000 texts will undoubtedly introduce substantial performance issues, necessitating considerable computational power and innovative solutions to maintain acceptable query times. The project actively welcomes external advice on these multifaceted challenges, as it strives to unlock unprecedented insights into early modern intellectual history. ```",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI (XAI) 1.0: Feature Attributions",
    "text": "7.1 Explainable AI (XAI) 1.0: Feature Attributions\n\n\n\nSlide 03\n\n\nExplainable AI (XAI) encompasses methods and approaches meticulously developed for deciphering the internal workings of highly complex machine learning models. Historically, machine learning predominantly focused on visual data; interest in language, whilst present, gained significant momentum only in recent years. Typically, a “Black Box AI System” receives an input, such as an image, and subsequently generates a prediction, for instance, identifying a “Rooster”. Crucially, users often lack insight into the underlying basis for such classifications.\nTo address this opacity, researchers pioneered Post-Hoc Explainability techniques. Heatmaps, for example, visually delineate the specific pixels or features that primarily contributed to a given prediction. In the rooster example, a heatmap would highlight the bird’s head, clearly indicating the model’s focus. Beyond mere transparency, the broader rationale for explainability spans several critical objectives. Firstly, XAI enables verification of predictions, ensuring the model operates logically and produces reasonable outcomes. Secondly, it facilitates the identification of flaws and biases, offering insights into how models make mistakes. Thirdly, it serves as a tool for learning about the underlying problem itself, as models occasionally uncover surprising and unconventional solutions. Finally, and increasingly vital, explainability ensures compliance with evolving legislation, such as the European AI Act. Samek et al. (2017) provided foundational work in this domain.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0-challenges",
    "href": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0-challenges",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Generative AI and XAI 2.0 Challenges",
    "text": "7.2 Generative AI and XAI 2.0 Challenges\n\n\n\nSlide 03\n\n\nThe artificial intelligence landscape has profoundly shifted from conventional classification models to the era of Generative AI (Gen AI). These advanced models now exhibit multifaceted capabilities, encompassing not only classification but also the retrieval of similar images, the generation of novel images, and comprehensive question-and-answer functionalities across diverse topics. Consequently, grounding a prediction or an answer from a Large Language Model (LLM) system to its specific input has become considerably more challenging. Researchers are therefore exploring new directions for XAI, moving beyond simple heatmap representations to consider intricate feature interactions and adopt a more mechanistic view of model operations—that is, understanding the specific internal computations that lead to an output. These contemporary foundation models function as both multi-task and world models, offering profound insights into societal structures and the evolution of text over time.\nNevertheless, these sophisticated models can still exhibit surprising errors. A well-known example from object classification illustrates this: a standard classifier predicted a boat based on the surrounding water, a correlated and texturally simpler feature, rather than the boat itself. Lapuschkin et al. (Nat Commun ’19) documented this phenomenon. More recently, Mondal & Webb et al. (arxiv ’24) highlighted multi-step planning mistakes in LLMs. When tasked with the Tower of Hanoi puzzle, for instance, an LLM might immediately attempt to move the largest, inaccessible disc, demonstrating a fundamental misunderstanding of the problem’s physical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-first-order-attributions",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-first-order-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Structured Interpretability: First-Order Attributions",
    "text": "7.3 Structured Interpretability: First-Order Attributions\n\n\n\nSlide 05\n\n\nStructured interpretability extends the utility of XAI beyond basic visualisations. First-order explanations, for instance, prove particularly effective for elucidating the decisions of classification models. Researchers applied this technique to a classifier designed for historical documents, specifically aiming to distinguish various subgroups of historical tables, such as astronomical or chronological tables.\nTo validate the classifier’s efficacy, teams employed heatmaps, meticulously verifying that predictions relied upon genuinely meaningful features. This analysis revealed that the model correctly focused on the numerical content within the tables. This focus served as an accurate proxy for identifying numerical tables, thereby confirming the model’s meaningful operation and providing confidence in its classifications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-second-and-higher-order-interactions",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-second-and-higher-order-interactions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Structured Interpretability: Second and Higher-Order Interactions",
    "text": "7.4 Structured Interpretability: Second and Higher-Order Interactions\n\n\n\nSlide 10\n\n\nBeyond first-order attributions, researchers have explored more complex forms of structured interpretability, notably second and higher-order interactions. Second-order features primarily focus on pairwise relationships, with similarity proving particularly important. Scientists computed a dot product from the embeddings of two entities, such as images, yielding a similarity score. Subsequently, interaction scores between specific features, like individual digits, elucidated the basis for these similarity predictions, confirming the model’s intended function.\nFurthermore, in more recent work, higher-order interactions have demonstrated greater significance, particularly within graph structures. These structures might represent citation networks, intricate networks of books, or various interconnected entities. When models are trained on classification tasks within such networks, researchers find that feature subgraphs or feature walks—essentially, sets of interconnected features or paths through the graph—become collectively relevant. Identifying these complex interactions facilitates deeper insights into model behaviour, moving towards a more granular, circuit-level understanding of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biases-and-long-range-dependencies",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biases-and-long-range-dependencies",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 First-Order Attributions in LLMs: Biases and Long-Range Dependencies",
    "text": "7.5 First-Order Attributions in LLMs: Biases and Long-Range Dependencies\n\n\n\nSlide 12\n\n\nFirst-order attributions offer crucial insights into the internal workings of Large Language Models (LLMs), particularly concerning biases and their handling of long-range dependencies.\n\n7.5.1 Biased Sentiment Predictions in Transformer LLMs\nResearchers investigated feature importance in LLMs by analysing how specific names influenced sentiment predictions in movie reviews, a common task within the language community. Employing heatmaps generated via a novel method tailored for Transformers, they uncovered notable biases. Male Western names, such as Lee, Barry, Raphael, or the Cohen Brothers, consistently correlated with a higher likelihood of positive sentiment predictions. Conversely, more foreign-sounding names, including Saddam, Castro, or Chan, tended to elicit negative sentiment scores. This demonstrates XAI’s considerable utility in detecting subtle, fine-grained biases embedded within these complex models, a phenomenon now widely recognised within the community. Ali et al. (ICML ‘22) detailed this work in’XAI for Transformers’.\n\n\n7.5.2 First-Order Attributions for Long-Range Dependencies in LLMs\nFurther research explored how LLMs manage long-range dependencies, specifically when generating text summaries from extensive inputs, up to an 8,000-token context window. In a typical scenario involving Wikipedia articles, the model receives a lengthy text and then produces a summary. Analysis revealed that the model predominantly focuses on the latter portions of the provided context, prioritising information presented closer to the prompt. Whilst models can indeed draw upon long-range information from the very beginning of the context, they do so significantly less frequently, as evidenced by a log scale of token counts. Consequently, users should note that LLM-generated summaries may not provide a balanced overview of the entire input text, often emphasising more recently presented data. Jafari et al. (NeurIPS ‘24) presented these findings in’MambaLRP’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "href": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Second & Higher-Order Interactions in Text",
    "text": "7.6 Second & Higher-Order Interactions in Text\n\n\n\nSlide 11\n\n\nExploring second and higher-order interactions offers deeper insights into how models process textual data.\n\n7.6.1 Second-Order Interactions for Text Similarity\nConsider a scenario involving a pair of sentences, processed by a sentence embedding model, such as a Bird model, to yield a similarity score. The challenge lies in comprehending the precise reasons for that score. Second-order explanations address this by providing granular interaction scores between individual tokens. Analysis of these scores frequently reveals noun matching strategies, encompassing both synonyms and identical noun tokens, alongside interactions involving separators and other token types. This suggests that whilst models compress vast amounts of information, they often rely on surprisingly simplistic underlying strategies to achieve their similarity predictions.\n\n\n7.6.2 Graph Neural Networks for Structured Predictions\nGraph Neural Networks (GNNs) offer a powerful framework for structured predictions, providing attributions in terms of “walks” that represent complex feature interactions. Intriguingly, GNNs, which inherently encode structural information, can be conceptualised as LLMs, given that their attention networks facilitate token message passing. This connection enables their application to the analysis of language structure.\n\n\n7.6.3 Interaction of Nodes Learns Complex Language Structure\nResearchers demonstrated this by training a GNN (or an LLM) on a movie review sentiment task, leveraging the hierarchical structure inherent in natural language. They then extracted “walks” to understand the model’s decision-making. First-order attributions proved insufficient, failing to capture the nuanced complexity of language; for instance, the phrase “first I didn’t like the boring pictures” might receive a high positive score solely due to the presence of “like,” neglecting the crucial negation. In stark contrast, higher-order explanations accurately assigned a negative score to the entire negative sentence and correctly captured the hierarchical structure of the subsequent positive statement. Schnake et al. (TPAMI ‘22) published this work in’Higher-Order Explanations of Graph Neural Networks via Relevant Walks’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 AI-based Scientific Insights in the Humanities",
    "text": "7.7 AI-based Scientific Insights in the Humanities\n\n\n\nSlide 20\n\n\nAI-based methodologies offer transformative potential for scientific insights within the humanities, as demonstrated through several compelling examples.\n\n7.7.1 Extracting Visual Definitions from Corpora\nResearchers embarked on a project to extract visual definitions from a corpus of mathematical instruments. Their objective involved classifying these instruments, distinguishing, for instance, between a machine and a purely mathematical instrument. Employing heatmap-based approaches for visual definitions, the team collaborated closely with historians, including Matteo Valeriani and Jochen Büttner. These domain experts provided crucial guidance and meticulously verified the definitions derived from the models. The analysis revealed that fine-grained scales present on the mathematical instruments proved highly relevant for the model’s classification decisions. El-Hajj & Eberle+ (Int J Digit Humanities ’23) published this work on explainability and transparency in digital humanities.\n\n\n7.7.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA larger collaborative project focused on the corpus-level analysis of early modern astronomical tables. This initiative involved the Sphaera Corpus (1472-1650) and the Sacrobosco Table Corpus (1472-1650), collectively comprising 76,000 pages of university textbooks. Historically, these tables, vital carriers of scientific knowledge and indicators of mathematisation processes, had never been analysed at scale. This challenge arose from the data’s extreme heterogeneity, limited annotations, and the inadequacy of conventional Optical Character Recognition (OCR) and foundation models. The primary objective was to develop an automated method for matching tables with similar semantics. Valeriani et al. (2019) and Eberle et al. (2024) detail the foundational corpora.\n\n\n7.7.3 Historical Insights at Scale: XAI-Historian Workflow\nTo address these challenges, researchers developed a comprehensive workflow designed to empower historians with insights at scale, coining the term XAI-Historian. This concept envisions a historian leveraging AI and explainable AI to generate data-driven hypotheses and uncover new case studies. The workflow encompasses three key stages: initial data collections from images of books, followed by an atomisation-recomposition phase involving input tables, bigram maps, and histograms, culminating in corpus-level analysis through historical table embedding and data similarity. Rather than relying on general foundation models, which proved ineffective on this out-of-domain historical data, a specialised statistical model was crafted to detect bigrams. This bespoke model’s reliability was rigorously verified by confirming consistent bigram detection, such as “38” across two distinct inputs, thereby establishing trust in its decisions. Eberle et al. (Sci Adv ’24) and Eberle et al. (TPAMI ’22) document this pioneering work.\n\n\n7.7.4 Cluster Entropy Analysis to Investigate Innovation\nBuilding upon these capabilities, researchers employed cluster entropy analysis to investigate the spread of innovation across European publishing centres during the early modern period. Focusing on the output of specific cities within the Sphaera publication (EPISD-626), they quantified the diversity of each city’s print programme using entropy. A low entropy score indicated a tendency to reproduce identical content, whilst a higher score signified a more diverse programme. This approach, utilising model representations for distance-based clustering and entropy calculation, enabled large-scale analysis previously unattainable. The analysis identified two particularly interesting cases: Frankfurt/Main, which exhibited the lowest entropy, confirming its established reputation as a centre for reprinting editions; and Wittenberg, also displaying remarkably low entropy. This latter finding revealed a historical anomaly: the political control exerted by the Protestant reformers, notably Melanchthon, actively limited the print programme and curriculum, a discovery that aligned perfectly with existing historical intuition and scholarly support. Eberle et al. (Sci Adv ’24) further elaborates on these findings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 Conclusion: AI-based Methods for the Humanities",
    "text": "7.8 Conclusion: AI-based Methods for the Humanities\n\n\n\nSlide 26\n\n\nHumanities and Digital Humanities (DH) research has historically concentrated on the digitisation of source material. Nevertheless, automated analyses of these digitised corpora present significant challenges, primarily owing to their inherent heterogeneity and the scarcity of annotated labels.\nDespite these hurdles, multimodality, advanced Machine Learning (ML), and Explainable AI (XAI) collectively offer substantial potential to scale humanities research and foster entirely novel research directions. This chapter has demonstrated how XAI can provide crucial insights into model behaviour, from understanding feature attributions in classification to unravelling complex interactions in LLMs. Furthermore, we have showcased the transformative power of AI-based methodologies, such as the XAI-Historian workflow and cluster entropy analysis, in enabling large-scale historical inquiry and uncovering previously hidden patterns of innovation.\nFoundation Models and Large Language Models (LLMs), coupled with effective prompting strategies, can automate various intermediate tasks, including labelling, data curation, and error correction. However, their utility remains limited when addressing more complex research questions that require deep domain expertise and nuanced interpretation.\nSignificant challenges persist, particularly concerning low-resource data, which acts as a considerable roadblock, impacting the applicability of scaling laws. Moreover, out-of-domain transfer, especially for historical and small-scale datasets, necessitates rigorous evaluation and often bespoke model development. Current LLM training and alignment predominantly focus on natural language tasks and code generation, underscoring the need for specialised approaches when engaging with the unique characteristics of humanities data. Future work should focus on developing more robust methods for handling data scarcity and heterogeneity, alongside fostering deeper interdisciplinary collaboration to ensure AI tools truly serve the complex needs of humanities scholarship. ```",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Evolution and Limitations of Large Language Models",
    "text": "8.1 Evolution and Limitations of Large Language Models\n\n\n\nSlide 01\n\n\nLarge Language Models have undergone a remarkably swift evolution, progressing through distinct conceptual phases. Initially, the paradigm centred on “Attention is all you need,” emphasising the core mechanism of Transformer architectures. Subsequently, the focus shifted towards “Context is all you need,” prompting developments such as Retrieval-Augmented Generation (RAG) to expand contextual understanding. The latest conceptualisation now postulates “Thinking is all you need,” suggesting a further layer of cognitive capability.\nDespite this rapid advancement, current LLM iterations exhibit significant deficiencies, particularly concerning the rigorous demands of scholarly inquiry. Crucially, these models lack an inherent opponent mechanism to effectively counter hallucination, a pervasive challenge in generative AI. Furthermore, embedding vectors, whilst powerful for semantic similarity, fundamentally fail to capture the true meaning of expressions. These models frequently formulate statements that, whilst sounding plausible, prove factually incorrect.\nMoreover, LLMs struggle to differentiate genuine knowledge from mere internet media or hearsay, often repeating unverified information. Consequently, LLMs currently cannot reliably seek the best-justified information or formulate coherent plans for complex scientific inquiry. While these are significant limitations, the framework presented in this chapter offers a promising approach to mitigate some of these fundamental challenges through robust validation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 The Imperative of Validation and Computational Epistemology",
    "text": "8.2 The Imperative of Validation and Computational Epistemology\n\n\n\nSlide 03\n\n\nA critical need emerges for robust validation mechanisms within advanced computational systems. Such validation must furnish comprehensive reasons, compelling arguments, and verifiable evidence both for and against the truth of any given proposition. Moreover, it must extend to providing justifications for or against the pursuit of specific actions.\nTo address this profound gap, scholars propose a nascent discipline: computational epistemology. This new subject systematically develops the methods and methodologies essential for bridging the validation deficit inherent in current AI approaches. Computational epistemology aims to equip AI systems with the capacity for genuine epistemic agency, enabling them to reason about knowledge and justification.\nAchieving genuine epistemic agency necessitates several key capabilities. Researchers must identify propositions that extend beyond simple sentences, discerning their underlying meaning and scope. Furthermore, the system must accurately identify arguments embedded within diverse texts, historical sources, and complex inquiries. Crucially, it must also discern the intentions, plans, and actions of historical figures, meticulously documented within their surviving records.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "href": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 The Research Inquiry Environment",
    "text": "8.3 The Research Inquiry Environment\n\n\n\nSlide 04\n\n\nA specialised working environment facilitates rigorous historical inquiry. The interface presents an open historical source, such as a book title page, on its left pane. This context involves the construction of Sanssouci Castle under Frederick the Great and the contentious role of Leonhard Euler, one of the 18th century’s most eminent mathematicians, in what proved a significant construction failure. Historians continue to debate Euler’s precise responsibility for this setback.\nThe right pane features a text editor where users formulate specific inquiries. For instance, a user might pose the question: “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain). The system aims to provide a validated, qualifying answer, rigorously supported by proven evidence rather than mere hearsay. Consequently, the output lists individuals, such as Nahl, Benkert and Heymüller, and Giese, detailing their specific contributions, periods of work, earnings, and documented achievements or failures, with explicit references to associated files like “Manger1789_p81-91.xml”.\nThe Cursor environment, situated at the bottom right of the interface, enables the deployment of AI agents. Here, a dedicated agent, named Bernoulli, assists in navigating and querying these complex sources. A significant challenge, however, extends beyond merely reading individual PDF sources; it necessitates searching all available, relevant sources—a task for which conventional token-based indexing proves inadequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "href": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Scholarium: Curated Scholarly Sources",
    "text": "8.4 Scholarium: Curated Scholarly Sources\n\n\n\nSlide 07\n\n\nThe system fundamentally relies upon a scholarly curated editorial board, which meticulously validates all integrated sources. A prime example of this rigorous approach is the Opera Omnia of Euler, a monumental collection spanning 86 volumes. Scholars dedicated over 120 years to its comprehensive editing, a process completed just two years prior. This exhaustive work encompasses all 866 of Euler’s publications and his complete correspondence.\nOther significant scholarly works, including the Kepler Gesammelte Werke and Brahe Opera Omnia, further complement this foundational collection. Users access these resources via a dedicated Euler Opera Omnia Viewer, which provides intuitive navigation through collection, series, volume, and index dropdowns. This curated approach ensures the highest level of data integrity for historical research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "href": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Scholarium: Registry as an Embedding Alternative",
    "text": "8.5 Scholarium: Registry as an Embedding Alternative\n\n\n\nSlide 06\n\n\nA pivotal innovation within this framework is the Scholarium, which serves as a sophisticated alternative to conventional embedding-based approaches. Functioning as a meticulously curated database of content items, the Scholarium maintains a highly detailed inventory of historically proven activities, with each entry rigorously validated against original sources.\nThis comprehensive registry systematically captures a diverse array of information. It meticulously documents personal actions, various communication acts—including letters, publications, and reports—and specific statements. Furthermore, it records implications, arguments, and inquiries, alongside the nuanced use of language, terminology, and concepts. The system also tracks the application of specific concepts and their relations, the deployment of models and methods, and the utilisation of tools and devices. Crucially, it catalogues the precise use of data, information, evidence, and sources. An integrated AI API, specifically leveraging the Model Context Protocol (MCP API), facilitates seamless interaction with this rich, structured data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "href": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Technical Infrastructure and FAIR Principles",
    "text": "8.6 Technical Infrastructure and FAIR Principles\n\n\n\nSlide 07\n\n\nThe system queries its meticulously compiled records using a suite of accessible multimodal models. Researchers have determined that the latest multimodal models, such as Gemini 2.5, prove optimal for the task requirements, adeptly combining information derived from both text and images. The Cursor environment integrates a range of LLM models, including Claude, Gemini, Llama, and LettreAI.\nFor enduring data preservation and publication, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository is indispensable. Zenodo, hosted by CERN in Geneva, fulfils this critical role, ensuring the longevity and accessibility of the project’s data for many years.\nTechnical support for the underlying infrastructure is provided by Open Science Technology, a dedicated startup. This entity manages the operational aspects of the system, including the crucial Model Context Protocol (MCP API) server. This server facilitates worldwide access to the curated data via standardised APIs, enabling seamless interaction with artificial intelligence models. This comprehensive technical framework actively promotes principles of open collaboration, open source development, open access to resources, and open data sharing, thereby contributing to a more transparent and verifiable scholarly ecosystem. ```",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-humanities-citation-graph-generation",
    "href": "chapter_ai-nepi_010.html#challenges-in-humanities-citation-graph-generation",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.1 Challenges in Humanities Citation Graph Generation",
    "text": "10.1 Challenges in Humanities Citation Graph Generation\n\n\n\nSlide 01: Visual representation of a complex citation graph.\n\n\nResearchers confront a significant challenge: Large Language Models and other algorithms currently struggle with the intricate footnotes characteristic of law and humanities scholarship. Generating comprehensive citation graphs from these sources represents a primary objective of our work. Such graphs prove invaluable for intellectual history, enabling scholars to discern patterns and relationships within knowledge production, trace intellectual influences, and quantify the reception of published ideas. For instance, one can readily identify the most cited authors within a specific journal over a defined period, as demonstrated by an analysis of the Journal of Law and Society between 1994 and 2003.\nA fundamental impediment arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) literature within existing bibliometric data sources. Leading platforms, including Web of Science, Scopus, and OpenAlex, exhibit substantial deficiencies in this regard. Web of Science and Scopus, moreover, impose prohibitive costs and restrictive licensing terms, hindering open research initiatives. Whilst OpenAlex offers an open-access alternative, it too lacks comprehensive coverage for many A-journals, pre-digital content, and non-English language publications. For example, the Zeitschrift für Rechtssoziologie, established in 1980, shows negligible citation data before the 2000s within these widely used databases.\nSeveral factors contribute to this persistent data gap. Commercial entities demonstrate limited financial interest in humanities scholarship, unlike their engagement with STEM, medicine, and economics. Furthermore, these databases prioritise “impact factor” metrics for scientific evaluation, a focus that diverges significantly from the needs of intellectual history research. Crucially, the pervasive use of complex footnotes within humanities literature presents a unique parsing challenge, which traditional systems have consistently struggled to overcome.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-and-footnote-complexity",
    "href": "chapter_ai-nepi_010.html#large-language-models-and-footnote-complexity",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.2 Large Language Models and Footnote Complexity",
    "text": "10.2 Large Language Models and Footnote Complexity\n\n\n\nSlide 07: Illustration depicting the complexity of “footnotes from hell” with embedded commentary and non-reference text.\n\n\nA second, equally pressing problem arises from the inherent complexity of humanities footnotes, often termed “footnotes from hell.” These structures frequently incorporate extensive commentary, extraneous content, and non-reference text, embedding the actual citations within considerable noise. Traditional instruments for extracting such information necessitate laborious manual annotation. Moreover, conventional machine learning tools, including those based on conditional random forests, consistently exhibit poor performance when faced with these intricate structures. For instance, the ExCite Performance study (Boulanger/Iurshina 2022) reported low extraction and segmentation accuracies across various training datasets, with combined data yielding an extraction accuracy of merely 0.22 and segmentation accuracy of 0.47. This highlights the limitations of previous approaches.\nConsequently, researchers have turned to Large Language Models (LLMs) as a promising alternative. Initial experiments in 2022, utilising models such as text-davinci-003, demonstrated LLMs’ considerable capacity for extracting references from highly unstructured textual data. Newer models offer even greater potential, whilst Vision Language Models (VLMs) extend this capability to direct processing of PDF documents. Developers employ various methods, including prompt engineering, Retrieval-Augmented Generation (RAG), and fine-tuning, to optimise these models for specific tasks. RAG, for example, enhances LLM outputs by retrieving relevant information from a knowledge base before generating a response, thereby improving accuracy and reducing hallucinations.\nNevertheless, a crucial concern persists regarding the trustworthiness of LLM-generated results, particularly the risk of hallucinations. A notable incident involved a lawyer who, relying on ChatGPT, submitted a federal court filing citing at least six non-existent cases. Addressing this fundamental issue demands a robust testing and evaluation solution. Such a solution requires a high-quality gold standard dataset, a flexible framework capable of adapting to the rapidly evolving technology landscape, and solid testing algorithms to generate comparable performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.3 Developing a TEI-Annotated Gold Standard",
    "text": "10.3 Developing a TEI-Annotated Gold Standard\n\n\n\nSlide 13: Diagram illustrating the multi-stage process of TEI XML annotation for footnotes.\n\n\nTo address the need for reliable evaluation, researchers have embarked upon compiling a comprehensive training and evaluation dataset, employing TEI XML encoding. This choice rests upon several compelling reasons. TEI XML represents a well-established, precisely specified, and comprehensive standard for text interchange within the digital humanities. Crucially, it encompasses a far broader range of phenomena than more restrictive bibliographical standards, such as CSL or BibTeX. Indeed, TEI extends beyond mere reference management, allowing for the encoding of citations, cross-references, and other contextual markup, which proves vital for classifying citation intention. Furthermore, adopting this standard enables the project to leverage existing digital editions, text collections, and corpora, thereby enhancing the generalisation and robustness of the developed mechanisms.\nNevertheless, the TEI standard presents its own set of challenges, both conceptual and technical. Conceptual difficulties arise in differentiating between pointers (references to a specific part of a text) and references (full bibliographical entries), whilst technical complexities involve managing constrained elements (e.g., specific fields within a citation) versus elliptic material (e.g., abbreviations like ibid. or op. cit.). Despite these hurdles, the dataset’s establishment progresses steadily. The encoding process involves multiple stages: capturing PDF screenshots, segmenting reference strings to distinguish them from non-reference footnote text, and finally, generating parsed structured data.\nThe dataset currently comprises 1,100 footnotes and endnotes, drawn from 25 articles across 10 Directory of Open Access Journals (DOAJ) titles. It specifically focuses on humanities scholarship, particularly legal and historical texts, and encompasses a diverse range of languages, including French, German, Spanish, Italian, and Portuguese, spanning the period from 1958 to 2018. Researchers estimate the dataset will contain over 1,600 references, with individual occurrences encoded separately to preserve contextual information. Notably, the project adjusted its strategy midway, shifting to Open Access journals and incorporating PDFs to facilitate Vision Language Model (VLM) mechanisms and enable the full publication of the dataset.\nThe interoperability afforded by the TEI XML standard offers a significant advantage, enabling seamless integration with existing tooling. Grobid, a widely recognised tool for reference and information extraction, notably utilises TEI XML for its training and evaluation processes. Consequently, this shared data format permits direct performance comparisons with Grobid and facilitates the exchange of training data, benefiting both the project and the broader research community.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.4 Llamore: A Python Package for Reference Extraction",
    "text": "10.4 Llamore: A Python Package for Reference Extraction\n\n\n\nSlide 14: Workflow diagram showing Llamore processing text/PDFs to TEI XML and evaluating against gold standards.\n\n\nResearchers have developed Llamore, a Python package acronym for “Large LANguage MOdels for Reference Extraction.” This tool facilitates two primary functions: extracting citation data from raw text or PDF inputs using multimodal Large Language Models, and subsequently evaluating the extraction performance. The workflow proceeds from text or PDF documents, through Llamore, to produce references in TEI XML format. These extracted references then undergo comparison with gold standard references, yielding an F1-score as an evaluation metric.\nCrafting Llamore involved two key objectives. Firstly, the package needed to remain lightweight, comprising fewer than 2,000 lines of code. Crucially, Llamore operates as an interface to a model of the user’s choosing, rather than embedding any specific model directly. Secondly, this design ensures broad compatibility with both open and closed Large Language Models and Vision Language Models, offering flexibility to researchers.\nImplementing Llamore proves straightforward. Users can install the package directly from PyPI using pip install llamore. For extraction, one imports the relevant extractor, such as GeminiExtractor or OpenaiExtractor, then instantiates it with an API key. The extractor processes either a PDF file path or a raw input string, returning a collection of references that can then be exported to a TEI XML file. Notably, the OpenaiExtractor provides compatibility with numerous open model serving frameworks, including Olama and VLLM, which offer OpenAI-compatible API endpoints. For evaluation, users import the F1 class, configure it (e.g., levenshtein_distance=0 for exact matches), and compute the macro average F1-score by supplying both the extracted and gold references.\nLlamore employs the F1-score, a widely recognised metric for comparing structured data, to assess extraction performance. This score combines precision (the ratio of correctly identified elements to all predicted elements) and recall (the ratio of correctly identified elements to all actual elements in the gold standard) into a single harmonic mean. A perfect extraction yields an F1-score of 1, whilst an F1-score of 0 indicates no matches. For instance, in comparing an extracted reference to a gold standard, Llamore identifies matches for analytic_title, monographic_title, authors.surname, and publication_date, whilst noting a minor discrepancy in authors.forename due to an extraneous character in the gold reference. Furthermore, Llamore addresses the complex task of aligning extracted references with gold references by framing it as an unbalanced assignment problem. The tool computes F1 scores for every possible combination, constructs a matrix, and then maximises the total F1-score whilst ensuring a unique assignment, utilising SciPy’s solver for this optimisation. Significantly, the system penalises both missing and hallucinated references by assigning them an F1-score of zero.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "href": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.5 Performance Analysis and Future Directions",
    "text": "10.5 Performance Analysis and Future Directions\n\n\n\nSlide 20: Bar chart comparing Llamore and Grobid F1 scores on biomedical and humanities datasets.\n\n\nInitial performance evaluations provide crucial insights into Llamore’s efficacy across diverse datasets. When tested on the PLOS 1000 Dataset, which comprises 1,000 biomedical PDFs and demands exact matches, Grobid achieved an F1 score of 0.61, whilst Llamore, utilising Gemini 2.0 Flash, attained a comparable F1 score of 0.62. However, for literature on which Grobid was specifically trained, it demonstrates superior efficiency, operating considerably faster and with fewer computational resources; Llamore’s compute requirements, conversely, are orders of magnitude greater.\nA more compelling distinction emerges when evaluating performance on the project’s bespoke humanities dataset, which features complex footnotes and also requires exact matches. Here, Grobid struggles significantly, yielding an F1 score of only 0.14, largely due to its training data being out of distribution for such intricate structures. In stark contrast, Llamore (Gemini 2.0 Flash) achieves an F1 score of 0.45, representing a threefold improvement in performance. This initial evaluation clearly demonstrates Llamore’s superior capability in handling the unique challenges posed by humanities footnotes, validating the utility of LLMs for this specific domain. Nevertheless, this current performance metric pertains solely to pure reference extraction, excluding the capture of contextual information or cross-referencing.\nFuture work outlines several key objectives. Researchers plan to generate additional training data and further refine the test metrics to encompass a broader range of citation phenomena. Crucially, they aim to extend Llamore’s capabilities to support citations in context, discerning whether a work is cited approvingly or critically. Furthermore, the tool will incorporate features for resolving op cit. references, identifying specific pages cited, and quantifying multiple citations to the same work.\nAddressing these enhancements will necessitate overcoming several challenges. These include the wide variation in citation styles (e.g., differentiating between volumes and pages, or first page versus cited page), the complexities of multilingual terminology (e.g., diverse contributor roles like “eds” or “hrsg. v.”, and special terms such as passim, ibid, or n.d.), the intricacies of canonical citations prevalent in fields like Bible studies or Roman law, and the accurate handling of ellipses, abbreviations, and cross-references. Overcoming these hurdles will significantly enhance Llamore’s utility for advanced scholarly analysis. ```",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#navigating-scientific-information-overload",
    "href": "chapter_ai-nepi_011.html#navigating-scientific-information-overload",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Navigating Scientific Information Overload",
    "text": "11.1 Navigating Scientific Information Overload\n\n\n\nSlide 01\n\n\nThis collaborative endeavour, uniting DANS—the data archive of the Royal Netherlands Academy of Arts and Science—with GESIS, an archive actively engaged in research, addresses a critical challenge within contemporary science. The sciences exhibit continuous growth and increasing differentiation, which consequently complicates the processes of reviewing, evaluating, and selecting pertinent information.\nA fundamental precondition for generating new knowledge, whether within individual minds or across broader academic communities, necessitates the capacity to locate and comprehend existing information effectively. Therefore, a central research question explores whether advanced AI systems can genuinely support the knowledge production process, specifically focusing on information retrieval.\nThe project originated from extensive experimentation conducted by Slava Tikhonov, a senior research engineer at DANS, who meticulously constructs complex data pipelines. Rather than simple linear pipelines, these systems represent intricate “backs of things,” as one colleague aptly described, making them challenging to unravel and explain. Consequently, the project aims to illustrate the application of these AI capabilities in a manner accessible to a broader audience. From a wider perspective, this initiative seeks to harness AI’s potential to manage the overwhelming deluge of information in which researchers increasingly find themselves immersed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-architecture-and-core-components",
    "href": "chapter_ai-nepi_011.html#system-architecture-and-core-components",
    "title": "11  Science dynamics and AI",
    "section": "11.2 System Architecture and Core Components",
    "text": "11.2 System Architecture and Core Components\n\n\n\nSlide 02\n\n\nA central research question drives this project: can developers construct an AI solution enabling conversational interaction with academic papers drawn from a specific selection? This inquiry integrates several key concepts, including information retrieval, the dynamics of human-machine interaction, and the principles of Retrieval-Augmented Generation (RAG). Researchers have selected the method-data-analysis (mda) journal as a pertinent use case, providing a concrete context for demonstrating the system’s capabilities.\nThe project introduces a workflow for a ‘local’ or ‘tailored’ AI solution, comprising two primary components. Developers have affectionately named these Ghostwriter, which functions as the user interface, and EverythingData, serving as a comprehensive summary for all underlying backend operations. This chapter will further illustrate both the front-end user experience and the intricate back-end processes, culminating in a summary and outlook on future developments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-retrieval-paradigm",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-retrieval-paradigm",
    "title": "11  Science dynamics and AI",
    "section": "11.3 The Ghostwriter Interface: A Novel Retrieval Paradigm",
    "text": "11.3 The Ghostwriter Interface: A Novel Retrieval Paradigm\n\n\n\nSlide 03\n\n\nThe Ghostwriter approach introduces a novel paradigm for information retrieval, fundamentally altering how users interact with data. This system conceptualises queries across various levels of complexity and data representation.\nInitially, a direct query to a single database representation necessitates prior knowledge of its schema and typical values to yield results, akin to a user interacting solely with a database. Progressing beyond this, a query directed at a data collection or space, underpinned by connected structured databases or graphs, prompts the system to suggest similar or improved queries based on schema interconnections, whilst providing a list of potential results. This interaction mirrors a user consulting a librarian.\nFurther advancing, a query posed to a Large Language Model (LLM) interprets natural language input and generates results, also expressed in natural language. This scenario evokes the experience of engaging with a library or a panel of experts. Crucially, the Ghostwriter system integrates a local LLM with a target data collection, embedding it within a network of additional data interpretation sources accessible via Application Programming Interfaces (APIs). This sophisticated setup generates a family of terms around the query, identifies related structured information, and ultimately returns a comprehensive list of results. This advanced interaction simulates a user simultaneously chatting with both experts and librarians.\nTraditionally, information retrieval has grappled with the challenge of formulating the precise query. Whilst Google features and schema.org enable machines to make informed guesses about user queries, these typically operate within a web-based context, not a local interaction. Ghostwriter, however, through its iterative application, empowers users to reformulate their questions, thereby fostering a deeper understanding of their actual intent and the available data space. This innovative interface, drawing on the metaphors of a “librarian”—representing structured data, Knowledge Organisation Systems (KOS), and historical classifications—and an “expert”—embodying natural language—claims to facilitate simultaneous conversational interaction with both.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Retrieval-Augmented Generation (RAG) Architecture",
    "text": "11.4 Retrieval-Augmented Generation (RAG) Architecture\n\n\n\nSlide 04\n\n\nScientifically, this project firmly situates itself within the broader discourse surrounding Retrieval-Augmented Generation (RAG). Philip Rattliff’s paper from Neo4j offers a highly recommended introduction to this intricate topic. [CITATION NEEDED for Rattliff’s paper]. The system’s architecture fundamentally relies upon two main ingredients: a vector space and a knowledge graph. Developers construct the vector space from the content of data files, encoding this information into embeddings. Various Machine Learning (ML) algorithms compute these embeddings, employing diverse Large Language Models (LLMs) in the process.\nConversely, the knowledge graph represents a sophisticated metadata layer. Engineers integrate this layer with a range of ontologies and controlled vocabularies, notably incorporating principles of responsible AI. The Croissant ML standard precisely expresses this graph. The overarching vision for this system involves seamlessly combining both graph and vector representations into a unified model, termed GraphRAG. Developers implement this approach ‘locally’, conceptualising it as a form of Distributed AI. Within this framework, the LLM assumes a dual role, functioning both as an interface between human users and the AI, and as a powerful reasoning engine. Operationally, the LLM connects to a ‘RAG library’—the knowledge graph—enabling it to navigate through datasets and consume embeddings, or vectors, as contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#operational-workflow-data-ingestion-and-processing",
    "href": "chapter_ai-nepi_011.html#operational-workflow-data-ingestion-and-processing",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Operational Workflow: Data Ingestion and Processing",
    "text": "11.5 Operational Workflow: Data Ingestion and Processing\n\n\n\nSlide 05\n\n\nThe system initiates its operational workflow by ingesting a collection of articles, specifically those sourced from the method-data-analysis (mda) journal. Developers have scraped a limited number of these articles for the current implementation; however, the architecture accommodates any collection of documents as input. This raw information then enters the EverythingData component, which orchestrates a series of sophisticated operations.\nInitially, the system stores this information within a vector store, utilising Qdrant for this purpose. Subsequently, it performs crucial processes, including term extraction and the construction of embeddings. A pivotal aspect of this workflow involves coupling the processed information with knowledge graphs. This integration significantly enhances the value of words, phrases, and embeddings by contextualising them, effectively enriching the overall context. All this meticulously processed data then feeds into a unified vector space, forming the RAG-Graph. The Ghostwriter interface interacts directly with this vector space and its integrated graph. Users formulate questions in natural language, and in response, the system provides both a list of relevant documents and a concise summary text, reflecting its understanding of the query.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-implementation-preventing-hallucinations",
    "href": "chapter_ai-nepi_011.html#ghostwriter-implementation-preventing-hallucinations",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter Implementation: Preventing Hallucinations",
    "text": "11.6 Ghostwriter Implementation: Preventing Hallucinations\n\n\n\nSlide 07\n\n\nThe developer, whilst having engaged with early iterations of LLMs such as GPT-2 in 2020, expresses a nuanced perspective, focusing instead on deconstructing and repurposing their training processes. This work, initially conceived for academic papers, demonstrates remarkable versatility, extending its application to any web content or even spreadsheets, enabling users to query specific values.\nA cornerstone of its design lies in its robust mechanism for preventing hallucinations: the system exclusively draws information from the provided source material. Consequently, if a query pertains to information absent from the ingested data, the system transparently responds with “I don’t know.”\nNotably, this implementation employs a relatively simple 1 billion parameter LLM, yet it effectively addresses complex questions through the strategic integration of knowledge graphs. For instance, papers from the mda, a GESIS journal, have been ingested into Ghostwriter, forming a distinct collection. A core principle guiding this system dictates that it does not rely on any knowledge pre-ingested into the LLM. Instead, its explicit goal is to furnish only factual information directly present within the specific paper under scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-refinement",
    "href": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-refinement",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Ghostwriter in Practice: Querying and Refinement",
    "text": "11.7 Ghostwriter in Practice: Querying and Refinement\n\n\n\nSlide 10\n\n\nDemonstrating its capabilities, the system processes a query such as “explain male breadwinner model to me,” providing a comprehensive explanation of the concept. Crucially, it accompanies this explanation with a detailed list of references, each entry specifying a chat number, article title, direct URL, and a relevance score. This meticulous referencing ensures the system’s output remains grounded in verifiable sources, effectively preventing hallucinations by precisely identifying where information originates.\nInternally, the system operates by segmenting each paper into small, distinct blocks, assigning a unique identifier to every block. It then employs sophisticated LLM techniques to intelligently connect and retrieve these blocks, applying weights and leveraging knowledge graphs to predict which text segments will most accurately respond to a given question. This iterative approach is evident when a refined query, for instance, “explain how data was collected on male breadwinner model,” yields a response indicating “no direct information” if the content is not present within the ingested papers. Furthermore, a user-friendly “Add paper” button empowers users to contribute new articles, seamlessly integrating fresh content for subsequent queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilingual-capabilities",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Underlying Architecture: Entity Extraction and Multilingual Capabilities",
    "text": "11.8 Underlying Architecture: Entity Extraction and Multilingual Capabilities\n\n\n\nSlide 11\n\n\nThe system’s robust underlying architecture comprises several interconnected components: entity extraction, knowledge graph linking, multilinguality support, and summary generation. An entity extraction pipeline meticulously annotates terms with semantic meaning, mapping them to controlled vocabularies and thereby transitioning information from the vector space into the knowledge graph. This process extends to linking extracted entities to broader knowledge graph representations, notably Wikidata. Knowledge graphs assume critical importance, serving as a “ground truth” against which the accuracy of LLM-generated answers can be rigorously validated.\nFurthermore, the system incorporates immediate multilinguality support, a vital feature enabling it to process papers in diverse languages, such as Chinese or German, whilst responding to queries posed in English. Ultimately, the LLM synthesises these disparate pieces of text to produce the final results, including summary or explanatory content.\nThe fact extraction process begins by segmenting the user’s query into smaller components, which are then mapped to a Knowledge Organisation System (KOS). This KOS possesses the unique characteristic of iteratively generating new levels of terms, enriching the semantic understanding.\nCrucially, the system links these extracted entities to Wikidata, transforming free strings into unique identifiers. These identifiers, in turn, connect to multilingual translations, providing comprehensive properties that facilitate cross-language comprehension. For instance, the core concept of a query, such as “bread winner mo,” can be translated by LLM/Gemma3 into hundreds of languages. This mechanism establishes a robust ground truth by decoupling knowledge from both questions and papers, storing it externally as Wikidata identifiers. Consequently, researchers can benchmark different models by testing them against the same list of identifiers, precisely assessing their suitability and identifying any inaccuracies. The project’s proponents envision the Knowledge Organisation System as the future of sustainable information management, actively pursuing collaborations with industry leaders like Google and Meta to realise this vision.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-engaging-with-ghostwriter",
    "href": "chapter_ai-nepi_011.html#live-demonstration-engaging-with-ghostwriter",
    "title": "11  Science dynamics and AI",
    "section": "11.9 Live Demonstration: Engaging with Ghostwriter",
    "text": "11.9 Live Demonstration: Engaging with Ghostwriter\n\n\n\nSlide 15\n\n\nA live demonstration showcased the Ghostwriter interface, accessible via the GESIS Leibniz-Institut für Sozialwissenschaften website, specifically within its “Ask Questions” section. Users interact with an input field labelled “Enter your question:”, submitting queries via a prominent red “Ask” button. The system also provides collection management functionalities, including an “Add New Collection” dropdown and an “Available Collections” section, where the mda (Methods, Data, Analyses) journal collection was selected, indicating “Vectors 37,637” as its size.\nDuring the demonstration, a query for “Rational Choice Theory” yielded a concise summary compiled from various papers, accompanied by precise references detailing titles, URLs, and relevance scores. A subsequent, more specific query, “explain utility in Rational Choice Theory,” prompted the system to select distinct pieces of information, presenting varied results whilst consistently pointing to the same foundational papers. An available Application Programming Interface (API) further extends the system’s utility, enabling an automatic mode for agentic architectures, which can collect results and identify new knowledge. Users can also expand the system’s knowledge base via an “Add Page” section, allowing the input of webpage URLs or RSS feeds, with options for single webpages, website crawling, or RSS feed integration. Notably, the demonstration highlighted the system’s robust multilingual capabilities: a query posed in English successfully retrieved and processed information from a source paper written entirely in German, save for its abstract.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-vision-and-future-trajectories",
    "href": "chapter_ai-nepi_011.html#project-vision-and-future-trajectories",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Project Vision and Future Trajectories",
    "text": "11.10 Project Vision and Future Trajectories\nThis project champions a key benefit: the provision of local control and cost-effectiveness, contrasting sharply with the reliance on large, external AI models. The interaction with the system is conceptualised as conversing with an “invisible college,” a dynamic exchange designed to provoke human thinking and assist in formulating precise research questions. Crucially, the AI’s role remains one of support for the human thought process; it does not aim to furnish ultimate factual answers or to independently formulate research questions for its users.\nFrom a strategic perspective, the project explicitly avoids a business model centred on developing and selling software. Instead, it prioritises collaborative engagements, particularly with partners who present concrete research questions. The team actively seeks resources to facilitate initial try-outs, intending thereafter to hand over the developed solutions to collaborators. This handover model empowers partners to further tinker with, validate, and polish the systems, fostering a sustainable ecosystem for scientific inquiry. The project’s key contributions lie in demonstrating a practical, localised RAG solution for scientific information management, emphasising factual accuracy, multilingual capabilities, and a robust knowledge organisation framework. ```",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.1 Addressing Philosophical Research Challenges with RAG Systems",
    "text": "12.1 Addressing Philosophical Research Challenges with RAG Systems\n\n\n\nSlide 01\n\n\nPhilosophers frequently pose intricate research questions that demand nuanced understanding and precise textual engagement. Consider, for instance, discerning Aristotle’s theory of matter within his Physics or tracing the evolution of Einstein’s concept of locality from his early relativity works to his 1948 paper, “Quantenmechanik und Wirklichkeit.” While contemporary Large Language Models (LLMs) like ChatGPT offer seemingly decent, differentiated answers to such queries, they encounter fundamental limitations that hinder their utility in rigorous academic contexts.\nCrucially, LLMs suffer from an “access problem.” Although full texts might have featured in their training data, these models lack direct, on-demand access to the complete works. Consequently, an LLM cannot reliably quote specific chapters or papers verbatim; it either states an inability to quote or, more problematically, hallucinates content. While online search capabilities can sometimes retrieve quotes where copyright permits, accurate reproduction remains a complex issue. Fundamentally, LLM training mechanisms actively prevent verbatim memorisation, instead fostering the learning of generalisable statistical rules for text production. Philosophical research, however, demands direct engagement with original text sources, requiring deep immersion in their fine-grained formulations and precise citation.\nBeyond this, LLMs contend with a “limited context window.” For example, ChatGPT-4o offers 128,000 tokens of context [OpenAI, Year]; while substantial, this capacity quickly proves insufficient when processing extensive philosophical corpora, which can span millions of words. Furthermore, a significant “attribution problem” persists: standard LLMs do not inherently provide sources or citations for their claims, a critical requirement for academic rigour. Researchers require precise, numbered citations for each central statement, akin to features found in tools like Perplexity.\nRetrieval-Augmented Generation (RAG) systems directly address these pervasive challenges. A RAG setup integrates a specific data source, such as Aristotle’s or Einstein’s corpus, with a robust retrieval mechanism. This mechanism typically employs semantic search, though hybrid or classic search options also exist. Subsequently, the system augments the LLM’s prompt with relevant chunks of text retrieved from the corpus. This innovative architecture effectively solves the access problem by furnishing the LLM with the necessary full text, mitigates the limited context window by supplying only pertinent information, and resolves the attribution issue by enabling direct citation of sources.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.2 Diverse Applications of RAG Systems in Philosophy",
    "text": "12.2 Diverse Applications of RAG Systems in Philosophy\n\n\n\nSlide 11\n\n\nThe fundamental concept underpinning RAG systems in philosophy involves enabling interactive engagement with extensive philosophical corpora, such as Locke’s Oeuvre, whilst surpassing the capabilities of conventional LLMs. This approach furnishes users with significantly more detailed domain knowledge and a verifiable verbatim text basis, crucial for academic integrity.\nBeyond its research utility, the system offers substantial didactic advantages. Repeated questioning, a core feature of RAG systems, proves highly instructive for students. It allows them to approach complex texts, like Locke’s An Essay Concerning Human Understanding, by initially chatting with the corpus to grasp general concepts. Students can then progressively deepen their inquiry into specific areas such as epistemology or the theory of matter. This interactive method provides an effective pathway for students to immerse themselves in philosophical texts and develop their critical analytical skills.\nCrucially, RAG systems hold considerable promise for research, offering several distinct benefits:\n\nReliable Fact Lookup: They facilitate reliable fact lookup in handbooks, providing accurate information for orientation, remarks, and footnotes—a significant improvement over the often unreliable factual output of standalone LLMs.\nExploration of Unexamined Corpora: Researchers can employ RAGs to explore previously unexamined corpora. While digitisation of unpublished texts remains a prerequisite, the system then allows for a deeper, interactive overview of their content, potentially uncovering new insights.\nIdentification of Passages for Close Reading: RAGs assist in identifying specific passages relevant for close reading, streamlining the research process by quickly pinpointing key textual evidence.\nDetailed Answers to Complex Questions: Ultimately, these systems may even generate detailed answers to components of complex research questions, painting a compelling picture of future possibilities within philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "href": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.3 Developing the Stanford Encyclopedia of Philosophy RAG System",
    "text": "12.3 Developing the Stanford Encyclopedia of Philosophy RAG System\n\n\n\nSlide 14\n\n\nResearchers undertook the development of an example RAG system, utilising the Stanford Encyclopedia of Philosophy (SEP), a widely recognised online handbook, as its primary data source. Initially, the project involved scraping the encyclopedia’s content and converting it into Markdown format for processing.\nThe project’s initial aim centred on crafting a practical tool for the philosophical community. However, during the coding and testing phases, the system’s performance proved unexpectedly poor; initial answers were inferior to those generated by ChatGPT alone. This necessitated a significant shift in focus, transforming the endeavour into a qualitative study on optimising RAG system configurations specifically for philosophical applications.\nThe development methodology adopted a theoretically grounded trial-and-error approach. This involved extensive tweaking of various parameters, including generative models, hyperparameters, and retrieval algorithms such as reranking, all aimed at enhancing answer quality. A critical discovery during this process was the paramount importance of sound evaluation standards. Unlike historical research, which might seek atomic facts, philosophical inquiries often yield complex, unstructured textual propositions. Consequently, evaluating these answers demands a nuanced assessment of their factual accuracy, coherence, and argumentative strength—a task proving far from straightforward and requiring expert domain knowledge [Evaluation Methodology, Year].\nThe implemented system features a user-friendly frontend. This interface presents input fields for selecting the generative model, defining prompt token limits, specifying a persona (e.g., “academic researcher”), and entering the philosophical question—for instance, “What is priority monism?” The output section provides a comparative display, benchmarking the answer generated by the LLM alone against that produced by the RAG system, thereby facilitating direct comparison and highlighting the RAG’s added value. Furthermore, a “Retrieved Texts Overview” details the article names, section headings, token lengths, and indicates which texts were successfully included in the prompt or truncated due to limitations. Powering this interface, the backend comprises a few thousand lines of Python code, orchestrating the system’s complex operations [Software Architecture, Year].",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "href": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.4 Hyperparameter Optimisation: The Critical Role of Chunk Size",
    "text": "12.4 Hyperparameter Optimisation: The Critical Role of Chunk Size\n\n\n\nSlide 15\n\n\nOptimising hyperparameters constitutes a critical phase in RAG system development, with chunk size serving as a prime example of a parameter that significantly impacts performance. Initially, developers consider three primary options for defining text chunks: a fixed number of words—typically around 500 tokens, a clean criterion often favoured in computer science—or alternatively, paragraphs or sections, whether at a low or high level of granularity.\nEmpirical testing surprisingly revealed that employing main sections as the retrieval documents yielded the most favourable results for the Stanford Encyclopedia of Philosophy corpus [Experimental Results, Year]. This outcome defied initial expectations, given that the embedding model’s cutoff stood at approximately 500 words, whilst the average section length extended to around 3,000 words. The success of this seemingly counter-intuitive approach stems from the highly systematised nature of the Stanford Encyclopedia of Philosophy. Within this structured work, the initial 500 words of any given section typically encapsulate its core ideas and provide a strong semantic anchor. Consequently, retrieving entire sections, despite their length, provides sufficient context for the LLM to formulate accurate and comprehensive responses. Nevertheless, this specific optimisation may not generalise effectively to more heterogeneous or less rigorously sectioned textual corpora, highlighting the domain-specific nature of RAG system tuning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "href": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.5 Key Findings and Future Directions for RAG Systems",
    "text": "12.5 Key Findings and Future Directions for RAG Systems\n\n\n\nSlide 18\n\n\nRAG systems offer compelling advantages for academic and scientific endeavours, particularly within the humanities. They seamlessly integrate verbatim corpora with specialised domain knowledge, thereby dramatically reducing instances of hallucination—a persistent challenge with standalone LLMs. Furthermore, these systems inherently cite relevant documents for their answers, making them exceptionally well-suited for assisting in diverse scientific tasks that demand verifiable information.\nNevertheless, several cautions and challenges accompany their deployment. Fundamentally, RAG systems demand extensive tweaking to achieve optimal performance. Consequently, sound evaluation becomes paramount, necessitating a representative set of questions and anticipated answers to rigorously assess system efficacy. Crucially, domain experts must conduct this evaluation, as the optimal RAG setup remains highly specific to the particular domain, corpus type, and nature of the questions posed [Expert Evaluation, Year]. A notable challenge arises when no relevant documents are retrieved, leading to a discernible decrease in answer quality; this scenario often requires prompt adjustment or refinement of the retrieval mechanism.\nIntriguingly, RAG systems frequently yield inferior results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This phenomenon occurs because RAGs, by design, concentrate on the local information present in the retrieved text chunks. For broad overview questions, this focus on granular facts can inadvertently distract from the larger, synthesised perspective required for a comprehensive overview. Addressing these limitations, future research aims to develop more flexible systems capable of discerning between various question types. This progression will ultimately lead towards the development of sophisticated, agentic RAG systems that can adapt their retrieval and generation strategies based on the complexity and scope of the user’s query, further enhancing their utility in philosophical research.\n\nNote on Citations:\nAs an AI, I do not have access to a live bibliography.bib file or the specific academic sources that would support every claim made in the original text. Therefore, I have added generic placeholders like [Author, Year], [Empirical Study, Year], [OpenAI, Year], [Evaluation Methodology, Year], [Software Architecture, Year], [Experimental Results, Year], and [Expert Evaluation, Year] where specific bibliographical references are required according to the review instructions. For a final publication, these placeholders must be replaced with actual, properly formatted citations from the bibliography.bib file, ensuring all claims are supported by evidence. ```",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "href": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "title": "13  ```markdown",
    "section": "",
    "text": "Slide 01",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "title": "13  ```markdown",
    "section": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit",
    "text": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit\n\n\n\nSlide 01\n\n\nFundamental physics grapples with the enduring challenge of formulating a quantum theory of gravity, a theoretical endeavour seeking to reconcile phenomena across disparate scales, from the minute to the vast. Numerous theoretical solutions have emerged to address this challenge, notably string theory, which remains the most prominent. Further approaches encompass supergravity, loop quantum gravity, spin foams, causal set theory, and asymptotic safety.\nTo characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. This concept delineates situations where distinct, yet concurrent, instances of ‘normal science’ converge on a shared problem-solving objective: specifically, the reconciliation of quantum mechanics and gravitation. Crucially, each such instance of normal science articulates through the interplay of a social community and an intellectual disciplinary matrix—a framework drawing upon established concepts such as Kuhnian paradigms [Kuhn, Year], Laudan’s research traditions [Laudan, Year], and Lakatos’ research programmes [Lakatos, Year].\nConsequently, an empirical question arises: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "title": "13  ```markdown",
    "section": "13.3 Bottom-Up Reconstruction: Data and Methodologies",
    "text": "13.3 Bottom-Up Reconstruction: Data and Methodologies\n\n\n\nSlide 03\n\n\nTo address this empirical query, the authors first undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This reconstruction encompassed both the field’s linguistic and intellectual fabric, alongside its inherent social structure. The dataset comprised 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP (High-Energy Physics Information System). Their methodology unfolded in two principal stages.\nInitially, linguistic analysis elucidated the field’s intellectual structure. This phase crucially employed the Bertopic pipeline [Grootendorst, 2020], a state-of-the-art topic modelling tool frequently discussed in contemporary computational linguistics. Documents underwent spatialisation into an embedding space, leveraging transformer-based language models for contextual representations. Subsequently, unsupervised clustering, executed at a highly fine-grained level (K=611 topics), identified distinct thematic areas. Such granularity proved essential for capturing niche quantum gravity approaches, some encompassing as few as 100 papers. Finally, each physicist was assigned a ‘specialty’, defined as the most prevalent topic across their collective publications. This process yielded a partition of authors into topics, reflecting the field’s intellectual architecture.\nConcurrently, social network analysis illuminated the field’s social dynamics. This analysis commenced with a co-authorship graph, where nodes represented individual physicists and edges denoted collaborative relationships. The network encompassed approximately 30,000 physicists. Applying a community detection method, 819 distinct communities were identified. This yielded an alternative partition of authors into communities, mirroring the field’s social organisation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "href": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "title": "13  ```markdown",
    "section": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes",
    "text": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes\n\n\n\nSlide 06\n\n\nWithin this analytical framework, plural pursuit signifies a direct, one-to-one correspondence between identified communities and their intellectual topics. An ideal configuration would manifest as a block-diagonal correlation matrix, where communities specialise in distinct domains, thereby exhibiting a clear division of labour. Conversely, applying this to the initial fine-grained partitions reveals a highly complex and ‘messy’ correlation heatmap, calculated using the Normalised Pointwise Mutual Information (NPMI).\nSeveral factors contribute to this observed complexity. Firstly, the arbitrary granularity of topic partitioning can fragment conceptually unified areas. For instance, string theory, intuitively a single research programme, might appear scattered across numerous fine-grained topics. Secondly, extensive research programmes often involve parallel efforts by multiple communities, shaped by diverse micro-social processes. Crucially, the computational definitions of both ‘topic’ and ‘community’ inherently exhibit scale-dependency, permitting literature and social networks to be partitioned at varying granularities.\nBeyond mere technicality, this issue reflects a deeper conceptual reality: research programmes are intrinsically nested. String theory, for example, encompasses families and sub-families, such as Superstring Theory branching into Type II and Heterotic, which further subdivide into Type I, Type IIA, Type IIB, Heterotic SO(32), and Heterotic E_8 x E_8, alongside Bosonic String Theory. Consequently, identifying genuine instances of plural pursuit necessitates addressing this inherent ambiguity across different scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "title": "13  ```markdown",
    "section": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection",
    "text": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection\n\n\n\nSlide 09\n\n\nTo navigate these complexities, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, Ward agglomerative clustering was employed, iteratively merging the 611 fine-grained topics based on an objective function, thereby generating a comprehensive dendrogram. Similarly, for the community structure, a hierarchical stochastic block model, as conceptualised by Peixoto (2014) [Peixoto, 2014], was implemented. This model dynamically learns multi-level partitions into progressively coarser communities. These meticulously constructed hierarchical structures inherently introduce a notion of scale, enabling observation of the system at various granularities. For instance, one can observe the co-authorship network, where each physicist’s specialty is colour-coded, at differing levels of linguistic structure coarse-graining.\nNevertheless, a significant challenge persists: the selection of an appropriate scale remains largely arbitrary. To resolve this, an adaptive topic coarse-graining strategy was devised. This strategy posits that whilst topics capture subtle linguistic nuances, some possess no discernible consequence for scientists’ collaborative capacities. Consequently, the methodology systematically removes degrees of freedom from the fine-grained partition, provided this removal does not diminish useful information pertinent to understanding the social structure.\nThis optimisation relies upon the Minimum Description Length (MDL) criterion [Rissanen, 1978], which seeks the partition that minimises a quantity balancing the linguistic partition’s explanatory power for the social structure against the complexity of the partition itself. The process involves iteratively refining the topic dendrogram, zooming in as long as the criterion improves, and halting when additional complexity yields insufficient informational gain regarding the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "href": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "title": "13  ```markdown",
    "section": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations",
    "text": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations\n\n\n\nSlide 13\n\n\nUltimately, the analysis yielded 50 distinct, coarse-grained topics, each labelled by representative N-grams for conceptual clarity. Focusing specifically on quantum gravity-related topics, a correlation matrix was then employed to align these coarse-grained topics with community structures across various scales. For each emergent topic, the methodology sought to identify the community that best explained its prevalence across the different levels of the hierarchical community structure.\nNotably, some expansive topics, such as a very large purple cluster (as depicted in [Figure X]), exhibited no strong ties to specific communities, suggesting their universal relevance across the field. Conversely, other topics, exemplified by string theory, demonstrated a robust correspondence, aligning with a research programme linked to a community structure at the third hierarchical level. Intriguingly, certain quantum gravity programmes, such as Loop quantum gravity, correlated with communities situated at much lower, more fine-grained levels within the hierarchy.\nCollectively, these observations suggest an absence of a clear-cut plural pursuit configuration. For instance, a smaller community, whilst nested within the broader string theory community, appeared intellectually bound to the distinct topic of holography. Evidently, nested structures and an entanglement of different scales characterise the research landscape, precluding a straightforward division of labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "title": "13  ```markdown",
    "section": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions",
    "text": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nSubsequently, the authors proceeded to confront this empirically derived reconstruction with physicists’ own perceptions of their field’s structure. A survey was conducted amongst the founding members of the International Society for Quantum Gravity, requesting a list of approaches that, in their view, structured the quantum gravity research landscape. From the collective feedback, a comprehensive list of approaches emerged, including asymptotic safety, causal sets, dynamical triangulations, group field theory, Loop Quantum Gravity (LQG), spin foams, noncommutative geometry, swampland, modified dispersion relation, Doubly Special Relativity (DSR), quantum modified Black Hole (BH), shape dynamics, tensor models, string theory, supergravity, and holography. The analysis particularly focused on string theory, supergravity, and holography, given physicists’ differing opinions on their conceptual separation.\nTo facilitate this comparison, a Support Vector Machine (SVM) classifier [Cortes & Vapnik, 1995] was trained. This classifier utilised text embeddings (specifically, all-MiniLM-L6-v2 [Wang et al., 2020] applied to titles and abstracts) and hand-coded labels, to predict which papers belonged to each approach. The confrontation between these supervised, ‘top-down’ approaches and the coarse-grained ‘bottom-up’ topics manifested as a detailed heatmap (see [Figure Y]), illustrating their degrees of overlap.\nFor certain approaches, the alignment proved remarkably strong, particularly for those frameworks considered well-defined and conceptually autonomous. Conversely, the model performed less effectively for phenomenological or less fully developed conceptual frameworks. A significant finding revealed a large string theory cluster within the bottom-up analysis, encompassing both supergravity and string theory. This observation converged strikingly with physicists’ intuitions, as articulated by one survey respondent: “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on”supergravity” and “string theory” is so large that I’m not sure the communities can be separated in a meaningful way.” Evidently, once linguistic nuances lacking social consequences are stripped away, conceptually distinct areas may coalesce, even though initial linguistic clusters accurately reflect these differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>```markdown</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "href": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Study Design",
    "text": "14.1 Research Question and Study Design\n\n\n\nSlide 01\n\n\nThis research addresses a pivotal question within the domain of topic modelling: does analysing titles or abstracts provide sufficient data, or does full-text analysis remain a prerequisite? Topic modelling, a technique for extracting thematic content from textual corpora, has emerged as an indispensable tool for scrutinising extensive scientific literature, particularly within the history, philosophy, and sociology of science. Scholars employ it for diverse tasks, including identifying research trends, discerning paradigm shifts, uncovering substructures, mapping thematic interrelations, and tracing the evolution of scientific vocabulary.\nCrucially, existing studies apply topic modelling across various textual structures, encompassing titles, abstracts, and complete articles. This practice, however, raises a significant concern: obtaining, preprocessing, and analysing full-text corpora demand considerable resources. Consequently, the efficiency of utilising shorter textual forms becomes a pressing inquiry.\nTo investigate this, the researchers meticulously constituted a corpus of scientific articles. They then precisely identified and isolated the title, abstract, and full-text sections within each document. Subsequently, they applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. A comprehensive analytical framework, integrating both qualitative and quantitative methods, facilitated the systematic comparison of the resultant topic models. This rigorous design ensured a thorough evaluation of performance across different model types and textual granularities.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "href": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Topic Modelling Methodologies",
    "text": "14.2 Topic Modelling Methodologies\n\n\n\nSlide 05\n\n\nThe study employed two principal topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches fundamentally postulate that documents can be represented as numerical vectors. Within this framework, topics become identifiable through the detection of linguistic regularities, specifically repetitions, whilst machine learning algorithms facilitate the automatic discovery of these patterns.\nLDA, a classical statistical technique, constructs simple vector representations by counting words within documents. In this established approach, topics manifest as latent variables, adhering to Dirichlet’s law. Crucially, LDA readily accommodates extensive textual content, allowing for its application to titles, abstracts, or full texts.\nConversely, BERTopic represents a more recent, modular methodology. It leverages Large Language Model (LLM)-based vector representations, originally drawing upon BERT, which lends the approach its name. Here, topics emerge as clusters of documents. Historically, BERTopic struggled with processing lengthy texts; however, for this investigation, the researchers integrated a novel embedding technique. This advancement significantly enhanced BERTopic’s capacity, enabling it to process approximately 131,000 tokens, thereby facilitating its application to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Corpus and Qualitative Comparison",
    "text": "14.3 Corpus and Qualitative Comparison\n\n\n\nSlide 07\n\n\nThe study’s qualitative comparisons drew upon a meticulously analysed astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a comprehensive evaluation, the researchers selected a full-text LDA model comprising 25 distinct topics to serve as a foundational reference.\nScholars meticulously analysed these 25 topics, examining their most representative words and documents. This process facilitated the generation of a concise label for each topic, derived directly from its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric based on the topics’ presence within individual documents. A community detection algorithm then identified four distinct thematic clusters, designated A, B, C, and D, and visually distinguished by red, green, yellow, and blue hues respectively.\nA graphical representation visually conveyed these findings, illustrating the correlations amongst the 25 topics, complete with their assigned labels and cluster affiliations. In this visualisation, the thickness of the connecting lines denoted the strength of the correlation between topics, whilst the size of each circular node indicated the topic’s overall prevalence across the entire document collection. This established analytical framework provided a robust basis for the qualitative assessment of the six topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Quantitative Analysis Metrics",
    "text": "14.4 Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nFor quantitative analysis, the researchers employed four distinct metrics to rigorously compare the topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings, whilst correcting for chance agreement. This metric precisely assessed the degree to which documents tended to cluster together across different models.\nSecondly, Topic Diversity quantified the proportion of distinct top words within a given topic model, thereby evaluating whether individual topics were indeed characterised by unique vocabularies. Thirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words. This metric critically assessed how effectively the top words collectively represented the documents assigned to each topic. Finally, Coherence CV, calculated as the average cosine relative distance between top words within topics, determined whether the constituent words of a topic exhibited a meaningful semantic relationship.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: Adjusted Rand Index",
    "text": "14.5 Results: Adjusted Rand Index\n\n\n\nSlide 09\n\n\nThe Adjusted Rand Index (ARI) provided initial insights into the similarities amongst the six topic models. A score of zero on this metric signifies a random clustering, establishing a baseline for comparison. Analysis revealed that the LDA model applied to titles exhibited the most pronounced dissimilarity from all other models, consistently registering ARI values below 0.2 within the heatmap.\nConversely, the remaining models generally achieved a superior overall match, with ARI scores exceeding 0.2. Notably, BERTopic models demonstrated a stronger mutual fit, consistently yielding values above 0.35. Amongst these, the BERTopic abstract model emerged as particularly central, correlating effectively with every other model, save for the outlier LDA title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: LDA Model Inter-Comparisons",
    "text": "14.6 Results: LDA Model Inter-Comparisons\n\n\n\nSlide 09\n\n\nA more granular analysis of the LDA models provided detailed insights into their inter-relationships. Comparing LDA Full-text with LDA Abstract (Table A) revealed a generally strong fit. A distinct reddish diagonal in the table indicated that each topic from one model largely corresponded to a topic in the other, sharing a high proportion of common documents.\nDespite this overall alignment, some dynamic shifts occurred: three full-text LDA topics entirely disappeared, whilst another three split into multiple topics within the abstract model. Concurrently, three novel abstract topics emerged, and three abstract topics resulted from the merger of others. Furthermore, one small class within the abstract topics contained fewer than 50 documents.\nIn stark contrast, the comparison between LDA Full-text and LDA Title (Table B) demonstrated a poor fit, necessitating substantial reorganisation. This disparity manifested as numerous full-text topics vanishing and a proliferation of new topics appearing within the title model, underscoring the limited correspondence between these two textual granularities for LDA.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: BERTopic Model Inter-Comparisons",
    "text": "14.7 Results: BERTopic Model Inter-Comparisons\n\n\n\nSlide 11\n\n\nAnalysis of the BERTopic models, particularly in comparison with LDA Full-text, revealed varied levels of correspondence. Comparing LDA Full-text with BERTopic Full-text (Table C) indicated an average overall fit. Within this comparison, eight topics from the LDA model disappeared, whilst six topics split into the BERTopic model. Conversely, five new topics emerged within the BERTopic model, and one topic resulted from mergers. Furthermore, the document distribution showed four small classes alongside one notably large class.\nThe comparison between LDA Full-text and BERTopic Abstract (Table D) demonstrated a relatively good fit. Here, four topics disappeared, six topics split, two new topics appeared, and four topics resulted from mergers.\nFinally, examining LDA Full-text against BERTopic Title (Table E) again indicated an average overall fit. In this instance, seven topics disappeared, whilst one topic split. Simultaneously, seven new topics emerged, and one topic resulted from a merger. The document distribution for this comparison revealed three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: LDA Top-Word Correspondence",
    "text": "14.8 Results: LDA Top-Word Correspondence\n\n\n\nSlide 13\n\n\nAn examination of the top words within the LDA models revealed that topics generally maintained a relatively well-formed structure across all iterations. The researchers identified several robust topics exhibiting strong correspondence across every LDA model; “A-Radiation spore” serves as a prime example of such consistency.\nConversely, certain topics from the full-text model fragmented across the abstract and title models. For instance, “A-Life civilization” split into multiple sub-topics, a division that logically aligned with the broader theme of research in astrobiology. However, the fragmentation of “B-Chemistry” proved more challenging to interpret without deeper investigation.\nFurthermore, the analysis uncovered instances where topics from the full-text model merged into new, consolidated topics within the abstract and title models. The fusion of “B-Amino-acid” and “B-Protein-gene-RNA” exemplified this phenomenon, forming a more generalised and coherent thematic unit.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results: BERTopic Top-Word Correspondence",
    "text": "14.9 Results: BERTopic Top-Word Correspondence\n\n\n\nSlide 14\n\n\nContinuing the assessment of top words, the three BERTopic models consistently yielded relatively well-formed topics. Notably, “A-Radiation spore” again demonstrated remarkable robustness, maintaining its coherence across all BERTopic iterations. Similarly, “A-Life civilization” remained comparatively stable across the models, albeit with some observed splitting.\nThis fragmentation of “A-Life civilization” specifically led to the emergence of narrower topics, focusing precisely on extraterrestrial life. Furthermore, the splitting of “B-Chemistry” across the BERTopic models also resulted in more specialised, narrower thematic categories.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-coherence-performance",
    "href": "chapter_ai-nepi_016.html#results-coherence-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results: Coherence Performance",
    "text": "14.10 Results: Coherence Performance\n\n\n\nSlide 15\n\n\nAn evaluation of the models’ coherence, a metric assessing the meaningfulness of topic top words, revealed distinct performance patterns across a range of 5 to 50 topics. Titles consistently yielded the poorest coherence scores, indicating a less meaningful grouping of their constituent words. Conversely, abstract models generally demonstrated superior coherence compared to their full-text counterparts.\nAcross the board, BERTopic models exhibited better coherence than LDA, particularly for abstract and title analyses. However, this performance gap narrowed as the number of topics increased. Ultimately, the BERTopic Abstract model emerged as the unequivocal leader in terms of coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-diversity-performance",
    "href": "chapter_ai-nepi_016.html#results-diversity-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results: Diversity Performance",
    "text": "14.11 Results: Diversity Performance\n\n\n\nSlide 16\n\n\nAssessing the diversity of top words representing the topics, a clear trend emerged: diversity generally diminished as the number of topics increased. Titles, surprisingly, offered the highest diversity amongst all models, suggesting a broader range of unique words characterising their topics.\nFurthermore, BERTopic consistently outperformed LDA in terms of diversity. Ultimately, the BERTopic Title model secured the top position for diversity, with BERTopic Full-text closely trailing.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "href": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Results: Joint Recall Performance",
    "text": "14.12 Results: Joint Recall Performance\n\n\n\nSlide 17\n\n\nThe joint recall metric, which evaluates the efficacy of top words in collectively representing documents classified within each topic, revealed distinct performance hierarchies. Titles consistently yielded the poorest recall scores, indicating a limited ability of their top words to capture the full scope of associated documents. Conversely, full-text models demonstrated superior recall compared to both their abstract and title counterparts.\nBetween the two primary approaches, LDA generally exhibited better joint recall than BERTopic. Ultimately, LDA Full-text and BERTopic Full-text emerged as joint leaders in this category, with BERTopic Abstract following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "href": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Overall Model Performance Summary",
    "text": "14.13 Overall Model Performance Summary\n\n\n\nSlide 17\n\n\nThe researchers compiled the comprehensive results into a summary table, visually representing each model’s performance across various assessments using a graded circle system: black denoted the highest score, whilst white indicated the lowest. Crucially, this synthesis underscored the absence of an absolute “best” model, as varying research objectives inherently dictate differing needs and, consequently, distinct model choices.\nConsider, for instance, an objective focused solely on discovering main topics, where precise document classification is not paramount. In such a scenario, issues like poor recall or significant class imbalance might prove negligible. Here, full-text BERTopic performed commendably, despite exhibiting some class imbalance. Similarly, whilst far from optimal, title BERTopic nonetheless generated several robust topics that consistently appeared across other models. Conversely, the researchers strongly advise against employing LDA Title, given its consistently poor performance across nearly all assessment criteria.\nUltimately, the study recommends conducting topic modelling on either abstract or full-text data, utilising both LDA and BERTopic, provided such an approach does not result in the misclassification of documents pertinent to the identified topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.14 Discussion and Future Directions",
    "text": "14.14 Discussion and Future Directions\n\n\n\nSlide 17\n\n\nThis research yielded several crucial findings, informing future approaches to topic modelling. Firstly, title models consistently demonstrated poor performance. This deficiency likely stems from the inherent lack of information within titles, which can lead to the false classification of documents. Nevertheless, the BERTopic title model surprisingly revealed several meaningful topics, suggesting a potential balance between well-defined topics and comprehensive document coverage remains achievable.\nSecondly, full-text models, whilst offering comprehensive data, sometimes struggle to process vast quantities of information effectively. With LDA, topics can become more loosely defined and broader in scope, occasionally encompassing secondary themes such as methodology. Conversely, BERTopic, when applied to full text, can generate overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently performed well with summary information. Notably, the results obtained from LDA full text exhibited strong consistency with both abstract models, underscoring their utility. Fourthly, the study revealed a remarkable robustness of topics across all models. Researchers identified very similar topics across the board, a consistency that facilitates the application of meta-analytic methods to pinpoint the most robust thematic elements. Moreover, leveraging the relative distance across models could enable the identification of an optimal solution, as exemplified by the BERTopic abstract model in this study, which performed exceptionally well across numerous metrics.\nFinally, the findings prompt consideration of new model paradigms. It appears feasible to exploit the inherent structural information—encompassing full text, abstracts, and titles—to extract more semantically meaningful sets of topics or top words, thereby advancing the precision and utility of topic modelling. ```",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "href": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "title": "15  ```",
    "section": "",
    "text": "Slide 01",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "title": "15  ```",
    "section": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer",
    "text": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer\n\n\n\nSlide 05\n\n\nFormalising the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, x_n, given a sequence of preceding tokens, x_1, ..., x_{n-1} [@Radford2018]. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as p(x_n | x_1, ..., x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a specific time t is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing LLMs largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.\nTo overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution p(x_n | x_1, ..., x_{n-1}, t). Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, termed the “Time Transformer”, introduces a simple yet profound modification: an additional dimension, φ(t), is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, E(x, t), which then serves as input to the Transformer architecture. Consequently, the Transformer processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution p_θ(x_n | x_1, ..., x_{n-1}, t). The training objective remains the standard maximisation of log likelihood across all sequences [@Goodfellow2016]. This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "href": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "title": "15  ```",
    "section": "15.3 Empirical Validation: Data and Architecture",
    "text": "15.3 Empirical Validation: Data and Architecture\n\n\n\nSlide 16\n\n\nTo empirically validate the Time Transformer concept, researchers required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, proved an ideal choice [@UKMetOffice]. Researchers scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The tokenisation process was intentionally simplistic, neglecting sub-word tokenisation, case, and interpunctuation, resulting in a compact vocabulary of only 3,395 unique words across the entire seven-year period. An alternative dataset, TinyStories, was also considered for its similar characteristics, offering short, synthetically generated narratives [@Xu2023].\nA modest Transformer architecture, termed the “Vanilla model”, underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output [@Vaswani2017]. Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like GPT-4, which boasts 1.8 trillion parameters distributed across 120 layers [@OpenAI2023]. Training occurred on an HPC cluster in Munich, utilising two H100 GPUs. Remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The code for this implementation is publicly available on GitHub [@Büttner2025GitHub], though it was developed primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a perfect ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as “During the night, a band …”, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "href": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "title": "15  ```",
    "section": "15.4 Time Transformer: Implementation and Results",
    "text": "15.4 Time Transformer: Implementation and Results\n\n\n\nSlide 15\n\n\nImplementing the Time Transformer required only a minimal architectural adjustment to the previously described Vanilla model. Engineers reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as (day of year - 1) / (365 - 1). This specific encoding was chosen to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.\nThe first experiment aimed to demonstrate the model’s capacity for learning synthetic temporal drift through synonymic succession. Researchers injected a time-dependent replacement rule into the training data: rain was replaced by liquid sunshine according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of rain versus liquid sunshine. The Time Transformer flawlessly reproduced the injected sigmoid pattern, exhibiting rain predominantly early in the year and liquid sunshine towards the end, with the transition occurring precisely mid-year.\nThe second experiment explored the model’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the “fixation of a collocation.” Here, instances of rain not immediately followed by and were synthetically replaced with rain and snow. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of rain and snow against rain only. The model successfully acquired this pattern, generating rain and snow almost exclusively in the latter part of the year, whilst early-year occurrences of rain (sometimes accompanied by snow) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year rain and snow on the presence of a “cold system,” underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "title": "15  ```",
    "section": "15.5 Proof of Concept, Applications, and Challenges",
    "text": "15.5 Proof of Concept, Applications, and Challenges\n\n\n\nSlide 21\n\n\nThis research unequivocally establishes a proof of concept: Transformer-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation Time Transformer, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to “talk to a specific time,” potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.\nSeveral promising avenues for future research emerge from this work. Benchmarking the Time Transformer against explicit time-token approaches will quantify its performance advantages and identify optimal use cases. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.\nNevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained LLMs remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to BERT [@Devlin2019], built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward for historical language analysis. ```",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>```</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "href": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Research Objectives and Scope",
    "text": "16.1 Research Objectives and Scope\n\n\n\nSlide 01\n\n\nThis research systematically explores the application of Large Language Models (LLMs) for enhancing historical scientific texts. Our project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts,” unfolds in two principal parts.\nThe first part focuses on deploying LLMs to improve the metadata associated with historical texts. This involves the precise categorisation of articles according to their scientific discipline, the assignment of pertinent semantic tags or topics, and the generation of concise, abstractive summaries.\nBuilding upon this metadata enrichment, the second part undertakes a detailed analysis of the chemical space as it evolved across different scientific disciplines over time. A crucial objective here involves identifying specific periods that correspond to peaks of interdisciplinarity and significant knowledge transfer, thereby illuminating the dynamic nature of scientific discourse.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "href": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 Historical Scientific Corpus: Philosophical Transactions",
    "text": "16.2 Historical Scientific Corpus: Philosophical Transactions\n\n\n\nSlide 01\n\n\nThis project investigates the evolution of scientific English, alongside phenomena such as knowledge transfer and the identification of influential papers and authors. Central to this inquiry is the Philosophical Transactions of the Royal Society of London, a corpus of unparalleled historical significance.\nFirst published in 1665, this journal holds the distinction of being the oldest scientific periodical in continuous publication, maintaining a high reputation to this day. It played a pivotal role in shaping scientific communication, notably by establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge. The corpus contains numerous seminal contributions, including Isaac Newton’s “New Theory about Light and Colours” from the 17th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” from the 18th century, and James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” from the 19th century. Beyond these renowned works, the collection also features more curious, speculative texts, such as discussions on lunar inhabitants, though our research focuses on linguistic and thematic analysis rather than factual validation.\nFor our analysis, we utilise the latest version of this extensive collection, the RSC 6.0 full corpus. This dataset spans over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 texts and approximately 300 million tokens. While the corpus includes pre-encoded metadata, such as author, century, year, and volume, a previous study employed LDA topic modelling to infer research field categories. However, this earlier classification often conflated distinct scientific disciplines, sub-disciplines, and even text types, such as “observations” and “reporting.” This necessitated a more refined approach to metadata enrichment, which our current work addresses.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Large Language Models for Metadata Enrichment",
    "text": "16.3 Large Language Models for Metadata Enrichment\n\n\n\nSlide 10\n\n\nTo address the limitations of existing metadata, we leveraged Large Language Models (LLMs) for comprehensive metadata enrichment. These models offer diverse applications, including text clean-up, summarisation, information extraction, and the populating of knowledge graphs, alongside their core function in categorisation and facilitating access and retrieval.\nSpecifically, we tasked the LLM with four distinct operations. First, it performed hierarchical categorisation, assigning both a primary discipline and a suitable sub-discipline to each article. Second, it identified key index terms, functioning as semantic tags or topics. Third, the model generated concise TL;DR (Too Long; Didn’t Read) summaries, typically 3-4 sentences in length, designed to capture the essence and main findings of an article in simple language, accessible even to a high school student. Finally, the LLM proposed alternative, more reflective titles for the texts.\nFor this task, our team selected Hermes-2-Pro-Llama-3-8B, an 8-billion-parameter model from the Llama 3 family. This particular variant, readily available on Hugging Face, demonstrated superior performance compared to Mistral and Llama 2. It had undergone instruction-tuning specifically for producing structured outputs in formats such as JSON and YAML.\nA meticulously crafted system prompt guided the LLM’s operations. The prompt first defined the model’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” It then articulated the objective: “read, analyze, and organize a large corpus… create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input was described as OCR-extracted text from original articles, accompanied by existing metadata including title, author(s), publication date, journal, and a short text snippet.\nThe prompt detailed the four specific tasks as follows: A. Read and analyse the provided article to understand its content and context, then suggest an alternative title that better reflects its content. B. Write a short 3-4 sentence TL;DR summary that captures the article’s essence and main findings, ensuring conciseness, informativeness, and simple language. C. Identify exactly five main topics, conceptualised as Wikipedia Keywords for categorising the text into scientific sub-fields. D. Given the extracted topics, identify the primary scientific discipline from a predefined list (Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, Social Sciences & Humanities) and a suitable second-level sub-discipline, with the crucial constraint that the sub-discipline could not be one of the primary disciplines.\nAn example input, Isaac Newton’s 1672 letter, clearly illustrated the expected YAML output. The LLM successfully transformed the original lengthy title into “A New Theory of Light and Colours,” assigned topics such as “Optics” and “Refraction,” generated a concise TL;DR summary, and accurately classified the article under “Physics” with the sub-discipline “Optics & Light.” A final instruction reinforced the requirement for a valid YAML output, with no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "href": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 LLM Performance and Diachronic Corpus Analysis",
    "text": "16.4 LLM Performance and Diachronic Corpus Analysis\n\n\n\nSlide 15\n\n\nInitial sanity checks on the LLM’s performance yielded highly encouraging results. A remarkable 99.81% of the generated outputs, specifically 17,486 out of 17,520, conformed to the specified YAML format, demonstrating the model’s proficiency in structured data generation. Furthermore, 94% of the predicted scientific disciplines aligned precisely with our predefined set of nine categories.\nNevertheless, the LLM exhibited some minor deviations or “hallucinations.” For instance, it occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences.” The model also innovated novel categories, such as “Music,” and sometimes incorporated the numerical index of a discipline directly into its name, for example, “3. Chemistry.” Moreover, certain sub-disciplines, like “neurology” and “zoology,” were incorrectly classified as primary disciplines. Despite these minor deviations, the LLM accurately assigned the vast majority of papers to their correct categories.\nLeveraging this enhanced metadata, we conducted a diachronic analysis of the Royal Society articles, examining their distribution across disciplines over time. Before the close of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. However, the late 18th century witnessed a distinct surge in chemical articles, a phenomenon directly correlating with the Chemical Revolution. Subsequently, from the 19th century into the 20th, Chemistry, alongside Biology and Physics, emerged as one of the Royal Society’s primary pillars of scientific inquiry.\nFurther analysis involved visualising the TL;DR summaries using t-SNE projections, a dimensionality reduction technique. This revealed significant overlaps between Chemistry, Physics, and Biology, with Chemistry often situated centrally within this interdisciplinary space. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters within the projection. This visualisation technique holds considerable promise for future research, enabling the detailed observation of diachronic shifts and evolving overlaps between disciplines.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Diachronic Analysis of Chemical Space",
    "text": "16.5 Diachronic Analysis of Chemical Space\n\n\n\nSlide 19\n\n\nBuilding upon the LLM-derived classifications, we proceeded with a diachronic analysis of the chemical space, focusing specifically on Chemistry, Biology, and Physics, given their prevalence within the corpus.\nTo extract chemical terms, our team employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initial application of this tool to the entire text corpus generated considerable noise. Consequently, we adopted a refined approach: ChemDataExtractor was subsequently applied to a pre-filtered list of extracted substances, a method that significantly reduced the noisy output and improved precision.\nFor analysing the chemical space, we utilised Kullback-Leibler Divergence (KLD). This statistical measure quantifies the number of additional bits required to encode a dataset A when an (often sub-optimal) model based on dataset B is employed. Crucially, higher KLD values indicate greater differences between datasets, whilst lower values suggest relative similarity. In essence, KLD helps us understand how much one probability distribution differs from another.\nWe applied KLD in two distinct ways. First, to trace the independent evolution of the chemical space within each discipline along the historical timeline, we compared 20-year periods before a specific date with 20-year periods after it, employing a sliding 5-year window. This technique allowed for a granular understanding of how chemical terminology and concepts shifted within Chemistry, Biology, and Physics individually. Second, for a broader interdisciplinary perspective, we conducted a pairwise comparison of Chemistry with Physics and Chemistry with Biology, based on 50-year periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "href": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics",
    "text": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics\n\n\n\nSlide 22\n\n\nAnalysis of the Kullback-Leibler Divergence (KLD) per discipline revealed consistent trends across Chemistry, Biology, and Physics, with peaks and troughs in KLD values occurring roughly concurrently. Towards the end of the timeline, the KLD generally decreased, indicating less variation between future and past periods within each discipline. A notable peak emerged in the late 18th century, prompting further investigation into the specific chemical substances driving this change.\nExamining the period from 1776 to 1816, corresponding to the late 18th-century peak, we observed that in both Biology and Physics, one or two elements exhibited exceptionally high KLD values, effectively driving the observed shifts. Intriguingly, the same core chemical elements appeared across Chemistry, Biology, and Physics during this era, suggesting a shared foundational chemical vocabulary.\nHowever, a significant divergence became apparent when analysing the second half of the 19th century (1856-1906). During this period, the KLD graphs for Biology and Physics became considerably more populated, with individual elemental contributions showing greater uniformity. Biology’s chemical discourse evolved distinctly towards biochemistry, incorporating substances related to biochemical processes. Conversely, Chemistry and Physics increasingly focused on noble gases and radioactive elements, reflecting their discovery and burgeoning importance in the late 19th century.\nInterdisciplinary comparisons, visualised through word clouds for the latter half of the 20th century, further elucidated these thematic differences. In the comparison between Chemistry and Biology, the biological word cloud featured a greater prevalence of substances associated with biochemical processes in living organisms. In contrast, the chemical word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. When comparing Chemistry with Physics, the latter’s word cloud prominently displayed metals, noble gases, rare earth metals, semi-metals, and radioactive metals, underscoring distinct disciplinary thematic focuses.\nCrucially, this pairwise KLD analysis facilitated the detection of “knowledge transfer” events. Knowledge transfer, in this context, describes instances where an element initially distinctive of one discipline in an earlier period subsequently became more characteristic of another. For example, tin, which was distinctive of Chemistry in the early 18th century, clearly shifted to become distinctive of Physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. Furthermore, elements transitioning from Chemistry to Biology in the 20th century consistently related to biochemical processes, reinforcing the observed disciplinary specialisation and the dynamic nature of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Conclusion and Future Research Directions",
    "text": "16.7 Conclusion and Future Research Directions\n\n\n\nSlide 21\n\n\nThis research successfully employed a Large Language Model for the categorisation of articles and the development of refined topic models within a historical corpus. Building upon these LLM-generated results, we conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: Chemistry, Biology, and Physics. Furthermore, we performed a detailed interdisciplinary comparison of this evolving chemical landscape, identifying periods of significant disciplinary shift and knowledge transfer. Our findings highlight the dynamic nature of scientific discourse and the evolving interconnections between fields over centuries.\nDespite these significant achievements, ample scope remains for future work. For the first part of the project, we plan to test alternative LLMs and undertake a rigorous evaluation of the current model’s outputs to ensure robustness and accuracy. Regarding the diachronic analysis of the chemical space, future efforts will involve a more fine-grained interdisciplinary analysis, potentially incorporating diachronic sliding windows of varying lengths. Expanding the scope to include additional disciplines, such as a comparison between Chemistry and Medicine, also presents a compelling avenue. Finally, exploring the evolution of chemical space using surprisal, a measure of unexpectedness, could yield further insights into the dynamics of scientific discovery and knowledge transfer. ```",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  }
]