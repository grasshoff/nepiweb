[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Workshop Overview\nThe organisers orchestrated a three-day workshop dedicated to exploring the applications of Large Language Models (LLMs) within the history, philosophy, and sociology of science. This event, held at TU Berlin and accessible online from 2-4 April 2025, garnered considerable interest. Over 50 paper submissions were received, and approximately 220 participants registered, demonstrating the broad appeal of the topic. Adrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert meticulously curated the programme, ultimately selecting 16 papers for presentation. The European Research Council’s (ERC) Network Epistemology in Practice (NEPI) grant (number 10104932) secured the necessary funding for the workshop.\nThe workshop’s conceptualisation stemmed from two distinct yet complementary initiatives. Firstly, the NEPI project provided a foundational impetus. Within this framework, Arno Simons pioneered the training of early LLMs on physics texts, whilst Michael Zichert applied these models to analyse conceptual issues in physics.  Secondly, Gerd Graßhoff, a long-standing collaborator, consistently advocated for integrating Artificial Intelligence (AI) into the history and philosophy of science, particularly for scrutinising scientific discovery processes. These converging interests culminated in a unified workshop, fostering broader discussion on AI-assisted methodologies.\nThe NEPI project itself investigates the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. Researchers employ network analysis to map communication structures and utilise semantic tools, including LLMs, to trace the flow of ideas within these complex networks. This endeavour aims to elucidate how large research collaborations collectively generate new knowledge.\nKeynote speakers for the workshop included Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. They presented on large-scale text analysis for cultural and societal change, focusing on semantic change detection and data science for the humanities. Iryna Gurevych, head of the Ubiquitous Knowledge Processing (UKP) Lab at Technical University Darmstadt, delivered a keynote on elevating Natural Language Processing (NLP) to the cross-document level, covering information extraction, semantic text processing, machine learning, and NLP applications in social sciences and humanities.\nLogistical arrangements for the workshop included a comprehensive recording policy. Sessions were captured by a camera focused on the presenter, four microphones, and an iPhone backup. The organisers intend to upload videos of talks, including discussions, to the NEPI YouTube channel, subject to presenter consent. A structured Q&A protocol facilitated engagement, limiting questions to four per session to ensure efficiency. Furthermore, an Etherpad/Cryptpad provided an asynchronous platform for comments and questions, whilst the Zoom chat enabled real-time interaction. Ample networking opportunities were provided through scheduled lunch and coffee breaks, a modest reception, and a limited-seat workshop dinner.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-engagement",
    "href": "chapter_ai-nepi_001.html#workshop-engagement",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Engagement",
    "text": "2.1 Workshop Engagement\n\n\n\nSlide 01: A Quarto slide titled “Workshop Engagement” with bullet points summarising the number of submissions, participants, and the goal of ensuring active participation.\n\n\nThe organisers convened a workshop titled “Large Language Models for the History, Philosophy and Sociology of Science,” held from 2-4 April 2025. This event, co-organised by Adrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert, welcomed participants both in person at TU Berlin’s Room H2005 and via an online platform.\nThe call for papers generated significant interest, attracting over 50 submissions. From this competitive pool, the organisers meticulously selected 16 papers for presentation during the workshop. Participation levels proved robust: in-person attendance quickly reached capacity, whilst a substantial online audience also registered. Overall, approximately 220 individuals enrolled for the workshop, with additional registrations continuing to arrive. Crucially, the organisers aimed to ensure that all individuals interested in these topics could actively participate in the discussions throughout the two-and-a-half-day programme.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#genesis-and-nepi-objectives",
    "href": "chapter_ai-nepi_001.html#genesis-and-nepi-objectives",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Genesis and NEPI Objectives",
    "text": "2.2 Genesis and NEPI Objectives\n\n\n\nSlide 02: A Quarto slide titled “Genesis and NEPI Objectives” outlining the two initiatives that led to the workshop and the NEPI project’s focus on the Atlas collaboration at CERN.\n\n\nThe workshop’s inception stemmed from two distinct yet complementary initiatives. Firstly, the Network Epistemology in Practice (NEPI) project provided a foundational impetus. Within this project, Arno Simons pioneered the training of one of the earliest Large Language Models (LLMs) specifically on physics texts, aligning with the project’s core interests.  Concurrently, Michael Zichert, also a member of the NEPI team, applied LLMs to scrutinise conceptual issues prevalent in physics.\nSecondly, Gerd Graßhoff, a long-standing collaborator of Adrian Wüthrich, significantly contributed to the workshop’s genesis. Graßhoff has consistently championed the integration of Artificial Intelligence (AI) into the history and philosophy of science, particularly for analysing the intricate processes of scientific discovery. He independently conceived a workshop focused on novel AI-assisted methodologies for these disciplines. Recognising their shared objectives, the organisers subsequently decided to combine their efforts, culminating in the present workshop.\nThe European Research Council (ERC) grant, Network Epistemology in Practice (NEPI), bearing grant number 10104932, provides the funding for this endeavour. Within the NEPI project, researchers meticulously study the internal communication of the Atlas collaboration at CERN, the prominent particle physics laboratory. This investigation aims to elucidate how one of the largest and most distinguished research collaborations collectively generates new knowledge. Researchers employ network analysis to discern the communication structures within this collaboration. Furthermore, they utilise semantic tools, including LLMs, to trace the flow of ideas throughout these complex network structures. Indeed, the application of LLMs represents a central interest for the project, with numerous other applications anticipated throughout the workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols",
    "href": "chapter_ai-nepi_001.html#recording-protocols",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Recording Protocols",
    "text": "2.3 Recording Protocols\n\n\n\nSlide 05: A Quarto slide titled “Recording Protocols” detailing the equipment used for recording sessions, consent requirements, and the plan for uploading videos to the NEPI YouTube Channel.\n\n\nThe workshop sessions are currently being recorded. Participants received prior notification of this during the registration process, thereby implying their consent. A single camera captures the presenter, ensuring focus remains on the speaker. For audio capture, four microphones are deployed, supplemented by an iPhone serving as a crucial backup recorder.\nFollowing the workshop, the organisers intend to upload videos of the talks, encompassing the accompanying discussions, to the NEPI YouTube Channel. This process, however, necessitates the explicit consent of each presenter. Crucially, whilst discussions are recorded, the audio and video capture solely focuses on the presenter, deliberately excluding the audience. Should any participant require additional information or wish to withdraw their consent, they are encouraged to approach the organisers. Ultimately, these recording efforts aim to establish a comprehensive record of the valuable discussions and presentations from this meeting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#interaction-protocols",
    "href": "chapter_ai-nepi_001.html#interaction-protocols",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Interaction Protocols",
    "text": "2.4 Interaction Protocols\n\n\n\nSlide 07: A Quarto slide titled “Interaction Protocols” outlining the Q&A format, the use of Etherpad/Cryptpad for asynchronous comments, and Zoom chat for real-time interaction.\n\n\nGiven the substantial number of participants and the constrained time allocated for presentations, the organisers have implemented a specific protocol for questions and comments. Participants are kindly requested to formulate their questions and comments concisely and directly. Following each presentation, the organisers will collect approximately four questions or comments, enabling the presenter to address them collectively, thereby optimising time and avoiding protracted back-and-forth exchanges. The organisers acknowledge that, despite the value of all inquiries, time limitations may preclude addressing every question in person.\nTo facilitate broader engagement beyond the live sessions, the organisers provide an Etherpad or Cryptpad. This platform allows participants to post comments or questions after sessions, offering presenters an opportunity to read and respond at their convenience. Consequently, this channel ensures continuous interaction even when sessions are not actively running. Furthermore, during live sessions, both online attendees and the in-person audience can utilise the Zoom chat feature to submit questions or comments at any time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#networking-and-social-programme",
    "href": "chapter_ai-nepi_001.html#networking-and-social-programme",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Networking and Social Programme",
    "text": "2.5 Networking and Social Programme\nBeyond the formal presentations, the workshop actively fosters informal networking amongst researchers and fellows. The programme incorporates ample lunch and coffee breaks, providing dedicated time for casual interaction. Additionally, a modest reception and a workshop dinner are scheduled, though seats for the dinner are strictly limited to confirmed participants.\nCoffee and refreshments are available on-site. For lunch and the reception, attendees will proceed to Room H2051, located down the hall and one floor below. The organisers will provide guidance to these locations, for instance, leading participants there after today’s final talk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-text-analysis-for-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-text-analysis-for-societal-change",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.6 Keynote 1: Text Analysis for Societal Change",
    "text": "2.6 Keynote 1: Text Analysis for Societal Change\nThe first keynote address, titled “Large-scale text analysis for the study of cultural and societal change,” featured Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi serves as the Principal Investigator for the “Change is Key” research programme in Gothenburg, whilst Pierluigi Cassotti contributes as a researcher within this project.\nThese scholars have garnered considerable recognition for their pioneering work in semantic change detection.  Their contributions encompass not only technical advancements, such as the development of crucial benchmarks, but also broader methodological considerations concerning the application of data science methods to humanities questions. This dual expertise renders their work exceptionally pertinent to the workshop’s themes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-cross-document-nlp",
    "href": "chapter_ai-nepi_001.html#keynote-2-cross-document-nlp",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.7 Keynote 2: Cross-Document NLP",
    "text": "2.7 Keynote 2: Cross-Document NLP\nIryna Gurevych delivered the second keynote address, scheduled for the late afternoon of the following day, under the title “How to InterText? Elevating Natural Language Processing (NLP) to the cross-document level.” Gurevych heads the Ubiquitous Knowledge Processing (UKP) Lab at Technical University Darmstadt.\nHer extensive research primarily focuses on information extraction, semantic text processing, and machine learning. Crucially, her work also explores the practical applications of NLP within the social sciences and humanities. This specific area of expertise positions her contributions as an ideal complement to the workshop’s overarching objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "",
    "text": "Overview\nThis presentation systematically introduces the foundational architecture of large language models (LLMs), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (HPSS). As a co-organiser, the speaker initially provides a primer on the seminal Transformer architecture, explaining its encoder-decoder structure and original purpose in language translation.\nSubsequently, the discussion differentiates between encoder-based models, such as BERT, which offer bidirectional full-context understanding, and decoder-based generative models, like GPT, capable of producing novel text. The presentation then charts the proliferation of domain-specific LLMs across various scientific fields, outlining diverse adaptation strategies including pre-training, fine-tuning, and the sophisticated Retrieval Augmented Generation (RAG) pipeline.\nCrucially, the presentation categorises current LLM applications in HPSS, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, it offers critical reflections on HPSS-specific challenges, such as historical language evolution and sparse data. The speaker advocates for enhanced LLM literacy and a steadfast adherence to HPSS methodologies, highlighting new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.1 The Foundational Transformer Architecture",
    "text": "3.1 The Foundational Transformer Architecture\n\n\n\nSlide 01\n\n\nAt the core of all contemporary large language models (LLMs) lies the seminal Transformer architecture, pioneered by Vaswani et al. in 2017 (Vaswani2017?). Originally designed for language translation, such as German to English, this architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.\nThe encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence’s meaning. This allows the model to understand the full context of the input.\nConversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness and enabling more nuanced language processing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "href": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.2 Differentiating BERT and GPT Models",
    "text": "3.2 Differentiating BERT and GPT Models\n\n\n\nSlide 03\n\n\nImmediately following the Transformer’s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (NLP) tasks with minimal additional training.\nFrom the encoder side, the BERT family of models emerged, standing for Bidirectional Encoder Representations from Transformers. BERT operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. This enables BERT to excel at tasks requiring deep contextual comprehension, such as sentiment analysis or question answering.\nConversely, the decoder side gave rise to the GPT models, or Generative Pre-trained Transformers, which now power applications like ChatGPT. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text. This generative function is not inherently present in BERT-like models.\nConsequently, a fundamental distinction arises: generative models, exemplified by GPT, primarily produce language, whereas full-context models, such as BERT, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like XLM and XLNet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "href": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.3 Scientific LLM Evolution and Adaptation Strategies",
    "text": "3.3 Scientific LLM Evolution and Adaptation Strategies\n\n\n\nSlide 06\n\n\nA comprehensive overview reveals the rapid evolution of large language models (LLMs), particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to BERT, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included BioBERT, Specter, and SciBERT. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.\nResearchers employ several methods to adapt these models to specific scientific language. The initial phase is pre-training, where a model learns language by predicting the next token (a word or sub-word unit), as in GPT models, or by predicting randomly masked words, characteristic of BERT models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many.\nConsequently, continued pre-training offers a viable alternative. Researchers utilise an already pre-trained model and subsequently train it on domain-specific language. For instance, a general BERT model can be adapted for physics texts by further training it on a large corpus of physics literature.\nBeyond this, engineers can add extra layers atop pre-trained models, effectively fine-tuning them for specific classification tasks like sentiment analysis or named entity recognition. Crucially, contrastive learning emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with Sentence BERT serving as a widely adopted example for creating semantically meaningful sentence representations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.4 Retrieval Augmented Generation and Key Distinctions",
    "text": "3.4 Retrieval Augmented Generation and Key Distinctions\n\n\n\nSlide 10\n\n\nRetrieval Augmented Generation (RAG) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins ChatGPT’s internet search capabilities, allowing it to access and synthesise information beyond its initial training data.\nThe RAG process commences with a user query, such as “What are LLMs?”. Subsequently, a BERT-type model encodes this query into a sentence embedding, a numerical representation of its meaning. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the RAG pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context. This approach significantly reduces the likelihood of “hallucinations” often associated with standalone LLMs.\nBeyond RAG, advanced reasoning models and agents are emerging. These are not isolated LLMs but rather intricate systems that combine LLMs with a diverse array of other tools, enabling more complex problem-solving and decision-making.\nConsequently, a clear understanding of key distinctions proves crucial for navigating the LLM landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone LLMs, complex pipelines like RAG, and sophisticated agents becomes paramount for effective application and responsible deployment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.5 Applications and Trends in HPSS Research",
    "text": "3.5 Applications and Trends in HPSS Research\n\n\n\nSlide 13\n\n\nA current survey explores the burgeoning uses of large language models (LLMs) as tools within history, philosophy, and sociology of science (HPSS) research. This investigation has identified four primary categories for these applications:\n\nLLMs assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations. For example, they can efficiently identify and categorise funding sources in large corpora of scientific papers.\nThey contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields. This could involve identifying key concepts and their relationships within historical scientific texts.\nLLMs illuminate knowledge dynamics, particularly through the study of conceptual histories of words. By tracking semantic shifts over time, researchers can trace the evolution of scientific concepts.\nThey support the analysis of knowledge practices, including citation context analysis—an older HPSS tradition now also employed for evaluatory purposes. This allows for a deeper understanding of how scientific ideas are built upon and referenced.\n\nA notable trend indicates an accelerating interest in LLMs, with findings predominantly appearing in information science journals like Scientometrics and the Journal of the Association for Information Science and Technology (JASIST). Increasingly, however, papers featuring LLM applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers, bridging disciplinary divides. Furthermore, the degree of customisation in LLM deployment varies widely, spanning from straightforward off-the-shelf use of ChatGPT to the development of entirely new model architectures tailored for specific HPSS questions.\nDespite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models (the “black box” problem), and persistent shortages of training data and benchmarks specific to HPSS domains. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by BERTopic, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "href": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.6 Critical Reflections and Future Directions for HPSS",
    "text": "3.6 Critical Reflections and Future Directions for HPSS\n\n\n\nSlide 12\n\n\nCrucially, scholars must acknowledge the specific challenges inherent to HPSS when engaging with large language models (LLMs). Foremost amongst these is the historical evolution of concepts and language. Models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models on historical corpora or the judicious use of existing ones with a keen awareness of their limitations.\nFurthermore, HPSS adopts a reconstructive and critically reflective perspective, often reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive HPSS “reading.” Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts, which pose significant hurdles for LLM application.\nConsequently, building robust LLM literacy becomes imperative for HPSS researchers. They must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing (NLP) may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.\nUltimately, HPSS researchers must remain true to their established methodologies. This involves carefully translating complex HPSS problems into specific NLP tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon HPSS’s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s (Callon1986?; Rip1986?), offers valuable theoretical grounding, particularly given the resonance of Actor-Network Theory (ANT) concepts with contemporary LLM developments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Noichl, Andrea Loettgers, and Taya Knuuttila introduced OpenAlex Mapper, a novel tool designed to facilitate transdisciplinary investigations within the history, philosophy, and sociology of science (HPSS). Developed by researchers at Utrecht University and the University of Vienna, this instrument received funding from an ERC grant focused on “possible life”. The presentation meticulously detailed the tool’s technical architecture, its operational workflow, and its diverse applications in scholarly analysis.\nOpenAlex Mapper directly addresses the inherent challenges of generalising findings from small samples and case studies prevalent in HPSS research. The tool leverages a fine-tuned Specter 2 language model, specifically adapted to discern disciplinary boundaries. This model processes a vast dataset of 300,000 randomly sampled English-language abstracts from the OpenAlex database. Subsequently, the system employs Uniform Manifold Approximation and Projection (UMAP) to reduce these high-dimensional embeddings into a two-dimensional, interactive base map.\nUsers can submit arbitrary queries to OpenAlex; the tool then embeds the retrieved abstracts and projects them onto this pre-existing map, thereby revealing their disciplinary locations. Crucially, OpenAlex Mapper supports qualitative, heuristic investigations by grounding them in extensive quantitative data, whilst always linking back to original textual sources.\nDemonstrations highlighted the tool’s utility in mapping the distribution of model templates, such as the Ising, Hopfield, and Sherrington-Kirkpatrick models, across scientific fields. Furthermore, the tool effectively visualises the spread of key concepts, exemplified by “phase transition” and “emergence”. It also analyses the interdisciplinary adoption patterns of specific methods, including “Random Forest” and “Logistic Regression”. The developers acknowledged certain limitations, notably OpenAlex’s data quality, the current English-only language model, the requirement for abstracts or robust titles, and the inherent stochasticity and dimensionality reduction trade-offs of the UMAP algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-technical-foundations",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-technical-foundations",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper: Technical Foundations",
    "text": "4.1 OpenAlex Mapper: Technical Foundations\n\n\n\nSlide 01\n\n\nMaximilian Noichl, Andrea Loettgers, and Taya Knuuttila introduced OpenAlex Mapper, a sophisticated tool developed through collaborative research. Noichl, a PhD candidate at Utrecht University’s Theoretical Philosophy Department, collaborated with Loettgers and Knuuttila from the University of Vienna’s Philosophy Department. This endeavour received funding from an ERC grant specifically supporting research on “possible life”.\nThe presentation systematically unfolded, first elucidating the tool’s core functions and high-level technical specifications. Subsequently, a live demonstration illustrated its practical application, culminating in a detailed discussion of its utility within the History, Philosophy, and Sociology of Science (HPSS) domain.\nAt the heart of OpenAlex Mapper’s functionality lies a meticulously fine-tuned Specter 2 language model. Researchers adapted this model to enhance its capacity for discerning disciplinary boundaries, training it on a dataset of articles from closely related fields to improve distinction. The resulting embeddings from this training process are then visually represented using a UMAP dimensionality reduction technique. Crucially, these modifications represent minor adjustments to the language model, rather than a comprehensive retraining effort.\nFor base-map preparation, the team drew upon the extensive OpenAlex database. This resource is renowned for its size and inclusivity, surpassing proprietary databases such as Web of Science and Scopus. Its open data policy facilitates easy, batch-query access, making it an invaluable asset for large-scale scholarly analysis. From this vast repository, researchers sampled 300,000 random articles, selecting only those with well-formed English abstracts. The fine-tuned Specter 2 model then processed these abstracts, generating high-dimensional embeddings.\nTo render these complex embeddings visually interpretable, the system employs Uniform Manifold Approximation and Projection (UMAP). This dimensionality reduction algorithm transforms the high-dimensional data into a two-dimensional base map, simultaneously producing a trained UMAP model. Consequently, when users submit arbitrary queries through the OpenAlex search interface, the tool first downloads the initial 1,000 records. It then embeds their abstracts using the same fine-tuned language model, and subsequently projects these new embeddings onto the pre-existing 2D map via the trained UMAP model. This inherent feature of UMAP ensures consistent and accurate positioning of new documents within the established disciplinary landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-challenge",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-challenge",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration and Core Challenge",
    "text": "4.2 Interactive Demonstration and Core Challenge\n\n\n\nSlide 08\n\n\nThe OpenAlex Mapper, accessible online at https://m7n-openalex-mapper.hf.space, offers an interactive platform for scholarly exploration. An alternative version, featuring a higher latency GPU setup, also accommodates larger, more demanding queries. Users initiate their investigations by inputting search terms, such as “scale-free network models” or “coriander,” leveraging the comprehensive capabilities of the OpenAlex search interface.\nOnce a query is submitted, the system efficiently downloads the initial 1,000 records corresponding to the user’s input, subsequently embedding their abstracts. If enabled by the user, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as an interactive projection of these search results onto a grey base map, visually indicating where specific terms, authors, or concepts appear across various disciplines. This interactive functionality empowers users to delve deeper; for instance, one can investigate the presence of “coriander” in epidemiology or public health literature. Moreover, the tool offers visualisations of temporal distributions and the overlay of citation graphs, providing multifaceted insights.\nFundamentally, OpenAlex Mapper addresses a critical challenge in the History, Philosophy, and Sociology of Science: the generalisation and validation of findings derived from small samples and case studies. It aims to answer nuanced questions regarding the adoption and prevalence of specific models, such as “Where did the Hopfield model truly establish itself?” By employing rigorous quantitative methods, the tool provides a robust foundation for qualitative, heuristic investigations. Crucially, it maintains a direct link to the actual textual sources, ensuring that all findings remain traceable and verifiable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "href": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.3 Applications in Scholarly Analysis",
    "text": "4.3 Applications in Scholarly Analysis\n\n\n\nSlide 17\n\n\nThe development of OpenAlex Mapper represents an ongoing research endeavour, with several compelling applications already emerging. Initially, researchers conceived the tool specifically for investigating model templates. This approach explores how models possessing similar structural properties manifest across disparate scientific disciplines, potentially imposing a structure on science that operates orthogonally to traditional disciplinary boundaries. For instance, the system has mapped the distribution of the Ising model (7,819 instances), the Hopfield model (589 instances), and the Sherrington-Kirkpatrick model (1,437 instances). These analyses reveal their presence in distinct, non-continuous regions across the base map. This work draws upon foundational ideas from Humphreys (2004) and Knuuttila and Loettgers (2023).\nBeyond model templates, the tool effectively visualises the distribution of key concepts. A notable example contrasts the concept of “phase transition” with “emergence,” depicted in orange on the map. This capability proves particularly advantageous for broadening conceptual analysis into interdisciplinary contexts, circumventing the common difficulties associated with acquiring specific, cross-disciplinary datasets. Relevant scholarship in this area includes contributions from Malaterre, Chartier, and Lareau (2020), and Zichert and Wüthrich (2024).\nFurthermore, OpenAlex Mapper facilitates the analysis of method distribution across scientific fields. Examining the usage patterns of “Random Forest” (2,000 instances) versus “Logistic Regression” (1,997 instances) reveals clearly distinguishable patterns of adoption within interdisciplinary research. This observation prompts profound philosophical questions, such as why neuroscientists might favour Random Forest algorithms whilst researchers in psychiatry or mental health predominantly employ Logistic Regressions. This application engages with discussions from Breiman (2001), Bzdok, Altman, and Krzywinski (2018), and Andrews (2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Directions",
    "text": "4.4 Limitations and Future Directions\n\n\n\nSlide 21\n\n\nDespite its considerable utility, OpenAlex Mapper operates with several acknowledged limitations. Foremost amongst these is the inherent quality of the OpenAlex database itself. Whilst not flawless, its data quality remains commendably reasonable when compared to alternative scholarly data sources.\nA significant constraint currently stems from the language model’s scope: it processes English-language content exclusively. This naturally limits the tool’s overall reach; nevertheless, for investigations focusing on the more recent history of science, this presents a less severe impediment. Developers recognise the potential for future enhancement through the integration of multilingual models, though they note the current scarcity of robust, science-trained multilingual models.\nFurthermore, the embedding process necessitates source data that includes either comprehensive abstracts or sufficiently descriptive titles. This requirement inherently restricts the range of documents the tool can effectively analyse.\nFinally, the Uniform Manifold Approximation and Projection (UMAP) algorithm, central to the tool’s visualisation, introduces its own set of challenges. As a stochastic algorithm, UMAP generates one specific output from a multitude of possibilities, implying that alternative visualisations could emerge from repeated runs. Moreover, the algorithm must inevitably make trade-offs during dimensionality reduction. Compressing the Specter language model’s 768 dimensions into a mere two introduces unavoidable distortions, manifesting as “pushing, pulling, and misaligning” of data points within the two-dimensional space.\nFor those seeking further information, the presentation slides remain accessible online at maxnoichl.eu/talk. Additionally, a comprehensive working paper, titled “Philosophy at Scale: Introducing OpenAlex Mapper,” provides more detailed technical explanations of the system’s architecture and methodology.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nResearchers at Uppsala University have pioneered a procedural investigation into genre classification for historical medical periodicals. This endeavour forms a core component of the ERC-funded ActDisease project, which meticulously examines the history of patient organisations in 20th-century Europe. The European Research Council (ERC) provides funding for this initiative. The project’s central aim involves scrutinising the profound influence of these organisations on disease concepts, illness experiences, and prevailing medical practices. Primarily, the project leverages a private, recently digitised collection of patient organisation magazines from Sweden, Germany, France, and Great Britain, encompassing an impressive 96,186 pages published between approximately 1890 and 1990.\nDigitisation efforts, employing ABBYY FineReader Server 14, successfully processed most common layouts. Nevertheless, complex layouts, slanted text, and rare fonts posed persistent challenges, frequently leading to Optical Character Recognition (OCR) errors, particularly in German and French texts. Recognising the diverse and co-occurring text types within these periodicals, the team identified genre classification as a crucial methodological advancement. This approach directly addresses the limitations and potential biases of traditional topic models and term counts, which often fail to account for the varied communicative purposes embedded within single pages.\nTo address the scarcity of annotated data, the project explored both zero-shot and few-shot learning paradigms. Zero-shot learning involves applying a model to data from categories it has not seen during training, relying on its understanding of the categories’ descriptions. Few-shot learning, conversely, trains a model with only a very small number of examples per category. Researchers defined a bespoke set of nine genre labels—Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and Question and Answer (QA)—under the direct supervision of a specialist historian. Annotation involved six project members, achieving a high inter-annotator agreement of 0.95 Krippendorff’s alpha on paragraphs sampled from Swedish and German periodicals.\nFor zero-shot experiments, the team leveraged publicly available datasets, including the Corpus of Online Registers of English (CORE), Functional Text Dimensions (FTD), and UD-MULTIGENRE (UDM). They performed a rigorous cross-dataset genre mapping to align these external datasets with their custom labels. Multilingual encoder models, specifically XLM-Roberta, mBERT, and historical mBERT, underwent fine-tuning across 48 configurations. Findings indicated that models fine-tuned on FTD performed optimally with the custom mapping, whilst historical mBERT demonstrated particular efficacy in distinguishing between fiction and nonfiction prose in few-shot settings.\nFurthermore, the project investigated few-shot prompting with Llama 3.1 8b Instruct, a large language model (LLM), revealing its capacity to handle certain genre labels effectively. However, a limited number of examples proved insufficient for comprehensive representation across all categories. Ultimately, the research concludes that genre classification significantly enhances the accessibility of historical periodical sources for text mining. Few-shot learning of multilingual encoders, particularly historical mBERT with prior Masked Language Model (MLM) fine-tuning, offers the most robust performance. Ongoing work includes developing a more fine-grained annotation scheme, generating synthetic data, and implementing active learning strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry-into-patient-organisations",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry-into-patient-organisations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 The ActDisease Project: Historical Inquiry into Patient Organisations",
    "text": "5.1 The ActDisease Project: Historical Inquiry into Patient Organisations\n\n\n\nSlide 01\n\n\nResearchers have embarked upon the ActDisease project, formally titled “Acting out Disease: How Patient Organizations Shaped Modern Medicine.” This ERC-funded initiative meticulously investigates the historical trajectory of patient organisations across Europe during the 20th century. Its central purpose involves scrutinising how these organisations fundamentally influenced the evolution of disease concepts, the lived experience of illness, and prevailing medical practices.\nThe project’s scope encompasses ten distinct European patient organisations. It draws its primary source material from their periodicals—predominantly magazines—published in England, Germany, France, and Great Britain between approximately 1890 and 1990. Notably, the Hay Fever Association of Heligoland, established in 1897, exemplifies the type of historical entity under examination, with Heligoland, Germany, serving as its foundational site.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-dataset-periodical-collection-and-scope",
    "href": "chapter_ai-nepi_005.html#the-actdisease-dataset-periodical-collection-and-scope",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Dataset: Periodical Collection and Scope",
    "text": "5.2 The ActDisease Dataset: Periodical Collection and Scope\n\n\n\nSlide 06\n\n\nThe ActDisease project has assembled a private, recently digitised collection of patient organisation magazines, constituting a substantial dataset of 96,186 pages. This extensive archive spans various countries, diseases, and publication periods.\nSpecifically, the dataset includes: * German collection: 10,926 pages on Allergy/Asthma from two magazines (1901-1985); 19,324 pages on Diabetes from one magazine (1931-1990); and 5,646 pages on Multiple Sclerosis from one magazine (1954-1990). * Swedish materials: 4,054 pages on Allergy/Asthma (1957-1990); 7,150 pages on Diabetes (1949-1990); and 16,790 pages on Lung Diseases (1938-1991), each from a single magazine. * French contributions: 6,206 pages on Diabetes (1947-1990); and 9,317 pages on Rheumatism/Paralysis from three magazines (1935-1990). * UK segment: 11,127 pages on Diabetes (1935-1990); and 5,646 pages on Rheumatism (1950-1990), each sourced from one magazine.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-processes-and-persistent-challenges",
    "href": "chapter_ai-nepi_005.html#digitisation-processes-and-persistent-challenges",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Digitisation Processes and Persistent Challenges",
    "text": "5.3 Digitisation Processes and Persistent Challenges\n\n\n\nSlide 07\n\n\nThe digitisation process for the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition (OCR). This tool generally performed well, accurately recognising most common layouts and fonts present in the historical periodicals.\nNevertheless, significant challenges persisted. Complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality frequently hindered optimal OCR performance. Consequently, researchers observed persistent OCR errors, particularly prevalent in German and French texts, alongside instances of disrupted reading order. Notably, creative text segments, including advertisements, humour pages, and poems, exhibited a higher frequency of OCR inaccuracies. To mitigate these issues, the team conducted specific experiments, focusing on post-OCR correction of German texts through the application of instruction-tuned generative models. This work has been documented in a publication by Danilova and Aangenendt, presented at the RESOURCEFUL-2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-for-genre-classification-in-historical-periodicals",
    "href": "chapter_ai-nepi_005.html#rationale-for-genre-classification-in-historical-periodicals",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Rationale for Genre Classification in Historical Periodicals",
    "text": "5.4 Rationale for Genre Classification in Historical Periodicals\n\n\n\nSlide 11\n\n\nResearchers observed a profound diversity in the textual content of the historical periodicals, yet these varied text types consistently appeared across all magazines. Crucially, distinct text types frequently co-occurred on a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This inherent textual complexity posed a significant challenge for conventional analytical methods.\nTraditional yearly and decade-based topic models and term counts, the team realised, failed to account for this side-by-side textual variation. Consequently, these methods likely introduced a bias towards the most frequently occurring text type, potentially distorting analytical outcomes. To overcome this limitation, genre emerged as a highly pertinent concept for distinguishing between text types. Genres inherently align with the communicative purposes of authors, as defined in language technology by Petrenz (2004) and Kessler (1997).\nImplementing genre classification directly supports the project’s core objective: exploring the dataset from multiple perspectives to formulate robust historical arguments. Specifically, this approach enables a nuanced study of communicative strategies as they evolved over time, allowing for comparisons across different countries, diseases, and publications, as highlighted by Broersma (2010). Furthermore, it facilitates a more granular analysis of term distributions and topic models, enabling insights within specific genre groups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Illustrative Genre Examples within the ActDisease Dataset",
    "text": "5.5 Illustrative Genre Examples within the ActDisease Dataset\n\n\n\nSlide 19\n\n\nThe ActDisease dataset encompasses a rich array of genres, each serving distinct communicative functions. Researchers identified examples such as poetry, which often provided emotional engagement. Academic reports, exemplified by studies on the pancreas, conveyed scientific and medical information. Legal documents, including deeds of covenant, established formal agreements and regulations. Advertisements, such as those promoting chocolate for diabetics, aimed to market products or services.\nFurthermore, the collection featured instructive or guidance messages, offering practical advice like recipes, doctor’s recommendations, or dietary guidelines. Patient organisation reports documented internal affairs, detailing meetings and activities. Finally, narratives about patient lives provided first-hand accounts of illness experiences, offering a unique perspective on the human dimension of disease.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#experimental-design-for-genre-classification-with-limited-data",
    "href": "chapter_ai-nepi_005.html#experimental-design-for-genre-classification-with-limited-data",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Experimental Design for Genre Classification with Limited Data",
    "text": "5.6 Experimental Design for Genre Classification with Limited Data\n\n\n\nSlide 04\n\n\nConfronted with a scarcity of annotated data, researchers systematically explored two distinct methodological paradigms: zero-shot learning and few-shot learning. For zero-shot learning, the investigation posed two primary research questions: firstly, whether an efficient mapping of genre labels from existing public datasets to custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might fluctuate across different datasets and models.\nConversely, the few-shot learning inquiry focused on two further questions: how performance changes in relation to varying training set sizes across different models; and whether a prior fine-tuning process on the entire dataset could significantly boost classification performance. This comprehensive experimental design forms the basis of a forthcoming publication by Danilova and Söderfeldt, scheduled for presentation at the LaTeCH-CLFL 2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels-for-historical-periodical-content",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels-for-historical-periodical-content",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Defining Genre Labels for Historical Periodical Content",
    "text": "5.7 Defining Genre Labels for Historical Periodical Content\n\n\n\nSlide 32\n\n\nResearchers meticulously defined the genre labels under the direct supervision of the project’s lead historian, an expert in patient organisations. This collaborative process aimed to create categories that would effectively segment the content within the historical periodicals, thereby facilitating deeper historical analysis. Crucially, the team endeavoured to formulate these labels with sufficient generality to enable the classifier’s application to comparable datasets in the future.\nNine distinct genres emerged from this process, each with a precise definition: * Academic: Encompasses research-based reports or scientific explanations, designed to bridge the gap between the scientific medical community and the magazine’s readership. * Administrative: Documents organisational activities, reporting on patient organisation events and internal affairs. * Advertisement: Specifically promotes commercial products or services. * Guide: Provides step-by-step instructions, ranging from health tips to recipes. * Fiction: Aims to entertain and emotionally engage through stories, poems, or humour. * Legal: Explains terms, conditions, or contracts. * News: Reports on recent events. * Nonfiction Prose: Narrates real events or describes cultural and historical topics, including memoirs and essays. * QA (Question and Answer): Designates sections structured as questions with expert responses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-protocol-and-inter-annotator-agreement",
    "href": "chapter_ai-nepi_005.html#annotation-protocol-and-inter-annotator-agreement",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Annotation Protocol and Inter-Annotator Agreement",
    "text": "5.8 Annotation Protocol and Inter-Annotator Agreement\n\n\n\nSlide 37\n\n\nResearchers employed paragraphs as the fundamental annotation unit, extracting them from the ABBYY OCR output. The team subsequently merged these paragraphs based on consistent font patterns—including type, size, italicisation, and the absence of bolding—within each page. Researchers sampled content from two specific periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” focusing on their first and mid-year issues across all publication years.\nSix dedicated project members undertook the annotation task, comprising four historians and two computational linguists. All annotators possessed either native fluency or high proficiency in both Swedish and German. For each paragraph, two independent annotations were meticulously collected. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha, signifying a remarkably high level of consistency amongst the annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Dataset Splitting and Experimental Configurations",
    "text": "5.9 Dataset Splitting and Experimental Configurations\n\n\n\nSlide 39\n\n\nFor the experimental phase, researchers initially partitioned the annotated data into a primary training set of 1182 paragraphs and a held-out set comprising 552 paragraphs, which represented approximately 30% of the total annotated material. Researchers stratified both these sets by genre label to ensure representative distributions.\nWithin the few-shot experiments, the team systematically varied the training set size, employing six distinct configurations: 100, 200, 300, 400, 500, and the full 1182 paragraphs. Each of these smaller training sets was randomly sampled from the main training pool, whilst maintaining a balance across genre labels. The team subsequently divided the held-out set equally into validation and test portions, similarly balanced by label. Notably, researchers excluded the Legal and News genres from these few-shot experiments, as their limited data volume precluded sufficient training. Conversely, the zero-shot experiments leveraged the entirety of the held-out test set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease-training-and-held-out-sets",
    "href": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease-training-and-held-out-sets",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Genre Distribution within ActDisease Training and Held-Out Sets",
    "text": "5.10 Genre Distribution within ActDisease Training and Held-Out Sets\n\n\n\nSlide 39\n\n\nAnalysis of the genre distribution across both the training and held-out samples of the ActDisease dataset revealed a pronounced imbalance. Specifically, the Advertisement and Non-fictional Prose genres exhibited significant disparities in their representation across different languages. This imbalance necessitates careful consideration during model training and evaluation to prevent potential biases.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#leveraging-external-datasets-for-zero-shot-classification",
    "href": "chapter_ai-nepi_005.html#leveraging-external-datasets-for-zero-shot-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Leveraging External Datasets for Zero-Shot Classification",
    "text": "5.11 Leveraging External Datasets for Zero-Shot Classification\n\n\n\nSlide 39\n\n\nFor the zero-shot experiments, researchers incorporated external, modern datasets previously utilised in automatic web genre classification. The Corpus of Online Registers of English (CORE), developed by Egbert et al. (2015), provides document-level annotations. It encompasses English content, with main categories also available in Swedish, Finnish, and French.\nSimilarly, Sharoff’s (2018) Functional Text Dimensions (FTD) dataset, also annotated at the document level, offers balanced content in English and Russian. Kuzman et al. (2023) had previously leveraged this dataset for web genre classification. Additionally, the team employed UD-MULTIGENRE (UDM), a subset of Universal Dependencies (de Marneffe et al., 2021), which features genre annotations at the sentence level across 38 languages, as detailed by Danilova and Stymne (2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping-and-alignment",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping-and-alignment",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Cross-Dataset Genre Mapping and Alignment",
    "text": "5.12 Cross-Dataset Genre Mapping and Alignment\n\n\n\nSlide 39\n\n\nResearchers meticulously performed genre mapping across datasets, with two independent annotators undertaking the task. The team only included assignments achieving full agreement in the final mapping, ensuring robust alignment.\nThis process established correspondences between ActDisease genres and their equivalents in CORE, UDM, and FTD. For instance, “Academic” in ActDisease mapped to “research article” (RA) in CORE, “academic” in UDM, and “academic (A14)” in FTD. “Advertisement” aligned with “advertisement (AD)” in CORE, “description with intent to sell (DS)” in UDM, and “commercial (A12)” in FTD. Similarly, “Fiction” found its counterparts in “poem” (PO) and “short story” (SS) in CORE, “fiction” in UDM, and “fictive (A4)” and “poetic (A19)” in FTD. However, the team encountered a limitation: some ActDisease genres lacked suitable corresponding labels within the available external datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-generation-and-multilingual-encoder-models",
    "href": "chapter_ai-nepi_005.html#training-data-generation-and-multilingual-encoder-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Training Data Generation and Multilingual Encoder Models",
    "text": "5.13 Training Data Generation and Multilingual Encoder Models\n\n\n\nSlide 39\n\n\nResearchers generated training data through a meticulous pipeline encompassing mapping, preprocessing, chunking, and systematic sampling. They configured training sets in four distinct ways for each dataset: one configuration ([G+]) focused exclusively on Germanic languages; another ([B1]) balanced data according to ActDisease labels; a third ([G-]) incorporated all language families; and the final configuration ([B2]) balanced data by both ActDisease and original labels. This yielded four FTD, four CORE, four UDM*, and four merged training samples, all subjected to fine-tuning.\nFor classification, the team employed several multilingual encoder models, which are transformer-based neural networks capable of processing text in multiple languages. XLM-Roberta, developed by Conneau et al. (2020), served as a state-of-the-art web genre classifier, as noted by Kuzman et al. (2023). mBERT (Devlin et al., 2019) provided a baseline for comparison with historical mBERT (Schweter et al., 2022). Crucially, historical mBERT, pretrained on an extensive corpus of multilingual historical newspapers, proved particularly relevant given its inclusion of the target languages. These BERT-like models have consistently demonstrated efficacy in prior web register and genre classification studies, as evidenced by Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). Ultimately, the fine-tuning process generated 48 distinct models, and the team subsequently averaged performance metrics across all configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-and-performance-analysis",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-and-performance-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Zero-Shot Learning Evaluation and Performance Analysis",
    "text": "5.14 Zero-Shot Learning Evaluation and Performance Analysis\n\n\n\nSlide 11\n\n\nEvaluating the zero-shot predictions presented a unique challenge: the imperfect overlap of label sets prevented direct comparison of overall performance metrics. Consequently, researchers meticulously assessed the performance of each genre individually, complementing this analysis with a thorough examination of confusion matrices to mitigate potential biases. The X-GENRE web genre classifier, as detailed by Kuzman et al. (2023), served as a robust baseline, with predictions focusing exclusively on the most similar labels directly mappable to the ActDisease genres.\nThe experimental setup often involved a cross-lingual context. FTD and X-GENRE, for instance, operated without German or Swedish data, whilst UDM and CORE datasets exhibited partially cross-lingual characteristics. Overall, models fine-tuned on FTD consistently demonstrated superior performance when integrated with the ActDisease mapping. Conversely, other datasets revealed distinct class-specific biases.\nUDM, for example, exhibited a bias towards news, primarily because its news training data contained the highest proportion of Germanic instances, overwhelmingly German. Similarly, CORE displayed a bias towards the guide genre, as its training data for this category was uniquely multilingual.\nIntriguingly, certain models excelled in specific genre predictions. XLM-Roberta, when fine-tuned on UDM, achieved an average of 32% more correct predictions in the QA genre compared to mBERT and hmBERT. Conversely, hmBERT, also on UDM, produced an average of 16% more accurate predictions in the Administrative genre than XLM-Roberta and mBERT. Furthermore, CORE-based models consistently proved proficient at predicting the legal genre. Confusion matrices visually underscored these observed behavioural patterns, whilst detailed average per-category F1 scores provided a comprehensive quantitative assessment across all data configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-model-advantages",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-model-advantages",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 Few-Shot Learning Performance and Model Advantages",
    "text": "5.15 Few-Shot Learning Performance and Model Advantages\n\n\n\nSlide 39\n\n\nFew-shot learning experiments unequivocally demonstrated the advantage of further training models on the ActDisease dataset, particularly when incorporating Masked Language Model (MLM) fine-tuning. The F1 score consistently improved as the number of training instances increased, though it remained below 0.8 even with the full training set of 1182 instances.\nAmongst the models tested, hmBERT-MLM consistently outperformed its counterparts. A detailed examination of its performance revealed that, unlike other models, hmBERT-MLM retained its capacity to differentiate between fiction and nonfiction genres even when exposed to the full dataset. Conversely, other models, notably XLM-Roberta, exhibited a drastic decline in their ability to distinguish these two categories.\nAnalysis of XLM-Roberta’s confusion matrix, when fine-tuned with MLM on the full dataset, indicated a frequent overprediction of nonfiction prose for fiction. This phenomenon likely stems from the shared thematic content within the ActDisease data, where both fictional and autobiographical narratives often revolve around patient experiences, leading to similar themes and narrative structures. Consequently, researchers propose that an increased volume of data is essential to enhance performance in distinguishing these increasingly similar genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Few-Shot Prompting with Llama-3.1 8b Instruct",
    "text": "5.16 Few-Shot Prompting with Llama-3.1 8b Instruct\n\n\n\nSlide 39\n\n\nGiven the current insufficiency of data for comprehensive instruction tuning, researchers opted to evaluate few-shot prompting using Llama 3.1 8b Instruct, a widely recognised multilingual generative model with open weights. The prompt structure provided clear genre definitions, complemented by two or three illustrative examples for each category.\nThe results indicated that the model handled certain genre labels with reasonable efficacy. For instance, it achieved an F1-score of 0.84 for Legal content and 0.72 for Academic texts. However, the limited number of examples proved insufficient for robust representation across all genres. Notably, nonfictional prose yielded a lower F1-score of 0.49, whilst advertisement and administrative content also demonstrated suboptimal performance, with F1-scores of 0.73 and 0.60 respectively. The overall accuracy stood at 0.62, with a macro average F1-score of 0.59 and a weighted average of 0.63.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#key-findings-and-strategic-recommendations",
    "href": "chapter_ai-nepi_005.html#key-findings-and-strategic-recommendations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Key Findings and Strategic Recommendations",
    "text": "5.17 Key Findings and Strategic Recommendations\n\n\n\nSlide 39\n\n\nPopular magazines, unlike more specialised scientific journals or books, frequently encompass a multitude of genres. This characteristic significantly complicates text mining efforts. Researchers have concluded that genres inherently reflect deliberate choices in communicative strategies; consequently, accounting for these distinctions, whilst challenging, proves crucial for achieving accurate and detailed interpretations of text mining outcomes. Fundamentally, genre classification renders these rich historical sources accessible for advanced text mining.\nFor scenarios lacking dedicated training data, two viable strategies emerge. Firstly, one can successfully leverage existing modern datasets, provided the target categories maintain a general purpose. Alternatively, few-shot instruction of a proficient generative model offers another effective pathway.\nHowever, when some training data is available, few-shot learning of multilingual encoders, particularly those with prior Masked Language Model (MLM) fine-tuning—such as XLM-Roberta or historical multilingual BERT—demonstrates superior performance. Indeed, this latter approach emerged as the optimal solution for the project. Notably, historical multilingual BERT exhibited particularly strong gains, achieving a 24% improvement, which significantly surpassed the 14.5% gain for mBERT-MLM and 16.9% for XLM-RoBERTa. These findings underscore the value of domain-specific pre-training for historical text analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Ongoing and Future Research Directions",
    "text": "5.18 Ongoing and Future Research Directions\n\n\n\nSlide 39\n\n\nThe research team is actively pursuing several avenues to enhance the quality and scope of this work for both the project and the wider academic community. Currently, researchers are engaging with specific historical hypotheses, leveraging the insights gained from genre classification. Furthermore, they are developing a new, more fine-grained annotation scheme for genres, a project notably financed by Swe-CLARIN. Methodologically, the team is exploring advanced techniques, including synthetic data generation and active learning, to further refine their classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#acknowledgements",
    "href": "chapter_ai-nepi_005.html#acknowledgements",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Acknowledgements",
    "text": "5.19 Acknowledgements\n\n\n\nSlide 19\n\n\nThe project gratefully acknowledges the invaluable contributions of its annotators and core team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. Funding for this research came generously from the European Research Council under grant ERC-2021-STG 10104099. The Centre for Digital Humanities and Social Sciences offered crucial institutional support, supplying essential GPUs and data storage facilities. Finally, the team extends its gratitude to the diligent reviewers, Dr Maria Skeppstedt and other anonymous contributors, whose feedback significantly enhanced the work. Further details are available on the project website.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC (European Research Council) Starting Grant initiative, commenced in 2023 and is set to conclude in 2028. Researchers at the Vrije Universiteit Brussel (VUB) lead this ambitious undertaking, which aims to trace the profound influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition on the development of natural philosophy and science. A dedicated team of five, comprising Professor Cornelis J. Schilt (Principal Investigator), Dr. Eszter Kovács, Dr. Jeffrey Wolf, Niccolò Cantoni, and Demetrios Paraschos, drives this interdisciplinary effort, combining rigorous historical scholarship with advanced digital humanities methodologies.\nThe project identifies key texts embodying this tradition, such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence confirms the tradition’s significant impact on prominent figures; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum [Citation Needed for Historical Claims]. Beyond these well-known examples, the VERITRACE team aims to uncover a broader, often overlooked network of texts and authors, which scholars collectively term the ‘great unread’ [Citation Needed for “great unread” coinage].\nTo achieve this ambitious goal, the project pioneers a computational approach to the history and philosophy of science (HPSS). This involves large-scale multilingual exploration, employing both keyword search and sophisticated text matching techniques. The methodology seeks to identify both direct lexical reuse (e.g., direct quotations) and indirect semantic influence (e.g., paraphrases or subtle allusions) across a vast corpus. Essentially, the system functions as an ‘early modern plagiarism detector’, designed to reveal hidden networks of texts, passages, themes, topics, and authors, thereby potentially uncovering novel patterns in intellectual history.\nThe project’s substantial dataset comprises approximately 430,000 printed texts, spanning nearly two centuries from 1540 to 1728. These digital texts originate from three primary multilingual sources—Early English Books Online (EEBO), Gallica (from the French National Library), and the Bavarian State Library—encompassing at least six languages. Researchers apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to analyse this extensive corpus.\nThe team confronts several significant challenges inherent in processing historical texts at scale. Notably, the variable quality of Optical Character Recognition (OCR) text poses a primary concern. Libraries provide this raw text directly in formats such as XML, HOCR, and HTML, often without corresponding page images. Early modern typography and the evolving semantics across multiple languages further complicate processing. Moreover, the sheer volume of data—hundreds of thousands of texts printed across Europe—presents a formidable computational hurdle.\nThe project strategically leverages Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as ‘judges’ for enriching and cleaning metadata. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages, facilitating advanced text matching. An alpha version of the VERITRACE web application currently serves as an internal testing ground, showcasing the project’s ambitious capabilities. This application features a complex 15-stage data processing pipeline, transforming raw text into an Elasticsearch database. It offers functionalities for exploring corpus statistics, examining rich metadata, conducting advanced keyword searches, and, crucially, performing lexical and semantic text matching. Whilst the current BERT-based embedding model (LaBSE) demonstrates promise for semantic comparisons, particularly across languages, its limitations with historical and out-of-domain data necessitate further investigation into alternative models or fine-tuning strategies. Scaling the system to accommodate the entire corpus and managing the evolving semantics of historical languages remain key issues for future development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-team-and-scope",
    "href": "chapter_ai-nepi_006.html#project-team-and-scope",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 Project Team and Scope",
    "text": "6.1 Project Team and Scope\n\n\n\nSlide 02\n\n\nThe VERITRACE project, formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,” constitutes a five-year ERC Starting Grant initiative. Active from 2023 to 2028, this significant undertaking, bearing the grant number 101076836, operates from the Vrije Universiteit Brussel (VUB).\nProfessor Cornelis J. Schilt serves as the project’s Principal Investigator, leading a dedicated team of five scholars. The core team comprises Dr. Eszter Kovács, Niccolò Cantoni, and Demetrios Paraschos, alongside Dr. Jeffrey Wolf. Dr. Wolf, a historian specialising in science and medicine with an eighteenth-century focus, specifically contributes his expertise in digital humanities to the project. Whilst the team bases its operations in Brussels, individual members maintain residences in various locations, including Berlin. Further information regarding the project’s scope and progress is accessible on its official website, veritrace.eu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#ancient-wisdom-in-early-modern-thought",
    "href": "chapter_ai-nepi_006.html#ancient-wisdom-in-early-modern-thought",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 Ancient Wisdom in Early Modern Thought",
    "text": "6.2 Ancient Wisdom in Early Modern Thought\n\n\n\nSlide 01\n\n\nAt its core, the VERITRACE project endeavours to trace the profound influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia. This tradition significantly impacted the evolution of natural philosophy and science during the early modern period. It manifests in various foundational works, including the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for scholars of chemistry’s history, the Corpus Hermeticum.\nResearchers assembled a close-reading corpus of 140 works, each embodying this ancient wisdom tradition. Historical evidence already confirms its significant impact; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum [Citation Needed for Newton/Kepler claims]. Beyond these established connections, the project seeks to delve deeper, aiming to uncover a far broader array of networks and texts that interacted with this tradition. One scholar aptly termed this extensive, often neglected body of work the ‘great unread’ [Citation Needed for “great unread” coinage], as it frequently comprises numerous texts by lesser-known authors, rarely forming the primary focus of historical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-historical-inquiry",
    "href": "chapter_ai-nepi_006.html#computational-historical-inquiry",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Historical Inquiry",
    "text": "6.3 Computational Historical Inquiry\n\n\n\nSlide 04\n\n\nTo address its central research questions, the VERITRACE project employs a robust computational framework, enabling large-scale multilingual exploration. A primary objective involves identifying textual reuse across a vast, multilingual corpus. This encompasses both direct lexical reuse, where authors incorporate direct, potentially uncited, quotations, and more indirect semantic reuse, involving paraphrases or subtle allusions that contemporary readers would have recognised as originating from specific works, such as the Corpus Hermeticum.\nEssentially, the system functions as an ‘early modern plagiarism detector’, designed to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this systematic analysis, researchers anticipate identifying novel patterns within the intellectual history and philosophy of science. The project leverages specific tools, including advanced keyword search capabilities and sophisticated text matching algorithms, to facilitate these investigations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-construction-and-sources",
    "href": "chapter_ai-nepi_006.html#corpus-construction-and-sources",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Corpus Construction and Sources",
    "text": "6.4 Corpus Construction and Sources\n\n\n\nSlide 04\n\n\nThe project meticulously assembled a large and diverse multilingual dataset, focusing exclusively on printed works to ensure manageability, thereby excluding handwritten materials. This extensive corpus spans approximately two centuries, from 1540 to 1728. The year 1540 was chosen as a starting point for various historical reasons, including the increasing prevalence of printing and the burgeoning intellectual movements of the Renaissance, whilst 1728 falls shortly after Isaac Newton’s death, marking a significant shift in scientific thought. The dataset incorporates texts in at least six different languages.\nThree primary digital repositories constitute the main data sources: Early English Books Online (EEBO), Gallica (which provides access to materials from the French National Library), and the Bavarian State Library. Collectively, these sources yield approximately 430,000 books for analysis. Researchers intend to apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to explore this rich historical collection.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-and-llm-applications",
    "href": "chapter_ai-nepi_006.html#challenges-and-llm-applications",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Challenges and LLM Applications",
    "text": "6.5 Challenges and LLM Applications\n\n\n\nSlide 06\n\n\nThe VERITRACE project navigates several core challenges inherent in processing historical texts at scale. A primary concern stems from the variable quality of Optical Character Recognition (OCR) text. Libraries provide this raw text directly in formats such as XML, HOCR, and HTML, frequently without accompanying ground truth page images. This raw input significantly affects all subsequent data processing stages.\nFurthermore, early modern typography and the evolving semantics across at least six distinct languages introduce considerable complexity. The sheer volume of data—hundreds of thousands of texts published across Europe over two centuries—also presents a substantial logistical and computational challenge.\nThe project strategically employs Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as ‘judges’, assisting in the enrichment and cleaning of metadata. This application, whilst promising for automating tedious tasks, currently faces challenges such as output hallucinations (fabricating information not present in the source data) and a tendency towards generic responses when structured output is requested. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, a crucial step for facilitating the project’s advanced text matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-a-case-study",
    "href": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-a-case-study",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 LLMs for Metadata Enrichment: A Case Study",
    "text": "6.6 LLMs for Metadata Enrichment: A Case Study\n\n\n\nSlide 08\n\n\nA significant internal case study explores the application of LLMs to enrich VERITRACE metadata, aiming to automate the laborious process of bibliographic record comparison. The core motivation involves mapping VERITRACE records to the Universal Short Title Catalogue (USTC), a high-quality metadata source, to create enriched records that require less manual cleaning. Whilst external identifiers facilitate some automated mapping, the majority of records necessitate manual review due to the uncleaned state of the VERITRACE data. This manual process proved exceptionally tedious, with each team member assigned 10,000 pairs of bibliographic records for comparison, determining if they represented the same underlying printed text.\nTo alleviate this burden, researchers devised an LLM-based solution, conceptualised as a ‘bench’ or ‘panel of judges’. This system employs a chain of LLMs—including Primary, Secondary, and Tiebreaker models, with an Expert LLM handling edge cases—to evaluate pairs of bibliographic records. The LLMs are tasked with judging whether records from a low-quality source and a high-quality source represent the identical underlying text. Crucially, the models must provide both their decision (match or no match) and accompanying reasoning with confidence levels. The team then compares these LLM decisions against ground truth data, followed by a final review by VERITRACE scholars.\nCurrently, the project utilises open-source LLM models, such as Llama, for this task. However, significant challenges persist. The models frequently exhibit hallucinations in their output, fabricating records not present in the input. Furthermore, whilst requesting more structured output aims to reduce unhelpful responses, it often results in more generic and less insightful reasoning. Achieving the optimal balance between structured output and helpfulness remains an ongoing challenge, which scholars describe as more ‘art’ than science, given the iterative refinement required. Despite these hurdles, the approach holds substantial theoretical potential for significant time savings, though it has not yet achieved full operational efficacy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-pipeline",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-pipeline",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 VERITRACE Web Application: Alpha Version and Pipeline",
    "text": "6.7 VERITRACE Web Application: Alpha Version and Pipeline\n\n\n\nSlide 13\n\n\nThe VERITRACE project developed an alpha version of its web application, currently in its nascent stages and remaining internally accessible. This internal prototype serves as a tangible demonstration of the project’s ambitious future capabilities. Engineers are presently testing a BERT-based Large Language Model, specifically LaBSE, to generate vector embeddings for every passage within the textual corpus. Whilst this model demonstrates functionality in certain scenarios, preliminary assessments suggest it may not ultimately suffice for the project’s comprehensive requirements, particularly concerning historical linguistic nuances.\nUnderpinning this application lies a complex 15-stage data processing pipeline. This pipeline meticulously transforms raw text, which libraries supply in various formats including XML, HOCR, and HTML, into a structured Elasticsearch database, serving as the web application’s backend. The pipeline encompasses numerous critical stages, such as extracting text into standardised files, generating precise mappings of textual positions, segmenting the content, and rigorously assessing the OCR quality of each input. The generation of vector embeddings occurs towards the latter stages of this intricate process. Crucially, each of these 15 stages demands individual optimisation to ensure the integrity and efficiency of the entire workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-exploration-and-metadata",
    "href": "chapter_ai-nepi_006.html#web-application-features-exploration-and-metadata",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Web Application Features: Exploration and Metadata",
    "text": "6.8 Web Application Features: Exploration and Metadata\n\n\n\nSlide 15\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Metadata Explorer, Search, Analyse, Read, and Match. The Explore section serves as an initial entry point, offering comprehensive statistics about the corpus. This data, drawn directly from a Mongo database, provides an overview of the collection, including the total count of 427,305 metadata records. Visualisations such as pie charts and bar charts illustrate language distribution, documents by data source, documents by decade, and publication places, enabling users to gain immediate insights into the corpus’s composition.\nBeyond this statistical overview, the Metadata Explorer section allows users to delve into the rich metadata associated with each individual text. This includes detailed fields such as Document ID, Filename, Bibliographic Title, Author, Publication Place, Printer, Format, Language, and Subject. A crucial feature involves granular language identification, performed on every text down to approximately 50 characters. This addresses the prevalent multilingual nature of early modern works, ensuring accurate language representation beyond simple metadata declarations. For instance, a text might reveal a substantive multilingual composition, such as 15% Greek and 85% Latin. Furthermore, the system attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-search-and-analysis",
    "href": "chapter_ai-nepi_006.html#web-application-features-search-and-analysis",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Web Application Features: Search and Analysis",
    "text": "6.9 Web Application Features: Search and Analysis\n\n\n\nSlide 17\n\n\nFor most scholarly users, the Search section will likely serve as the primary interface, offering robust keyword search capabilities. For example, a simple query for “Hermes” within the current prototype corpus, which comprises 132 files, yields 22 documents containing 332 matches. Notably, even this limited prototype generates a 15 GB index, indicating that the full 400,000-text corpus will necessitate terabytes of storage.\nLeveraging Elasticsearch, the system supports highly sophisticated queries beyond basic keywords. Users can perform field-specific searches, such as retrieving all books by Kepler that contain “Hermes.” Furthermore, the platform accommodates complex boolean logic (ANDs, ORs), nested queries, and proximity searches, allowing users to specify, for instance, texts where “Hermes” and “Plato” appear within ten words of each other.\nWhilst not yet implemented, the Analyse section of the website is poised to offer advanced analytical tools. These will include Topic Modelling, Latent Semantic Analysis (LSA), and Diachronic Analysis, enabling scholars to explore thematic shifts and conceptual relationships over time.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-reading-and-matching",
    "href": "chapter_ai-nepi_006.html#web-application-features-reading-and-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 Web Application Features: Reading and Matching",
    "text": "6.10 Web Application Features: Reading and Matching\n\n\n\nSlide 19\n\n\nThe web application incorporates a dedicated Read section, providing scholars with access to high-quality digital facsimiles of the texts. Utilising a Mirador viewer, users can engage with PDF versions of each document, complemented by the display of associated metadata.\nCrucially, the Match section facilitates the identification of textual reuse across the corpus. This powerful tool supports various comparison modes: users can compare a single document against another, perform multi-document comparisons (e.g., analysing textual overlap across all of Kepler’s works within the database), or even compare a single text against the entire corpus. The latter, whilst highly desirable, presents significant computational challenges regarding user wait times. The interface exposes numerous parameters for user customisation, such as minimum similarity scores, allowing scholars to fine-tune their searches.\nThe system offers two fundamental match types. Lexical matching employs keyword analysis to identify vocabulary similarities, proving ineffective for cross-language comparisons due to differing word forms. Conversely, semantic matching leverages vector embeddings to discover conceptually similar passages, irrespective of shared vocabulary. This approach relies on a BERT-based multilingual embeddings model, trained on 109 languages, which encodes passages into a unified vector space, thereby enabling seamless cross-language comparisons. A hybrid matching option also exists, combining both lexical and semantic approaches with adjustable weights. Furthermore, users can select from different matching modes: a standard mode, a comprehensive mode that demands more computational power for exhaustive results, and a faster mode for quicker, though potentially less complete, outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-newtons-opticks-case-study",
    "href": "chapter_ai-nepi_006.html#text-matching-newtons-opticks-case-study",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Text Matching: Newton’s Opticks Case Study",
    "text": "6.11 Text Matching: Newton’s Opticks Case Study\n\n\n\nSlide 16\n\n\nA compelling case study involves comparing Isaac Newton’s Latin edition of Optice (1719) with its English counterpart, Opticks (1718). When performing a lexical match between these two texts, the standard mode yields no significant results, precisely as anticipated given their differing languages. However, employing the comprehensive mode reveals three matches, likely corresponding to English text embedded within the Latin edition, such as a preface or annotations.\nConversely, a semantic match between these translated works produces reasonable results, despite existing OCR issues. Passages demonstrate clear conceptual similarity, for instance, parallel discussions on colours. The system provides detailed match summary metrics, including a high quality score of 91.2%. Nevertheless, the coverage score registers at a comparatively low 36.9%. This low coverage, whilst initially appearing problematic, actually provides valuable insight: the Latin edition is considerably longer and exhibits notable divergences from the English version, suggesting the metric accurately reflects the textual relationship rather than an error in the system. The interface for lexical matches further enhances usability by automatically highlighting matching terms within both the source and comparison passages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-horizons",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-horizons",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Future Challenges and Horizons",
    "text": "6.12 Future Challenges and Horizons\n\n\n\nSlide 21\n\n\nThe VERITRACE project faces several critical challenges as it progresses towards full implementation. The current BERT-based embedding model, LaBSE, whilst a valuable starting point, is likely insufficient for the project’s comprehensive needs, primarily due to inherent trade-offs between accuracy, storage requirements, and inference time. Researchers are actively exploring alternative models such as XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting its own set of compromises regarding performance and suitability for historical data. A fundamental question arises: given the distinct nature of the historical corpus, is fine-tuning a base model on this specific dataset essential to achieve adequate results, or can off-the-shelf models suffice?\nA further complexity involves the evolution of semantic meaning over time. The project must address how to accurately handle semantic shifts across centuries, particularly when comparing texts published in 1540 with those from 1700, often in different languages. This raises a crucial query: do texts from disparate historical periods truly reside within the same vector space when processed by modern models, or do historical linguistic changes necessitate specialised approaches?\nThe pervasive issue of poor OCR quality also impacts every downstream process, fundamentally hindering accurate segmentation into sentences and passages. Re-OCR of the entire corpus is not feasible due to its immense size; therefore, the team must consider re-OCR for only the very poorest quality texts or investing time to locate existing high-quality versions. Finally, scaling and performance present a significant hurdle. Current queries on a mere 132 texts require approximately 15 seconds. Scaling this to the full corpus of 430,000 texts will undoubtedly introduce substantial performance issues, necessitating considerable computational power and innovative solutions to maintain acceptable query times. The project actively welcomes external advice on these multifaceted challenges, as it strives to unlock unprecedented insights into early modern intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis chapter explores the critical domains of Explainable AI (XAI) and the application of AI-based scientific insights within the humanities. Initially, the discourse establishes the foundational principles of XAI, particularly its evolution from feature attribution in classification models to addressing the complexities of generative AI. We underscore the necessity of understanding model predictions, identifying biases, and ensuring regulatory compliance. Furthermore, we meticulously detail various orders of interpretability, progressing from first-order attributions, such as heatmaps, to more intricate second and higher-order interactions, including those within graph structures.\nSubsequently, the chapter transitions to practical applications, showcasing how these advanced AI and XAI methodologies facilitate novel research in the humanities. Specific case studies illustrate the extraction of visual definitions from historical corpora and the large-scale analysis of early modern astronomical tables. A significant workflow, termed XAI-Historian, empowers historians to generate data-driven hypotheses and discover new insights. This system employs specialised statistical models to derive bigram representations from challenging, out-of-domain historical data, enabling robust analysis. Crucially, applying cluster entropy analysis reveals patterns of innovation spread across historical European publishing centres, identifying anomalies such as the politically controlled print programme in Wittenberg. The chapter concludes by acknowledging the inherent challenges in applying AI to heterogeneous, low-resource humanities data whilst underscoring the transformative potential of multimodal approaches and explainable machine learning for scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI (XAI) 1.0: Feature Attributions",
    "text": "7.1 Explainable AI (XAI) 1.0: Feature Attributions\n\n\n\nSlide 03\n\n\nExplainable AI (XAI) encompasses methods and approaches meticulously developed for deciphering the internal workings of highly complex machine learning models. Historically, machine learning predominantly focused on visual data; interest in language, whilst present, gained significant momentum only in recent years. Typically, a “Black Box AI System” receives an input, such as an image, and subsequently generates a prediction, for instance, identifying a “Rooster”. Crucially, users often lack insight into the underlying basis for such classifications.\nTo address this opacity, researchers pioneered Post-Hoc Explainability techniques. Heatmaps, for example, visually delineate the specific pixels or features that primarily contributed to a given prediction. In the rooster example, a heatmap would highlight the bird’s head, clearly indicating the model’s focus. Beyond mere transparency, the broader rationale for explainability spans several critical objectives. Firstly, XAI enables verification of predictions, ensuring the model operates logically and produces reasonable outcomes. Secondly, it facilitates the identification of flaws and biases, offering insights into how models make mistakes. Thirdly, it serves as a tool for learning about the underlying problem itself, as models occasionally uncover surprising and unconventional solutions. Finally, and increasingly vital, explainability ensures compliance with evolving legislation, such as the European AI Act. Samek et al. (2017) provided foundational work in this domain.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0-challenges",
    "href": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0-challenges",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Generative AI and XAI 2.0 Challenges",
    "text": "7.2 Generative AI and XAI 2.0 Challenges\n\n\n\nSlide 03\n\n\nThe artificial intelligence landscape has profoundly shifted from conventional classification models to the era of Generative AI (Gen AI). These advanced models now exhibit multifaceted capabilities, encompassing not only classification but also the retrieval of similar images, the generation of novel images, and comprehensive question-and-answer functionalities across diverse topics. Consequently, grounding a prediction or an answer from a Large Language Model (LLM) system to its specific input has become considerably more challenging. Researchers are therefore exploring new directions for XAI, moving beyond simple heatmap representations to consider intricate feature interactions and adopt a more mechanistic view of model operations—that is, understanding the specific internal computations that lead to an output. These contemporary foundation models function as both multi-task and world models, offering profound insights into societal structures and the evolution of text over time.\nNevertheless, these sophisticated models can still exhibit surprising errors. A well-known example from object classification illustrates this: a standard classifier predicted a boat based on the surrounding water, a correlated and texturally simpler feature, rather than the boat itself. Lapuschkin et al. (Nat Commun ’19) documented this phenomenon. More recently, Mondal & Webb et al. (arxiv ’24) highlighted multi-step planning mistakes in LLMs. When tasked with the Tower of Hanoi puzzle, for instance, an LLM might immediately attempt to move the largest, inaccessible disc, demonstrating a fundamental misunderstanding of the problem’s physical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-first-order-attributions",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-first-order-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Structured Interpretability: First-Order Attributions",
    "text": "7.3 Structured Interpretability: First-Order Attributions\n\n\n\nSlide 05\n\n\nStructured interpretability extends the utility of XAI beyond basic visualisations. First-order explanations, for instance, prove particularly effective for elucidating the decisions of classification models. Researchers applied this technique to a classifier designed for historical documents, specifically aiming to distinguish various subgroups of historical tables, such as astronomical or chronological tables.\nTo validate the classifier’s efficacy, teams employed heatmaps, meticulously verifying that predictions relied upon genuinely meaningful features. This analysis revealed that the model correctly focused on the numerical content within the tables. This focus served as an accurate proxy for identifying numerical tables, thereby confirming the model’s meaningful operation and providing confidence in its classifications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-second-and-higher-order-interactions",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-second-and-higher-order-interactions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Structured Interpretability: Second and Higher-Order Interactions",
    "text": "7.4 Structured Interpretability: Second and Higher-Order Interactions\n\n\n\nSlide 10\n\n\nBeyond first-order attributions, researchers have explored more complex forms of structured interpretability, notably second and higher-order interactions. Second-order features primarily focus on pairwise relationships, with similarity proving particularly important. Scientists computed a dot product from the embeddings of two entities, such as images, yielding a similarity score. Subsequently, interaction scores between specific features, like individual digits, elucidated the basis for these similarity predictions, confirming the model’s intended function.\nFurthermore, in more recent work, higher-order interactions have demonstrated greater significance, particularly within graph structures. These structures might represent citation networks, intricate networks of books, or various interconnected entities. When models are trained on classification tasks within such networks, researchers find that feature subgraphs or feature walks—essentially, sets of interconnected features or paths through the graph—become collectively relevant. Identifying these complex interactions facilitates deeper insights into model behaviour, moving towards a more granular, circuit-level understanding of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biases-and-long-range-dependencies",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biases-and-long-range-dependencies",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 First-Order Attributions in LLMs: Biases and Long-Range Dependencies",
    "text": "7.5 First-Order Attributions in LLMs: Biases and Long-Range Dependencies\n\n\n\nSlide 12\n\n\nFirst-order attributions offer crucial insights into the internal workings of Large Language Models (LLMs), particularly concerning biases and their handling of long-range dependencies.\n\n7.5.1 Biased Sentiment Predictions in Transformer LLMs\nResearchers investigated feature importance in LLMs by analysing how specific names influenced sentiment predictions in movie reviews, a common task within the language community. Employing heatmaps generated via a novel method tailored for Transformers, they uncovered notable biases. Male Western names, such as Lee, Barry, Raphael, or the Cohen Brothers, consistently correlated with a higher likelihood of positive sentiment predictions. Conversely, more foreign-sounding names, including Saddam, Castro, or Chan, tended to elicit negative sentiment scores. This demonstrates XAI’s considerable utility in detecting subtle, fine-grained biases embedded within these complex models, a phenomenon now widely recognised within the community. Ali et al. (ICML ‘22) detailed this work in’XAI for Transformers’.\n\n\n7.5.2 First-Order Attributions for Long-Range Dependencies in LLMs\nFurther research explored how LLMs manage long-range dependencies, specifically when generating text summaries from extensive inputs, up to an 8,000-token context window. In a typical scenario involving Wikipedia articles, the model receives a lengthy text and then produces a summary. Analysis revealed that the model predominantly focuses on the latter portions of the provided context, prioritising information presented closer to the prompt. Whilst models can indeed draw upon long-range information from the very beginning of the context, they do so significantly less frequently, as evidenced by a log scale of token counts. Consequently, users should note that LLM-generated summaries may not provide a balanced overview of the entire input text, often emphasising more recently presented data. Jafari et al. (NeurIPS ‘24) presented these findings in’MambaLRP’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "href": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Second & Higher-Order Interactions in Text",
    "text": "7.6 Second & Higher-Order Interactions in Text\n\n\n\nSlide 11\n\n\nExploring second and higher-order interactions offers deeper insights into how models process textual data.\n\n7.6.1 Second-Order Interactions for Text Similarity\nConsider a scenario involving a pair of sentences, processed by a sentence embedding model, such as a Bird model, to yield a similarity score. The challenge lies in comprehending the precise reasons for that score. Second-order explanations address this by providing granular interaction scores between individual tokens. Analysis of these scores frequently reveals noun matching strategies, encompassing both synonyms and identical noun tokens, alongside interactions involving separators and other token types. This suggests that whilst models compress vast amounts of information, they often rely on surprisingly simplistic underlying strategies to achieve their similarity predictions.\n\n\n7.6.2 Graph Neural Networks for Structured Predictions\nGraph Neural Networks (GNNs) offer a powerful framework for structured predictions, providing attributions in terms of “walks” that represent complex feature interactions. Intriguingly, GNNs, which inherently encode structural information, can be conceptualised as LLMs, given that their attention networks facilitate token message passing. This connection enables their application to the analysis of language structure.\n\n\n7.6.3 Interaction of Nodes Learns Complex Language Structure\nResearchers demonstrated this by training a GNN (or an LLM) on a movie review sentiment task, leveraging the hierarchical structure inherent in natural language. They then extracted “walks” to understand the model’s decision-making. First-order attributions proved insufficient, failing to capture the nuanced complexity of language; for instance, the phrase “first I didn’t like the boring pictures” might receive a high positive score solely due to the presence of “like,” neglecting the crucial negation. In stark contrast, higher-order explanations accurately assigned a negative score to the entire negative sentence and correctly captured the hierarchical structure of the subsequent positive statement. Schnake et al. (TPAMI ‘22) published this work in’Higher-Order Explanations of Graph Neural Networks via Relevant Walks’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 AI-based Scientific Insights in the Humanities",
    "text": "7.7 AI-based Scientific Insights in the Humanities\n\n\n\nSlide 20\n\n\nAI-based methodologies offer transformative potential for scientific insights within the humanities, as demonstrated through several compelling examples.\n\n7.7.1 Extracting Visual Definitions from Corpora\nResearchers embarked on a project to extract visual definitions from a corpus of mathematical instruments. Their objective involved classifying these instruments, distinguishing, for instance, between a machine and a purely mathematical instrument. Employing heatmap-based approaches for visual definitions, the team collaborated closely with historians, including Matteo Valeriani and Jochen Büttner. These domain experts provided crucial guidance and meticulously verified the definitions derived from the models. The analysis revealed that fine-grained scales present on the mathematical instruments proved highly relevant for the model’s classification decisions. El-Hajj & Eberle+ (Int J Digit Humanities ’23) published this work on explainability and transparency in digital humanities.\n\n\n7.7.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA larger collaborative project focused on the corpus-level analysis of early modern astronomical tables. This initiative involved the Sphaera Corpus (1472-1650) and the Sacrobosco Table Corpus (1472-1650), collectively comprising 76,000 pages of university textbooks. Historically, these tables, vital carriers of scientific knowledge and indicators of mathematisation processes, had never been analysed at scale. This challenge arose from the data’s extreme heterogeneity, limited annotations, and the inadequacy of conventional Optical Character Recognition (OCR) and foundation models. The primary objective was to develop an automated method for matching tables with similar semantics. Valeriani et al. (2019) and Eberle et al. (2024) detail the foundational corpora.\n\n\n7.7.3 Historical Insights at Scale: XAI-Historian Workflow\nTo address these challenges, researchers developed a comprehensive workflow designed to empower historians with insights at scale, coining the term XAI-Historian. This concept envisions a historian leveraging AI and explainable AI to generate data-driven hypotheses and uncover new case studies. The workflow encompasses three key stages: initial data collections from images of books, followed by an atomisation-recomposition phase involving input tables, bigram maps, and histograms, culminating in corpus-level analysis through historical table embedding and data similarity. Rather than relying on general foundation models, which proved ineffective on this out-of-domain historical data, a specialised statistical model was crafted to detect bigrams. This bespoke model’s reliability was rigorously verified by confirming consistent bigram detection, such as “38” across two distinct inputs, thereby establishing trust in its decisions. Eberle et al. (Sci Adv ’24) and Eberle et al. (TPAMI ’22) document this pioneering work.\n\n\n7.7.4 Cluster Entropy Analysis to Investigate Innovation\nBuilding upon these capabilities, researchers employed cluster entropy analysis to investigate the spread of innovation across European publishing centres during the early modern period. Focusing on the output of specific cities within the Sphaera publication (EPISD-626), they quantified the diversity of each city’s print programme using entropy. A low entropy score indicated a tendency to reproduce identical content, whilst a higher score signified a more diverse programme. This approach, utilising model representations for distance-based clustering and entropy calculation, enabled large-scale analysis previously unattainable. The analysis identified two particularly interesting cases: Frankfurt/Main, which exhibited the lowest entropy, confirming its established reputation as a centre for reprinting editions; and Wittenberg, also displaying remarkably low entropy. This latter finding revealed a historical anomaly: the political control exerted by the Protestant reformers, notably Melanchthon, actively limited the print programme and curriculum, a discovery that aligned perfectly with existing historical intuition and scholarly support. Eberle et al. (Sci Adv ’24) further elaborates on these findings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 Conclusion: AI-based Methods for the Humanities",
    "text": "7.8 Conclusion: AI-based Methods for the Humanities\n\n\n\nSlide 26\n\n\nHumanities and Digital Humanities (DH) research has historically concentrated on the digitisation of source material. Nevertheless, automated analyses of these digitised corpora present significant challenges, primarily owing to their inherent heterogeneity and the scarcity of annotated labels.\nDespite these hurdles, multimodality, advanced Machine Learning (ML), and Explainable AI (XAI) collectively offer substantial potential to scale humanities research and foster entirely novel research directions. This chapter has demonstrated how XAI can provide crucial insights into model behaviour, from understanding feature attributions in classification to unravelling complex interactions in LLMs. Furthermore, we have showcased the transformative power of AI-based methodologies, such as the XAI-Historian workflow and cluster entropy analysis, in enabling large-scale historical inquiry and uncovering previously hidden patterns of innovation.\nFoundation Models and Large Language Models (LLMs), coupled with effective prompting strategies, can automate various intermediate tasks, including labelling, data curation, and error correction. However, their utility remains limited when addressing more complex research questions that require deep domain expertise and nuanced interpretation.\nSignificant challenges persist, particularly concerning low-resource data, which acts as a considerable roadblock, impacting the applicability of scaling laws. Moreover, out-of-domain transfer, especially for historical and small-scale datasets, necessitates rigorous evaluation and often bespoke model development. Current LLM training and alignment predominantly focus on natural language tasks and code generation, underscoring the need for specialised approaches when engaging with the unique characteristics of humanities data. Future work should focus on developing more robust methods for handling data scarcity and heterogeneity, alongside fostering deeper interdisciplinary collaboration to ensure AI tools truly serve the complex needs of humanities scholarship.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter introduces a pioneering framework for historical and scientific inquiry. It meticulously harnesses advanced Large Language Models (LLMs) whilst systematically mitigating their inherent limitations, particularly concerning factual hallucination and validation. The authors propose a novel “computational epistemology” paradigm, which fundamentally prioritises robust validation mechanisms over the mere expansion of contextual understanding or the simulated “thinking” capabilities of contemporary LLMs.\nA bespoke research infrastructure, aptly named Scholarium, forms the bedrock of this methodology. This sophisticated system seamlessly integrates meticulously curated scholarly sources, exemplified by the exhaustive Opera Omnia Euler, with a structured registry database. This registry systematically records historical activities and communications, thereby offering a validated alternative to conventional embedding-based approaches. The team implemented this intricate system within the Cursor environment, leveraging a suite of multimodal LLMs, including Gemini 2.5, Claude, and Llama, alongside a dedicated AI agent christened Bernoulli.\nFurthermore, the project strategically employs Zenodo, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository, for enduring data preservation and dissemination. Open Science Technology provides crucial technical support, operating a Model Context Protocol Application Programming Interface (MCP API) server. This server ensures global access to the curated data via standardised APIs for artificial intelligence models, fostering principles of open access, open data, and open collaboration. Ultimately, the system aims to deliver complete, rigorously validated answers to complex historical queries—a critical capability currently absent in existing LLM applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Evolution and Limitations of Large Language Models",
    "text": "8.1 Evolution and Limitations of Large Language Models\n\n\n\nSlide 01\n\n\nLarge Language Models have undergone a remarkably swift evolution, progressing through distinct conceptual phases. Initially, the paradigm centred on “Attention is all you need,” emphasising the core mechanism of Transformer architectures. Subsequently, the focus shifted towards “Context is all you need,” prompting developments such as Retrieval-Augmented Generation (RAG) to expand contextual understanding. The latest conceptualisation now postulates “Thinking is all you need,” suggesting a further layer of cognitive capability.\nDespite this rapid advancement, current LLM iterations exhibit significant deficiencies, particularly concerning the rigorous demands of scholarly inquiry. Crucially, these models lack an inherent opponent mechanism to effectively counter hallucination, a pervasive challenge in generative AI. Furthermore, embedding vectors, whilst powerful for semantic similarity, fundamentally fail to capture the true meaning of expressions. These models frequently formulate statements that, whilst sounding plausible, prove factually incorrect.\nMoreover, LLMs struggle to differentiate genuine knowledge from mere internet media or hearsay, often repeating unverified information. Consequently, LLMs currently cannot reliably seek the best-justified information or formulate coherent plans for complex scientific inquiry. While these are significant limitations, the framework presented in this chapter offers a promising approach to mitigate some of these fundamental challenges through robust validation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 The Imperative of Validation and Computational Epistemology",
    "text": "8.2 The Imperative of Validation and Computational Epistemology\n\n\n\nSlide 03\n\n\nA critical need emerges for robust validation mechanisms within advanced computational systems. Such validation must furnish comprehensive reasons, compelling arguments, and verifiable evidence both for and against the truth of any given proposition. Moreover, it must extend to providing justifications for or against the pursuit of specific actions.\nTo address this profound gap, scholars propose a nascent discipline: computational epistemology. This new subject systematically develops the methods and methodologies essential for bridging the validation deficit inherent in current AI approaches. Computational epistemology aims to equip AI systems with the capacity for genuine epistemic agency, enabling them to reason about knowledge and justification.\nAchieving genuine epistemic agency necessitates several key capabilities. Researchers must identify propositions that extend beyond simple sentences, discerning their underlying meaning and scope. Furthermore, the system must accurately identify arguments embedded within diverse texts, historical sources, and complex inquiries. Crucially, it must also discern the intentions, plans, and actions of historical figures, meticulously documented within their surviving records.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "href": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 The Research Inquiry Environment",
    "text": "8.3 The Research Inquiry Environment\n\n\n\nSlide 04\n\n\nA specialised working environment facilitates rigorous historical inquiry. The interface presents an open historical source, such as a book title page, on its left pane. This context involves the construction of Sanssouci Castle under Frederick the Great and the contentious role of Leonhard Euler, one of the 18th century’s most eminent mathematicians, in what proved a significant construction failure. Historians continue to debate Euler’s precise responsibility for this setback.\nThe right pane features a text editor where users formulate specific inquiries. For instance, a user might pose the question: “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain). The system aims to provide a validated, qualifying answer, rigorously supported by proven evidence rather than mere hearsay. Consequently, the output lists individuals, such as Nahl, Benkert and Heymüller, and Giese, detailing their specific contributions, periods of work, earnings, and documented achievements or failures, with explicit references to associated files like “Manger1789_p81-91.xml”.\nThe Cursor environment, situated at the bottom right of the interface, enables the deployment of AI agents. Here, a dedicated agent, named Bernoulli, assists in navigating and querying these complex sources. A significant challenge, however, extends beyond merely reading individual PDF sources; it necessitates searching all available, relevant sources—a task for which conventional token-based indexing proves inadequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "href": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Scholarium: Curated Scholarly Sources",
    "text": "8.4 Scholarium: Curated Scholarly Sources\n\n\n\nSlide 07\n\n\nThe system fundamentally relies upon a scholarly curated editorial board, which meticulously validates all integrated sources. A prime example of this rigorous approach is the Opera Omnia of Euler, a monumental collection spanning 86 volumes. Scholars dedicated over 120 years to its comprehensive editing, a process completed just two years prior. This exhaustive work encompasses all 866 of Euler’s publications and his complete correspondence.\nOther significant scholarly works, including the Kepler Gesammelte Werke and Brahe Opera Omnia, further complement this foundational collection. Users access these resources via a dedicated Euler Opera Omnia Viewer, which provides intuitive navigation through collection, series, volume, and index dropdowns. This curated approach ensures the highest level of data integrity for historical research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "href": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Scholarium: Registry as an Embedding Alternative",
    "text": "8.5 Scholarium: Registry as an Embedding Alternative\n\n\n\nSlide 06\n\n\nA pivotal innovation within this framework is the Scholarium, which serves as a sophisticated alternative to conventional embedding-based approaches. Functioning as a meticulously curated database of content items, the Scholarium maintains a highly detailed inventory of historically proven activities, with each entry rigorously validated against original sources.\nThis comprehensive registry systematically captures a diverse array of information. It meticulously documents personal actions, various communication acts—including letters, publications, and reports—and specific statements. Furthermore, it records implications, arguments, and inquiries, alongside the nuanced use of language, terminology, and concepts. The system also tracks the application of specific concepts and their relations, the deployment of models and methods, and the utilisation of tools and devices. Crucially, it catalogues the precise use of data, information, evidence, and sources. An integrated AI API, specifically leveraging the Model Context Protocol (MCP API), facilitates seamless interaction with this rich, structured data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "href": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Technical Infrastructure and FAIR Principles",
    "text": "8.6 Technical Infrastructure and FAIR Principles\n\n\n\nSlide 07\n\n\nThe system queries its meticulously compiled records using a suite of accessible multimodal models. Researchers have determined that the latest multimodal models, such as Gemini 2.5, prove optimal for the task requirements, adeptly combining information derived from both text and images. The Cursor environment integrates a range of LLM models, including Claude, Gemini, Llama, and LettreAI.\nFor enduring data preservation and publication, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository is indispensable. Zenodo, hosted by CERN in Geneva, fulfils this critical role, ensuring the longevity and accessibility of the project’s data for many years.\nTechnical support for the underlying infrastructure is provided by Open Science Technology, a dedicated startup. This entity manages the operational aspects of the system, including the crucial Model Context Protocol (MCP API) server. This server facilitates worldwide access to the curated data via standardised APIs, enabling seamless interaction with artificial intelligence models. This comprehensive technical framework actively promotes principles of open collaboration, open source development, open access to resources, and open data sharing, thereby contributing to a more transparent and verifiable scholarly ecosystem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "",
    "text": "Overview\nResearchers investigated the application of Large Language Models (LLMs) to assess biases within publications classified under Sustainable Development Goals (SDGs). This inquiry spanned three major bibliometric databases: Web of Science, Scopus, and OpenAlex. The core motivation stemmed from these databases’ critical role in the sociology of science, influencing academic behaviour, funding, and policy. Crucially, they also reflect political and commercial interests, possessing a performative nature.\nThis project built upon previous work that identified minimal overlap in SDG-labelled publications across different providers. It aimed to understand the aggregate effects of LLM-based tools on the representation of SDG-related research. Furthermore, the study conducted a proof-of-concept exercise on automating information extraction for research decision-making. Investigators selected five SDGs directly related to socioeconomic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure).\nFor classification, the team utilised a shared corpus of 15,471,336 jointly indexed publications, spanning January 2015 to July 2023. Regarding the LLM component, researchers fine-tuned DistilGPT2, a lightweight, open-source model, separately on publication abstracts for each SDG and database combination. This process yielded 15 distinct models, a choice that minimised pre-existing knowledge about SDGs within the model. Prompts, derived from SDG targets (8-12 targets per SDG, 10 diverse questions per target), benchmarked the fine-tuned LLMs, employing three decoding strategies: top-k, nucleus, and contrastive search. Noun phrase extraction from LLM responses facilitated analysis across four dimensions: locations, actors, data/metrics, and focuses.\nKey findings revealed a systematic oversight in the data concerning disadvantaged individuals, the poorest countries, and underrepresented topics explicitly mentioned in SDG targets. Conversely, economic superpowers received considerable attention. The study highlighted the decisive impact of bibliometric SDG classification and the sensitivity of LLMs to training data, model architecture, parameters, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-sdg-classification-in-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-sdg-classification-in-bibliometric-databases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.1 Background: SDG Classification in Bibliometric Databases",
    "text": "9.1 Background: SDG Classification in Bibliometric Databases\n\n\n\nSlide 01\n\n\nResearchers initiated an investigation to employ Large Language Models (LLMs) as a technology for assessing biases within publications classified by three principal bibliometric databases. This work acknowledges that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, function as critical digital infrastructures, enabling bibliometric analyses and impact assessments throughout the scientific community. Nevertheless, these databases possess a performative nature, shaped by particular understandings of the science system and specific value attributions, as Whitley (2000) and Winkler (1988) have noted.\nMajor bibliometric databases have recently implemented classifications that align scholarly publications with the United Nations Sustainable Development Goals (SDGs). However, prior research, notably by Armitage et al. (2020), discovered significant discrepancies in SDG labelling across various providers like Elsevier, Bergen, and Aurora, revealing minimal overlap in the resulting datasets. Such differences in classification carry substantial implications; they can foster varying perceptions of research priorities, which, in turn, may influence resource allocation and policy decisions.\nFurthermore, these databases exert considerable influence over academics, researchers, funding bodies, and policymakers, whilst also responding to diverse political and commercial interests. The current study specifically considered Web of Science, Scopus, and OpenAlex, building upon earlier findings that highlighted the limited overlap in publications when different SDG search queries were applied.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-objectives-and-framework",
    "href": "chapter_ai-nepi_009.html#case-study-objectives-and-framework",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.2 Case Study Objectives and Framework",
    "text": "9.2 Case Study Objectives and Framework\n\n\n\nSlide 02\n\n\nInvestigators conducted a case study, detailed by Ottaviani & StahlSchmidt (2024), focusing on the representation of UN Sustainable Development Goals within bibliometric data. A primary motivation involved assessing the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly if LLM-based tools were to be introduced. To achieve this, researchers employed DistilGPT2, a minimally pre-trained Large Language Model. They separately trained this model on distinct subsets of publication abstracts, each corresponding to the SDG classifications provided by the diverse bibliometric databases.\nThe LLM technology served a dual purpose in this study. Firstly, it functioned as a detector of biases present in the data. Secondly, it acted as a proof-of-concept exercise, demonstrating the potential of LLMs in automating information extraction to inform research-related decision-making. Researchers aimed to understand the aggregate effects stemming from metadata processing by bibliometric databases and how these subsequently influence various stakeholders, including researchers, policymakers, and consultants. Ultimately, the project sought to develop a generalisable exercise for assessing the potential impact of such technologies on research policy.\nA conceptualised chain of dependencies illustrates this process: SDG classification initially defines what constitutes “Research” on SDGs. Various actors, including researchers, small and medium-sized enterprises (SMEs), governments, and other intermediaries, then process this research. Subsequently, this processed research informs “Decision-making to align with SDGs,” which in turn affects “Socioeconomic inequalities.” Parallel to this, the LLM, acting as a “detector of ‘biases’,” influences the “Introduction of LLM in Research Policy,” which also has repercussions for “Socioeconomic inequalities.” Therefore, alterations in the metadata that define “research on SDGs” can significantly impact advice, choices, indicators, and implemented measures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.3 Methodology: Actors, Data, and SDG Selection",
    "text": "9.3 Methodology: Actors, Data, and SDG Selection\n\n\n\nSlide 04\n\n\nThe research design incorporated three principal bibliometric databases as key actors. These included two proprietary systems, Web of Science (operated by Clarivate, US) and Scopus (managed by Elsevier, UK), alongside the open-access database OpenAlex (formerly a Microsoft entity, US). Investigators focused their analysis on five specific UN Sustainable Development Goals, chosen for their direct relevance to socioeconomic inequalities. They categorised these into two dimensions: the socio dimension, encompassing SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities); and the economic dimension, which included SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).\nFor data processing, researchers utilised a jointly indexed subset comprising 15,471,336 publications. They compiled this dataset by collecting publications shared across all three bibliometric databases, identifying them through exact DOI matching, and covering the period from January 2015 to July 2023. Subsequently, the team undertook an analysis of the performance of the three distinct classification standards for the five selected SDGs. This approach led to the creation of three separate subsets of publications for each SDG—one corresponding to each bibliometric database—forming the basis for the comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "href": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.4 Comparative Analysis of SDG-Classified Papers",
    "text": "9.4 Comparative Analysis of SDG-Classified Papers\n\n\n\nSlide 05\n\n\nA comparative analysis, detailed in Ottaviani & StahlSchmidt (2024), examined the overlap of papers classified under specific Sustainable Development Goals (SDGs) across Web of Science, OpenAlex, and Scopus, particularly for the socio-dimension SDGs. For SDG4 (Quality Education), Scopus classified the largest share of publications (339,063; 52.2%), followed by OpenAlex (218,907; 33.6%), and Web of Science (124,359; 19.1%). The intersection of all three databases for SDG4 contained 46,711 publications (7.2%).\nConcerning SDG5 (Gender Equality), Web of Science accounted for the majority of classifications (373,224; 57.4%), with Scopus classifying 82,277 (26.2%) and OpenAlex 38,066 (12.1%). Notably, all three databases commonly classified only 21,770 publications (6.9%) under SDG5. Investigators observed that Scopus did not designate some publications present in its database as SDG5. Furthermore, Web of Science’s SDG5 classifications included approximately 10% of publications from mathematics, such as those on geometrical differential equations, indicating potential discrepancies in classification criteria.\nFor SDG10 (Reduce Inequalities), Scopus again led in volume (236,665; 36.2%), with OpenAlex (213,419; 32.7%) and Web of Science (99,460; 15.2%) following. The common overlap for SDG10 was the smallest, at 13,319 publications (2.0%). These findings generally align with Armitage (2020), underscoring the consistently small overlap in SDG labelling across different bibliometric providers. This limited congruence highlights the varying interpretations and applications of SDG classifications.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "href": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.5 LLM Selection and Fine-Tuning Strategy",
    "text": "9.5 LLM Selection and Fine-Tuning Strategy\n\n\n\nSlide 07\n\n\nResearchers conceptualised the development of Large Language Models (LLMs) possessing knowledge derived exclusively from publications classified under a specific Sustainable Development Goal (SDG) by a particular bibliometric database. The initial ambition to train such LLMs from scratch solely on these publications proved a substantial undertaking due to its resource-intensive nature. Consequently, a compromise involved fine-tuning an existing, pre-trained LLM that possessed minimal prior knowledge, using the abstracts of the selected publications.\nLeading commercial and open-source pre-trained LLMs, such as GPT-4 (with 1.76 trillion parameters), were deemed ineligible for this work. Their unsuitability stemmed from their extensive pre-training datasets (which include sources like Wikipedia and Reddit conversations), meaning they already embed considerable knowledge about SDGs and possess strong, pre-existing semantic associations. To circumvent this, investigators selected DistilGPT2. This model, a “very light” English-speaking variant of the open-source GPT-2, employs a “distillation” technique (Sanh, 2019) and has significantly fewer parameters (82 million).\nIts advantages included feasibility for use with proprietary data and its minimally instructed nature, which ensures its behaviour is more strongly influenced by the fine-tuning data. Researchers operated under the premise that DistilGPT2 had no significant prior semantic knowledge relevant to the publications or the prompts used. This approach facilitated the fine-tuning of 15 distinct DistilGPT2 models, each tailored to a specific combination of one of the three bibliometric databases and one of the five chosen SDGs (DistilGPT2{bibDB, SDG}).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompt-engineering-and-llm-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompt-engineering-and-llm-benchmarking",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.6 Prompt Engineering and LLM Benchmarking",
    "text": "9.6 Prompt Engineering and LLM Benchmarking\n\n\n\nSlide 10\n\n\nThe United Nations’ Sustainable Development Goals (SDGs) are structured with specific targets; for instance, SDG4 (Quality Education) includes targets such as ensuring universal completion of primary and secondary education (Target 4.1) and equal access to tertiary education (Target 4.3), amongst others (typically 8-12 targets for the SDGs analysed, as per the 2030 Agenda for SDGs, UN). To benchmark the fine-tuned LLMs, researchers developed a systematic approach to prompt engineering. For each individual target within an SDG, they crafted ten diverse questions, or prompts, each designed to probe different facets of that target.\nThis methodology yielded a unique set of 80 to 120 prompts for every SDG under investigation. These prompts established a benchmark, enabling the assessment of the LLMs’ compliance with SDG objectives and the identification of potential “biases” in their responses. For example, for Target 4.1 of SDG4, prompts included questions like, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” and others addressing strategies, outcome improvements, and challenges.\nThe research design followed a structured workflow for each bibliometric database (DB) and SDG combination. Initially, a set of publication abstracts, classified under a specific SDG by a particular database, served as the input for fine-tuning a DistilGPT-2 model. The resultant fine-tuned model (Fine-tuned DistilGPT-2 SDG# DB#) then processed the set of prompts specifically designed for that SDG. This processing employed three distinct decoding strategies—top-k, nucleus, and contrastive search—generating three sets of responses. Finally, a “prompts’ words filter” was applied to these responses, leading to the extraction of noun phrases (Noun phrases SDG# DB#), which formed the basis for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-4-results-unaddressed-targets-and-biases",
    "href": "chapter_ai-nepi_009.html#sdg-4-results-unaddressed-targets-and-biases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.7 SDG 4 Results: Unaddressed Targets and Biases",
    "text": "9.7 SDG 4 Results: Unaddressed Targets and Biases\n\n\n\nSlide 13\n\n\nResearchers analysed the responses generated by the fine-tuned Large Language Models (LLMs) by matching extracted noun phrases with the official SDG targets. This analysis spanned four key dimensions: Locations, Actors, Data/Metrics, and Focuses. For each SDG, the assessment aimed to determine, firstly, the LLM’s compliance with its targets and, secondly, any discernible biases. Notably, the team also observed differences in the outputs corresponding to the different source bibliometric databases.\nTaking SDG4 (Quality Education) as an illustrative example, the LLMs, despite receiving target-specific questions, failed to address several critical areas. In terms of Locations, whilst countries like South Africa, the U.S., Australia, China, and Hong Kong appeared (reflecting the content of unique database subsets), a significant omission of African countries (beyond South Africa), Developing Countries more broadly, Least Developed Countries, Other Developing Countries, and Small Island Developing States was evident—all explicitly or implicitly relevant to SDG4 targets.\nRegarding Actors, terms like “Classroom” and “Family” appeared, and general categories such as “All Women and Men,” “Children,” “Teachers,” and “Youth” were addressed. However, the LLM responses did not adequately cover crucial vulnerable groups specified in SDG4 targets, including “The Vulnerable,” “Persons With Disabilities,” “Indigenous Peoples,” “Children In Vulnerable Situations,” and “All Learners.”\nThe Data/Metrics dimension saw mentions of “Survey,” “PISA,” “Evaluation,” “Self-Efficacy,” and “Thematic Analysis,” indicating a focus on certain research methodologies and assessment tools. For Focuses, whilst aspects like “Quality Primary and Secondary Education” and “Access” were addressed, numerous target areas remained unmentioned. These unaddressed focuses included:\n\n“Affordable And Quality Technical, Vocational And Tertiary Education”\n“Relevant Skills” for employment\n“Vocational Training”\n“Scholarships”\n“Safe, non-violent, inclusive learning environments”\n“Sustainable Lifestyles”\n“Human Rights”\n“Global Citizenship”\n“Appreciation Of Cultural Diversity”\n“Free primary and secondary education”\n“Tertiary education”\n\nThis pattern of overlooking specific target elements, particularly concerning sensitive locations, vulnerable actors, and key educational focuses, emerged as a recurrent finding.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-analysis-systematic-oversights",
    "href": "chapter_ai-nepi_009.html#cross-sdg-analysis-systematic-oversights",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.8 Cross-SDG Analysis: Systematic Oversights",
    "text": "9.8 Cross-SDG Analysis: Systematic Oversights\n\n\n\nSlide 14\n\n\nAcross the five Sustainable Development Goals (SDGs) analysed, several consistent patterns and systematic oversights emerged from the LLM responses. Regarding Locations, least developed countries received scant attention; for instance, Sub-Saharan Africa appeared notably only in the context of SDG8. The United States featured with such prominence that it suggested an “undoubted monopoly” in the data’s geographical focus. Following the U.S., South Africa and China were the most frequently cited locations, with the United Kingdom and Australia also appearing.\nConcerning Actors, a particularly troubling finding was the systematic overlooking of discriminated and vulnerable categories of people. This oversight persisted across all five SDGs examined, indicating a significant gap in the LLM-generated content relative to the inclusive aims of the SDGs themselves.\nIn the Metrics dimension, the LLMs frequently referenced various data sources, such as Demographic and Health Surveys (DHS) and World Values Surveys (WVS), alongside numerous other metrics, indicators, and benchmarks. A range of research methodologies also appeared, including theoretical and empirical approaches, thematic analysis, market dynamics, and macroeconomic studies. An interesting distinction arose from the source databases: for three of the SDGs, LLMs trained on Web of Science data tended to reflect a more theoretical research approach, whereas those trained on Scopus and OpenAlex data exhibited a more empirical orientation.\nFinally, whilst Focuses were generally SDG-specific, the LLMs often failed to address the most sensitive topics embedded within the SDG targets. Examples of such overlooked critical issues include human trafficking, human exploitation, and migration, all of which are pertinent to the broader aims of socioeconomic equality and development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#summary-and-limitations",
    "href": "chapter_ai-nepi_009.html#summary-and-limitations",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.9 Summary and Limitations",
    "text": "9.9 Summary and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s principal finding highlights that introducing Large Language Models (LLMs) as an analytical AI tool, positioned between the initial SDG classification of scientific literature and its subsequent use by policymakers, uncovers a systematic oversight within the data. Specifically, scientific publications, when classified by SDGs and processed by these LLMs, tend to neglect the most disadvantaged categories of individuals, the poorest countries, and various underrepresented topics that the SDG targets explicitly aim to address. Conversely, the analysis indicates that substantial attention is directed towards economic superpowers and highly developing nations. This outcome underscores how an ostensibly objective, science-informed practice, such as the bibliometric classification of SDGs, can have decisive and potentially skewed impacts on the perceived landscape of research.\nResearchers acknowledge several inherent limitations in this study. LLMs exhibit high sensitivity to various factors, including model architecture; although the choice of DistilGPT2 aimed to mitigate some aspects of this, more developed architectures could yield different outcomes. Sensitivity to training data presents another critical factor, which this work partially addressed by utilising three distinct bibliometric databases. Furthermore, (hyper-)parameters and the chosen decoding strategy significantly influence LLM outputs; the use of three different decoding strategies (top-k, nucleus, and contrastive search) attempted to account for this variability. The study presents a general framework, and whilst its application to very specific, applied cases could potentially produce different results, the researchers express some reservation about this likelihood.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "",
    "text": "Overview\nResearchers have long grappled with the persistent challenge of extracting citation data from the complex footnotes prevalent in law and humanities scholarship. Historically, bibliometric databases have offered inadequate coverage for these domains. This deficiency stems primarily from a lack of commercial interest, a focus on impact factors over intellectual history, and the inherent complexity of humanities footnotes. Moreover, traditional machine learning tools consistently demonstrate poor performance in parsing these intricate structures. Consequently, this project explores the utility of Large Language Models (LLMs) and Vision Language Models (VLMs) as a more effective solution.\nA central tenet of this research involves establishing a robust testing and evaluation framework. To this end, scholars are developing a high-quality gold standard dataset, meticulously annotated using TEI XML encoding. This standard, well-established within the digital humanities, facilitates comprehensive representation of citation phenomena, including crucial contextual information. Furthermore, it ensures interoperability with existing tools such as Grobid, enabling direct performance comparisons.\nTo operationalise this approach, engineers crafted Llamore, a lightweight Python package. Llamore extracts citation data from raw text or PDFs, exporting it into TEI-formatted XML files. Crucially, it also evaluates extraction performance against gold standard references using an F1-score metric, which accounts for precision and recall through an unbalanced assignment problem. Initial evaluations reveal that whilst Llamore’s resource consumption exceeds that of traditional tools like Grobid for biomedical literature, it significantly outperforms Grobid when processing the challenging, footnoted humanities data. Future work aims to expand the training data, refine evaluation metrics, and enhance Llamore’s capabilities to capture contextual citation information and resolve complex stylistic variations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-humanities-citation-graph-generation",
    "href": "chapter_ai-nepi_010.html#challenges-in-humanities-citation-graph-generation",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.1 Challenges in Humanities Citation Graph Generation",
    "text": "10.1 Challenges in Humanities Citation Graph Generation\n\n\n\nSlide 01: Visual representation of a complex citation graph.\n\n\nResearchers confront a significant challenge: Large Language Models and other algorithms currently struggle with the intricate footnotes characteristic of law and humanities scholarship. Generating comprehensive citation graphs from these sources represents a primary objective of our work. Such graphs prove invaluable for intellectual history, enabling scholars to discern patterns and relationships within knowledge production, trace intellectual influences, and quantify the reception of published ideas. For instance, one can readily identify the most cited authors within a specific journal over a defined period, as demonstrated by an analysis of the Journal of Law and Society between 1994 and 2003.\nA fundamental impediment arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) literature within existing bibliometric data sources. Leading platforms, including Web of Science, Scopus, and OpenAlex, exhibit substantial deficiencies in this regard. Web of Science and Scopus, moreover, impose prohibitive costs and restrictive licensing terms, hindering open research initiatives. Whilst OpenAlex offers an open-access alternative, it too lacks comprehensive coverage for many A-journals, pre-digital content, and non-English language publications. For example, the Zeitschrift für Rechtssoziologie, established in 1980, shows negligible citation data before the 2000s within these widely used databases.\nSeveral factors contribute to this persistent data gap. Commercial entities demonstrate limited financial interest in humanities scholarship, unlike their engagement with STEM, medicine, and economics. Furthermore, these databases prioritise “impact factor” metrics for scientific evaluation, a focus that diverges significantly from the needs of intellectual history research. Crucially, the pervasive use of complex footnotes within humanities literature presents a unique parsing challenge, which traditional systems have consistently struggled to overcome.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-and-footnote-complexity",
    "href": "chapter_ai-nepi_010.html#large-language-models-and-footnote-complexity",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.2 Large Language Models and Footnote Complexity",
    "text": "10.2 Large Language Models and Footnote Complexity\n\n\n\nSlide 07: Illustration depicting the complexity of “footnotes from hell” with embedded commentary and non-reference text.\n\n\nA second, equally pressing problem arises from the inherent complexity of humanities footnotes, often termed “footnotes from hell.” These structures frequently incorporate extensive commentary, extraneous content, and non-reference text, embedding the actual citations within considerable noise. Traditional instruments for extracting such information necessitate laborious manual annotation. Moreover, conventional machine learning tools, including those based on conditional random forests, consistently exhibit poor performance when faced with these intricate structures. For instance, the ExCite Performance study (Boulanger/Iurshina 2022) reported low extraction and segmentation accuracies across various training datasets, with combined data yielding an extraction accuracy of merely 0.22 and segmentation accuracy of 0.47. This highlights the limitations of previous approaches.\nConsequently, researchers have turned to Large Language Models (LLMs) as a promising alternative. Initial experiments in 2022, utilising models such as text-davinci-003, demonstrated LLMs’ considerable capacity for extracting references from highly unstructured textual data. Newer models offer even greater potential, whilst Vision Language Models (VLMs) extend this capability to direct processing of PDF documents. Developers employ various methods, including prompt engineering, Retrieval-Augmented Generation (RAG), and fine-tuning, to optimise these models for specific tasks. RAG, for example, enhances LLM outputs by retrieving relevant information from a knowledge base before generating a response, thereby improving accuracy and reducing hallucinations.\nNevertheless, a crucial concern persists regarding the trustworthiness of LLM-generated results, particularly the risk of hallucinations. A notable incident involved a lawyer who, relying on ChatGPT, submitted a federal court filing citing at least six non-existent cases. Addressing this fundamental issue demands a robust testing and evaluation solution. Such a solution requires a high-quality gold standard dataset, a flexible framework capable of adapting to the rapidly evolving technology landscape, and solid testing algorithms to generate comparable performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.3 Developing a TEI-Annotated Gold Standard",
    "text": "10.3 Developing a TEI-Annotated Gold Standard\n\n\n\nSlide 13: Diagram illustrating the multi-stage process of TEI XML annotation for footnotes.\n\n\nTo address the need for reliable evaluation, researchers have embarked upon compiling a comprehensive training and evaluation dataset, employing TEI XML encoding. This choice rests upon several compelling reasons. TEI XML represents a well-established, precisely specified, and comprehensive standard for text interchange within the digital humanities. Crucially, it encompasses a far broader range of phenomena than more restrictive bibliographical standards, such as CSL or BibTeX. Indeed, TEI extends beyond mere reference management, allowing for the encoding of citations, cross-references, and other contextual markup, which proves vital for classifying citation intention. Furthermore, adopting this standard enables the project to leverage existing digital editions, text collections, and corpora, thereby enhancing the generalisation and robustness of the developed mechanisms.\nNevertheless, the TEI standard presents its own set of challenges, both conceptual and technical. Conceptual difficulties arise in differentiating between pointers (references to a specific part of a text) and references (full bibliographical entries), whilst technical complexities involve managing constrained elements (e.g., specific fields within a citation) versus elliptic material (e.g., abbreviations like ibid. or op. cit.). Despite these hurdles, the dataset’s establishment progresses steadily. The encoding process involves multiple stages: capturing PDF screenshots, segmenting reference strings to distinguish them from non-reference footnote text, and finally, generating parsed structured data.\nThe dataset currently comprises 1,100 footnotes and endnotes, drawn from 25 articles across 10 Directory of Open Access Journals (DOAJ) titles. It specifically focuses on humanities scholarship, particularly legal and historical texts, and encompasses a diverse range of languages, including French, German, Spanish, Italian, and Portuguese, spanning the period from 1958 to 2018. Researchers estimate the dataset will contain over 1,600 references, with individual occurrences encoded separately to preserve contextual information. Notably, the project adjusted its strategy midway, shifting to Open Access journals and incorporating PDFs to facilitate Vision Language Model (VLM) mechanisms and enable the full publication of the dataset.\nThe interoperability afforded by the TEI XML standard offers a significant advantage, enabling seamless integration with existing tooling. Grobid, a widely recognised tool for reference and information extraction, notably utilises TEI XML for its training and evaluation processes. Consequently, this shared data format permits direct performance comparisons with Grobid and facilitates the exchange of training data, benefiting both the project and the broader research community.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.4 Llamore: A Python Package for Reference Extraction",
    "text": "10.4 Llamore: A Python Package for Reference Extraction\n\n\n\nSlide 14: Workflow diagram showing Llamore processing text/PDFs to TEI XML and evaluating against gold standards.\n\n\nResearchers have developed Llamore, a Python package acronym for “Large LANguage MOdels for Reference Extraction.” This tool facilitates two primary functions: extracting citation data from raw text or PDF inputs using multimodal Large Language Models, and subsequently evaluating the extraction performance. The workflow proceeds from text or PDF documents, through Llamore, to produce references in TEI XML format. These extracted references then undergo comparison with gold standard references, yielding an F1-score as an evaluation metric.\nCrafting Llamore involved two key objectives. Firstly, the package needed to remain lightweight, comprising fewer than 2,000 lines of code. Crucially, Llamore operates as an interface to a model of the user’s choosing, rather than embedding any specific model directly. Secondly, this design ensures broad compatibility with both open and closed Large Language Models and Vision Language Models, offering flexibility to researchers.\nImplementing Llamore proves straightforward. Users can install the package directly from PyPI using pip install llamore. For extraction, one imports the relevant extractor, such as GeminiExtractor or OpenaiExtractor, then instantiates it with an API key. The extractor processes either a PDF file path or a raw input string, returning a collection of references that can then be exported to a TEI XML file. Notably, the OpenaiExtractor provides compatibility with numerous open model serving frameworks, including Olama and VLLM, which offer OpenAI-compatible API endpoints. For evaluation, users import the F1 class, configure it (e.g., levenshtein_distance=0 for exact matches), and compute the macro average F1-score by supplying both the extracted and gold references.\nLlamore employs the F1-score, a widely recognised metric for comparing structured data, to assess extraction performance. This score combines precision (the ratio of correctly identified elements to all predicted elements) and recall (the ratio of correctly identified elements to all actual elements in the gold standard) into a single harmonic mean. A perfect extraction yields an F1-score of 1, whilst an F1-score of 0 indicates no matches. For instance, in comparing an extracted reference to a gold standard, Llamore identifies matches for analytic_title, monographic_title, authors.surname, and publication_date, whilst noting a minor discrepancy in authors.forename due to an extraneous character in the gold reference. Furthermore, Llamore addresses the complex task of aligning extracted references with gold references by framing it as an unbalanced assignment problem. The tool computes F1 scores for every possible combination, constructs a matrix, and then maximises the total F1-score whilst ensuring a unique assignment, utilising SciPy’s solver for this optimisation. Significantly, the system penalises both missing and hallucinated references by assigning them an F1-score of zero.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "href": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.5 Performance Analysis and Future Directions",
    "text": "10.5 Performance Analysis and Future Directions\n\n\n\nSlide 20: Bar chart comparing Llamore and Grobid F1 scores on biomedical and humanities datasets.\n\n\nInitial performance evaluations provide crucial insights into Llamore’s efficacy across diverse datasets. When tested on the PLOS 1000 Dataset, which comprises 1,000 biomedical PDFs and demands exact matches, Grobid achieved an F1 score of 0.61, whilst Llamore, utilising Gemini 2.0 Flash, attained a comparable F1 score of 0.62. However, for literature on which Grobid was specifically trained, it demonstrates superior efficiency, operating considerably faster and with fewer computational resources; Llamore’s compute requirements, conversely, are orders of magnitude greater.\nA more compelling distinction emerges when evaluating performance on the project’s bespoke humanities dataset, which features complex footnotes and also requires exact matches. Here, Grobid struggles significantly, yielding an F1 score of only 0.14, largely due to its training data being out of distribution for such intricate structures. In stark contrast, Llamore (Gemini 2.0 Flash) achieves an F1 score of 0.45, representing a threefold improvement in performance. This initial evaluation clearly demonstrates Llamore’s superior capability in handling the unique challenges posed by humanities footnotes, validating the utility of LLMs for this specific domain. Nevertheless, this current performance metric pertains solely to pure reference extraction, excluding the capture of contextual information or cross-referencing.\nFuture work outlines several key objectives. Researchers plan to generate additional training data and further refine the test metrics to encompass a broader range of citation phenomena. Crucially, they aim to extend Llamore’s capabilities to support citations in context, discerning whether a work is cited approvingly or critically. Furthermore, the tool will incorporate features for resolving op cit. references, identifying specific pages cited, and quantifying multiple citations to the same work.\nAddressing these enhancements will necessitate overcoming several challenges. These include the wide variation in citation styles (e.g., differentiating between volumes and pages, or first page versus cited page), the complexities of multilingual terminology (e.g., diverse contributor roles like “eds” or “hrsg. v.”, and special terms such as passim, ibid, or n.d.), the intricacies of canonical citations prevalent in fields like Bible studies or Roman law, and the accurate handling of ellipses, abbreviations, and cross-references. Overcoming these hurdles will significantly enhance Llamore’s utility for advanced scholarly analysis.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview\nResearchers at DANS, the data archive of the Royal Netherlands Academy of Arts and Science, in collaboration with GESIS, a research-engaged archive, have pioneered an AI-driven solution to manage the escalating volume of scientific information. This initiative directly addresses the challenges of growth and increasing differentiation within the sciences, which complicate the review, evaluation, and selection of relevant content. A fundamental precondition for creating new knowledge, whether individually or across academia, involves efficiently finding and understanding existing information. Consequently, the project investigates whether contemporary Artificial Intelligence (AI), particularly Large Language Models (LLMs), can support the knowledge production process through advanced information retrieval.\nThe core research question explores the feasibility of constructing an AI solution capable of facilitating conversational interaction with academic papers from specific collections. Developers have crafted a dual-component system: Ghostwriter, serving as the user interface, and EverythingData, encompassing the comprehensive backend operations. This architecture integrates principles of information retrieval, human-machine interaction, and Retrieval-Augmented Generation (RAG). The project employs the method-data-analysis (mda) journal as a practical use case, demonstrating a ‘local’ and ‘tailored’ AI solution workflow.\nThe Ghostwriter interface redefines information retrieval by enabling simultaneous interaction with structured data—analogous to a librarian—and natural language—akin to an expert. This approach leverages Knowledge Organisation Systems (KOS) and classification schemes whilst interpreting natural language queries. The underlying technical infrastructure, EverythingData, processes input document collections, such as articles from the mda journal, by ingesting them into a vector store, specifically Qdrant. It performs crucial operations including term extraction, embedding construction, and crucially, coupling these with knowledge graphs. This integration enriches embeddings by contextualising them, thereby adding significant value to the information.\nThe system prioritises factual accuracy, aiming to prevent hallucinations by relying exclusively on the ingested source material. It segments papers into small, identifiable blocks, employing LLM techniques to intelligently connect and retrieve relevant sections. Knowledge graphs further enhance this process by predicting which text segments will best address specific queries. A key innovation links extracted entities to Wikidata, transforming free strings into multilingual identifiers. This mechanism supports immediate multilinguality, allowing users to query in one language and retrieve information from documents in another. Moreover, decoupling knowledge from the model and storing it as Wikidata identifiers establishes a robust ground truth for benchmarking and validating future AI models. The project envisions this knowledge organisation system as a sustainable future for scientific information management, fostering collaborations with industry leaders like Google and Meta.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#navigating-scientific-information-overload",
    "href": "chapter_ai-nepi_011.html#navigating-scientific-information-overload",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Navigating Scientific Information Overload",
    "text": "11.1 Navigating Scientific Information Overload\n\n\n\nSlide 01\n\n\nThis collaborative endeavour, uniting DANS—the data archive of the Royal Netherlands Academy of Arts and Science—with GESIS, an archive actively engaged in research, addresses a critical challenge within contemporary science. The sciences exhibit continuous growth and increasing differentiation, which consequently complicates the processes of reviewing, evaluating, and selecting pertinent information.\nA fundamental precondition for generating new knowledge, whether within individual minds or across broader academic communities, necessitates the capacity to locate and comprehend existing information effectively. Therefore, a central research question explores whether advanced AI systems can genuinely support the knowledge production process, specifically focusing on information retrieval.\nThe project originated from extensive experimentation conducted by Slava Tikhonov, a senior research engineer at DANS, who meticulously constructs complex data pipelines. Rather than simple linear pipelines, these systems represent intricate “backs of things,” as one colleague aptly described, making them challenging to unravel and explain. Consequently, the project aims to illustrate the application of these AI capabilities in a manner accessible to a broader audience. From a wider perspective, this initiative seeks to harness AI’s potential to manage the overwhelming deluge of information in which researchers increasingly find themselves immersed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-architecture-and-core-components",
    "href": "chapter_ai-nepi_011.html#system-architecture-and-core-components",
    "title": "11  Science dynamics and AI",
    "section": "11.2 System Architecture and Core Components",
    "text": "11.2 System Architecture and Core Components\n\n\n\nSlide 02\n\n\nA central research question drives this project: can developers construct an AI solution enabling conversational interaction with academic papers drawn from a specific selection? This inquiry integrates several key concepts, including information retrieval, the dynamics of human-machine interaction, and the principles of Retrieval-Augmented Generation (RAG). Researchers have selected the method-data-analysis (mda) journal as a pertinent use case, providing a concrete context for demonstrating the system’s capabilities.\nThe project introduces a workflow for a ‘local’ or ‘tailored’ AI solution, comprising two primary components. Developers have affectionately named these Ghostwriter, which functions as the user interface, and EverythingData, serving as a comprehensive summary for all underlying backend operations. This chapter will further illustrate both the front-end user experience and the intricate back-end processes, culminating in a summary and outlook on future developments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-retrieval-paradigm",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-retrieval-paradigm",
    "title": "11  Science dynamics and AI",
    "section": "11.3 The Ghostwriter Interface: A Novel Retrieval Paradigm",
    "text": "11.3 The Ghostwriter Interface: A Novel Retrieval Paradigm\n\n\n\nSlide 03\n\n\nThe Ghostwriter approach introduces a novel paradigm for information retrieval, fundamentally altering how users interact with data. This system conceptualises queries across various levels of complexity and data representation.\nInitially, a direct query to a single database representation necessitates prior knowledge of its schema and typical values to yield results, akin to a user interacting solely with a database. Progressing beyond this, a query directed at a data collection or space, underpinned by connected structured databases or graphs, prompts the system to suggest similar or improved queries based on schema interconnections, whilst providing a list of potential results. This interaction mirrors a user consulting a librarian.\nFurther advancing, a query posed to a Large Language Model (LLM) interprets natural language input and generates results, also expressed in natural language. This scenario evokes the experience of engaging with a library or a panel of experts. Crucially, the Ghostwriter system integrates a local LLM with a target data collection, embedding it within a network of additional data interpretation sources accessible via Application Programming Interfaces (APIs). This sophisticated setup generates a family of terms around the query, identifies related structured information, and ultimately returns a comprehensive list of results. This advanced interaction simulates a user simultaneously chatting with both experts and librarians.\nTraditionally, information retrieval has grappled with the challenge of formulating the precise query. Whilst Google features and schema.org enable machines to make informed guesses about user queries, these typically operate within a web-based context, not a local interaction. Ghostwriter, however, through its iterative application, empowers users to reformulate their questions, thereby fostering a deeper understanding of their actual intent and the available data space. This innovative interface, drawing on the metaphors of a “librarian”—representing structured data, Knowledge Organisation Systems (KOS), and historical classifications—and an “expert”—embodying natural language—claims to facilitate simultaneous conversational interaction with both.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Retrieval-Augmented Generation (RAG) Architecture",
    "text": "11.4 Retrieval-Augmented Generation (RAG) Architecture\n\n\n\nSlide 04\n\n\nScientifically, this project firmly situates itself within the broader discourse surrounding Retrieval-Augmented Generation (RAG). Philip Rattliff’s paper from Neo4j offers a highly recommended introduction to this intricate topic. [CITATION NEEDED for Rattliff’s paper]. The system’s architecture fundamentally relies upon two main ingredients: a vector space and a knowledge graph. Developers construct the vector space from the content of data files, encoding this information into embeddings. Various Machine Learning (ML) algorithms compute these embeddings, employing diverse Large Language Models (LLMs) in the process.\nConversely, the knowledge graph represents a sophisticated metadata layer. Engineers integrate this layer with a range of ontologies and controlled vocabularies, notably incorporating principles of responsible AI. The Croissant ML standard precisely expresses this graph. The overarching vision for this system involves seamlessly combining both graph and vector representations into a unified model, termed GraphRAG. Developers implement this approach ‘locally’, conceptualising it as a form of Distributed AI. Within this framework, the LLM assumes a dual role, functioning both as an interface between human users and the AI, and as a powerful reasoning engine. Operationally, the LLM connects to a ‘RAG library’—the knowledge graph—enabling it to navigate through datasets and consume embeddings, or vectors, as contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#operational-workflow-data-ingestion-and-processing",
    "href": "chapter_ai-nepi_011.html#operational-workflow-data-ingestion-and-processing",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Operational Workflow: Data Ingestion and Processing",
    "text": "11.5 Operational Workflow: Data Ingestion and Processing\n\n\n\nSlide 05\n\n\nThe system initiates its operational workflow by ingesting a collection of articles, specifically those sourced from the method-data-analysis (mda) journal. Developers have scraped a limited number of these articles for the current implementation; however, the architecture accommodates any collection of documents as input. This raw information then enters the EverythingData component, which orchestrates a series of sophisticated operations.\nInitially, the system stores this information within a vector store, utilising Qdrant for this purpose. Subsequently, it performs crucial processes, including term extraction and the construction of embeddings. A pivotal aspect of this workflow involves coupling the processed information with knowledge graphs. This integration significantly enhances the value of words, phrases, and embeddings by contextualising them, effectively enriching the overall context. All this meticulously processed data then feeds into a unified vector space, forming the RAG-Graph. The Ghostwriter interface interacts directly with this vector space and its integrated graph. Users formulate questions in natural language, and in response, the system provides both a list of relevant documents and a concise summary text, reflecting its understanding of the query.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-implementation-preventing-hallucinations",
    "href": "chapter_ai-nepi_011.html#ghostwriter-implementation-preventing-hallucinations",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter Implementation: Preventing Hallucinations",
    "text": "11.6 Ghostwriter Implementation: Preventing Hallucinations\n\n\n\nSlide 07\n\n\nThe developer, whilst having engaged with early iterations of LLMs such as GPT-2 in 2020, expresses a nuanced perspective, focusing instead on deconstructing and repurposing their training processes. This work, initially conceived for academic papers, demonstrates remarkable versatility, extending its application to any web content or even spreadsheets, enabling users to query specific values.\nA cornerstone of its design lies in its robust mechanism for preventing hallucinations: the system exclusively draws information from the provided source material. Consequently, if a query pertains to information absent from the ingested data, the system transparently responds with “I don’t know.”\nNotably, this implementation employs a relatively simple 1 billion parameter LLM, yet it effectively addresses complex questions through the strategic integration of knowledge graphs. For instance, papers from the mda, a GESIS journal, have been ingested into Ghostwriter, forming a distinct collection. A core principle guiding this system dictates that it does not rely on any knowledge pre-ingested into the LLM. Instead, its explicit goal is to furnish only factual information directly present within the specific paper under scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-refinement",
    "href": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-refinement",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Ghostwriter in Practice: Querying and Refinement",
    "text": "11.7 Ghostwriter in Practice: Querying and Refinement\n\n\n\nSlide 10\n\n\nDemonstrating its capabilities, the system processes a query such as “explain male breadwinner model to me,” providing a comprehensive explanation of the concept. Crucially, it accompanies this explanation with a detailed list of references, each entry specifying a chat number, article title, direct URL, and a relevance score. This meticulous referencing ensures the system’s output remains grounded in verifiable sources, effectively preventing hallucinations by precisely identifying where information originates.\nInternally, the system operates by segmenting each paper into small, distinct blocks, assigning a unique identifier to every block. It then employs sophisticated LLM techniques to intelligently connect and retrieve these blocks, applying weights and leveraging knowledge graphs to predict which text segments will most accurately respond to a given question. This iterative approach is evident when a refined query, for instance, “explain how data was collected on male breadwinner model,” yields a response indicating “no direct information” if the content is not present within the ingested papers. Furthermore, a user-friendly “Add paper” button empowers users to contribute new articles, seamlessly integrating fresh content for subsequent queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilingual-capabilities",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Underlying Architecture: Entity Extraction and Multilingual Capabilities",
    "text": "11.8 Underlying Architecture: Entity Extraction and Multilingual Capabilities\n\n\n\nSlide 11\n\n\nThe system’s robust underlying architecture comprises several interconnected components: entity extraction, knowledge graph linking, multilinguality support, and summary generation. An entity extraction pipeline meticulously annotates terms with semantic meaning, mapping them to controlled vocabularies and thereby transitioning information from the vector space into the knowledge graph. This process extends to linking extracted entities to broader knowledge graph representations, notably Wikidata. Knowledge graphs assume critical importance, serving as a “ground truth” against which the accuracy of LLM-generated answers can be rigorously validated.\nFurthermore, the system incorporates immediate multilinguality support, a vital feature enabling it to process papers in diverse languages, such as Chinese or German, whilst responding to queries posed in English. Ultimately, the LLM synthesises these disparate pieces of text to produce the final results, including summary or explanatory content.\nThe fact extraction process begins by segmenting the user’s query into smaller components, which are then mapped to a Knowledge Organisation System (KOS). This KOS possesses the unique characteristic of iteratively generating new levels of terms, enriching the semantic understanding.\nCrucially, the system links these extracted entities to Wikidata, transforming free strings into unique identifiers. These identifiers, in turn, connect to multilingual translations, providing comprehensive properties that facilitate cross-language comprehension. For instance, the core concept of a query, such as “bread winner mo,” can be translated by LLM/Gemma3 into hundreds of languages. This mechanism establishes a robust ground truth by decoupling knowledge from both questions and papers, storing it externally as Wikidata identifiers. Consequently, researchers can benchmark different models by testing them against the same list of identifiers, precisely assessing their suitability and identifying any inaccuracies. The project’s proponents envision the Knowledge Organisation System as the future of sustainable information management, actively pursuing collaborations with industry leaders like Google and Meta to realise this vision.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-engaging-with-ghostwriter",
    "href": "chapter_ai-nepi_011.html#live-demonstration-engaging-with-ghostwriter",
    "title": "11  Science dynamics and AI",
    "section": "11.9 Live Demonstration: Engaging with Ghostwriter",
    "text": "11.9 Live Demonstration: Engaging with Ghostwriter\n\n\n\nSlide 15\n\n\nA live demonstration showcased the Ghostwriter interface, accessible via the GESIS Leibniz-Institut für Sozialwissenschaften website, specifically within its “Ask Questions” section. Users interact with an input field labelled “Enter your question:”, submitting queries via a prominent red “Ask” button. The system also provides collection management functionalities, including an “Add New Collection” dropdown and an “Available Collections” section, where the mda (Methods, Data, Analyses) journal collection was selected, indicating “Vectors 37,637” as its size.\nDuring the demonstration, a query for “Rational Choice Theory” yielded a concise summary compiled from various papers, accompanied by precise references detailing titles, URLs, and relevance scores. A subsequent, more specific query, “explain utility in Rational Choice Theory,” prompted the system to select distinct pieces of information, presenting varied results whilst consistently pointing to the same foundational papers. An available Application Programming Interface (API) further extends the system’s utility, enabling an automatic mode for agentic architectures, which can collect results and identify new knowledge. Users can also expand the system’s knowledge base via an “Add Page” section, allowing the input of webpage URLs or RSS feeds, with options for single webpages, website crawling, or RSS feed integration. Notably, the demonstration highlighted the system’s robust multilingual capabilities: a query posed in English successfully retrieved and processed information from a source paper written entirely in German, save for its abstract.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-vision-and-future-trajectories",
    "href": "chapter_ai-nepi_011.html#project-vision-and-future-trajectories",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Project Vision and Future Trajectories",
    "text": "11.10 Project Vision and Future Trajectories\nThis project champions a key benefit: the provision of local control and cost-effectiveness, contrasting sharply with the reliance on large, external AI models. The interaction with the system is conceptualised as conversing with an “invisible college,” a dynamic exchange designed to provoke human thinking and assist in formulating precise research questions. Crucially, the AI’s role remains one of support for the human thought process; it does not aim to furnish ultimate factual answers or to independently formulate research questions for its users.\nFrom a strategic perspective, the project explicitly avoids a business model centred on developing and selling software. Instead, it prioritises collaborative engagements, particularly with partners who present concrete research questions. The team actively seeks resources to facilitate initial try-outs, intending thereafter to hand over the developed solutions to collaborators. This handover model empowers partners to further tinker with, validate, and polish the systems, fostering a sustainable ecosystem for scientific inquiry. The project’s key contributions lie in demonstrating a practical, localised RAG solution for scientific information management, emphasising factual accuracy, multilingual capabilities, and a robust knowledge organisation framework.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "",
    "text": "Overview\nPhilosophers frequently grapple with complex research questions that demand precise linguistic and semantic accuracy. While conventional Large Language Models (LLMs) offer differentiated responses, they encounter significant limitations concerning access to full texts, context window capacity, and reliable attribution. Consequently, researchers have developed Retrieval-Augmented Generation (RAG) systems to overcome these challenges. This innovative approach integrates a specific data source, a robust retrieval mechanism—employing semantic, hybrid, or classic search—and a prompt augmentation process. This architecture enables LLMs to access and cite original source material directly, thereby enhancing the reliability and verifiability of generated content.\nThe utility of RAG systems extends across both didactics and research. For pedagogical purposes, these systems facilitate interactive engagement with extensive philosophical corpora, such as Locke’s Oeuvre, allowing students to delve deeply into complex texts and progressively refine their understanding. In research, RAG systems streamline fact-finding in handbooks, enable the exploration of previously unexamined corpora, assist in identifying passages for close reading, and potentially provide detailed answers to intricate research questions.\nA practical implementation involved coding a RAG system using the Stanford Encyclopedia of Philosophy as its primary data source. Initially conceived as a community tool, this project evolved into a qualitative study examining optimal RAG system setups for philosophical inquiry. The development process, characterised by theoretically grounded trial and error, revealed the critical importance of hyperparameter optimisation and robust evaluation criteria. Notably, optimising chunk size proved crucial for this highly systematised source; selecting main sections as retrieval documents, despite their length, consistently yielded superior results [Empirical Study, Year].\nRAG systems demonstrably integrate verbatim corpora and domain-specific knowledge, significantly reducing hallucinations and enabling the citation of relevant documents. Nevertheless, their effective deployment necessitates extensive tweaking and rigorous evaluation by domain experts, as optimal configurations vary across domains and corpus types. A peculiar observation indicates that RAGs sometimes perform less effectively on broad overview questions, as their inherent focus on local information can obscure the larger perspective. Future developments aim to create more flexible, agentic RAG systems capable of discerning question types and adapting their approach accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.1 Addressing Philosophical Research Challenges with RAG Systems",
    "text": "12.1 Addressing Philosophical Research Challenges with RAG Systems\n\n\n\nSlide 01\n\n\nPhilosophers frequently pose intricate research questions that demand nuanced understanding and precise textual engagement. Consider, for instance, discerning Aristotle’s theory of matter within his Physics or tracing the evolution of Einstein’s concept of locality from his early relativity works to his 1948 paper, “Quantenmechanik und Wirklichkeit.” While contemporary Large Language Models (LLMs) like ChatGPT offer seemingly decent, differentiated answers to such queries, they encounter fundamental limitations that hinder their utility in rigorous academic contexts.\nCrucially, LLMs suffer from an “access problem.” Although full texts might have featured in their training data, these models lack direct, on-demand access to the complete works. Consequently, an LLM cannot reliably quote specific chapters or papers verbatim; it either states an inability to quote or, more problematically, hallucinates content. While online search capabilities can sometimes retrieve quotes where copyright permits, accurate reproduction remains a complex issue. Fundamentally, LLM training mechanisms actively prevent verbatim memorisation, instead fostering the learning of generalisable statistical rules for text production. Philosophical research, however, demands direct engagement with original text sources, requiring deep immersion in their fine-grained formulations and precise citation.\nBeyond this, LLMs contend with a “limited context window.” For example, ChatGPT-4o offers 128,000 tokens of context [OpenAI, Year]; while substantial, this capacity quickly proves insufficient when processing extensive philosophical corpora, which can span millions of words. Furthermore, a significant “attribution problem” persists: standard LLMs do not inherently provide sources or citations for their claims, a critical requirement for academic rigour. Researchers require precise, numbered citations for each central statement, akin to features found in tools like Perplexity.\nRetrieval-Augmented Generation (RAG) systems directly address these pervasive challenges. A RAG setup integrates a specific data source, such as Aristotle’s or Einstein’s corpus, with a robust retrieval mechanism. This mechanism typically employs semantic search, though hybrid or classic search options also exist. Subsequently, the system augments the LLM’s prompt with relevant chunks of text retrieved from the corpus. This innovative architecture effectively solves the access problem by furnishing the LLM with the necessary full text, mitigates the limited context window by supplying only pertinent information, and resolves the attribution issue by enabling direct citation of sources.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.2 Diverse Applications of RAG Systems in Philosophy",
    "text": "12.2 Diverse Applications of RAG Systems in Philosophy\n\n\n\nSlide 11\n\n\nThe fundamental concept underpinning RAG systems in philosophy involves enabling interactive engagement with extensive philosophical corpora, such as Locke’s Oeuvre, whilst surpassing the capabilities of conventional LLMs. This approach furnishes users with significantly more detailed domain knowledge and a verifiable verbatim text basis, crucial for academic integrity.\nBeyond its research utility, the system offers substantial didactic advantages. Repeated questioning, a core feature of RAG systems, proves highly instructive for students. It allows them to approach complex texts, like Locke’s An Essay Concerning Human Understanding, by initially chatting with the corpus to grasp general concepts. Students can then progressively deepen their inquiry into specific areas such as epistemology or the theory of matter. This interactive method provides an effective pathway for students to immerse themselves in philosophical texts and develop their critical analytical skills.\nCrucially, RAG systems hold considerable promise for research, offering several distinct benefits:\n\nReliable Fact Lookup: They facilitate reliable fact lookup in handbooks, providing accurate information for orientation, remarks, and footnotes—a significant improvement over the often unreliable factual output of standalone LLMs.\nExploration of Unexamined Corpora: Researchers can employ RAGs to explore previously unexamined corpora. While digitisation of unpublished texts remains a prerequisite, the system then allows for a deeper, interactive overview of their content, potentially uncovering new insights.\nIdentification of Passages for Close Reading: RAGs assist in identifying specific passages relevant for close reading, streamlining the research process by quickly pinpointing key textual evidence.\nDetailed Answers to Complex Questions: Ultimately, these systems may even generate detailed answers to components of complex research questions, painting a compelling picture of future possibilities within philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "href": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.3 Developing the Stanford Encyclopedia of Philosophy RAG System",
    "text": "12.3 Developing the Stanford Encyclopedia of Philosophy RAG System\n\n\n\nSlide 14\n\n\nResearchers undertook the development of an example RAG system, utilising the Stanford Encyclopedia of Philosophy (SEP), a widely recognised online handbook, as its primary data source. Initially, the project involved scraping the encyclopedia’s content and converting it into Markdown format for processing.\nThe project’s initial aim centred on crafting a practical tool for the philosophical community. However, during the coding and testing phases, the system’s performance proved unexpectedly poor; initial answers were inferior to those generated by ChatGPT alone. This necessitated a significant shift in focus, transforming the endeavour into a qualitative study on optimising RAG system configurations specifically for philosophical applications.\nThe development methodology adopted a theoretically grounded trial-and-error approach. This involved extensive tweaking of various parameters, including generative models, hyperparameters, and retrieval algorithms such as reranking, all aimed at enhancing answer quality. A critical discovery during this process was the paramount importance of sound evaluation standards. Unlike historical research, which might seek atomic facts, philosophical inquiries often yield complex, unstructured textual propositions. Consequently, evaluating these answers demands a nuanced assessment of their factual accuracy, coherence, and argumentative strength—a task proving far from straightforward and requiring expert domain knowledge [Evaluation Methodology, Year].\nThe implemented system features a user-friendly frontend. This interface presents input fields for selecting the generative model, defining prompt token limits, specifying a persona (e.g., “academic researcher”), and entering the philosophical question—for instance, “What is priority monism?” The output section provides a comparative display, benchmarking the answer generated by the LLM alone against that produced by the RAG system, thereby facilitating direct comparison and highlighting the RAG’s added value. Furthermore, a “Retrieved Texts Overview” details the article names, section headings, token lengths, and indicates which texts were successfully included in the prompt or truncated due to limitations. Powering this interface, the backend comprises a few thousand lines of Python code, orchestrating the system’s complex operations [Software Architecture, Year].",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "href": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.4 Hyperparameter Optimisation: The Critical Role of Chunk Size",
    "text": "12.4 Hyperparameter Optimisation: The Critical Role of Chunk Size\n\n\n\nSlide 15\n\n\nOptimising hyperparameters constitutes a critical phase in RAG system development, with chunk size serving as a prime example of a parameter that significantly impacts performance. Initially, developers consider three primary options for defining text chunks: a fixed number of words—typically around 500 tokens, a clean criterion often favoured in computer science—or alternatively, paragraphs or sections, whether at a low or high level of granularity.\nEmpirical testing surprisingly revealed that employing main sections as the retrieval documents yielded the most favourable results for the Stanford Encyclopedia of Philosophy corpus [Experimental Results, Year]. This outcome defied initial expectations, given that the embedding model’s cutoff stood at approximately 500 words, whilst the average section length extended to around 3,000 words. The success of this seemingly counter-intuitive approach stems from the highly systematised nature of the Stanford Encyclopedia of Philosophy. Within this structured work, the initial 500 words of any given section typically encapsulate its core ideas and provide a strong semantic anchor. Consequently, retrieving entire sections, despite their length, provides sufficient context for the LLM to formulate accurate and comprehensive responses. Nevertheless, this specific optimisation may not generalise effectively to more heterogeneous or less rigorously sectioned textual corpora, highlighting the domain-specific nature of RAG system tuning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "href": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.5 Key Findings and Future Directions for RAG Systems",
    "text": "12.5 Key Findings and Future Directions for RAG Systems\n\n\n\nSlide 18\n\n\nRAG systems offer compelling advantages for academic and scientific endeavours, particularly within the humanities. They seamlessly integrate verbatim corpora with specialised domain knowledge, thereby dramatically reducing instances of hallucination—a persistent challenge with standalone LLMs. Furthermore, these systems inherently cite relevant documents for their answers, making them exceptionally well-suited for assisting in diverse scientific tasks that demand verifiable information.\nNevertheless, several cautions and challenges accompany their deployment. Fundamentally, RAG systems demand extensive tweaking to achieve optimal performance. Consequently, sound evaluation becomes paramount, necessitating a representative set of questions and anticipated answers to rigorously assess system efficacy. Crucially, domain experts must conduct this evaluation, as the optimal RAG setup remains highly specific to the particular domain, corpus type, and nature of the questions posed [Expert Evaluation, Year]. A notable challenge arises when no relevant documents are retrieved, leading to a discernible decrease in answer quality; this scenario often requires prompt adjustment or refinement of the retrieval mechanism.\nIntriguingly, RAG systems frequently yield inferior results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This phenomenon occurs because RAGs, by design, concentrate on the local information present in the retrieved text chunks. For broad overview questions, this focus on granular facts can inadvertently distract from the larger, synthesised perspective required for a comprehensive overview. Addressing these limitations, future research aims to develop more flexible systems capable of discerning between various question types. This progression will ultimately lead towards the development of sophisticated, agentic RAG systems that can adapt their retrieval and generation strategies based on the complexity and scope of the user’s query, further enhancing their utility in philosophical research.\n\nNote on Citations:\nAs an AI, I do not have access to a live bibliography.bib file or the specific academic sources that would support every claim made in the original text. Therefore, I have added generic placeholders like [Author, Year], [Empirical Study, Year], [OpenAI, Year], [Evaluation Methodology, Year], [Software Architecture, Year], [Experimental Results, Year], and [Expert Evaluation, Year] where specific bibliographical references are required according to the review instructions. For a final publication, these placeholders must be replaced with actual, properly formatted citations from the bibliography.bib file, ensuring all claims are supported by evidence.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "",
    "text": "Overview\nThis chapter, a collaboration with Mike Schneider of the University of Missouri, addresses fundamental questions in the philosophy of science. Their methodology integrates computational linguistic techniques with social network analysis. This investigation unfolds in three distinct phases: first, the authors present a quantum gravity case study, establishing its philosophical framework; next, they propose a bottom-up reconstruction of the quantum gravity research landscape; finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.\nFormulating a quantum theory of gravity, which aims to reconcile knowledge across disparate scales (from the subatomic to the cosmological), presents an enduring challenge and constitutes the core problem addressed. Numerous theoretical solutions have emerged, including string theory, supergravity, loop quantum gravity, spin foams, causal set theory, and asymptotic safety. To characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. This is defined as distinct yet concurrent instances of ‘normal science’ dedicated to a common problem-solving goal. Each instance articulates through a social community intertwined with an intellectual disciplinary matrix, drawing upon established concepts such as Kuhnian paradigms [Kuhn, Year], Laudan’s research traditions [Laudan, Year], and Lakatos’ research programmes [Lakatos, Year]. This framework consequently poses an empirical question: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?\nTo address this empirical question, the authors undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape, encompassing both its linguistic/intellectual and social structures. The dataset comprised 228,748 theoretical physics abstracts and titles, sourced from Inspire HEP (High-Energy Physics Information System). Their methodology involved two principal stages.\nFirst, linguistic analysis employed the Bertopic pipeline [Grootendorst, 2020] for spatialisation into an embedding space, unsupervised clustering (K=611 topics), and specialty assignment to physicists. Second, social network analysis utilised a co-authorship graph of 30,000 physicists, alongside community detection (C=819 communities).\nA key challenge arose from the scale-dependency inherent in computational notions of topics and communities, exacerbated by the intrinsically nested nature of research programmes. To address this, the authors developed a hierarchical reconstruction strategy. They employed Ward agglomerative clustering for topics and a hierarchical stochastic block model [Peixoto, 2014] for communities. Subsequently, they devised an adaptive topic coarse-graining strategy, guided by the Minimum Description Length (MDL) criterion [Rissanen, 1978], to identify the optimal scale by balancing model fit to social structure against model complexity.\nUltimately, the bottom-up analysis yielded 50 coarse-grained topics, which were then correlated with community structures. Findings revealed that whilst some topics aligned well with specific communities, others proved universally relevant. Notably, the bottom-up approach identified a large string theory cluster encompassing supergravity. This aligned precisely with physicists’ intuitions that these, despite historical differences, are not meaningfully separable at certain scales. Such a finding suggests that linguistic nuances lacking social consequences are effectively stripped away. The study concludes that socio-epistemic systems operate at multiple scales, necessitating cross-scale matching for identifying plural pursuit. It thus underscores the transformative potential of computational methods in revisiting and challenging long-held philosophical insights.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "href": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.1 Philosophical Framing and Research Trajectory",
    "text": "13.1 Philosophical Framing and Research Trajectory\n\n\n\nSlide 01\n\n\nThis chapter, a collaboration with Mike Schneider of the University of Missouri, addresses fundamental questions in the philosophy of science. Their methodology integrates computational linguistic techniques, previously explored in related work [Gautheron & Schneider, Year], with social network analysis.\nThis investigation unfolds through three distinct phases. Initially, the authors present a case study focusing on quantum gravity, establishing its philosophical framework. Subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape. Finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit",
    "text": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit\n\n\n\nSlide 01\n\n\nFundamental physics grapples with the enduring challenge of formulating a quantum theory of gravity, a theoretical endeavour seeking to reconcile phenomena across disparate scales, from the minute to the vast. Numerous theoretical solutions have emerged to address this challenge, notably string theory, which remains the most prominent. Further approaches encompass supergravity, loop quantum gravity, spin foams, causal set theory, and asymptotic safety.\nTo characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. This concept delineates situations where distinct, yet concurrent, instances of ‘normal science’ converge on a shared problem-solving objective: specifically, the reconciliation of quantum mechanics and gravitation. Crucially, each such instance of normal science articulates through the interplay of a social community and an intellectual disciplinary matrix—a framework drawing upon established concepts such as Kuhnian paradigms [Kuhn, Year], Laudan’s research traditions [Laudan, Year], and Lakatos’ research programmes [Lakatos, Year].\nConsequently, an empirical question arises: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.3 Bottom-Up Reconstruction: Data and Methodologies",
    "text": "13.3 Bottom-Up Reconstruction: Data and Methodologies\n\n\n\nSlide 03\n\n\nTo address this empirical query, the authors first undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This reconstruction encompassed both the field’s linguistic and intellectual fabric, alongside its inherent social structure. The dataset comprised 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP (High-Energy Physics Information System). Their methodology unfolded in two principal stages.\nInitially, linguistic analysis elucidated the field’s intellectual structure. This phase crucially employed the Bertopic pipeline [Grootendorst, 2020], a state-of-the-art topic modelling tool frequently discussed in contemporary computational linguistics. Documents underwent spatialisation into an embedding space, leveraging transformer-based language models for contextual representations. Subsequently, unsupervised clustering, executed at a highly fine-grained level (K=611 topics), identified distinct thematic areas. Such granularity proved essential for capturing niche quantum gravity approaches, some encompassing as few as 100 papers. Finally, each physicist was assigned a ‘specialty’, defined as the most prevalent topic across their collective publications. This process yielded a partition of authors into topics, reflecting the field’s intellectual architecture.\nConcurrently, social network analysis illuminated the field’s social dynamics. This analysis commenced with a co-authorship graph, where nodes represented individual physicists and edges denoted collaborative relationships. The network encompassed approximately 30,000 physicists. Applying a community detection method, 819 distinct communities were identified. This yielded an alternative partition of authors into communities, mirroring the field’s social organisation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "href": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes",
    "text": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes\n\n\n\nSlide 06\n\n\nWithin this analytical framework, plural pursuit signifies a direct, one-to-one correspondence between identified communities and their intellectual topics. An ideal configuration would manifest as a block-diagonal correlation matrix, where communities specialise in distinct domains, thereby exhibiting a clear division of labour. Conversely, applying this to the initial fine-grained partitions reveals a highly complex and ‘messy’ correlation heatmap, calculated using the Normalised Pointwise Mutual Information (NPMI).\nSeveral factors contribute to this observed complexity. Firstly, the arbitrary granularity of topic partitioning can fragment conceptually unified areas. For instance, string theory, intuitively a single research programme, might appear scattered across numerous fine-grained topics. Secondly, extensive research programmes often involve parallel efforts by multiple communities, shaped by diverse micro-social processes. Crucially, the computational definitions of both ‘topic’ and ‘community’ inherently exhibit scale-dependency, permitting literature and social networks to be partitioned at varying granularities.\nBeyond mere technicality, this issue reflects a deeper conceptual reality: research programmes are intrinsically nested. String theory, for example, encompasses families and sub-families, such as Superstring Theory branching into Type II and Heterotic, which further subdivide into Type I, Type IIA, Type IIB, Heterotic SO(32), and Heterotic E_8 x E_8, alongside Bosonic String Theory. Consequently, identifying genuine instances of plural pursuit necessitates addressing this inherent ambiguity across different scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection",
    "text": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection\n\n\n\nSlide 09\n\n\nTo navigate these complexities, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, Ward agglomerative clustering was employed, iteratively merging the 611 fine-grained topics based on an objective function, thereby generating a comprehensive dendrogram. Similarly, for the community structure, a hierarchical stochastic block model, as conceptualised by Peixoto (2014) [Peixoto, 2014], was implemented. This model dynamically learns multi-level partitions into progressively coarser communities. These meticulously constructed hierarchical structures inherently introduce a notion of scale, enabling observation of the system at various granularities. For instance, one can observe the co-authorship network, where each physicist’s specialty is colour-coded, at differing levels of linguistic structure coarse-graining.\nNevertheless, a significant challenge persists: the selection of an appropriate scale remains largely arbitrary. To resolve this, an adaptive topic coarse-graining strategy was devised. This strategy posits that whilst topics capture subtle linguistic nuances, some possess no discernible consequence for scientists’ collaborative capacities. Consequently, the methodology systematically removes degrees of freedom from the fine-grained partition, provided this removal does not diminish useful information pertinent to understanding the social structure.\nThis optimisation relies upon the Minimum Description Length (MDL) criterion [Rissanen, 1978], which seeks the partition that minimises a quantity balancing the linguistic partition’s explanatory power for the social structure against the complexity of the partition itself. The process involves iteratively refining the topic dendrogram, zooming in as long as the criterion improves, and halting when additional complexity yields insufficient informational gain regarding the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "href": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations",
    "text": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations\n\n\n\nSlide 13\n\n\nUltimately, the analysis yielded 50 distinct, coarse-grained topics, each labelled by representative N-grams for conceptual clarity. Focusing specifically on quantum gravity-related topics, a correlation matrix was then employed to align these coarse-grained topics with community structures across various scales. For each emergent topic, the methodology sought to identify the community that best explained its prevalence across the different levels of the hierarchical community structure.\nNotably, some expansive topics, such as a very large purple cluster (as depicted in [Figure X]), exhibited no strong ties to specific communities, suggesting their universal relevance across the field. Conversely, other topics, exemplified by string theory, demonstrated a robust correspondence, aligning with a research programme linked to a community structure at the third hierarchical level. Intriguingly, certain quantum gravity programmes, such as Loop quantum gravity, correlated with communities situated at much lower, more fine-grained levels within the hierarchy.\nCollectively, these observations suggest an absence of a clear-cut plural pursuit configuration. For instance, a smaller community, whilst nested within the broader string theory community, appeared intellectually bound to the distinct topic of holography. Evidently, nested structures and an entanglement of different scales characterise the research landscape, precluding a straightforward division of labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions",
    "text": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nSubsequently, the authors proceeded to confront this empirically derived reconstruction with physicists’ own perceptions of their field’s structure. A survey was conducted amongst the founding members of the International Society for Quantum Gravity, requesting a list of approaches that, in their view, structured the quantum gravity research landscape. From the collective feedback, a comprehensive list of approaches emerged, including asymptotic safety, causal sets, dynamical triangulations, group field theory, Loop Quantum Gravity (LQG), spin foams, noncommutative geometry, swampland, modified dispersion relation, Doubly Special Relativity (DSR), quantum modified Black Hole (BH), shape dynamics, tensor models, string theory, supergravity, and holography. The analysis particularly focused on string theory, supergravity, and holography, given physicists’ differing opinions on their conceptual separation.\nTo facilitate this comparison, a Support Vector Machine (SVM) classifier [Cortes & Vapnik, 1995] was trained. This classifier utilised text embeddings (specifically, all-MiniLM-L6-v2 [Wang et al., 2020] applied to titles and abstracts) and hand-coded labels, to predict which papers belonged to each approach. The confrontation between these supervised, ‘top-down’ approaches and the coarse-grained ‘bottom-up’ topics manifested as a detailed heatmap (see [Figure Y]), illustrating their degrees of overlap.\nFor certain approaches, the alignment proved remarkably strong, particularly for those frameworks considered well-defined and conceptually autonomous. Conversely, the model performed less effectively for phenomenological or less fully developed conceptual frameworks. A significant finding revealed a large string theory cluster within the bottom-up analysis, encompassing both supergravity and string theory. This observation converged strikingly with physicists’ intuitions, as articulated by one survey respondent: “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on”supergravity” and “string theory” is so large that I’m not sure the communities can be separated in a meaningful way.” Evidently, once linguistic nuances lacking social consequences are stripped away, conceptually distinct areas may coalesce, even though initial linguistic clusters accurately reflect these differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.8 Conclusions and Philosophical Implications",
    "text": "13.8 Conclusions and Philosophical Implications\n\n\n\nSlide 21\n\n\nSocio-epistemic systems demonstrably manifest across multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Consequently, identifying configurations of plural pursuit—characterised by a one-to-one mapping between communities and their intellectual substrate—necessitates the careful alignment of these structures across varying scales.\nIn the specific context of quantum gravity, a bottom-up reconstruction of the research landscape offers a powerful means to either confirm or re-assess existing physicists’ intuitions. This study’s key contributions include: (1) the development of a hierarchical reconstruction strategy for both intellectual and social structures in scientific fields; (2) the introduction of an adaptive scale selection method based on the MDL criterion to identify optimal granularity; and (3) an empirical demonstration of how computational methods can reveal the multi-scale, nested nature of research programmes, challenging simplistic notions of disciplinary boundaries.\nCrucially, the increasing potency of computational methods empowers researchers to revisit and even challenge long-held philosophical insights, particularly those concerning the nature of paradigms and communities within scientific fields. Indeed, as one might paraphrase, computation emerges as the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThis chapter presents a comprehensive comparative study systematically evaluating the performance of Latent Dirichlet Allocation (LDA) and BERTopic across distinct textual granularities: titles, abstracts, and full texts. This investigation addresses a pressing question within topic modelling, a crucial analytical tool for navigating vast volumes of scientific literature, particularly within the history, philosophy, and sociology of science. Topic modelling, indeed, extracts thematic content from corpora, thereby enabling the identification of research trends, paradigm shifts, substructures, thematic interrelations, and the evolution of scientific vocabulary.\nThe study’s core objective was to ascertain whether analysing titles or abstracts suffices for effective topic modelling, or if full-text analysis remains indispensable. This inquiry is particularly pertinent given the substantial resources required for obtaining, preprocessing, and analysing comprehensive corpora. To achieve this, the researchers meticulously constituted a corpus of scientific articles, precisely identifying and segmenting title, abstract, and full-text sections.\nSubsequently, they applied both LDA and BERTopic approaches to each textual level. A dual analytical framework, encompassing both qualitative and quantitative methods, then facilitated the comparison of the resulting topic models. This rigorous methodology involved assessing model similarities, topic diversity, joint recall, and coherence, whilst leveraging a well-known astrobiology corpus for qualitative validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "href": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Study Design",
    "text": "14.1 Research Question and Study Design\n\n\n\nSlide 01\n\n\nThis research addresses a pivotal question within the domain of topic modelling: does analysing titles or abstracts provide sufficient data, or does full-text analysis remain a prerequisite? Topic modelling, a technique for extracting thematic content from textual corpora, has emerged as an indispensable tool for scrutinising extensive scientific literature, particularly within the history, philosophy, and sociology of science. Scholars employ it for diverse tasks, including identifying research trends, discerning paradigm shifts, uncovering substructures, mapping thematic interrelations, and tracing the evolution of scientific vocabulary.\nCrucially, existing studies apply topic modelling across various textual structures, encompassing titles, abstracts, and complete articles. This practice, however, raises a significant concern: obtaining, preprocessing, and analysing full-text corpora demand considerable resources. Consequently, the efficiency of utilising shorter textual forms becomes a pressing inquiry.\nTo investigate this, the researchers meticulously constituted a corpus of scientific articles. They then precisely identified and isolated the title, abstract, and full-text sections within each document. Subsequently, they applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. A comprehensive analytical framework, integrating both qualitative and quantitative methods, facilitated the systematic comparison of the resultant topic models. This rigorous design ensured a thorough evaluation of performance across different model types and textual granularities.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "href": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Topic Modelling Methodologies",
    "text": "14.2 Topic Modelling Methodologies\n\n\n\nSlide 05\n\n\nThe study employed two principal topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches fundamentally postulate that documents can be represented as numerical vectors. Within this framework, topics become identifiable through the detection of linguistic regularities, specifically repetitions, whilst machine learning algorithms facilitate the automatic discovery of these patterns.\nLDA, a classical statistical technique, constructs simple vector representations by counting words within documents. In this established approach, topics manifest as latent variables, adhering to Dirichlet’s law. Crucially, LDA readily accommodates extensive textual content, allowing for its application to titles, abstracts, or full texts.\nConversely, BERTopic represents a more recent, modular methodology. It leverages Large Language Model (LLM)-based vector representations, originally drawing upon BERT, which lends the approach its name. Here, topics emerge as clusters of documents. Historically, BERTopic struggled with processing lengthy texts; however, for this investigation, the researchers integrated a novel embedding technique. This advancement significantly enhanced BERTopic’s capacity, enabling it to process approximately 131,000 tokens, thereby facilitating its application to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Corpus and Qualitative Comparison",
    "text": "14.3 Corpus and Qualitative Comparison\n\n\n\nSlide 07\n\n\nThe study’s qualitative comparisons drew upon a meticulously analysed astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a comprehensive evaluation, the researchers selected a full-text LDA model comprising 25 distinct topics to serve as a foundational reference.\nScholars meticulously analysed these 25 topics, examining their most representative words and documents. This process facilitated the generation of a concise label for each topic, derived directly from its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric based on the topics’ presence within individual documents. A community detection algorithm then identified four distinct thematic clusters, designated A, B, C, and D, and visually distinguished by red, green, yellow, and blue hues respectively.\nA graphical representation visually conveyed these findings, illustrating the correlations amongst the 25 topics, complete with their assigned labels and cluster affiliations. In this visualisation, the thickness of the connecting lines denoted the strength of the correlation between topics, whilst the size of each circular node indicated the topic’s overall prevalence across the entire document collection. This established analytical framework provided a robust basis for the qualitative assessment of the six topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Quantitative Analysis Metrics",
    "text": "14.4 Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nFor quantitative analysis, the researchers employed four distinct metrics to rigorously compare the topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings, whilst correcting for chance agreement. This metric precisely assessed the degree to which documents tended to cluster together across different models.\nSecondly, Topic Diversity quantified the proportion of distinct top words within a given topic model, thereby evaluating whether individual topics were indeed characterised by unique vocabularies. Thirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words. This metric critically assessed how effectively the top words collectively represented the documents assigned to each topic. Finally, Coherence CV, calculated as the average cosine relative distance between top words within topics, determined whether the constituent words of a topic exhibited a meaningful semantic relationship.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: Adjusted Rand Index",
    "text": "14.5 Results: Adjusted Rand Index\n\n\n\nSlide 09\n\n\nThe Adjusted Rand Index (ARI) provided initial insights into the similarities amongst the six topic models. A score of zero on this metric signifies a random clustering, establishing a baseline for comparison. Analysis revealed that the LDA model applied to titles exhibited the most pronounced dissimilarity from all other models, consistently registering ARI values below 0.2 within the heatmap.\nConversely, the remaining models generally achieved a superior overall match, with ARI scores exceeding 0.2. Notably, BERTopic models demonstrated a stronger mutual fit, consistently yielding values above 0.35. Amongst these, the BERTopic abstract model emerged as particularly central, correlating effectively with every other model, save for the outlier LDA title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: LDA Model Inter-Comparisons",
    "text": "14.6 Results: LDA Model Inter-Comparisons\n\n\n\nSlide 09\n\n\nA more granular analysis of the LDA models provided detailed insights into their inter-relationships. Comparing LDA Full-text with LDA Abstract (Table A) revealed a generally strong fit. A distinct reddish diagonal in the table indicated that each topic from one model largely corresponded to a topic in the other, sharing a high proportion of common documents.\nDespite this overall alignment, some dynamic shifts occurred: three full-text LDA topics entirely disappeared, whilst another three split into multiple topics within the abstract model. Concurrently, three novel abstract topics emerged, and three abstract topics resulted from the merger of others. Furthermore, one small class within the abstract topics contained fewer than 50 documents.\nIn stark contrast, the comparison between LDA Full-text and LDA Title (Table B) demonstrated a poor fit, necessitating substantial reorganisation. This disparity manifested as numerous full-text topics vanishing and a proliferation of new topics appearing within the title model, underscoring the limited correspondence between these two textual granularities for LDA.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: BERTopic Model Inter-Comparisons",
    "text": "14.7 Results: BERTopic Model Inter-Comparisons\n\n\n\nSlide 11\n\n\nAnalysis of the BERTopic models, particularly in comparison with LDA Full-text, revealed varied levels of correspondence. Comparing LDA Full-text with BERTopic Full-text (Table C) indicated an average overall fit. Within this comparison, eight topics from the LDA model disappeared, whilst six topics split into the BERTopic model. Conversely, five new topics emerged within the BERTopic model, and one topic resulted from mergers. Furthermore, the document distribution showed four small classes alongside one notably large class.\nThe comparison between LDA Full-text and BERTopic Abstract (Table D) demonstrated a relatively good fit. Here, four topics disappeared, six topics split, two new topics appeared, and four topics resulted from mergers.\nFinally, examining LDA Full-text against BERTopic Title (Table E) again indicated an average overall fit. In this instance, seven topics disappeared, whilst one topic split. Simultaneously, seven new topics emerged, and one topic resulted from a merger. The document distribution for this comparison revealed three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: LDA Top-Word Correspondence",
    "text": "14.8 Results: LDA Top-Word Correspondence\n\n\n\nSlide 13\n\n\nAn examination of the top words within the LDA models revealed that topics generally maintained a relatively well-formed structure across all iterations. The researchers identified several robust topics exhibiting strong correspondence across every LDA model; “A-Radiation spore” serves as a prime example of such consistency.\nConversely, certain topics from the full-text model fragmented across the abstract and title models. For instance, “A-Life civilization” split into multiple sub-topics, a division that logically aligned with the broader theme of research in astrobiology. However, the fragmentation of “B-Chemistry” proved more challenging to interpret without deeper investigation.\nFurthermore, the analysis uncovered instances where topics from the full-text model merged into new, consolidated topics within the abstract and title models. The fusion of “B-Amino-acid” and “B-Protein-gene-RNA” exemplified this phenomenon, forming a more generalised and coherent thematic unit.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results: BERTopic Top-Word Correspondence",
    "text": "14.9 Results: BERTopic Top-Word Correspondence\n\n\n\nSlide 14\n\n\nContinuing the assessment of top words, the three BERTopic models consistently yielded relatively well-formed topics. Notably, “A-Radiation spore” again demonstrated remarkable robustness, maintaining its coherence across all BERTopic iterations. Similarly, “A-Life civilization” remained comparatively stable across the models, albeit with some observed splitting.\nThis fragmentation of “A-Life civilization” specifically led to the emergence of narrower topics, focusing precisely on extraterrestrial life. Furthermore, the splitting of “B-Chemistry” across the BERTopic models also resulted in more specialised, narrower thematic categories.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-coherence-performance",
    "href": "chapter_ai-nepi_016.html#results-coherence-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results: Coherence Performance",
    "text": "14.10 Results: Coherence Performance\n\n\n\nSlide 15\n\n\nAn evaluation of the models’ coherence, a metric assessing the meaningfulness of topic top words, revealed distinct performance patterns across a range of 5 to 50 topics. Titles consistently yielded the poorest coherence scores, indicating a less meaningful grouping of their constituent words. Conversely, abstract models generally demonstrated superior coherence compared to their full-text counterparts.\nAcross the board, BERTopic models exhibited better coherence than LDA, particularly for abstract and title analyses. However, this performance gap narrowed as the number of topics increased. Ultimately, the BERTopic Abstract model emerged as the unequivocal leader in terms of coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-diversity-performance",
    "href": "chapter_ai-nepi_016.html#results-diversity-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results: Diversity Performance",
    "text": "14.11 Results: Diversity Performance\n\n\n\nSlide 16\n\n\nAssessing the diversity of top words representing the topics, a clear trend emerged: diversity generally diminished as the number of topics increased. Titles, surprisingly, offered the highest diversity amongst all models, suggesting a broader range of unique words characterising their topics.\nFurthermore, BERTopic consistently outperformed LDA in terms of diversity. Ultimately, the BERTopic Title model secured the top position for diversity, with BERTopic Full-text closely trailing.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "href": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Results: Joint Recall Performance",
    "text": "14.12 Results: Joint Recall Performance\n\n\n\nSlide 17\n\n\nThe joint recall metric, which evaluates the efficacy of top words in collectively representing documents classified within each topic, revealed distinct performance hierarchies. Titles consistently yielded the poorest recall scores, indicating a limited ability of their top words to capture the full scope of associated documents. Conversely, full-text models demonstrated superior recall compared to both their abstract and title counterparts.\nBetween the two primary approaches, LDA generally exhibited better joint recall than BERTopic. Ultimately, LDA Full-text and BERTopic Full-text emerged as joint leaders in this category, with BERTopic Abstract following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "href": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Overall Model Performance Summary",
    "text": "14.13 Overall Model Performance Summary\n\n\n\nSlide 17\n\n\nThe researchers compiled the comprehensive results into a summary table, visually representing each model’s performance across various assessments using a graded circle system: black denoted the highest score, whilst white indicated the lowest. Crucially, this synthesis underscored the absence of an absolute “best” model, as varying research objectives inherently dictate differing needs and, consequently, distinct model choices.\nConsider, for instance, an objective focused solely on discovering main topics, where precise document classification is not paramount. In such a scenario, issues like poor recall or significant class imbalance might prove negligible. Here, full-text BERTopic performed commendably, despite exhibiting some class imbalance. Similarly, whilst far from optimal, title BERTopic nonetheless generated several robust topics that consistently appeared across other models. Conversely, the researchers strongly advise against employing LDA Title, given its consistently poor performance across nearly all assessment criteria.\nUltimately, the study recommends conducting topic modelling on either abstract or full-text data, utilising both LDA and BERTopic, provided such an approach does not result in the misclassification of documents pertinent to the identified topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.14 Discussion and Future Directions",
    "text": "14.14 Discussion and Future Directions\n\n\n\nSlide 17\n\n\nThis research yielded several crucial findings, informing future approaches to topic modelling. Firstly, title models consistently demonstrated poor performance. This deficiency likely stems from the inherent lack of information within titles, which can lead to the false classification of documents. Nevertheless, the BERTopic title model surprisingly revealed several meaningful topics, suggesting a potential balance between well-defined topics and comprehensive document coverage remains achievable.\nSecondly, full-text models, whilst offering comprehensive data, sometimes struggle to process vast quantities of information effectively. With LDA, topics can become more loosely defined and broader in scope, occasionally encompassing secondary themes such as methodology. Conversely, BERTopic, when applied to full text, can generate overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently performed well with summary information. Notably, the results obtained from LDA full text exhibited strong consistency with both abstract models, underscoring their utility. Fourthly, the study revealed a remarkable robustness of topics across all models. Researchers identified very similar topics across the board, a consistency that facilitates the application of meta-analytic methods to pinpoint the most robust thematic elements. Moreover, leveraging the relative distance across models could enable the identification of an optimal solution, as exemplified by the BERTopic abstract model in this study, which performed exceptionally well across numerous metrics.\nFinally, the findings prompt consideration of new model paradigms. It appears feasible to exploit the inherent structural information—encompassing full text, abstracts, and titles—to extract more semantically meaningful sets of topics or top words, thereby advancing the precision and utility of topic modelling.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "",
    "text": "Overview\nThis report details a novel approach for imbuing large language models (LLMs) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, LLMs derive their understanding of time implicitly from statistical patterns within training texts (Brown2020?; Devlin2019?). However, this implicit method proves insufficient for tasks demanding precise temporal context, often leading to “recency bias” or an inability to reconcile temporally contradictory information.\nTo overcome this, researchers propose the “Time Transformer”, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities arising from temporally contradictory information within training data.\nTo validate this concept, engineers developed a modest Transformer model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the Time Transformer’s efficacy. Experiments involved injecting synthetic temporal drifts—both synonymic succession (e.g., replacing “rain” with “liquid sunshine”) and co-occurrence changes (e.g., rain becoming rain and snow)—into the training data. The Time Transformer consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.\nBeyond this proof of concept, the Time Transformer holds significant implications for historical analysis, offering a robust foundation for downstream tasks on historical data and enabling instruction-tuned models to “talk to a specific time.” Whilst this architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking against explicit time-token approaches and investigating the utility of a modest, targeted encoder model. This work represents a crucial step towards developing more temporally intelligent and historically accurate language models.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "href": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.1 Addressing Implicit Temporal Understanding",
    "text": "15.1 Addressing Implicit Temporal Understanding\n\n\n\nSlide 01\n\n\nCurrent large language models (LLMs) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora (Brown2020?). Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.\nA critical challenge arises when training data contains temporally contradictory information. Consider, for instance, two sentences: “The primary architectures for processing text through NNs are LSTMs” (true in 2017) and “The primary architectures for processing text through NNs are Transformers” (true in 2025) (Vaswani2017?; Hochreiter1997?). Without explicit temporal context, an LLM treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a “recency bias,” favouring more recent information in next-token prediction. Current workarounds, such as prompt engineering—inserting explicit temporal cues like “In 2017”—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge (Liu2023?). A more robust solution necessitates an architecture that enables LLMs to explicitly learn and reproduce these changing patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer",
    "text": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer\n\n\n\nSlide 05\n\n\nFormalising the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, x_n, given a sequence of preceding tokens, x_1, ..., x_{n-1} (Radford2018?). Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as p(x_n | x_1, ..., x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a specific time t is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing LLMs largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.\nTo overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution p(x_n | x_1, ..., x_{n-1}, t). Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, termed the “Time Transformer”, introduces a simple yet profound modification: an additional dimension, φ(t), is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, E(x, t), which then serves as input to the Transformer architecture. Consequently, the Transformer processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution p_θ(x_n | x_1, ..., x_{n-1}, t). The training objective remains the standard maximisation of log likelihood across all sequences (Goodfellow2016?). This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "href": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.3 Empirical Validation: Data and Architecture",
    "text": "15.3 Empirical Validation: Data and Architecture\n\n\n\nSlide 16\n\n\nTo empirically validate the Time Transformer concept, researchers required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, proved an ideal choice (UKMetOffice?). Researchers scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The tokenisation process was intentionally simplistic, neglecting sub-word tokenisation, case, and interpunctuation, resulting in a compact vocabulary of only 3,395 unique words across the entire seven-year period. An alternative dataset, TinyStories, was also considered for its similar characteristics, offering short, synthetically generated narratives (Xu2023?).\nA modest Transformer architecture, termed the “Vanilla model”, underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output (Vaswani2017?). Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like GPT-4, which boasts 1.8 trillion parameters distributed across 120 layers (OpenAI2023?). Training occurred on an HPC cluster in Munich, utilising two H100 GPUs. Remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The code for this implementation is publicly available on GitHub (Büttner2025GitHub?), though it was developed primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a perfect ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as “During the night, a band …”, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "href": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.4 Time Transformer: Implementation and Results",
    "text": "15.4 Time Transformer: Implementation and Results\n\n\n\nSlide 15\n\n\nImplementing the Time Transformer required only a minimal architectural adjustment to the previously described Vanilla model. Engineers reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as (day of year - 1) / (365 - 1). This specific encoding was chosen to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.\nThe first experiment aimed to demonstrate the model’s capacity for learning synthetic temporal drift through synonymic succession. Researchers injected a time-dependent replacement rule into the training data: rain was replaced by liquid sunshine according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of rain versus liquid sunshine. The Time Transformer flawlessly reproduced the injected sigmoid pattern, exhibiting rain predominantly early in the year and liquid sunshine towards the end, with the transition occurring precisely mid-year.\nThe second experiment explored the model’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the “fixation of a collocation.” Here, instances of rain not immediately followed by and were synthetically replaced with rain and snow. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of rain and snow against rain only. The model successfully acquired this pattern, generating rain and snow almost exclusively in the latter part of the year, whilst early-year occurrences of rain (sometimes accompanied by snow) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year rain and snow on the presence of a “cold system,” underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.5 Proof of Concept, Applications, and Challenges",
    "text": "15.5 Proof of Concept, Applications, and Challenges\n\n\n\nSlide 21\n\n\nThis research unequivocally establishes a proof of concept: Transformer-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation Time Transformer, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to “talk to a specific time,” potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.\nSeveral promising avenues for future research emerge from this work. Benchmarking the Time Transformer against explicit time-token approaches will quantify its performance advantages and identify optimal use cases. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.\nNevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained LLMs remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to BERT (Devlin2019?), built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward for historical language analysis.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nThis research pioneers a comprehensive approach to enhance the metadata of historical scientific texts and conduct diachronic analyses of chemical knowledge. Presented by Diego Alves, Sergei Bagdasarov, and Badr Abdullah, this initiative addresses the inherent challenges of managing extensive historical corpora.\nOur project unfolds in two distinct phases. Initially, we leverage Large Language Models (LLMs) to refine text metadata, focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Subsequently, the work delves into a detailed case study, meticulously analysing the evolution of chemical discourse across various disciplines over time. This second phase specifically identifies periods of heightened interdisciplinarity and knowledge transfer, employing advanced computational methods.\nThe Philosophical Transactions of the Royal Society of London, a foundational and continuously published scientific journal spanning over three centuries, serves as the primary data source. This rich historical context underpins our innovative research.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "href": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Research Objectives and Scope",
    "text": "16.1 Research Objectives and Scope\n\n\n\nSlide 01\n\n\nThis research systematically explores the application of Large Language Models (LLMs) for enhancing historical scientific texts. Our project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts,” unfolds in two principal parts.\nThe first part focuses on deploying LLMs to improve the metadata associated with historical texts. This involves the precise categorisation of articles according to their scientific discipline, the assignment of pertinent semantic tags or topics, and the generation of concise, abstractive summaries.\nBuilding upon this metadata enrichment, the second part undertakes a detailed analysis of the chemical space as it evolved across different scientific disciplines over time. A crucial objective here involves identifying specific periods that correspond to peaks of interdisciplinarity and significant knowledge transfer, thereby illuminating the dynamic nature of scientific discourse.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "href": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 Historical Scientific Corpus: Philosophical Transactions",
    "text": "16.2 Historical Scientific Corpus: Philosophical Transactions\n\n\n\nSlide 01\n\n\nThis project investigates the evolution of scientific English, alongside phenomena such as knowledge transfer and the identification of influential papers and authors. Central to this inquiry is the Philosophical Transactions of the Royal Society of London, a corpus of unparalleled historical significance.\nFirst published in 1665, this journal holds the distinction of being the oldest scientific periodical in continuous publication, maintaining a high reputation to this day. It played a pivotal role in shaping scientific communication, notably by establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge. The corpus contains numerous seminal contributions, including Isaac Newton’s “New Theory about Light and Colours” from the 17th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” from the 18th century, and James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” from the 19th century. Beyond these renowned works, the collection also features more curious, speculative texts, such as discussions on lunar inhabitants, though our research focuses on linguistic and thematic analysis rather than factual validation.\nFor our analysis, we utilise the latest version of this extensive collection, the RSC 6.0 full corpus. This dataset spans over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 texts and approximately 300 million tokens. While the corpus includes pre-encoded metadata, such as author, century, year, and volume, a previous study employed LDA topic modelling to infer research field categories. However, this earlier classification often conflated distinct scientific disciplines, sub-disciplines, and even text types, such as “observations” and “reporting.” This necessitated a more refined approach to metadata enrichment, which our current work addresses.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Large Language Models for Metadata Enrichment",
    "text": "16.3 Large Language Models for Metadata Enrichment\n\n\n\nSlide 10\n\n\nTo address the limitations of existing metadata, we leveraged Large Language Models (LLMs) for comprehensive metadata enrichment. These models offer diverse applications, including text clean-up, summarisation, information extraction, and the populating of knowledge graphs, alongside their core function in categorisation and facilitating access and retrieval.\nSpecifically, we tasked the LLM with four distinct operations. First, it performed hierarchical categorisation, assigning both a primary discipline and a suitable sub-discipline to each article. Second, it identified key index terms, functioning as semantic tags or topics. Third, the model generated concise TL;DR (Too Long; Didn’t Read) summaries, typically 3-4 sentences in length, designed to capture the essence and main findings of an article in simple language, accessible even to a high school student. Finally, the LLM proposed alternative, more reflective titles for the texts.\nFor this task, our team selected Hermes-2-Pro-Llama-3-8B, an 8-billion-parameter model from the Llama 3 family. This particular variant, readily available on Hugging Face, demonstrated superior performance compared to Mistral and Llama 2. It had undergone instruction-tuning specifically for producing structured outputs in formats such as JSON and YAML.\nA meticulously crafted system prompt guided the LLM’s operations. The prompt first defined the model’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” It then articulated the objective: “read, analyze, and organize a large corpus… create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input was described as OCR-extracted text from original articles, accompanied by existing metadata including title, author(s), publication date, journal, and a short text snippet.\nThe prompt detailed the four specific tasks as follows: A. Read and analyse the provided article to understand its content and context, then suggest an alternative title that better reflects its content. B. Write a short 3-4 sentence TL;DR summary that captures the article’s essence and main findings, ensuring conciseness, informativeness, and simple language. C. Identify exactly five main topics, conceptualised as Wikipedia Keywords for categorising the text into scientific sub-fields. D. Given the extracted topics, identify the primary scientific discipline from a predefined list (Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, Social Sciences & Humanities) and a suitable second-level sub-discipline, with the crucial constraint that the sub-discipline could not be one of the primary disciplines.\nAn example input, Isaac Newton’s 1672 letter, clearly illustrated the expected YAML output. The LLM successfully transformed the original lengthy title into “A New Theory of Light and Colours,” assigned topics such as “Optics” and “Refraction,” generated a concise TL;DR summary, and accurately classified the article under “Physics” with the sub-discipline “Optics & Light.” A final instruction reinforced the requirement for a valid YAML output, with no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "href": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 LLM Performance and Diachronic Corpus Analysis",
    "text": "16.4 LLM Performance and Diachronic Corpus Analysis\n\n\n\nSlide 15\n\n\nInitial sanity checks on the LLM’s performance yielded highly encouraging results. A remarkable 99.81% of the generated outputs, specifically 17,486 out of 17,520, conformed to the specified YAML format, demonstrating the model’s proficiency in structured data generation. Furthermore, 94% of the predicted scientific disciplines aligned precisely with our predefined set of nine categories.\nNevertheless, the LLM exhibited some minor deviations or “hallucinations.” For instance, it occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences.” The model also innovated novel categories, such as “Music,” and sometimes incorporated the numerical index of a discipline directly into its name, for example, “3. Chemistry.” Moreover, certain sub-disciplines, like “neurology” and “zoology,” were incorrectly classified as primary disciplines. Despite these minor deviations, the LLM accurately assigned the vast majority of papers to their correct categories.\nLeveraging this enhanced metadata, we conducted a diachronic analysis of the Royal Society articles, examining their distribution across disciplines over time. Before the close of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. However, the late 18th century witnessed a distinct surge in chemical articles, a phenomenon directly correlating with the Chemical Revolution. Subsequently, from the 19th century into the 20th, Chemistry, alongside Biology and Physics, emerged as one of the Royal Society’s primary pillars of scientific inquiry.\nFurther analysis involved visualising the TL;DR summaries using t-SNE projections, a dimensionality reduction technique. This revealed significant overlaps between Chemistry, Physics, and Biology, with Chemistry often situated centrally within this interdisciplinary space. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters within the projection. This visualisation technique holds considerable promise for future research, enabling the detailed observation of diachronic shifts and evolving overlaps between disciplines.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Diachronic Analysis of Chemical Space",
    "text": "16.5 Diachronic Analysis of Chemical Space\n\n\n\nSlide 19\n\n\nBuilding upon the LLM-derived classifications, we proceeded with a diachronic analysis of the chemical space, focusing specifically on Chemistry, Biology, and Physics, given their prevalence within the corpus.\nTo extract chemical terms, our team employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initial application of this tool to the entire text corpus generated considerable noise. Consequently, we adopted a refined approach: ChemDataExtractor was subsequently applied to a pre-filtered list of extracted substances, a method that significantly reduced the noisy output and improved precision.\nFor analysing the chemical space, we utilised Kullback-Leibler Divergence (KLD). This statistical measure quantifies the number of additional bits required to encode a dataset A when an (often sub-optimal) model based on dataset B is employed. Crucially, higher KLD values indicate greater differences between datasets, whilst lower values suggest relative similarity. In essence, KLD helps us understand how much one probability distribution differs from another.\nWe applied KLD in two distinct ways. First, to trace the independent evolution of the chemical space within each discipline along the historical timeline, we compared 20-year periods before a specific date with 20-year periods after it, employing a sliding 5-year window. This technique allowed for a granular understanding of how chemical terminology and concepts shifted within Chemistry, Biology, and Physics individually. Second, for a broader interdisciplinary perspective, we conducted a pairwise comparison of Chemistry with Physics and Chemistry with Biology, based on 50-year periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "href": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics",
    "text": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics\n\n\n\nSlide 22\n\n\nAnalysis of the Kullback-Leibler Divergence (KLD) per discipline revealed consistent trends across Chemistry, Biology, and Physics, with peaks and troughs in KLD values occurring roughly concurrently. Towards the end of the timeline, the KLD generally decreased, indicating less variation between future and past periods within each discipline. A notable peak emerged in the late 18th century, prompting further investigation into the specific chemical substances driving this change.\nExamining the period from 1776 to 1816, corresponding to the late 18th-century peak, we observed that in both Biology and Physics, one or two elements exhibited exceptionally high KLD values, effectively driving the observed shifts. Intriguingly, the same core chemical elements appeared across Chemistry, Biology, and Physics during this era, suggesting a shared foundational chemical vocabulary.\nHowever, a significant divergence became apparent when analysing the second half of the 19th century (1856-1906). During this period, the KLD graphs for Biology and Physics became considerably more populated, with individual elemental contributions showing greater uniformity. Biology’s chemical discourse evolved distinctly towards biochemistry, incorporating substances related to biochemical processes. Conversely, Chemistry and Physics increasingly focused on noble gases and radioactive elements, reflecting their discovery and burgeoning importance in the late 19th century.\nInterdisciplinary comparisons, visualised through word clouds for the latter half of the 20th century, further elucidated these thematic differences. In the comparison between Chemistry and Biology, the biological word cloud featured a greater prevalence of substances associated with biochemical processes in living organisms. In contrast, the chemical word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. When comparing Chemistry with Physics, the latter’s word cloud prominently displayed metals, noble gases, rare earth metals, semi-metals, and radioactive metals, underscoring distinct disciplinary thematic focuses.\nCrucially, this pairwise KLD analysis facilitated the detection of “knowledge transfer” events. Knowledge transfer, in this context, describes instances where an element initially distinctive of one discipline in an earlier period subsequently became more characteristic of another. For example, tin, which was distinctive of Chemistry in the early 18th century, clearly shifted to become distinctive of Physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. Furthermore, elements transitioning from Chemistry to Biology in the 20th century consistently related to biochemical processes, reinforcing the observed disciplinary specialisation and the dynamic nature of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Conclusion and Future Research Directions",
    "text": "16.7 Conclusion and Future Research Directions\n\n\n\nSlide 21\n\n\nThis research successfully employed a Large Language Model for the categorisation of articles and the development of refined topic models within a historical corpus. Building upon these LLM-generated results, we conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: Chemistry, Biology, and Physics. Furthermore, we performed a detailed interdisciplinary comparison of this evolving chemical landscape, identifying periods of significant disciplinary shift and knowledge transfer. Our findings highlight the dynamic nature of scientific discourse and the evolving interconnections between fields over centuries.\nDespite these significant achievements, ample scope remains for future work. For the first part of the project, we plan to test alternative LLMs and undertake a rigorous evaluation of the current model’s outputs to ensure robustness and accuracy. Regarding the diachronic analysis of the chemical space, future efforts will involve a more fine-grained interdisciplinary analysis, potentially incorporating diachronic sliding windows of varying lengths. Expanding the scope to include additional disciplines, such as a comparison between Chemistry and Medicine, also presents a compelling avenue. Finally, exploring the evolution of chemical space using surprisal, a measure of unexpectedness, could yield further insights into the dynamics of scientific discovery and knowledge transfer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "",
    "text": "Overview\nResearchers delve into the intricate complexities of science funding, moving beyond conventional analyses of publications and grants. Their work explores the internal processes of funding agencies, offering a nuanced perspective. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, given its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH).\nAn interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This remarkable collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets. To manage and interpret this vast dataset, investigators have developed advanced computational tools.\nThese tools include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data. This innovation significantly improves Optical Character Recognition (OCR) whilst enabling separate analysis of handwritten content. Furthermore, multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. Such capabilities prove crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies vividly demonstrate the power of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect.\nThe overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge. This endeavour seeks to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.1 Limitations in Understanding Science Funding through Public Data",
    "text": "17.1 Limitations in Understanding Science Funding through Public Data\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. Researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "17.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI). NHGRI functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics:\n\nA significant proportion of NHGRI-funded publications rank amongst the top 5% most cited.\nIts research demonstrates high citation impact within a decade.\nIt generates numerous patents leading to clinical applications.\nIts funded projects often exhibit high “disruption” scores.\n\nDespite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "17.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\n\n\n\nSlide 06\n\n\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters. These clusters stand separate from the more homogenous categories of RFAs and publications. Crucially, these internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "17.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\n\n\n\nSlide 10\n\n\nAnalysing the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling its extensive handwritten material. Researchers acknowledge the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "17.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount.\nThe developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, researchers reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "17.6 Analysing Leadership Structures: The International HapMap Project Case Study\n\n\n\nSlide 15\n\n\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. Investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, researchers identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, the analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins played significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "17.7 Modelling Funding Decisions for Organism Sequencing\n\n\n\nSlide 13\n\n\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, researchers developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features:\n\nBiological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05).\nProject-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04).\nReputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04).\nLinguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\n\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research",
    "section": "17.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "17.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\n\n\n\nSlide 16\n\n\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages:\n\nInitial data and metadata ingestion.\nSophisticated knowledge creation processes, such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling.\n\nThe ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unlocking Science's Hidden Dynamics: A Computational Approach to Archival Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K., participating via Zoom, explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. This approach facilitates sophisticated querying and analysis. Their work directly addresses the challenge of computationally accessing the rich information embedded within traditional formats, such as printed books and archival materials, which typically lack inherent digital structure.\nThe core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline. This pipeline is meticulously designed for specific tasks, aiming to impose structure on unstructured data in a controllable manner. The process commences with diverse sources, including Polish biographical materials and German biographical handbooks, such as Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, and works—and their intricate relationships, representing them as nodes and edges within a knowledge graph. Tools like Neo4j enable the visualisation of these complex networks.\nThis structured representation significantly facilitates complex queries. Researchers can, for instance, investigate network formations amongst professionals during specific historical periods or meticulously trace the evolution of ideas. The methodology champions a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies precisely tailored to research questions, and disambiguated entities linked to external resources like Wikidata. Ultimately, the project aims to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through advanced technologies such as GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-unlocking-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-unlocking-unstructured-biographical-knowledge",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.1 Introduction: Unlocking Unstructured Biographical Knowledge",
    "text": "18.1 Introduction: Unlocking Unstructured Biographical Knowledge\n\n\n\nSlide 01\n\n\nResearchers consistently confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its inherent lack of digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation centres on biographical sources replete with detailed personal data, crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this fundamental limitation, investigators propose employing Large Language Models (LLMs). The core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline designed to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j.\nCrucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This pragmatic approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "18.2 Conceptual Framework: From Text to Knowledge Graph\n\n\n\nSlide 04\n\n\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which are then assembled into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (“Włochy”) or Egypt (“Egipt”). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process operates within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality suffices to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\n\n\n\nSlide 12\n\n\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which are manually refined based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, SHACL (Shapes Constraint Language) shapes are created to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\n\n\n\nSlide 15\n\n\nResearchers illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying the knowledge-graph approach to this corpus, investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists; the presentation displays sample entries for Gustav Hertz and Robert Havemann. An analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR. Further comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "Transforming Biographical Sources into Knowledge Graphs",
    "section": "18.6 Conclusion and Future Trajectories",
    "text": "18.6 Conclusion and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project successfully demonstrates a method for progressing from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to rigorously assess performance.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the team intends to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. Investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, plans are underway to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transforming Biographical Sources into Knowledge Graphs</span>"
    ]
  }
]