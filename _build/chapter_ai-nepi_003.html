<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-01-01">

<title>3&nbsp; Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-a126389619fad6dbfb296a5315d49fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Philosophy at Scale: Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modelling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">SDG-Research in Bibliometric DBs - LLMs for HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems for Philosophical Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">```markdown</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title"><code>markdown&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jaGFwdGVyX2FpLW5lcGlfMDE1Lmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjEzPC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPmBgYG1hcmtkb3duPC9zcGFuPg=="} [&lt;span class='chapter-number'&gt;14&lt;/span&gt;&nbsp; &lt;span class='chapter-title'&gt;Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jaGFwdGVyX2FpLW5lcGlfMDE2Lmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjE0PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPlRpdGxlcywtQWJzdHJhY3RzLC1vci1GdWxsLVRleHRzPy1BLUNvbXBhcmF0aXZlLVN0dWR5LW9mLUxEQS1hbmQtQkVSVG9waWMtUGVyZm9ybWFuY2U8L3NwYW4+"} [&lt;span class='chapter-number'&gt;15&lt;/span&gt;&nbsp; &lt;span class='chapter-title'&gt;</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Transforming Biographical Sources into Knowledge Graphs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#the-foundational-transformer-architecture" id="toc-the-foundational-transformer-architecture" class="nav-link" data-scroll-target="#the-foundational-transformer-architecture"><span class="header-section-number">3.1</span> The Foundational <em>Transformer</em> Architecture</a></li>
  <li><a href="#differentiating-bert-and-gpt-models" id="toc-differentiating-bert-and-gpt-models" class="nav-link" data-scroll-target="#differentiating-bert-and-gpt-models"><span class="header-section-number">3.2</span> Differentiating <em>BERT</em> and <em>GPT</em> Models</a></li>
  <li><a href="#scientific-llm-evolution-and-adaptation-strategies" id="toc-scientific-llm-evolution-and-adaptation-strategies" class="nav-link" data-scroll-target="#scientific-llm-evolution-and-adaptation-strategies"><span class="header-section-number">3.3</span> Scientific <em>LLM</em> Evolution and Adaptation Strategies</a></li>
  <li><a href="#retrieval-augmented-generation-and-key-distinctions" id="toc-retrieval-augmented-generation-and-key-distinctions" class="nav-link" data-scroll-target="#retrieval-augmented-generation-and-key-distinctions"><span class="header-section-number">3.4</span> <em>Retrieval Augmented Generation</em> and Key Distinctions</a></li>
  <li><a href="#applications-and-trends-in-hpss-research" id="toc-applications-and-trends-in-hpss-research" class="nav-link" data-scroll-target="#applications-and-trends-in-hpss-research"><span class="header-section-number">3.5</span> Applications and Trends in <em>HPSS</em> Research</a></li>
  <li><a href="#critical-reflections-and-future-directions-for-hpss" id="toc-critical-reflections-and-future-directions-for-hpss" class="nav-link" data-scroll-target="#critical-reflections-and-future-directions-for-hpss"><span class="header-section-number">3.6</span> Critical Reflections and Future Directions for <em>HPSS</em></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></h1>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            AI-NEPI Conference Participant
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This presentation systematically introduces the foundational architecture of large language models (<em>LLMs</em>), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (<em>HPSS</em>). As a co-organiser, the speaker initially provides a primer on the seminal <em>Transformer</em> architecture, explaining its encoder-decoder structure and original purpose in language translation.</p>
<p>Subsequently, the discussion differentiates between encoder-based models, such as <em>BERT</em>, which offer bidirectional full-context understanding, and decoder-based generative models, like <em>GPT</em>, capable of producing novel text. The presentation then charts the proliferation of domain-specific <em>LLMs</em> across various scientific fields, outlining diverse adaptation strategies including pre-training, fine-tuning, and the sophisticated <em>Retrieval Augmented Generation</em> (<em>RAG</em>) pipeline.</p>
<p>Crucially, the presentation categorises current <em>LLM</em> applications in <em>HPSS</em>, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, it offers critical reflections on <em>HPSS</em>-specific challenges, such as historical language evolution and sparse data. The speaker advocates for enhanced <em>LLM</em> literacy and a steadfast adherence to <em>HPSS</em> methodologies, highlighting new opportunities for bridging qualitative and quantitative research.</p>
</section>
<section id="the-foundational-transformer-architecture" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-foundational-transformer-architecture"><span class="header-section-number">3.1</span> The Foundational <em>Transformer</em> Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>At the core of all contemporary large language models (<em>LLMs</em>) lies the seminal <em>Transformer</em> architecture, pioneered by Vaswani et al.&nbsp;in 2017 <span class="citation" data-cites="Vaswani2017">(<a href="#ref-Vaswani2017" role="doc-biblioref"><strong>Vaswani2017?</strong></a>)</span>. Originally designed for language translation, such as German to English, this architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.</p>
<p>The encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence’s meaning. This allows the model to understand the full context of the input.</p>
<p>Conversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness and enabling more nuanced language processing.</p>
</section>
<section id="differentiating-bert-and-gpt-models" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="differentiating-bert-and-gpt-models"><span class="header-section-number">3.2</span> Differentiating <em>BERT</em> and <em>GPT</em> Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Immediately following the <em>Transformer</em>’s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (<em>NLP</em>) tasks with minimal additional training.</p>
<p>From the encoder side, the <em>BERT</em> family of models emerged, standing for Bidirectional Encoder Representations from <em>Transformers</em>. <em>BERT</em> operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. This enables <em>BERT</em> to excel at tasks requiring deep contextual comprehension, such as sentiment analysis or question answering.</p>
<p>Conversely, the decoder side gave rise to the <em>GPT</em> models, or Generative Pre-trained <em>Transformers</em>, which now power applications like <em>ChatGPT</em>. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text. This generative function is not inherently present in <em>BERT</em>-like models.</p>
<p>Consequently, a fundamental distinction arises: generative models, exemplified by <em>GPT</em>, primarily produce language, whereas full-context models, such as <em>BERT</em>, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like <em>XLM</em> and <em>XLNet</em>.</p>
</section>
<section id="scientific-llm-evolution-and-adaptation-strategies" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="scientific-llm-evolution-and-adaptation-strategies"><span class="header-section-number">3.3</span> Scientific <em>LLM</em> Evolution and Adaptation Strategies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>A comprehensive overview reveals the rapid evolution of large language models (<em>LLMs</em>), particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to <em>BERT</em>, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included <em>BioBERT</em>, <em>Specter</em>, and <em>SciBERT</em>. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.</p>
<p>Researchers employ several methods to adapt these models to specific scientific language. The initial phase is <em>pre-training</em>, where a model learns language by predicting the next token (a word or sub-word unit), as in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many.</p>
<p>Consequently, <em>continued pre-training</em> offers a viable alternative. Researchers utilise an already pre-trained model and subsequently train it on domain-specific language. For instance, a general <em>BERT</em> model can be adapted for physics texts by further training it on a large corpus of physics literature.</p>
<p>Beyond this, engineers can add extra layers atop pre-trained models, effectively fine-tuning them for specific classification tasks like sentiment analysis or named entity recognition. Crucially, <em>contrastive learning</em> emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with <em>Sentence BERT</em> serving as a widely adopted example for creating semantically meaningful sentence representations.</p>
</section>
<section id="retrieval-augmented-generation-and-key-distinctions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="retrieval-augmented-generation-and-key-distinctions"><span class="header-section-number">3.4</span> <em>Retrieval Augmented Generation</em> and Key Distinctions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p><em>Retrieval Augmented Generation</em> (<em>RAG</em>) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins <em>ChatGPT</em>’s internet search capabilities, allowing it to access and synthesise information beyond its initial training data.</p>
<p>The <em>RAG</em> process commences with a user query, such as “What are <em>LLMs</em>?”. Subsequently, a <em>BERT</em>-type model encodes this query into a sentence embedding, a numerical representation of its meaning. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the <em>RAG</em> pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context. This approach significantly reduces the likelihood of “hallucinations” often associated with standalone <em>LLMs</em>.</p>
<p>Beyond <em>RAG</em>, advanced reasoning models and agents are emerging. These are not isolated <em>LLMs</em> but rather intricate systems that combine <em>LLMs</em> with a diverse array of other tools, enabling more complex problem-solving and decision-making.</p>
<p>Consequently, a clear understanding of key distinctions proves crucial for navigating the <em>LLM</em> landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone <em>LLMs</em>, complex pipelines like <em>RAG</em>, and sophisticated agents becomes paramount for effective application and responsible deployment.</p>
</section>
<section id="applications-and-trends-in-hpss-research" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="applications-and-trends-in-hpss-research"><span class="header-section-number">3.5</span> Applications and Trends in <em>HPSS</em> Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>A current survey explores the burgeoning uses of large language models (<em>LLMs</em>) as tools within history, philosophy, and sociology of science (<em>HPSS</em>) research. This investigation has identified four primary categories for these applications:</p>
<ul>
<li><em>LLMs</em> assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations. For example, they can efficiently identify and categorise funding sources in large corpora of scientific papers.</li>
<li>They contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields. This could involve identifying key concepts and their relationships within historical scientific texts.</li>
<li><em>LLMs</em> illuminate knowledge dynamics, particularly through the study of conceptual histories of words. By tracking semantic shifts over time, researchers can trace the evolution of scientific concepts.</li>
<li>They support the analysis of knowledge practices, including citation context analysis—an older <em>HPSS</em> tradition now also employed for evaluatory purposes. This allows for a deeper understanding of how scientific ideas are built upon and referenced.</li>
</ul>
<p>A notable trend indicates an accelerating interest in <em>LLMs</em>, with findings predominantly appearing in information science journals like <em>Scientometrics</em> and the <em>Journal of the Association for Information Science and Technology</em> (<em>JASIST</em>). Increasingly, however, papers featuring <em>LLM</em> applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers, bridging disciplinary divides. Furthermore, the degree of customisation in <em>LLM</em> deployment varies widely, spanning from straightforward off-the-shelf use of <em>ChatGPT</em> to the development of entirely new model architectures tailored for specific <em>HPSS</em> questions.</p>
<p>Despite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models (the “black box” problem), and persistent shortages of training data and benchmarks specific to <em>HPSS</em> domains. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by <em>BERTopic</em>, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.</p>
</section>
<section id="critical-reflections-and-future-directions-for-hpss" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="critical-reflections-and-future-directions-for-hpss"><span class="header-section-number">3.6</span> Critical Reflections and Future Directions for <em>HPSS</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Crucially, scholars must acknowledge the specific challenges inherent to <em>HPSS</em> when engaging with large language models (<em>LLMs</em>). Foremost amongst these is the historical evolution of concepts and language. Models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models on historical corpora or the judicious use of existing ones with a keen awareness of their limitations.</p>
<p>Furthermore, <em>HPSS</em> adopts a reconstructive and critically reflective perspective, often reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive <em>HPSS</em> “reading.” Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts, which pose significant hurdles for <em>LLM</em> application.</p>
<p>Consequently, building robust <em>LLM</em> literacy becomes imperative for <em>HPSS</em> researchers. They must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing (<em>NLP</em>) may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.</p>
<p>Ultimately, <em>HPSS</em> researchers must remain true to their established methodologies. This involves carefully translating complex <em>HPSS</em> problems into specific <em>NLP</em> tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon <em>HPSS</em>’s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s <span class="citation" data-cites="Callon1986 Rip1986">(<a href="#ref-Callon1986" role="doc-biblioref"><strong>Callon1986?</strong></a>; <a href="#ref-Rip1986" role="doc-biblioref"><strong>Rip1986?</strong></a>)</span>, offers valuable theoretical grounding, particularly given the resonance of <em>Actor-Network Theory</em> (<em>ANT</em>) concepts with contemporary <em>LLM</em> developments. ```</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Philosophy at Scale: Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Philosophy at Scale: Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>