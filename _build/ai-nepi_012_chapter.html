<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paul M. Nager">

<title>12&nbsp; Retrieval Augmented Generation (RAG) in HPSS Research: Applications, Methodological Challenges, and Limitations – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./ai-nepi_011_chapter.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-09b140d2d032adf2aedb8b099be3ee13.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai-nepi_012_chapter.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in HPSS Research: Applications, Methodological Challenges, and Limitations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_001_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_003_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_004_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Workflow and Utility of OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_005_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_006_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_007_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_008_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_009_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_010_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_011_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Can We Build an AI Solution to Chat with Papers? Exploring the Ghostwriter and EverythingData Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_012_chapter.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in HPSS Research: Applications, Methodological Challenges, and Limitations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction-rag-in-hpss-research" id="toc-introduction-rag-in-hpss-research" class="nav-link" data-scroll-target="#introduction-rag-in-hpss-research"><span class="header-section-number">12.1</span> Introduction: RAG in HPSS Research</a></li>
  <li><a href="#addressing-core-llm-limitations-with-rag-systems" id="toc-addressing-core-llm-limitations-with-rag-systems" class="nav-link" data-scroll-target="#addressing-core-llm-limitations-with-rag-systems"><span class="header-section-number">12.2</span> Addressing Core LLM Limitations with RAG Systems</a>
  <ul class="collapse">
  <li><a href="#the-rag-process-illustrated" id="toc-the-rag-process-illustrated" class="nav-link" data-scroll-target="#the-rag-process-illustrated"><span class="header-section-number">12.2.1</span> The RAG Process Illustrated</a></li>
  <li><a href="#benefits-for-scholarly-inquiry" id="toc-benefits-for-scholarly-inquiry" class="nav-link" data-scroll-target="#benefits-for-scholarly-inquiry"><span class="header-section-number">12.2.2</span> Benefits for Scholarly Inquiry</a></li>
  </ul></li>
  <li><a href="#developing-a-rag-system-for-philosophical-corpora-a-case-study" id="toc-developing-a-rag-system-for-philosophical-corpora-a-case-study" class="nav-link" data-scroll-target="#developing-a-rag-system-for-philosophical-corpora-a-case-study"><span class="header-section-number">12.3</span> Developing a RAG System for Philosophical Corpora: A Case Study</a>
  <ul class="collapse">
  <li><a href="#initial-aims-and-evolving-objectives" id="toc-initial-aims-and-evolving-objectives" class="nav-link" data-scroll-target="#initial-aims-and-evolving-objectives"><span class="header-section-number">12.3.1</span> Initial Aims and Evolving Objectives</a></li>
  <li><a href="#the-iterative-process-of-system-refinement" id="toc-the-iterative-process-of-system-refinement" class="nav-link" data-scroll-target="#the-iterative-process-of-system-refinement"><span class="header-section-number">12.3.2</span> The Iterative Process of System Refinement</a></li>
  <li><a href="#methodological-approach-theoretically-grounded-experimentation" id="toc-methodological-approach-theoretically-grounded-experimentation" class="nav-link" data-scroll-target="#methodological-approach-theoretically-grounded-experimentation"><span class="header-section-number">12.3.3</span> Methodological Approach: Theoretically Grounded Experimentation</a></li>
  </ul></li>
  <li><a href="#potential-applications-in-philosophical-didactics-and-research" id="toc-potential-applications-in-philosophical-didactics-and-research" class="nav-link" data-scroll-target="#potential-applications-in-philosophical-didactics-and-research"><span class="header-section-number">12.4</span> Potential Applications in Philosophical Didactics and Research</a></li>
  <li><a href="#frontend-design-and-comparative-evaluation-framework" id="toc-frontend-design-and-comparative-evaluation-framework" class="nav-link" data-scroll-target="#frontend-design-and-comparative-evaluation-framework"><span class="header-section-number">12.5</span> Frontend Design and Comparative Evaluation Framework</a>
  <ul class="collapse">
  <li><a href="#interface-for-qualitative-assessment" id="toc-interface-for-qualitative-assessment" class="nav-link" data-scroll-target="#interface-for-qualitative-assessment"><span class="header-section-number">12.5.1</span> Interface for Qualitative Assessment</a></li>
  <li><a href="#analysing-retrieved-sources" id="toc-analysing-retrieved-sources" class="nav-link" data-scroll-target="#analysing-retrieved-sources"><span class="header-section-number">12.5.2</span> Analysing Retrieved Sources</a></li>
  </ul></li>
  <li><a href="#hyperparameter-optimisation-the-critical-role-of-chunk-size" id="toc-hyperparameter-optimisation-the-critical-role-of-chunk-size" class="nav-link" data-scroll-target="#hyperparameter-optimisation-the-critical-role-of-chunk-size"><span class="header-section-number">12.6</span> Hyperparameter Optimisation: The Critical Role of Chunk Size</a>
  <ul class="collapse">
  <li><a href="#exploring-chunking-strategies" id="toc-exploring-chunking-strategies" class="nav-link" data-scroll-target="#exploring-chunking-strategies"><span class="header-section-number">12.6.1</span> Exploring Chunking Strategies</a></li>
  <li><a href="#surprising-efficacy-of-section-based-chunking" id="toc-surprising-efficacy-of-section-based-chunking" class="nav-link" data-scroll-target="#surprising-efficacy-of-section-based-chunking"><span class="header-section-number">12.6.2</span> Surprising Efficacy of Section-Based Chunking</a></li>
  <li><a href="#implications-for-corpus-specific-tuning" id="toc-implications-for-corpus-specific-tuning" class="nav-link" data-scroll-target="#implications-for-corpus-specific-tuning"><span class="header-section-number">12.6.3</span> Implications for Corpus-Specific Tuning</a></li>
  </ul></li>
  <li><a href="#enhancing-retrieval-precision-through-reranking" id="toc-enhancing-retrieval-precision-through-reranking" class="nav-link" data-scroll-target="#enhancing-retrieval-precision-through-reranking"><span class="header-section-number">12.7</span> Enhancing Retrieval Precision through Reranking</a></li>
  <li><a href="#concluding-reflections-advantages-cautions-and-future-challenges" id="toc-concluding-reflections-advantages-cautions-and-future-challenges" class="nav-link" data-scroll-target="#concluding-reflections-advantages-cautions-and-future-challenges"><span class="header-section-number">12.8</span> Concluding Reflections: Advantages, Cautions, and Future Challenges</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in HPSS Research: Applications, Methodological Challenges, and Limitations</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paul M. Nager </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Retrieval Augmented Generation (RAG) systems present a promising avenue for research, particularly within the Humanities and Social Sciences (HPSS). These systems offer distinct advantages, notably their capacity to integrate verbatim corpora alongside domain-specific or specialised knowledge. This integration facilitates the generation of more detailed answers and commendably reduces the occurrence of hallucinations often associated with standalone large language models (LLMs). Furthermore, the ability of RAG setups to cite relevant source documents renders them exceptionally well-suited for assisting in a multitude of scientific tasks, thereby enhancing research transparency and reliability.</p>
<p>However, the implementation and optimisation of RAG systems necessitate careful consideration. Practitioners must recognise that these systems invariably require meticulous tweaking; appropriate settings are highly contingent upon the specific corpus in use and the nature of the questions posed. Consequently, rigorous evaluation of RAG systems is crucial. This involves establishing a representative set of questions and corresponding expected answers to benchmark performance. A pertinent question arises regarding the effective application of RAG in unexplored corpora where such benchmarks may not yet exist. The involvement of domain experts—philosophers, in the context of this exploration—is therefore an essential requirement for meaningful assessment and refinement.</p>
<p>Despite their strengths, RAG systems encounter certain challenges. The quality of generated answers can diminish if no relevant documents are retrieved from the knowledge base, a scenario that calls for prompt adjustments. Moreover, for widely discussed overview questions, such as “What are the central arguments against scientific realism?”, RAG systems may occasionally provide results inferior to those of general-purpose LLMs. This, too, suggests a need for tailored prompting strategies in specific instances. Looking ahead, the development of more flexible, agentic RAG systems appears to be a key area for future work, potentially addressing some of these current limitations and expanding the utility of RAG in scholarly pursuits.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>A slide summarising the advantages, cautions, and challenges of RAG systems, with bullet points under each category.</figcaption>
</figure>
</div>
</section>
<section id="introduction-rag-in-hpss-research" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="introduction-rag-in-hpss-research"><span class="header-section-number">12.1</span> Introduction: RAG in HPSS Research</h2>
<p>The exploration of Large Language Models (LLMs) for research in the History and Philosophy of Science and the broader Humanities and Social Sciences (HPSS) opens new frontiers. Amongst emerging methodologies, Retrieval Augmented Generation (RAG) commands particular attention. This chapter delves into the applications, methodological challenges, and inherent limitations of employing RAG systems within HPSS research, drawing upon experiences from a workshop on “LLMs for HPSS” held in Berlin. The central proposition is to harness the power of LLMs while grounding their outputs in verifiable textual sources, a critical desideratum for scholarly rigour.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Title slide for “Retrieval augmented generation (RAG) in HPSS research: Applications, methodological challenges, limitations” workshop presentation.</figcaption>
</figure>
</div>
</section>
<section id="addressing-core-llm-limitations-with-rag-systems" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="addressing-core-llm-limitations-with-rag-systems"><span class="header-section-number">12.2</span> Addressing Core LLM Limitations with RAG Systems</h2>
<p>Large Language Models, despite their impressive generative capabilities, present certain inherent challenges for academic use. These include the potential for generating plausible yet incorrect information (hallucinations), difficulties in attributing information to specific sources, limitations imposed by context window sizes, and restricted access to specialised or up-to-date textual corpora. RAG systems offer a structured approach to mitigate several of these fundamental problems. They achieve this by synergising the retrieval of pertinent information from a defined corpus with the generative prowess of LLMs.</p>
<section id="the-rag-process-illustrated" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="the-rag-process-illustrated"><span class="header-section-number">12.2.1</span> The RAG Process Illustrated</h3>
<p>The typical RAG architecture involves several key stages. A user initiates the process with a query. This query is first used by an application to retrieve relevant text chunks from a designated data source, often a vector database populated with embeddings of the corpus. This retrieval step, typically employing semantic search, aims to find the most relevant passages. These retrieved chunks are then combined with the original query and fed as an augmented prompt to an LLM. The LLM, now equipped with specific contextual information, generates an answer. This answer is subsequently returned to the user via the application. This process directly addresses issues of data access by explicitly providing texts to the LLM, tackles context window limitations by supplying only the most relevant segments, and aids in attribution by enabling the citation of sources from which chunks were retrieved.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_03.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram illustrating the RAG process, from user query to retrieval, augmentation, generation, and final answer, highlighting how RAG solves LLM problems.</figcaption>
</figure>
</div>
</section>
<section id="benefits-for-scholarly-inquiry" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="benefits-for-scholarly-inquiry"><span class="header-section-number">12.2.2</span> Benefits for Scholarly Inquiry</h3>
<p>For HPSS research, this mechanism is particularly beneficial. Consider typical research questions such as, “What is Aristotle’s theory of matter in the Physics?” or “Does Einstein’s idea of locality develop from his earlier to his later works?”. RAG systems can consult a corpus of philosophical texts or scientific papers, retrieve relevant sections, and then synthesise an answer grounded in those specific sources. This approach enhances the reliability and verifiability of the generated responses, crucial aspects of scholarly investigation.</p>
</section>
</section>
<section id="developing-a-rag-system-for-philosophical-corpora-a-case-study" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="developing-a-rag-system-for-philosophical-corpora-a-case-study"><span class="header-section-number">12.3</span> Developing a RAG System for Philosophical Corpora: A Case Study</h2>
<p>To explore the practicalities of RAG in a philosophical context, an example system was developed using the Stanford Encyclopedia of Philosophy as its data source. This online handbook, a well-known and highly structured resource, served as the corpus. The content was scraped and converted into markdown format for integration into the RAG pipeline.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_05.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide showing the Stanford Encyclopedia of Philosophy logo and a diagram of the RAG system, outlining aims and methods for the project.</figcaption>
</figure>
</div>
<section id="initial-aims-and-evolving-objectives" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="initial-aims-and-evolving-objectives"><span class="header-section-number">12.3.1</span> Initial Aims and Evolving Objectives</h3>
<p>The initial aim was straightforward: to create a useful tool for the philosophical community. However, early experiences with coding and testing the system revealed a significant challenge. Setting up the system with a basic retrieval component and a generation component, as one might find in textbook examples, did not yield high-quality answers. In fact, the initial outputs were often worse than those obtained by posing the same queries directly to a standalone LLM like ChatGPT.</p>
<p>This observation led to a shift in objectives. Beyond tool creation, the project evolved into a qualitative study focused on the optimal setup of RAG systems specifically for philosophy. This involved investigating:</p>
<ul>
<li>Model choices, such as selecting the most suitable generative LLM and embedding model.</li>
<li>Tuning of hyperparameters, including the number of documents to retrieve (top-k), maximum input/output token lengths, temperature or top-p for generation, and, critically, chunk size and overlap.</li>
<li>Addressing further methodological challenges, for instance, the problem of retrieval semantic mismatch, and exploring solutions like reranking.</li>
</ul>
</section>
<section id="the-iterative-process-of-system-refinement" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="the-iterative-process-of-system-refinement"><span class="header-section-number">12.3.2</span> The Iterative Process of System Refinement</h3>
<p>The process of improving the RAG system became one of iterative refinement. This involved extensive tweaking of models and hyperparameters. Algorithms were made more complex, for example by introducing reranking stages, in pursuit of better results. This iterative cycle of adjustment and testing underscored the empirical nature of RAG system development. Whilst a theoretical understanding of the components is essential, practical experimentation proves indispensable for optimisation.</p>
</section>
<section id="methodological-approach-theoretically-grounded-experimentation" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="methodological-approach-theoretically-grounded-experimentation"><span class="header-section-number">12.3.3</span> Methodological Approach: Theoretically Grounded Experimentation</h3>
<p>The core methodology adopted was one of theoretically grounded trial and error. The guiding question at each stage was: by which measures do the answers improve? This necessitates robust evaluation standards. In philosophy, answers are typically free-form, unstructured text, not simple atomic facts. Evaluating the correctness of complex propositions derived from philosophical texts is a non-trivial task. It requires careful assessment of whether the generated statements accurately reflect the source material, a process where sound evaluation becomes crucial. One cannot simply ask for easily verifiable data points, such as “What was Wittgenstein’s last place of living?”; rather, the system must handle nuanced arguments and interpretations.</p>
</section>
</section>
<section id="potential-applications-in-philosophical-didactics-and-research" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="potential-applications-in-philosophical-didactics-and-research"><span class="header-section-number">12.4</span> Potential Applications in Philosophical Didactics and Research</h2>
<p>The general idea underpinning the application of RAG systems in philosophy is to enable interactive engagement with philosophical corpora—for instance, conversing with the entirety of Locke’s Oeuvre—in a manner akin to ChatGPT, but with substantially more detailed domain knowledge and a foundation in verbatim text.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_04.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide outlining possible applications of RAG systems in philosophy, including didactics and research, alongside the RAG process diagram.</figcaption>
</figure>
</div>
<p>This capability opens up several promising applications:</p>
<ul>
<li>In didactics: The ability to engage in repeated questioning with a corpus can be highly instructive for students, allowing them to explore concepts and texts in depth.</li>
<li>In research:
<ul>
<li>Researchers can efficiently look up facts, remarks, or footnotes within extensive handbooks or collections.</li>
<li>Unexamined or less familiar corpora can be explored more systematically.</li>
<li>Specific passages relevant for close reading can be readily identified.</li>
<li>Directly finding detailed answers to research questions becomes more feasible, grounded in the provided textual evidence.</li>
</ul></li>
</ul>
</section>
<section id="frontend-design-and-comparative-evaluation-framework" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="frontend-design-and-comparative-evaluation-framework"><span class="header-section-number">12.5</span> Frontend Design and Comparative Evaluation Framework</h2>
<p>To facilitate the qualitative evaluation essential for refining the RAG system, a specific frontend was developed. This interface provides a comparative setup, allowing for a direct side-by-side assessment of answers generated by an LLM alone (the benchmark) versus those produced by the RAG system.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_08.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide detailing the frontend output section for answers, showing a comparative setup for qualitative evaluation between LLM alone and RAG.</figcaption>
</figure>
</div>
<section id="interface-for-qualitative-assessment" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="interface-for-qualitative-assessment"><span class="header-section-number">12.5.1</span> Interface for Qualitative Assessment</h3>
<p>The frontend displays the user’s query and then presents two responses. On one side, the answer from the chosen standalone LLM (e.g., ChatGPT) appears. On the other, the RAG system’s answer, generated using the Stanford Encyclopedia of Philosophy corpus, is shown. This comparative view significantly aids in discerning the added value of the RAG approach, such as increased detail, accuracy, or grounding in the source material. For example, when asked “What is priority monism?”, the RAG system can provide a response that directly references and synthesises information from specific texts within the encyclopedia, often highlighting key definitions, contrasting views (like existence monism or priority pluralism), and historical proponents (such as Plato or Spinoza), complete with references to the source texts (e.g., “Text 0”).</p>
</section>
<section id="analysing-retrieved-sources" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="analysing-retrieved-sources"><span class="header-section-number">12.5.2</span> Analysing Retrieved Sources</h3>
<p>A crucial component of the output section is the list of found texts. This feature provides transparency into the retrieval process. Users can see the names of the articles and the specific section headings that the system identified as relevant to the query. Furthermore, it indicates which of these retrieved texts were actually included in the prompt sent to the LLM for generation and which, if any, were truncated due to prompt length limitations. This insight is invaluable for understanding the system’s behaviour and for diagnosing potential issues in the retrieval or generation stages.</p>
</section>
</section>
<section id="hyperparameter-optimisation-the-critical-role-of-chunk-size" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="hyperparameter-optimisation-the-critical-role-of-chunk-size"><span class="header-section-number">12.6</span> Hyperparameter Optimisation: The Critical Role of Chunk Size</h2>
<p>Amongst the various hyperparameters requiring optimisation, chunk size emerged as particularly influential. Chunking refers to the strategy used to divide the source documents into smaller segments for the retrieval process. The choice of chunk size directly impacts what information is available to the LLM for generating an answer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_10.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide discussing chunk size as a hyperparameter, outlining options and the best result found.</figcaption>
</figure>
</div>
<section id="exploring-chunking-strategies" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="exploring-chunking-strategies"><span class="header-section-number">12.6.1</span> Exploring Chunking Strategies</h3>
<p>Several options for defining chunks were considered:</p>
<ul>
<li>A fixed number of words (e.g., 500 tokens or words), a common approach in computer science due to its clean, algorithmic definition. This method, however, risks cutting off text mid-sentence or mid-argument, irrespective of semantic coherence.</li>
<li>Paragraphs, which offer a more semantically meaningful unit.</li>
<li>Sections, as defined within the source corpus (e.g., main sections of an encyclopedia entry, including their headings).</li>
</ul>
</section>
<section id="surprising-efficacy-of-section-based-chunking" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="surprising-efficacy-of-section-based-chunking"><span class="header-section-number">12.6.2</span> Surprising Efficacy of Section-Based Chunking</h3>
<p>Counterintuitively, the investigation revealed that chunking the Stanford Encyclopedia of Philosophy into its main sections yielded the best results. This finding was surprising because the average length of these sections (approximately 3,000 words) significantly exceeded the embedding model’s cutoff (around 512 words). Standard practice might suggest that such long chunks would perform poorly.</p>
<p>The superior performance of section-based chunking in this instance is likely attributable to the highly systematised nature of the Stanford Encyclopedia. Philosophical facts and arguments are rarely short and isolated; they typically require considerable space for presentation and contextualisation. It appears that the initial 500 words of a main section in this well-structured handbook often encapsulate the core themes and ideas of that entire section. Consequently, even if the embedding model only processed the beginning of the chunk, it captured sufficient essence for effective retrieval. This suggests that sticking to longer, semantically coherent units can be beneficial, particularly with systematically ordered documents.</p>
</section>
<section id="implications-for-corpus-specific-tuning" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="implications-for-corpus-specific-tuning"><span class="header-section-number">12.6.3</span> Implications for Corpus-Specific Tuning</h3>
<p>This outcome underscores a critical lesson: effective chunking strategy highly depends on the specifics of the corpus and the kinds of questions being posed. A strategy optimal for a highly structured encyclopedia may not be suitable for more heterogeneous or less formally organised texts. Future work plans to explore embedding models with longer context windows, such as Cohere Embed v3, to assess their impact on handling lengthy, information-rich chunks.</p>
</section>
</section>
<section id="enhancing-retrieval-precision-through-reranking" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="enhancing-retrieval-precision-through-reranking"><span class="header-section-number">12.7</span> Enhancing Retrieval Precision through Reranking</h2>
<p>A common challenge in retrieval systems is the occurrence of false positives, where some retrieved texts are not genuinely relevant to the user’s query. To address this, an additional step known as reranking can be incorporated into the RAG pipeline after the initial retrieval phase.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_11.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide explaining reranking as an additional step to retrieval, detailing the problem, aim, solution, and evaluation.</figcaption>
</figure>
</div>
<p>The primary aim of reranking is to reorder the initially retrieved documents according to their actual relevance to the question. One effective solution involves leveraging a generative LLM (gLLM) to evaluate the relevance of each retrieved text. The gLLM can perform a much more advanced semantic differentiation than the embedding model used for initial retrieval.</p>
<p>For this evaluation, scoring categories such as informativeness and the length of the relevant passage within the text can be defined. The gLLM assigns scores based on these categories, and a total score is calculated for each document. These scores then determine the new ranking.</p>
<p>Evaluations of this reranking approach indicate very good results in improving the precision of the documents ultimately passed to the generation stage. However, this enhancement comes at a cost: employing an additional LLM call for each retrieved document significantly multiplies the computational expense and latency of the system.</p>
</section>
<section id="concluding-reflections-advantages-cautions-and-future-challenges" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="concluding-reflections-advantages-cautions-and-future-challenges"><span class="header-section-number">12.8</span> Concluding Reflections: Advantages, Cautions, and Future Challenges</h2>
<p>The exploration of Retrieval Augmented Generation systems within the Humanities and Social Sciences, particularly using philosophical corpora like the Stanford Encyclopedia of Philosophy, reveals a technology of considerable promise, albeit one that requires careful implementation and ongoing development.</p>
<p>RAG systems demonstrate clear advantages. They successfully integrate verbatim corpora and specialised domain knowledge, leading to more detailed and nuanced answers compared to standalone LLMs. This grounding in source material also results in a welcome reduction in the frequency of “hallucinations”. A key strength for scholarly work is the capacity for citation, allowing users to trace information back to relevant documents, thereby making the RAG setup highly suitable for assisting in numerous scientific tasks.</p>
<p>Nevertheless, several cautions must be observed. RAG systems are not “plug-and-play” solutions; they invariably necessitate careful tweaking and customisation. The optimal settings for retrieval, chunking, and generation are highly dependent on the specific characteristics of the corpus and the types of questions anticipated. Consequently, the evaluation of RAG systems is of paramount importance. This requires developing representative sets of questions and expected answers for benchmarking. For unexplored corpora, determining appropriate evaluation strategies remains an open question. Crucially, the expertise of domain specialists—in this case, philosophers—is indispensable for assessing the quality and relevance of RAG outputs and for guiding system refinement.</p>
<p>Looking forward, certain challenges persist. If the retrieval step fails to find relevant documents, the quality of the generated answer predictably decreases. In such scenarios, prompt engineering to adjust the query becomes necessary. For broad, widely discussed overview questions (e.g., “What are the central arguments against scientific realism?”), RAG systems can sometimes produce results that are less comprehensive or fluent than those from a general LLM. Again, prompt adjustments may be required. These observations point towards the need for more flexible and adaptive systems. The development of agentic RAG systems, capable of more sophisticated reasoning and strategy selection, represents a promising direction for future research, potentially overcoming some of the current limitations and further unlocking the potential of RAG for scholarly inquiry.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide summarising the advantages, cautions, and challenges of RAG systems, with bullet points under each category.</figcaption>
</figure>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ai-nepi_011_chapter.html" class="pagination-link" aria-label="Can We Build an AI Solution to Chat with Papers? Exploring the Ghostwriter and EverythingData Workflow">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Can We Build an AI Solution to Chat with Papers? Exploring the Ghostwriter and EverythingData Workflow</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">References</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>