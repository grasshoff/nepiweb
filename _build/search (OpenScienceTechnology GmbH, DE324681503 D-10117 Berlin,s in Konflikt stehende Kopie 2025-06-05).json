[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter introduces Large Language Models (LLMs), elucidating their foundational Transformer architecture and distinguishing the operational principles of encoder-based models, such as BERT, from decoder-based models like GPT. It surveys the evolution of LLMs within scientific domains, highlighting various adaptation techniques. These include continued pre-training, the addition of extra parameters, prompt-based methods, contrastive learning, and Retrieval Augmented Generation (RAG) pipelines.\nFurthermore, the discussion catalogues current LLM applications within History, Philosophy, and Sociology of Science (HPSS) research, categorising them into data management, knowledge structure analysis, knowledge dynamics exploration, and knowledge practice investigation. It also identifies emerging trends, such as accelerating interest and varying customisation levels, alongside persistent concerns like computational demands and model opaqueness. Finally, the presenter offers critical reflections on HPSS-specific challenges, the necessity for LLM literacy, and the importance of aligning NLP task adoption with core HPSS methodologies, whilst also pointing towards new opportunities for interdisciplinary research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#foundations-of-large-language-models-the-transformer-architecture-bert-and-gpt",
    "href": "chapter_ai-nepi_003.html#foundations-of-large-language-models-the-transformer-architecture-bert-and-gpt",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.1 Foundations of Large Language Models: The Transformer Architecture, BERT, and GPT",
    "text": "2.1 Foundations of Large Language Models: The Transformer Architecture, BERT, and GPT\n\n\n\nSlide 02\n\n\nThis presentation commenced by outlining its objectives: to provide a primer on Large Language Models (LLMs) and their scientific adaptations, to summarise current HPSS applications, and to offer critical reflections. At the core of modern LLMs lies the Transformer architecture, a pivotal development introduced by Vaswani and colleagues in 2017. Initially, researchers designed this architecture for language translation tasks, such as converting German to English. This ingenious architecture employs two interconnected streams: an encoder and a decoder.\nThe encoder processes an input sentence, for instance, in German, transforming its words into numerical representations. Crucially, within the encoder, every word interacts with all other words in the sentence, thereby constructing a comprehensive understanding of the sentence’s complete meaning. This mechanism operates bidirectionally. These numerical data then transition to the decoder stream. Here, the decoder generates words in the target language, say English, one by one. Each generated word references only its predecessors to predict the subsequent word; it cannot foresee future words in the sequence. This iterative process feeds each newly produced word back into the decoder until the entire translated sentence emerges. This part of the architecture functions unidirectionally. Both streams contain multiple layers that progressively refine and contextualise word embeddings.\nFrom this foundational design, researchers began re-engineering the encoder and decoder streams independently, leading to the advent of pre-trained language models (PLMs). These models possess a robust capacity to understand or generate language and serve as versatile bases for various Natural Language Processing (NLP) tasks, often requiring only minimal further training.\nOne prominent family of PLMs, derived from the encoder stream, is BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin and associates in 2018. BERT models distinguish themselves by allowing each input word to consider every other word in the context simultaneously, thereby achieving a thorough, full-context understanding. The term “bidirectional” encapsulates this ability to look both forwards and backwards within the text. Whilst powerful for comprehension, BERT models are not primarily designed for generating novel text. Conversely, GPT (Generative Pre-trained Transformers), developed by Radford and colleagues in 2018 and originating from the decoder stream, excels at text generation. Models like ChatGPT are powered by this technology. GPT’s architecture, where tokens only attend to their predecessors, underpins its generative strength. Beyond these, other architectures exist, including combined encoder-decoder models and sophisticated decoder applications that emulate encoder functionalities, such as XLM, which itself builds upon XLNet. A key takeaway highlights the fundamental difference between generative models like GPT, which produce language, and full-context models like BERT, which focus on coherent sentence understanding.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-and-adaptation-of-llms-for-scientific-applications",
    "href": "chapter_ai-nepi_003.html#evolution-and-adaptation-of-llms-for-scientific-applications",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.2 Evolution and Adaptation of LLMs for Scientific Applications",
    "text": "2.2 Evolution and Adaptation of LLMs for Scientific Applications\n\n\n\nSlide 06\n\n\nThe landscape of Large Language Models has evolved significantly, particularly with a focus on scientific domains, as documented by Ho and colleagues in their 2024 survey. This evolution reveals a diverse ecosystem where encoder models, such as BERT-types, are more prevalent than decoder, or GPT-type, models, alongside hybrid encoder-decoder systems. Early influential models in this scientific niche include BioBERT, Specter, and Cyber. Specialised LLMs now cater to a wide array of disciplines, including biomedicine, chemistry, material science, climate science, mathematics, physics, and the social sciences.\nAdapting these powerful models to the nuances of specific scientific language involves several distinct techniques. Full pre-training, where a model learns language from scratch by predicting next tokens (as in GPT) or masked words (as in BERT), demands vast computational power and data, often rendering it impractical for many research teams. A more feasible approach involves continued pre-training, where an existing pre-trained model undergoes further training on a specialised corpus; for instance, a general BERT model can be fine-tuned on physics literature, a method employed by the presenter. Another strategy involves adding extra parameters or layers atop a pre-trained model, then training these new components for specific downstream tasks like sentiment classification or Named Entity Recognition. Prompt-based adaptation offers another avenue, though its specifics were not elaborated upon. Furthermore, contrastive learning stands out as a crucial method for deriving sentence or document embeddings from word-level embeddings, with Sentence-BERT being a prominent example; the work of Irina Gurevich is notable in this area.\nBeyond direct model training, Retrieval Augmented Generation (RAG) offers a powerful pipeline for domain adaptation without necessitating complete model retraining. RAG systems are not monolithic; rather, they orchestrate multiple LLMs and other tools. A typical RAG workflow begins with a user query, such as “What are LLMs?”. A BERT-like model then encodes this query into a sentence embedding. This embedding facilitates a search across a database of relevant documents, retrieving the most similar passages. These retrieved texts subsequently augment the prompt fed to a generative model, which then formulates an answer based on this enriched context. This mechanism is commonly observed in systems like ChatGPT when they integrate information from internet searches. Indeed, the concept extends to more complex reasoning models and agents, which are sophisticated systems combining LLMs with a variety of other tools, representing a significant step beyond individual model capabilities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#llm-applications-trends-and-concerns-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#llm-applications-trends-and-concerns-in-hpss-research",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.3 LLM Applications, Trends, and Concerns in HPSS Research",
    "text": "2.3 LLM Applications, Trends, and Concerns in HPSS Research\n\n\n\nSlide 09\n\n\nTo navigate the complexities of LLMs effectively, researchers must bear in mind several key distinctions. These include the fundamental differences in architecture and pre-training methodologies (encoder-based, decoder-based, or hybrid), the variety of fine-tuning strategies available, the crucial conceptual gap between word embeddings and sentence embeddings, and the varying levels of abstraction from individual LLMs to integrated pipelines and sophisticated agents.\nA survey conducted on the utilisation of LLMs within HPSS research reveals applications falling into four broad categories:\n\nData and Source Management: Scholars employ LLMs for parsing and extracting specific information—such as publication types, acknowledgements, or citations—and for facilitating interaction with source materials through summarisation or RAG-style conversational interfaces.\nKnowledge Structure Analysis: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds, and in mapping intricate networks such as disciplinary formations, interdisciplinary fields, or science-policy discourses.\nKnowledge Dynamics Exploration: This domain benefits from LLM applications in tracing conceptual histories—for example, the evolution of terms like “theory” in Digital Humanities or “virtual” and “Planck” in physics—and in identifying novelty, such as breakthrough papers or emerging technologies.\nKnowledge Practice Investigation: LLMs are applied to reconstruct arguments by identifying premises and conclusions, to analyse citation contexts for purpose and sentiment (revitalising an older HPSS tradition), and to conduct discourse analysis to detect features like hedge sentences, jargon, or instances of boundary work.\n\nObservations indicate an accelerating interest in LLMs within HPSS, with research now appearing even in journals not traditionally associated with computational methods. This broadening appeal may stem from the enhanced semantic capabilities of modern LLMs, attracting qualitative researchers and philosophers. The degree of technical engagement varies widely, from researchers undertaking architectural modifications and custom pre-training, through those performing custom fine-tuning, to others using off-the-shelf tools like ChatGPT. Despite the enthusiasm, several concerns recur: the substantial computational resources required, the inherent opaqueness of many models, shortages of appropriate training data and standardised benchmarks for HPSS-specific tasks, and the inevitable trade-offs when selecting between model types, such as the comprehension strengths of BERT-like models versus the generative power of GPT-like alternatives. Nina’s pertinent point highlights that no single model suits all purposes; rather, one must find an adequate model for a given purpose. Encouragingly, a trend towards greater accessibility is evident, exemplified by tools like BERTTopic for topic modelling, which is gaining traction due to its user-friendliness and robust developer support, potentially emulating the widespread adoption previously seen with tools like pyLDAvis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#reflections-hpss-specific-challenges-and-methodological-considerations",
    "href": "chapter_ai-nepi_003.html#reflections-hpss-specific-challenges-and-methodological-considerations",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.4 Reflections: HPSS-Specific Challenges and Methodological Considerations",
    "text": "2.4 Reflections: HPSS-Specific Challenges and Methodological Considerations\n\n\n\nSlide 12\n\n\nEngaging with LLMs in HPSS necessitates acknowledging several unique challenges inherent to the discipline. A primary concern is the historical evolution of concepts and language. As Nina also highlighted, LLMs are predominantly trained on contemporary language, which can lead to biases or misinterpretations when applied to historical texts where meanings and linguistic conventions have shifted. Researchers must therefore devise strategies to train their own models on historical data or critically adapt existing models, remaining acutely aware of their inherent limitations.\nFurthermore, HPSS scholarship often adopts a reconstructive and critical perspective, seeking to read “between the lines,” understand the socio-historical context of texts, discern authorial intent, and identify subtle discursive strategies like boundary work. Current LLMs are not generally equipped for such nuanced analytical tasks, compelling the HPSS community to explore how models might be guided towards these deeper forms of interpretation. Data-specific issues also abound, including:\n\nThe frequent sparseness of relevant data.\nThe complexities of multilingual sources.\nThe difficulties presented by archaic scripts and orthographies.\nThe persistent lack of digitalisation for many historical archives.\n\nTo navigate these complexities, building robust LLM literacy within the HPSS community is paramount. This involves familiarising scholars not only with the practical application of LLMs, Natural Language Processing (NLP), and Deep Learning tools but also with their theoretical underpinnings. Researchers must cultivate the expertise to determine the most suitable model architectures and training approaches for their specific research questions. Developing shared datasets and benchmarks tailored to HPSS inquiries is also vital, preventing a reliance on off-the-shelf tools that might produce visually impressive but ultimately uninterpretable results.\nCrucially, whilst embracing these new technologies, HPSS scholars must remain true to their own methodologies. The objective should be to translate HPSS problems into solvable NLP tasks—such as classification, generation, or summarisation—without allowing the technical task to obscure or distort the original research focus. If approached thoughtfully, LLMs present new opportunities, particularly for bridging qualitative and quantitative research paradigms, a prospect the presenter finds especially compelling. Finally, it is worth reflecting on HPSS’s own “pre-history” in relation to LLM development. Concepts like co-word analysis, pioneered in the 1980s by Actor-Network Theory scholars such as Callon and Rip, demonstrate a longstanding disciplinary engagement with theoretically informed tool development, offering a rich intellectual heritage upon which to build.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nResearchers Maximilian Noichl and Andrea Loettgers, alongside Taja Knutila, pioneered OpenAlex Mapper, a sophisticated tool designed to investigate transdisciplinary connections within scholarly literature. This work, integral to the “Possible Life” ERC grant project involving Utrecht University and the University of Vienna, directly addresses challenges in the History and Philosophy of Science and Technology (HPSS) concerning the generalisation of case studies. OpenAlex Mapper enables users to project search queries from the OpenAlex database onto a background map of randomly sampled papers.\nThe technical approach involved fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, a base map originated from embedding 300,000 abstracts from OpenAlex using this refined model, followed by dimensionality reduction to two dimensions via Uniform Manifold Approximation and Projection (UMAP). The tool then embeds new query results and projects them onto this existing UMAP model. Key features include interactive exploration of results, options for temporal distribution analysis, and citation graph visualisation. The developers intend OpenAlex Mapper to support qualitative heuristic investigations by bridging detailed case studies with large-scale trends in contemporary science, allowing users to trace the influence and adoption of concepts or models across diverse fields. The tool is accessible online, with slides and a working paper providing further details.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#introduction-project-context",
    "href": "chapter_ai-nepi_004.html#introduction-project-context",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.1 Introduction: Project Context",
    "text": "3.1 Introduction: Project Context\n\n\n\nSlide 01\n\n\nMaximilian Noichl, a doctoral candidate in theoretical philosophy at Utrecht University, presented collaborative work undertaken with Andrea Loettgers and Taja Knutila from the University of Vienna’s philosophy department. This research received funding from an European Research Council (ERC) grant, supporting the “Possible Life” project. The presentation primarily introduced OpenAlex Mapper, a novel tool for scholarly investigation.\nTo foster engagement, attendees accessed the presentation slides interactively via the website maxnechel.eu/talk, enabling local exploration of elements. The presentation first elucidated the tool’s operational principles and its high-level technical architecture. Following this explanation, a practical demonstration showcased its capabilities. Finally, the discussion turned towards the tool’s intended applications and its significance for research in the History and Philosophy of Science and Technology (HPSS).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.2 OpenAlex Mapper: Architecture and Workflow",
    "text": "3.2 OpenAlex Mapper: Architecture and Workflow\n\n\n\nSlide 02\n\n\nResearchers developed OpenAlex Mapper by first refining the Specter 2 language model. They meticulously fine-tuned this model, enhancing its capacity to discern and delineate disciplinary boundaries. Training involved a curated dataset of articles from closely related fields, thereby improving differentiation. This process entailed minor adjustments rather than extensive retraining; UMAP dimensionality reduction visualised its progress.\nSubsequently, the team leveraged the OpenAlex database, a vast, open repository of scholarly material renowned for its inclusivity and accessibility, surpassing alternatives such as Web of Science or Scopus. From OpenAlex, they sampled 300,000 random articles, stipulating only that these possessed well-formed abstracts and were in English. The fine-tuned Specter 2 model then embedded these selected abstracts. Following embedding, Uniform Manifold Approximation and Projection (UMAP) reduced the high-dimensional data to a two-dimensional representation. Crucially, the team preserved this UMAP model.\nOpenAlex Mapper, accessible at https://m7n-openalex-mapper.hf.space, leverages this foundational work. The tool permits users to submit arbitrary queries to the OpenAlex database. It then downloads the results, embeds them using the identical Specter 2 model, and projects them onto the pre-existing two-dimensional UMAP map. A key feature of UMAP facilitates this, ensuring new data points position themselves as if they had formed part of the original map’s construction. As its informational page describes, OpenAlex Mapper projects search queries onto a background map of randomly sampled papers. This enables users to investigate interdisciplinary connections, for instance, by searching for topics such as the Kuramoto model or Wittgenstein’s Philosophical Investigations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#demonstration-interactive-features",
    "href": "chapter_ai-nepi_004.html#demonstration-interactive-features",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.3 Demonstration: Interactive Features",
    "text": "3.3 Demonstration: Interactive Features\n\n\n\nSlide 07\n\n\nA live demonstration vividly illustrated OpenAlex Mapper’s functionality. Users access the tool by navigating to OpenAlex, conducting a search—for instance, for “scale-free network models”—and then copying the resultant URL. They subsequently paste this URL into OpenAlex Mapper’s designated input field. Users can adjust various settings, including sample size and selection methods, before initiating the query.\nUpon query execution, the system commences its backend operations. Initially, it downloads a specified number of records from the search results; for demonstration purposes, this was limited to the first thousand to conserve time. The tool then embeds the abstracts of these downloaded articles. If selected, it also processes and prepares the citation graph associated with these results. The outcome presents a visual projection: search results appear as points on a grey base map, indicating their distribution across the scholarly landscape.\nThis fully interactive map empowers users to delve into specific areas and individual data points. One might, for example, investigate unexpected occurrences, such as references to “coriander” within epidemiological literature. Furthermore, clicking any paper within the visualisation directs the user to its original online source, ensuring a constant link to the underlying textual data. Additional configurable options include visualising temporal distributions of publications or overlaying the citation network. An alternative, more powerful version of the tool, utilising a higher latency GPU setup for larger queries, was also briefly made available.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#rationale-applications-in-hpss",
    "href": "chapter_ai-nepi_004.html#rationale-applications-in-hpss",
    "title": "3  Introducing OpenAlex Mapper",
    "section": "3.4 Rationale: Applications in HPSS",
    "text": "3.4 Rationale: Applications in HPSS\n\n\n\nSlide 13\n\n\nThe development of OpenAlex Mapper addresses specific challenges prevalent within the History and Philosophy of Science and Technology (HPSS). Primarily, it assists researchers in grappling with the inherent limitations of small samples and individual case studies. HPSS often yields rich, detailed understandings of scientific processes through methods such as close textual analysis, direct engagement with scientists, or ethnographic studies. A persistent concern, however, centres on generalising these nuanced insights or validating them against the backdrop of contemporary science, characterised by its global scale, vast output, and rapid pace of discovery.\nOpenAlex Mapper offers a potent means to bridge this gap. It enables researchers to pose questions such as: “Where did the Hopfield model, initially developed in a specific context, truly gain traction and see continued use across various scientific domains?” The tool facilitates tracing the dissemination and adoption of models, methods, or concepts. Consequently, it supports sophisticated quantitative methodologies, underpinning essentially qualitative and heuristic investigations. Its design fosters an iterative analytical process, allowing users to move fluidly between a broad overview on the map and in-depth scrutiny of individual papers, whilst always maintaining a direct link back to the original textual sources. Further technical details and discussions are available in a working paper and the online presentation slides.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#overview",
    "href": "chapter_ai-nepi_005.html#overview",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "",
    "text": "Researchers associated with the ActDisease project investigate the historical role of patient organisations in shaping modern medicine, primarily through analysing their periodicals. The project involves a substantial dataset of 96,186 pages from magazines published by patient organisations in Sweden, Germany, France, and Great Britain between approximately 1890 and 1990. Digitisation of these materials, using ABBYY FineReader Server 14 for Optical Character Recognition (OCR), encountered challenges such as complex layouts, varied fonts, and inconsistent scan quality, leading to OCR errors, particularly in German and French texts, and disrupted reading order. Consequently, the team conducted experiments on post-OCR correction for historical German periodicals using instruction-tuned generative models (Danilova  Aangenendt, 2023). A key analytical challenge arises from the diverse range of text genres within these periodicals—including administrative reports, advertisements, and patient narratives often appearing on the same page—which can bias standard text mining approaches like topic modelling and term counts.\n\n  To address this, investigators developed a genre classification system. They defined nine distinct genres (Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and Question  Answer) under the guidance of a historian specialising in patient organisations. For annotation, project members utilised paragraphs merged by font patterns from two specific periodicals: the Swedish \"Diabetes\" and the German \"Diabetiker Journal\", achieving a high inter-annotator agreement of 0.95 Krippendorff's alpha. Given the limited availability of annotated data (1,182 paragraphs for training and 552 for a held-out set), the research explored both zero-shot and few-shot learning techniques.\n\n  Zero-shot classification experiments involved mapping labels from publicly available datasets—CORE, FTD, and UD-MULTIGENRE—to the custom ActDisease genres. Multilingual encoder models, including XLM-Roberta, mBERT, and a historical mBERT variant (hmBERT), underwent fine-tuning on these mapped datasets. Models fine-tuned on the FTD dataset generally demonstrated better performance with the project's mapping. Notably, specific model-dataset pairings exhibited superior efficacy for certain genres; for instance, XLM-Roberta fine-tuned on UDM excelled at identifying QA sections, whilst hmBERT fine-tuned on UDM performed well on Administrative texts.\n\n  Few-shot learning experiments revealed that additional training on the ActDisease dataset, particularly with prior Masked Language Model (MLM) fine-tuning, significantly improved performance. The hmBERT-MLM model emerged as the top performer, showing particular strength in distinguishing between fiction and nonfiction, a common point of confusion for other models. Although F1 scores improved with increased training instances, they generally remained below 0.8 even with the full training set of 1,182 paragraphs. Further experiments involved few-shot prompting of the Llama-3.1 8b Instruct model, which managed some labels reasonably well but highlighted the insufficiency of only two to three examples per genre for capturing the complexity of categories like nonfiction prose.\n\n  The findings underscore the genre-rich nature of popular historical magazines, which complicates text mining efforts compared to more uniform sources like scientific journals. Genre classification offers a pathway to make these diverse sources more accessible for detailed analysis, enabling comparisons of communicative strategies across different countries, diseases, and publications over time. When training data is scarce, leveraging existing modern datasets or employing few-shot prompting with generative models present viable options. However, few-shot learning with multilingual encoders, especially historical mBERT enhanced by prior MLM fine-tuning, yielded the most promising results, with hmBERT showing substantial gains. Current and future work focuses on applying these methods to specific historical hypotheses, developing a new, more fine-grained annotation scheme (supported by Swe-CLARIN), exploring synthetic data generation, and implementing active learning strategies to refine the classification quality.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-and-dataset",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-and-dataset",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.1 The ActDisease Project and Dataset",
    "text": "4.1 The ActDisease Project and Dataset\n\n\n\nSlide 01\n\n\n    Researchers at Uppsala University are engaged in the ActDisease project, an ERC-funded initiative titled \"Acting out Disease - How Patient Organizations Shaped Modern Medicine.\" This project delves into the histories of patient organisations across 20th-century Europe, specifically examining their contributions to evolving disease concepts, illness experiences, and medical practices. The investigation centres on approximately ten patient organisations located in Sweden, Germany, France, and Great Britain, with a temporal focus spanning roughly from 1890 to 1990. Periodicals, predominantly magazines, published by these organisations constitute the main source material; an early example of such an organisation is the Hay Fever Association of Heligoland, established in Germany in 1897.\n\n    The ActDisease dataset comprises a private, recently digitised collection of these patient organisation magazines, amounting to a substantial 96,186 pages. This corpus includes materials from Germany covering Allergy/Asthma (10,926 pages, 1901-1985), Diabetes (19,324 pages, 1931-1990), and Multiple Sclerosis (5,646 pages, 1954-1990). Swedish contributions include periodicals on Allergy/Asthma (4,054 pages, 1957-1990), Diabetes (7,150 pages, 1949-1990), and Lung Diseases (16,790 pages, 1938-1991). From France, the dataset contains materials on Diabetes (6,206 pages, 1947-1990) and Rheumatism/Paralysis (9,317 pages, 1935-1990). Finally, UK sources cover Diabetes (11,127 pages, 1935-1990) and Rheumatism (5,646 pages, 1950-1990).\n\n    Engineers employed ABBYY FineReader Server 14 for the Optical Character Recognition (OCR) process. Whilst this tool performed well on most common layouts and fonts, significant challenges arose from complex page designs, slanted text, rare typefaces, and inconsistent quality of scans or photographs. Consequently, residual issues such as OCR errors—particularly prevalent in German and French documents—and disrupted reading order affect the digitised collection. To mitigate these problems, the team undertook experiments in post-OCR correction of historical German texts using instruction-tuned generative models (Danilova  Aangenendt, 2023). Observations also indicate a higher frequency of OCR errors within creative textual forms, including advertisements, humour sections, and poetry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-and-manifestations-of-genre-in-actdisease-periodicals",
    "href": "chapter_ai-nepi_005.html#rationale-and-manifestations-of-genre-in-actdisease-periodicals",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.2 Rationale and Manifestations of Genre in ActDisease Periodicals",
    "text": "4.2 Rationale and Manifestations of Genre in ActDisease Periodicals\n\n\n\nSlide 11\n\n\n    Investigators exploring the ActDisease materials discovered a wide diversity of textual content, which, paradoxically, exhibited similarities in composition across the various magazines. A significant challenge arises from this diversity: different text types, such as administrative reports, advertisements, and humour sections, frequently appear side-by-side on a single page. This heterogeneity means that conventional analytical methods, like yearly or decade-based topic models and term counts, fail to capture such intra-page variations. Consequently, these methods are likely to produce results biased towards the most prevalent text types within the corpus.\n\n    To address these analytical limitations, the concept of 'genre' proved useful for distinguishing between different kinds of texts, particularly as genres are intrinsically linked to the communicative purposes of authors. Adopting a definition from Language Technology, a genre is understood as a class of documents sharing a common communicative purpose (Petrenz, 2004; Kessler, 1997). The ability to classify texts by genre is crucial for achieving a key research objective: exploring the dataset from multiple perspectives to construct robust historical arguments. Specifically, genre classification facilitates the study of evolving communicative strategies over time, across different countries, diseases, and publications (Broersma, 2010). Furthermore, it allows for more nuanced, fine-grained analyses of term distributions and topic models within distinct genre categories.\n\n    The ActDisease periodicals manifest a rich tapestry of genres. Examples include poetry, academic reports detailing scientific studies (such as research on the pancreas), and legal documents like deeds of covenant. Commercial content appears in the form of advertisements, for instance, promoting chocolate suitable for diabetics. Instructive or guidance texts offer practical advice, encompassing recipes or medical counsel on diet. Patient organisation reports document internal activities, such as meetings. Crucially for the project, narratives detailing patient experiences and aspects of their lives also form a distinct category.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#methodology-for-genre-classification-experiments",
    "href": "chapter_ai-nepi_005.html#methodology-for-genre-classification-experiments",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.3 Methodology for Genre Classification Experiments",
    "text": "4.3 Methodology for Genre Classification Experiments\n\n\n\nSlide 04\n\n\n    Owing to a scarcity of annotated data, researchers embarked on an exploration of zero-shot and few-shot learning methodologies for genre classification. The zero-shot learning investigation posed two primary questions: firstly, whether genre labels from publicly available datasets could be efficiently mapped to the project's custom labels, and secondly, how classification performance would vary when using different datasets and models. For few-shot learning, the inquiry focused on how performance changes with varying training set sizes across different models, and whether prior fine-tuning on the complete dataset could substantially boost performance. Details of this experimental design are documented by Danilova and Söderfeldt (2025).\n\n    The genre labels themselves were meticulously defined under the supervision of the project's principal historian, an expert on patient organisations. These labels were crafted to be effective in segregating content within the historical materials for detailed analysis, whilst also aiming for a degree of general-purpose applicability to similar datasets. Nine distinct genres were established: Academic (research-based reports, aiming to disseminate scientific information), Administrative (documents on organisational activities), Advertisement (commercial promotions), Guide (instructional texts), Fiction (entertaining narratives), Legal (documents explaining terms and conditions), News (reports on recent events), Nonfiction Prose (narratives of real events or cultural topics), and QA (question-and-answer sections from periodicals).\n\n    For the annotation process, the fundamental unit was the paragraph, derived from ABBYY OCR output and subsequently merged based on font patterns (type, size, bold, italic) at the page level. Annotators sampled content from first and mid-year issues of two periodicals: the Swedish \"Diabetes\" and the German \"Diabetiker Journal\". A team of four historians and two computational linguists, all either native or proficient in Swedish and German, undertook the annotation, with two independent annotations collected for every paragraph. This process yielded a high average inter-annotator agreement of 0.95, measured by Krippendorff's alpha. Annotators utilised spreadsheet files (e.g., .numbers) where they made hard assignments of genres to paragraphs.\n\n    The annotated data was partitioned into a training set of 1,182 paragraphs and a held-out set of 552 paragraphs (approximately 30% of the total), with stratification by label. For few-shot experiments, six different training set sizes (100, 200, 300, 400, 500, and 1,182 paragraphs) were created by random, balanced sampling from the main training set. The held-out set was further divided equally into validation and test sets, also balanced by label. However, the 'legal' and 'news' genres were excluded from these few-shot experiments due to an insufficient number of training instances. For zero-shot experiments, the entire test portion of the held-out set served as the evaluation data. Analysis of genre distribution within the ActDisease training and held-out samples revealed a strong imbalance for the 'advertisement' and 'nonfictional prose' categories across the Swedish and German language materials.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-genre-classification-approach-and-results",
    "href": "chapter_ai-nepi_005.html#zero-shot-genre-classification-approach-and-results",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.4 Zero-Shot Genre Classification: Approach and Results",
    "text": "4.4 Zero-Shot Genre Classification: Approach and Results\n\n\n\nSlide 36\n\n\n    For the zero-shot experiments, investigators utilised several publicly available datasets. These included the Corpus of Online Registers of English (CORE), featuring English texts with main categories also available in Swedish, Finnish, and French, annotated at the document level (Egbert et al., 2015). Another source was the Functional Text Dimensions (FTD) dataset, a balanced collection of English and Russian web genres also annotated at document level (Sharoff, 2018), previously employed in web genre classification work (Kuzman et al., 2023). Additionally, UD-MULTIGENRE (UDM), a subset of Universal Dependencies spanning 38 languages with recovered sentence-level genre annotations, provided further training material (de Marneffe et al., 2021; Danilova and Stymne, 2023).\n\n    Two annotators independently performed the critical task of mapping genre labels from these external datasets to the ActDisease project's custom genre schema. Only mappings where both annotators reached full agreement were incorporated into the final schema. This process revealed that some ActDisease genres, such as 'Administrative' and 'QA' in certain datasets, lacked direct equivalents in the external sources. For instance, 'Academic' in ActDisease mapped to 'research article' in CORE, 'academic' in UDM, and 'academic (A14)' in FTD.\n\n    The creation of training data followed a pipeline involving mapping, preprocessing, chunking, and sampling. Each external dataset yielded training sets in four distinct configurations: considering only Germanic languages [G+], balancing by ActDisease labels [B1], including all language families [G-], and balancing by both ActDisease and original dataset labels [B2]. This resulted in four training samples each from FTD, CORE, and UDM. Researchers selected three multilingual encoder models for these experiments: XLM-Roberta (Conneau et al., 2020), noted as a state-of-the-art web genre classifier (Kuzman et al., 2023); mBERT (Devlin et al., 2019), included for comparison; and historical mBERT (hmBERT) (Schweter et al., 2022), which is pretrained on a large corpus of multilingual historical newspapers that include the languages present in the ActDisease data. These BERT-like models have seen extensive use in prior work on web register and genre classification. Fine-tuning these models across all configurations produced a total of 48 distinct fine-tuned models, with reported metrics representing averages across these configurations.\n\n    Evaluating zero-shot predictions presented a challenge due to the imperfect overlap between the label sets, making direct comparison of overall performance metrics problematic. Therefore, analysts examined the performance for each genre individually and scrutinised confusion matrices. The X-GENRE web genre classifier (Kuzman et al., 2023) served as a baseline, with predictions made on the most similar labels directly mappable to the ActDisease schema. The experimental setup was entirely cross-lingual for the FTD dataset and the X-GENRE baseline (which lack German or Swedish training data) and partially cross-lingual for the UDM and CORE datasets.\n\n    Overall, models fine-tuned on the FTD dataset, using the ActDisease mapping, generally performed better and exhibited less bias in most configurations, with per-genre metrics indicating good performance. Conversely, models trained on UDM and CORE datasets displayed class-specific biases: UDM-trained models tended to favour 'news' (as its news training data contained the highest number of Germanic instances, predominantly German), whilst CORE-trained models showed a bias towards 'guide' (its only multilingual training data). Interestingly, certain model-dataset combinations demonstrated particular strengths: XLM-Roberta fine-tuned on UDM achieved, on average, 32% more correct predictions for the QA genre compared to mBERT and hmBERT. Similarly, hmBERT fine-tuned on UDM yielded 16% more correct predictions for the 'Administrative' genre than XLM-Roberta and mBERT. Models based on CORE data proved effective at predicting the 'legal' genre. Analysis of per-category F1 scores, averaged across data configurations, helped identify performances not attributable to systematic biases.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-for-genre-classification-performance-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-for-genre-classification-performance-evaluation",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.5 Few-Shot Learning for Genre Classification: Performance Evaluation",
    "text": "4.5 Few-Shot Learning for Genre Classification: Performance Evaluation\n\n\n\nSlide 13\n\n\n    The investigation into few-shot learning revealed that further training on the ActDisease dataset yields clear advantages, particularly when models undergo prior fine-tuning using a Masked Language Model (MLM) objective. Performance, measured by F1 scores, consistently increased with the number of training instances. Nevertheless, even with the largest training set of 1,182 paragraphs, F1 scores generally remained below the 0.8 threshold.\n\n    Amongst the models tested, historical mBERT with MLM pre-training (hmBERT-MLM) demonstrated superior performance. A key factor in its success appears to be its sustained ability to differentiate between the 'fiction' and 'nonfiction' genres, even when trained on the full dataset. In contrast, other models, most notably XLM-Roberta, exhibited a significant decline in their capacity to distinguish these two genres as the training data size increased. Examination of detailed scores and confusion matrices for XLM-Roberta-MLM trained on the full dataset showed that it frequently misclassified 'fiction' instances as 'nonfictional prose'.\n\n    Researchers hypothesise that this growing confusion between 'fiction' and 'nonfictional prose' may stem from their increasing similarity within the specific domain of the ActDisease corpus. Since all genres are confined to patient organisation magazines largely focused on diabetes, both fictional narratives and (auto)biographical accounts often centre on the experiences of individuals with diabetes. This thematic and structural overlap likely makes them harder to distinguish as more examples are seen by the models. This suggests that acquiring even more annotated data might be necessary to enhance the classifier's ability to resolve these nuanced distinctions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#investigating-few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#investigating-few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.6 Investigating Few-Shot Prompting with Llama-3.1 8b Instruct",
    "text": "4.6 Investigating Few-Shot Prompting with Llama-3.1 8b Instruct\n\n\n\nSlide 36\n\n\n    Given the current insufficiency of annotated data for comprehensive instruction tuning, investigators explored few-shot prompting as an alternative. They selected Llama-3.1 8b Instruct, a widely recognised multilingual generative model with open weights, for this experiment. The prompt design incorporated definitions for each genre along with two or three carefully selected examples intended to illustrate the target category.\n\n    Evaluation of Llama-3.1 8b Instruct's few-shot prediction performance on the entire held-out set (the zero-shot test set) yielded varied results across genres. The model achieved an F1-score of 0.84 for 'legal' texts, 0.73 for 'advertisement', and 0.72 for 'academic' content. Other scores included 0.64 for 'fiction', 0.62 for 'QA', 0.61 for 'guide', and 0.60 for 'administrative'. Performance was notably lower for 'nonfictional prose' (0.49) and especially for 'news' (0.08). The overall accuracy reached 0.62, with a macro average F1-score of 0.59 and a weighted average F1-score of 0.63.\n\n    These outcomes indicate that the model can handle certain labels with a fair degree of success using only a few examples. However, the experiment also underscored a significant limitation: providing merely two or three examples per genre proved insufficient for the model to adequately represent and distinguish more complex or nuanced categories such as 'nonfictional prose', 'advertisement', and 'administrative' texts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#summary-of-findings-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#summary-of-findings-and-future-research-directions",
    "title": "4  Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments",
    "section": "4.7 Summary of Findings and Future Research Directions",
    "text": "4.7 Summary of Findings and Future Research Directions\n\n\n\nSlide 36\n\n\n    The research culminates in several key conclusions regarding genre classification in historical periodicals. Firstly, popular magazines, unlike more uniform scientific journals or books, often contain a multitude of genres. This richness, reflecting deliberate communicative strategies, complicates text mining efforts. Secondly, genre classification emerges as a vital tool to render these complex sources accessible for text mining, facilitating more accurate and detailed interpretations of results and enabling comparisons across diverse sources from novel perspectives.\n\n    For scenarios lacking specific training data, investigators found it possible to successfully leverage existing modern datasets, provided the target genre categories are sufficiently general-purpose. Another viable approach involves few-shot prompting of open generative models, which can achieve decent classification quality even with limited examples. However, when some annotated data is available, few-shot learning using multilingual encoders such as XLM-Roberta or historical multilingual BERT, particularly when augmented with prior MLM fine-tuning, proves to be a superior strategy; indeed, this was the most effective method in the reported experiments. Notably, historical multilingual BERT demonstrated particularly strong performance gains from MLM fine-tuning, exhibiting a 24% improvement, which surpassed the gains seen for mBERT-MLM (14.5%) and XLM-RoBERTa-MLM (16.9%).\n\n    Current and future endeavours aim to build upon these findings. The team is now working with specific historical hypotheses, leveraging the developed classification methods. A new annotation scheme featuring more fine-grained genres is under development, supported by a new annotation project financed by Swe-CLARIN. Furthermore, researchers are exploring synthetic data generation and the implementation of active learning techniques to enhance classification quality and efficiency. The project acknowledges the contributions of the annotation team (Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, Gijs Aangenendt), funding from the European Research Council (ERC-2021-STG 10104099), support from the Centre for Digital Humanities and Social Sciences (for GPUs and data storage), and valuable input from reviewers including Dr Maria Skeppstedt and anonymous reviewers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals: A Procedural Report on Experiments</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "5  VERITRACE",
    "section": "",
    "text": "Overview\nThe VERITRACE project, an ERC Starting Grant initiative spanning 2023 to 2028, is based at Vrije Universiteit Brussel. This ambitious endeavour investigates the profound influence of an early modern ‘ancient wisdom’ tradition on the development of natural philosophy. Researchers meticulously trace this intellectual lineage, evident in seminal texts such as the Chaldean Oracles and Corpus Hermeticum, extending their inquiry far beyond well-known figures like Newton and Kepler.\nTo achieve this, the project explores a vast corpus of approximately 430,000 multilingual printed works dating from 1540 to 1728. This extensive exploration employs sophisticated computational methods, including keyword searching, advanced text matching (both lexical and semantic), topic modelling, and sentiment analysis. Such techniques aim to uncover previously unrecognised networks of texts, authors, and thematic connections.\nThe project navigates several core challenges, amongst them variable OCR quality, the complexities of early modern typography across at least six languages, and the sheer volume of data. VERITRACE strategically utilises Large Language Models (LLMs) to address these issues. GPT-based LLMs function as “‘LLMs-as-Judges’” for metadata enrichment, whilst BERT-based models, specifically LaBSE, generate semantic embeddings to facilitate precise text matching.\nA sophisticated 15-stage data processing pipeline transforms raw textual data—received in XML, HOCR, and HTML formats—into a structured Elasticsearch database. This robust backend underpins a developmental web application, which features intuitive tools for exploring corpus statistics, searching metadata, reading texts via a Mirador viewer, and performing text matching with configurable parameters. Initial sanity checks on the text matching tool, comparing Latin and English editions of Newton’s Opticks, demonstrate the distinct functionalities of lexical and semantic matching, whilst also highlighting the limitations of current embedding models and the pervasive impact of OCR quality. Future work will focus on refining embedding model selection, potentially through fine-tuning, addressing semantic change over time, mitigating OCR issues, and ensuring scalability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-veritrace-context-and-objectives",
    "href": "chapter_ai-nepi_006.html#project-veritrace-context-and-objectives",
    "title": "5  VERITRACE",
    "section": "5.1 Project VERITRACE: Context and Objectives",
    "text": "5.1 Project VERITRACE: Context and Objectives\n\n\n\nSlide 02\n\n\nResearchers at Vrije Universiteit Brussel spearhead the VERITRACE project, a five-year initiative generously funded by an ERC Starting Grant (101076836) from 2023 to 2028. Formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy”, the project involves a dedicated team of five. Professor Cornelis J. Schilt serves as Principal Investigator, supported by classicist Dr Eszter Kovács, historians Niccolò Cantoni and Demetrios Paraschos, and Dr Jeffrey Wolf, a historian of science and medicine who functions as the project’s digital humanities specialist. Although the core team is based in Brussels, Dr Wolf resides in Berlin.\nThe central objective of VERITRACE is to meticulously trace the influence of an early modern ‘ancient wisdom’ or ‘Prisca Sapientia’ tradition upon the burgeoning field of natural philosophy during that era. This profound tradition manifests in significant texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the widely recognised Corpus Hermeticum. Indeed, a curated collection of 140 works forms the project’s close reading corpus for this tradition.\nWhilst established connections, such as Newton’s engagement with the Sibylline Oracles or Kepler’s knowledge of the Corpus Hermeticum, provide a foundational understanding, VERITRACE seeks to extend far beyond these known instances. The team endeavours to uncover a substantially broader network of texts and authors who interacted with this ancient wisdom, including many overlooked works often referred to as “the great Unread.” Further project details are available via their website: HTTPS://VERITRACE.EU.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-framework-and-multilingual-data-corpus",
    "href": "chapter_ai-nepi_006.html#computational-framework-and-multilingual-data-corpus",
    "title": "5  VERITRACE",
    "section": "5.2 Computational Framework and Multilingual Data Corpus",
    "text": "5.2 Computational Framework and Multilingual Data Corpus\n\n\n\nSlide 04\n\n\nTo address its ambitious research questions, the VERITRACE project employs a computational History and Philosophy of Science and Scholarship (HPSS) framework. This framework facilitates large-scale, multilingual exploration of the extensive textual data. Core to this approach is the identification of textual reuse, encompassing both direct lexical borrowings—such as unacknowledged quotations—and more subtle, indirect semantic appropriations, like paraphrases or allusions that contemporary readers would have readily recognised. Effectively, the team aims to construct what might be termed an “Early Modern Plagiarism Detector.” Beyond merely identifying reuse, the project seeks to uncover previously overlooked networks connecting texts, specific passages, overarching themes, topics, and authors, thereby hoping to reveal novel patterns in the intellectual history and philosophy of science.\nThe foundation for this computational inquiry is a substantial and diverse multilingual dataset. Researchers focus exclusively on digital texts of printed works, setting aside handwritten materials for this project. The corpus spans approximately two centuries, from 1540 to 1728, a period concluding just after Isaac Newton’s death. These texts originate from at least six different languages and are drawn from three primary sources: the freely downloadable Early English Books Online (EEBO), Gallica (the digital repository of the French National Library), and, most significantly, the Bavarian State Library. Collectively, these sources contribute to a corpus of around 430,000 texts. Analysis of this vast collection will leverage a suite of digital techniques, including keyword searching, sophisticated text matching algorithms, topic modelling, and sentiment analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-project-challenges-and-utilisation-of-large-language-models",
    "href": "chapter_ai-nepi_006.html#core-project-challenges-and-utilisation-of-large-language-models",
    "title": "5  VERITRACE",
    "section": "5.3 Core Project Challenges and Utilisation of Large Language Models",
    "text": "5.3 Core Project Challenges and Utilisation of Large Language Models\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several significant challenges inherent in working with historical textual data at scale. A primary concern is the variable quality of Optical Character Recognition (OCR) in the texts, which libraries provide in raw digital formats like XML, HOCR, or HTML, often without the corresponding page images that could serve as ground truth. This variability directly affects all subsequent analytical processes. Furthermore, the project must navigate the complexities of early modern typography and semantics across at least six different languages. Compounding these issues is the sheer volume of data: hundreds of thousands of texts printed across Europe over a span of roughly two hundred years.\nTo address some of these complexities, particularly in text analysis and metadata management, researchers are incorporating Large Language Models (LLMs). On the decoder side, GPT-based LLMs function as “‘LLMs-as-Judges’” to aid in the enrichment and cleaning of metadata, although this presentation did not delve into the specifics of this application. The main focus here is on the encoder-side utilisation of LLMs. Specifically, BERT-based models generate embeddings to capture the semantic meaning of sentences and passages within the vast textual corpus, a crucial step for enabling effective text matching.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-a-brief-overview",
    "href": "chapter_ai-nepi_006.html#metadata-enrichment-via-llms-as-judges-a-brief-overview",
    "title": "5  VERITRACE",
    "section": "5.4 Metadata Enrichment via “LLMs-as-Judges”: A Brief Overview",
    "text": "5.4 Metadata Enrichment via “LLMs-as-Judges”: A Brief Overview\n\n\n\nSlide 08\n\n\nResearchers explored the use of Large Language Models as adjudicators to enrich the project’s metadata, a task critical for ensuring data quality. The primary motivation involves mapping VERITRACE’s records, which initially possess variable quality metadata, against the high-quality records of the Universal Short Title Catalogue (USTC), accessible at https://www.ustc.ac.uk. This mapping, if successful, would yield “enriched” metadata, thereby diminishing the extensive manual cleaning otherwise required. Whilst some record matching can be automated using external identifiers, a substantial portion necessitates more sophisticated comparison, a process made more complex by the uncleaned nature of the initial VERITRACE data. The sheer scale of manual review—with team members potentially facing 10,000 pairs of records each—underscores the urgent need for an automated solution.\nConsequently, the team developed an “‘LLM Bench’”, a system employing a chain of LLMs to evaluate these bibliographic pairs. This panel of judges, including models such as Llama 3 (8B as primary), Qwen 2.5 (7B as secondary for architectural diversity), Mixtral (8x7B as a more powerful tiebreaker), and Llama 3.1 (the latest version for expert review of edge cases), assesses whether two records pertain to the same underlying printed text. Crucially, these LLMs are prompted to provide detailed reasoning and confidence levels for their decisions, guided by extensive “‘MATCHING_GUIDELINES’” that specify field priorities and criteria for matches or non-matches.\nDespite its promise, this “LLM-as-judge” system remains a work in progress. A significant hurdle is the occurrence of hallucinations, where the models generate information about records not actually presented to them. Attempts to mitigate this by enforcing more structured output have, paradoxically, often resulted in overly generic and less insightful responses, especially concerning the reasoning. Achieving the delicate balance between structured, reliable output and genuinely helpful, nuanced reasoning proves to be more an art than a science. Nevertheless, the potential for this approach to streamline the metadata enrichment process remains considerable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-data-processing-pipeline",
    "href": "chapter_ai-nepi_006.html#the-veritrace-web-application-architecture-and-data-processing-pipeline",
    "title": "5  VERITRACE",
    "section": "5.5 The VERITRACE Web Application: Architecture and Data Processing Pipeline",
    "text": "5.5 The VERITRACE Web Application: Architecture and Data Processing Pipeline\n\n\n\nSlide 13\n\n\nEngineers are developing the VERITRACE web application, currently an alpha version restricted to internal use and not yet publicly accessible. This early iteration serves as a proof-of-concept, representing the project’s aspirations for its digital research environment. A key component under current testing is a BERT-based Large Language Model, LaBSE (Language-agnostic BERT Sentence Embedding), tasked with generating vector embeddings for every passage within the corpus. However, initial assessments suggest that LaBSE, whilst functional in some scenarios, may not possess the requisite sophistication for the project’s full demands.\nUnderpinning the web application is a complex, 15-stage data processing pipeline. This pipeline is essential for transforming the raw textual data—received from libraries in formats such as XML, HOCR, and HTML—into a structured format suitable for analysis. The processed data ultimately populates an Elasticsearch database, which functions as the application’s backend. The pipeline encompasses numerous critical steps, including:\n\nBatch processing\nCharacter position mapping\nPage extraction\nLanguage analysis and mapping\nOCR quality assessment\nDocument segmentation\nSegment filtering\nRelationship tracking between segments\nData enrichment within a MongoDB instance before final sequence enrichment\n\nVector embeddings are generated towards the culmination of this multi-stage process. Each stage demands meticulous optimisation to ensure data integrity and processing efficiency.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#navigating-the-corpus-the-explore-and-search-functionalities",
    "href": "chapter_ai-nepi_006.html#navigating-the-corpus-the-explore-and-search-functionalities",
    "title": "5  VERITRACE",
    "section": "5.6 Navigating the Corpus: The Explore and Search Functionalities",
    "text": "5.6 Navigating the Corpus: The Explore and Search Functionalities\n\n\n\nSlide 15\n\n\nThe VERITRACE web application offers users several distinct modules for interacting with the corpus, including sections for Exploration, Searching, Matching, Analysis, and Reading. The “Explore” section furnishes users with statistical insights into the dataset, drawing live information from a MongoDB backend. As an example, on 20 March 2025, the system reported 427,395 metadata records. This section presents visualisations such as language distribution and data source breakdowns, alongside charts illustrating document distribution by decade and common publication places.\nFurthermore, an Elasticsearch Metadata Explorer enables detailed examination of individual records, showcasing the rich metadata compiled for each text. Notably, this includes granular language identification, capable of discerning multiple languages within a single document down to segments of about 50 characters—for example, identifying a text as 85% Latin and 15% Greek. The system also attempts a page-by-page OCR quality assessment, a challenging task without access to ground truth images.\nFor direct interrogation of the corpus, the “Search” section provides robust keyword search functionality. Currently, the prototype operates on a test set of 132 files, yet the index for this small subset already consumes 15 gigabytes, hinting at the terabytes of storage the full corpus will require. A simple keyword search for “hermes” within this prototype, for instance, identified 22 documents with 332 matches. Leveraging the power of Elasticsearch, the search interface supports more sophisticated queries. Users can perform field-specific searches, such as locating instances of “hermes” in works authored by “Kepler” (e.g., author:kepler 'hermes'). The system also accommodates complex Boolean queries, nested searches, and proximity searches, allowing, for example, the discovery of texts where “Hermes” and “Plato” appear within ten words of each other.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prospective-analytical-tools-and-integrated-reading-environment",
    "href": "chapter_ai-nepi_006.html#prospective-analytical-tools-and-integrated-reading-environment",
    "title": "5  VERITRACE",
    "section": "5.7 Prospective Analytical Tools and Integrated Reading Environment",
    "text": "5.7 Prospective Analytical Tools and Integrated Reading Environment\n\n\n\nSlide 14\n\n\nBeyond exploration and search, the VERITRACE web application plans to incorporate an “Analyse” section, although its features are yet to be fully implemented. This module will offer advanced analytical tools, including Topic Modelling capabilities to uncover thematic patterns across the entire corpus or user-selected document sets. Additionally, Latent Semantic Analysis (LSA) will be available for determining document similarity based on shared semantic content. The team also intends to integrate Diachronic Analysis tools, enabling researchers to visualise linguistic and conceptual changes as they unfold over the historical period covered by the corpus.\nComplementing these analytical functions, a “Read” section is already operational. This feature provides scholars with direct access to the source texts. It integrates a Mirador viewer, allowing users to read PDF versions of the documents seamlessly within the application. This reading experience is designed to be comparable to that of major digital library websites, offering the text alongside its pertinent metadata.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#detecting-textual-reuse-the-veritrace-match-functionality",
    "href": "chapter_ai-nepi_006.html#detecting-textual-reuse-the-veritrace-match-functionality",
    "title": "5  VERITRACE",
    "section": "5.8 Detecting Textual Reuse: The VERITRACE Match Functionality",
    "text": "5.8 Detecting Textual Reuse: The VERITRACE Match Functionality\n\n\n\nSlide 20\n\n\nA cornerstone of the VERITRACE web application is its “Match” section, engineered to detect textual reuse across documents. Users can select a query text and then compare it against a single comparison document, multiple documents (for instance, all works by a specific author like Kepler within the database), or, ambitiously, the entire corpus—though the latter presents significant computational challenges. Crucially, the system exposes numerous parameters, such as minimum similarity thresholds, allowing users to fine-tune the matching process.\nThe tool offers three primary types of matching. Lexical matching relies on keyword overlap and BM25 relevance ranking to identify passages with similar vocabulary. Semantic matching, by contrast, employs vector embeddings—currently generated by the LaBSE model—to find conceptually akin passages, even if they share little direct wording; this is vital for comparing texts across different languages. A hybrid approach, combining both lexical and semantic techniques with potentially adjustable weights, is also available. Users can further select a matching mode: “Standard” for default operation, “Comprehensive” for more exhaustive (and computationally intensive) searches, or “Selective” for quicker, less detailed results.\nTo illustrate its capabilities, several sanity checks were performed using Isaac Newton’s Latin Optice (1719) and his English Opticks (1718). A lexical match between the Latin and English versions, as anticipated, yielded no significant results in standard mode due to the language barrier. However, the comprehensive mode did identify three matches, likely corresponding to English text segments, perhaps in the preface of the Latin edition. Conversely, a lexical match of the English Opticks against itself produced perfect scores (100% normalised match, 99.7% coverage, 100% quality), based on nearly 1.3 million passage comparisons, with the interface highlighting identical terms.\nWhen performing a semantic match between the Latin and English Opticks, the system identified passages that seemed reasonably connected (e.g., discussions of colours), despite OCR imperfections. The resulting normalised match score was 58%, with a coverage score of 36.9% and a quality score of 91.2%. The lower coverage might accurately reflect that the Latin edition is considerably longer and potentially contains material not present in the English version. Nevertheless, these tests also underscored potential limitations of the LaBSE model for nuanced semantic comparisons across such historical and linguistically diverse texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#anticipated-challenges-and-future-directions-in-historical-text-analysis",
    "href": "chapter_ai-nepi_006.html#anticipated-challenges-and-future-directions-in-historical-text-analysis",
    "title": "5  VERITRACE",
    "section": "5.9 Anticipated Challenges and Future Directions in Historical Text Analysis",
    "text": "5.9 Anticipated Challenges and Future Directions in Historical Text Analysis\n\n\n\nSlide 21\n\n\nLooking ahead, the VERITRACE team anticipates several significant challenges that will require careful consideration and innovative solutions. The choice and refinement of embedding models remain a critical area. Whilst LaBSE served as an initial model, its limitations are apparent. Researchers are evaluating alternatives such as XLM-Roberta, intfloat multilingual-e5-large, and various historical mBERT implementations, each carrying its own balance of accuracy, computational overhead, and storage demands. An important strategic question is whether the unique characteristics of the early modern corpus—with its specific linguistic features and OCR artefacts—necessitate fine-tuning a base language model to achieve optimal performance, rather than relying on pre-trained models.\nThe phenomenon of semantic change over time presents another profound challenge. Current embedding models, largely trained on contemporary language data, may struggle to adequately represent the evolving meanings of words and concepts across several centuries (e.g., from 1540 to 1700) and across multiple languages. This raises fundamental questions about the coherence of a single vector space for such diachronically and linguistically diverse material.\nFurthermore, the pervasive issue of poor OCR quality continues to cast a long shadow. Errors in the digitised text directly impede crucial downstream processes, including accurate sentence and passage segmentation. Given that re-OCRing the entire corpus of 430,000 texts is impracticable, the team is considering more targeted interventions. These might include selectively re-processing the most problematic texts or dedicating resources to locate pre-existing, higher-quality digital versions.\nFinally, issues of scaling and performance loom large. Current queries on a test set of just 132 texts already take around 15 seconds to complete. Extrapolating this to the full corpus of 430,000 texts highlights an urgent need to optimise algorithms and infrastructure to maintain acceptable response times for users. The project actively welcomes advice and insights from the wider research community on tackling these multifaceted challenges.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>VERITRACE</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "",
    "text": "Overview\nAn exploration of Large Language Model (LLM) evolution charts a trajectory from “attention is all you need,” through “context is all you need”—exemplified by Retrieval Augmented Generation systems—to the current notion of “thinking is all you need.” Nevertheless, significant deficiencies persist in LLMs. These include their propensity for hallucination, the superficial nature of embedding vectors (which do not equate to meanings of expressions), their tendency to generate plausible yet false statements, and their inability to discern justified knowledge from mere internet-sourced repetitions or to formulate robust plans for scientific inquiry.\nTo address these shortcomings, a new paradigm, centred on validation, now emerges, encapsulated by the principle “validation is all you need.” This gives rise to a proposed discipline, Computational Epistemology, which equips systems with epistemic agency. Such agency involves identifying propositions beyond sentences, understanding argumentation, and recognising human intentions, plans, and actions within historical documents.\nA practical framework demonstrates this approach through a working environment built on the Cursor platform, featuring an AI agent named “Bernoulli.” This system tackles complex historical questions, such as Leonhard Euler’s role in the Sanssouci palace’s flawed water fountain construction, by processing historical sources like “Manger1789.pdf” to provide validated, evidence-based answers.\nThe infrastructure supporting this endeavour comprises several key components:\nThis comprehensive approach aims to transform AI into a reliable assistant for scholarly inquiry, delivering accuracy and evidential backing.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#overview",
    "href": "chapter_ai-nepi_008.html#overview",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "",
    "text": "Firstly, a “Scholarium” provides curated scholarly evidence from editorial board-approved sources, including the Opera Omnia Euler, Kepler Gesammelte Werke, and Brahe Opera Omnia.\nSecondly, this Scholarium employs a registry of content items—detailing personal actions, communication acts, statements, and terminology usage, all validated against historical records—as a robust alternative to embedding-based methods.\nThirdly, advanced multimodal LLMs, including Gemini 2.5, Claude, and Llama, integrate into the system via “LettreAI on Cursor.”\nFourthly, a FAIR infrastructure, utilising Zenodo (hosted by CERN), ensures long-term data storage and publication.\nFinally, technical support from the Open Science Technology startup underpins the system, offering an MCP API server to facilitate open collaboration and standardised data access for AI models worldwide.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-evolving-trajectory-and-persistent-deficiencies-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#the-evolving-trajectory-and-persistent-deficiencies-of-large-language-models",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.1 The Evolving Trajectory and Persistent Deficiencies of Large Language Models",
    "text": "6.1 The Evolving Trajectory and Persistent Deficiencies of Large Language Models\n\n\n\nSlide 01\n\n\nLarge Language Models (LLMs) underwent a rapid developmental trajectory. Initial advancements centred upon the principle that “attention is all you need.” Subsequently, the necessity for broader context gained prominence, leading to the idea that “context is all you need,” often addressed through techniques like Retrieval Augmented Generation (RAGs) designed to incorporate larger information stores. Current thinking posits that “thinking is all you need,” adding another dimension to their operational framework.\nDespite these advancements, critical deficiencies remain inherent in contemporary LLMs. A significant issue is the lack of an internal “opponent” or verification mechanism to effectively counter hallucinations. Furthermore, a fundamental misunderstanding persists if one considers embedding vectors as true representations of linguistic meaning; they are not. LLMs also exhibit a tendency to generate statements that, whilst sounding coherent or plausible, are demonstrably false. Their outputs often reflect repetitions of information found across internet media, rather than constituting genuine, verified knowledge.\nCrucially, these models currently lack the ability to systematically seek what is best justified or to formulate coherent and effective plans for scientific inquiry. Indeed, little indication suggests that current technological approaches will overcome these deep-seated limitations in the near future.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#validation-as-a-cornerstone-introducing-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#validation-as-a-cornerstone-introducing-computational-epistemology",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.2 Validation as a Cornerstone: Introducing Computational Epistemology",
    "text": "6.2 Validation as a Cornerstone: Introducing Computational Epistemology\n\n\n\nSlide 03\n\n\nA new guiding principle, “validation is all you need,” emerges as a critical requirement to address the shortcomings of current AI. Validation, in this context, encompasses several core functions. It involves providing substantive reasons both for and against the veracity of any given proposition. Furthermore, it requires the capacity to furnish cogent arguments and supply verifiable evidence that either supports or refutes a proposition’s truth. Beyond propositions, validation extends to actions, offering reasoned justifications for or against their pursuit.\nTo systematically develop these capabilities, researchers propose a new discipline termed “Computational Epistemology.” This nascent field would concern itself with the intricate methods and methodologies required to instil robust validation mechanisms within AI. A key outcome of such a discipline would be the cultivation of “epistemic agency” in AI systems. This agency necessitates the ability to identify underlying propositions, moving beyond superficial sentence interpretation. It also demands the capacity to recognise and deconstruct argumentation present in texts and historical inquiries. Crucially, epistemic agency involves discerning the intentions, plans, and actions of individuals as they are documented and leave their traces in historical records.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#a-practical-framework-the-bernoulli-agent-and-curation-driven-inquiry",
    "href": "chapter_ai-nepi_008.html#a-practical-framework-the-bernoulli-agent-and-curation-driven-inquiry",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.3 A Practical Framework: The Bernoulli Agent and Curation-Driven Inquiry",
    "text": "6.3 A Practical Framework: The Bernoulli Agent and Curation-Driven Inquiry\n\n\n\nSlide 04\n\n\nResearchers have developed a practical working environment to illustrate the principles of validated scholarly inquiry. This environment operates on the Cursor platform and features a specialised AI agent, named “Bernoulli,” tasked with conducting deep inquiries into historical sources. An illustrative application involves the complex history of the Sanssouci castle’s construction under Frederick the Great, specifically addressing the long-debated question of mathematician Leonhard Euler’s involvement and potential culpability in the failure of its water features—a significant engineering challenge of the 18th century.\nWithin this environment, users can pose specific questions. For instance, using a source document such as “Manger1789.pdf” (a historical German text), a query like “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain) prompts the system. The AI agent, Bernoulli, then endeavours to provide a validated and qualifying answer. This answer details, with supporting evidence, the individuals involved (such as Nahl, Benkert and Heymüller, and Giese), their specific tasks, the payments they received, and their contributions or shortcomings related to the water fountain or the Neptune group.\nA core challenge this system addresses is the inadequacy of traditional methods, like simple indexing or token concentration, for comprehensively searching and synthesising information from a multitude of available sources, moving far beyond single-document analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#architectural-components-for-robust-epistemic-systems",
    "href": "chapter_ai-nepi_008.html#architectural-components-for-robust-epistemic-systems",
    "title": "6  Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research",
    "section": "6.4 Architectural Components for Robust Epistemic Systems",
    "text": "6.4 Architectural Components for Robust Epistemic Systems\n\n\n\nSlide 08\n\n\nEngineers and scholars designed a multi-component architecture to support robust epistemic systems.\n\nThe first component, the “Scholarium,” serves as a repository of curated scholarly evidence. A dedicated scholarly editorial board oversees the validation of these sources. A prime example is the Opera Omnia Euler, an extensive collection of 86 volumes compiled over approximately 120 years of scholarly labour, with editing completed in 2022; this encompasses all 866 of Euler’s publications and his entire correspondence. Other significant sources integrated include the Kepler Gesammelte Werke and the Brahe Opera Omnia.\nSecondly, the Scholarium features a registry-based content management system, offering a structured alternative to conventional embeddings. This system functions as a curated database, meticulously cataloguing items such as chronologies of personal actions, communication acts (letters, publications, reports), formal statements (including implications, arguments, and inquiries), and detailed records of an individual’s use of language, terminology, concepts, models, methods, tools, and data. Each entry undergoes rigorous validation against original historical sources, thereby creating a comprehensive inventory of historically substantiated activities. Access to this registry is facilitated through an AI API and an MCP API, referencing Anthropic’s work on MCP.\nThirdly, the architecture integrates advanced multimodal Large Language Models. Gemini 2.5 is particularly favoured for its capacity to combine information from textual and visual sources. Other models, such as Claude and Llama, also form part of this integrated LLM suite, operating within the “LettreAI on Cursor” platform.\nFourthly, a FAIR (Findable, Accessible, Interoperable, Reusable) infrastructure ensures responsible data management. Researchers selected Zenodo, hosted by CERN in Geneva, as the repository for long-term storage and publication of the project’s data.\nFinally, technical support and an open collaboration framework are provided by Open Science Technology, a startup founded by the presenter. This organisation offers crucial technical support for the infrastructure’s operation, including an MCP API server. This server enables AI models worldwide to access the curated data through a standardised API, fostering an environment of Open Source, Open Access, Open Data, and Open Collaboration.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validation is All You Need: Towards Computational Epistemology in AI-Assisted Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "",
    "text": "Overview\nThis study critically examines the capacity of Large Language Models (LLMs) to assess biases within publications classified under the Sustainable Development Goals (SDGs) across three major bibliometric databases: Web of Science, Scopus, and OpenAlex. A primary objective involved employing LLMs, specifically a fine-tuned DistilGPT2 model, not only to detect these biases but also to demonstrate the feasibility of automating information extraction for informing research policy and decision-making.\nThe research team selected five SDGs pertaining to socioeconomic inequalities—SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure). They processed a common corpus of over 15 million publications, indexed across all three databases between January 2015 and July 2023. By fine-tuning separate DistilGPT2 instances for each SDG and database combination, utilising publication titles and abstracts, the team developed a robust method to benchmark LLM-generated content against official SDG targets. This involved crafting specific prompts derived from these targets and meticulously analysing the LLM responses across dimensions such as locations, actors, data/metrics, and thematic focuses.\nKey findings revealed a significant, systematic omission in the database classifications concerning disadvantaged individuals, the poorest nations, and numerous underrepresented topics explicitly mentioned in SDG targets. Conversely, the classifications demonstrated a marked emphasis on economic superpowers and highly developed countries. The study also highlighted divergent methodologies amongst the databases, with Web of Science exhibiting a more theoretical orientation compared to the empirical leanings of Scopus and OpenAlex. These outcomes underscore the profound influence of ostensibly objective bibliometric classification practices on the perceived landscape of SDG-related research, potentially impacting resource allocation and policy formulation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.1 Background and Motivation: The Performative Nature of Bibliometric Databases",
    "text": "7.1 Background and Motivation: The Performative Nature of Bibliometric Databases\n\n\n\nSlide 01\n\n\nMatteo Ottaviani and Stephan Stahlschmidt initiated an investigation into the application of Large Language Models (LLMs) for assessing biases within scientific publications, as classified by major bibliometric databases. This work acknowledges the critical role that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, fulfil within the sociology of science. Indeed, these platforms significantly influence the behaviours and decisions of academics, researchers, funding bodies, and policymakers alike.\nThese databases, however, are far from neutral entities; they respond to diverse political and commercial interests, inherently possessing a performative nature. This performativity shapes how the science system is understood and how value is attributed within it—a concept explored by scholars such as Whitley (2000) and Winkler (1988). The current study specifically considers Web of Science, Scopus, and OpenAlex.\nBuilding upon prior research examining the labelling of Sustainable Development Goals (SDGs) and the construction of search queries, this study addresses a persistent challenge. Armitage et al. (2020), for instance, observed that SDG labelling by various providers yielded disparate results with minimal overlap. Such classification discrepancies can foster divergent perceptions of research priorities. Consequently, these disparities may profoundly impact resource allocation and policy decisions, frequently intertwined with underlying political and commercial interests. This investigation, therefore, scrutinises the aggregate effects arising from how bibliometric databases process metadata and how this subsequently influences diverse stakeholders.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "href": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.2 Case Study: LLMs for SDG Research Analysis",
    "text": "7.2 Case Study: LLMs for SDG Research Analysis\n\n\n\nSlide 04\n\n\nThis investigation centres on a case study analysing the representation of United Nations Sustainable Development Goals (SDGs) within bibliometric data, as detailed by Ottaviani & Stahlschmidt (2024). A primary motivation for this research stems from a desire to comprehend the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly given the prospective integration of LLM-based analytical tools. To this end, the investigators employed relatively small pre-trained Large Language Models, selecting DistilGPT2 for its specific characteristics.\nThe core methodological approach involved fine-tuning these LLMs. Researchers trained separate models on distinct subsets of publication abstracts, with each subset corresponding to a particular SDG classification provided by one of the bibliometric databases under review. This strategy enabled the LLM technology to fulfil a dual role: firstly, as an instrument for detecting inherent data biases; and secondly, as a demonstration of concept. This latter role explored the feasibility of LLMs in automating information extraction processes, thereby informing decision-making within the research domain. Ultimately, the project aimed to conduct a broadly applicable exercise, assessing these aggregate effects and gauging their potential impact on research policy.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "href": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons",
    "text": "7.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons\n\n\n\nSlide 03\n\n\nThe researchers conceptualised a chain of dependencies to frame their analysis. This chain posits that SDG classification practices define “Research on SDGs,” which in turn informs decision-making aimed at aligning with these goals, ultimately impacting socioeconomic inequalities. Various actors—including researchers, small and medium-sized enterprises (SMEs), and governments—process this “Research on SDGs.” The introduction of an LLM as a bias detector, it is posited, influences the adoption of LLMs in research policy, which itself can affect socioeconomic inequalities.\nThe study focused on three principal bibliometric databases: the proprietary Web of Science (Clarivate, US) and Scopus (Elsevier, UK), alongside the open-access OpenAlex (formerly Microsoft, US). Researchers selected five SDGs directly relating to socioeconomic inequalities. These comprised SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduced Inequalities) for the socio-equity dimension, complemented by SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure) for the economic development dimension.\nA substantial dataset formed the basis of the analysis: a jointly indexed subset of 15,471,336 publications. These publications, shared across all three databases and identified via exact DOI matching, spanned from January 2015 to July 2023. Investigators then applied the distinct SDG classification standards of Web of Science, Scopus, and OpenAlex to this common corpus for the five chosen SDGs. This process yielded three unique subsets of publications for each SDG, one corresponding to each database’s classification.\nInitial comparisons of these SDG-classified papers revealed a strikingly low overlap amongst the databases, a finding consistent with earlier work by Armitage (2020). For instance, concerning SDG4 (Quality Education), only 7.2% of the relevant publications in the shared corpus were classified as such by all three databases. Similarly low intersection rates were observed for SDG5 (Gender Equality) at 4.8%, SDG10 (Reduced Inequalities) at 2.9%, SDG8 (Decent Work) at 2.5%, and SDG9 (Industry/Innovation) at a mere 2.0%. An interesting anomaly noted was that Web of Science classified approximately 10% of its SDG5-related publications as originating from the field of mathematics, including topics like geometrical differential equations, indicating potential classification idiosyncrasies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "href": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning",
    "text": "7.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning\n\n\n\nSlide 07\n\n\nInvestigators initially conceived of building bespoke Large Language Models, each trained exclusively on publications classified under a specific SDG by a particular bibliometric database. However, developing LLMs entirely from scratch proved a prohibitively resource-intensive endeavour. Consequently, the team adopted a pragmatic compromise: fine-tuning an existing, pre-trained LLM known for having limited prior knowledge, using publication abstracts as the training material.\nThe choice fell upon DistilGPT2. This selection was deliberate, as prominent commercial and large open-source LLMs were deemed ineligible. Such models often possess pre-existing knowledge about SDGs and strong semantic associations derived from their extensive training datasets, which can include sources like Wikipedia and Reddit discussions. DistilGPT2, in contrast, is a lightweight, English-speaking variant of the open-source GPT2 model that utilises a technique called “distillation,” as described by Sanh (2019). With 82 million parameters—significantly fewer than models like GPT-4—DistilGPT2 offered feasibility for working with proprietary datasets; importantly, it was assessed to have no significant prior semantic understanding of the specific publication domain or the prompts to be used.\nThe fine-tuning procedure involved creating 15 distinct LLM instances: one for each of the five selected SDGs, replicated across the three bibliometric databases. For this fine-tuning, researchers utilised the titles and abstracts of the classified publications. The task was structured such that the LLM, when given a new title as a prompt, would generate a new abstract, with the training aimed at maximising the similarity of this output to the characteristics of the source corpus.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "href": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.5 Benchmarking: SDG Targets and Prompt Engineering",
    "text": "7.5 Benchmarking: SDG Targets and Prompt Engineering\n\n\n\nSlide 10\n\n\nTo evaluate the fine-tuned LLMs, researchers developed a benchmarking methodology rooted in the official structure of the UN Sustainable Development Goals. Each SDG is defined by a series of specific targets; for the SDGs under analysis, this typically ranged from eight to twelve targets per goal. For instance, SDG4 (Quality Education) includes targets such as ensuring all children complete primary and secondary education (Target 4.1), providing access to early childhood development, guaranteeing equal access to vocational and tertiary education for all, enhancing youth and adult skills for employment, eliminating gender disparities in education, and ensuring literacy and numeracy for all learners by 2030, as outlined in the UN’s 2030 Agenda for SDGs.\nBased on this structure, the team implemented a systematic prompt generation strategy. For every individual target within each of the five selected SDGs, ten distinct questions, or prompts, were carefully crafted. Each of these prompts was designed to probe different aspects and nuances of its corresponding target. This meticulous process yielded a specific set of 80 to 120 prompts for each SDG.\nThese target-derived prompts formed the cornerstone of the benchmarking standard. Their primary purpose was to establish a ground truth against which the LLM responses could be measured, thereby defining compliance with the stated objectives of the SDGs. Furthermore, this approach facilitated the identification of “biases” or significant informational omissions. The underlying rationale is straightforward: if an LLM, fine-tuned on a corpus of literature purportedly related to a specific SDG, cannot generate relevant responses to prompts directly addressing that SDG’s official targets, it indicates that information crucial to those targets is either missing or substantially underrepresented within the dataset upon which the LLM was trained. This method provides a systematic way to assess both the completeness of the information captured by the database classifications and the potential biases therein.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "href": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.6 Research Design and Analytical Workflow",
    "text": "7.6 Research Design and Analytical Workflow\n\n\n\nSlide 11\n\n\nThe comprehensive research design involved several distinct stages, beginning with the input data: sets of publication abstracts classified under a specific Sustainable Development Goal (SDG#) by one of the three bibliometric databases (DB#). Each of these curated sets of abstracts then served as the training material to fine-tune an instance of the DistilGPT-2 model. This procedure resulted in a collection of specialised LLMs, each uniquely adapted to the content associated with a particular SDG as represented by a specific database (denoted as Fine-tuned DistilGPT-2 SDG# DB#).\nSubsequently, the prompting process commenced. Researchers utilised the previously developed sets of prompts, each tailored to a specific SDG#. These prompts were systematically inputted into the corresponding fine-tuned DistilGPT-2 model. To explore the variability in LLM output and ensure robustness, the team applied three distinct decoding strategies for generating responses: top-k sampling, nucleus (or top-p) sampling, and contrastive search. This approach yielded three distinct sets of responses for every SDG and database combination, reflecting the nuances of each decoding method.\nFor the initial analysis of these generated responses, researchers applied a filter based on the words used in the original prompts. Following this, noun phrases were extracted from the filtered responses, creating a structured dataset of key terms (Noun phrases SDG# DB#). However, the analytical scrutiny extended beyond this. To ensure a thorough and nuanced comparison, investigators also conducted direct searches within the full text of the LLM-generated responses, complementing the insights derived from noun phrase analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "href": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.7 LLM Response Analysis: Uncovering Biases",
    "text": "7.7 LLM Response Analysis: Uncovering Biases\n\n\n\nSlide 12\n\n\nResearchers analysed the LLM-generated responses by matching the extracted noun phrases against the official targets of each Sustainable Development Goal. This analysis was structured around four key data dimensions: Locations, Actors, Data/Metrics, and Focuses. For every SDG under review, the team assessed the degree of compliance with its targets and identified any discernible biases. Importantly, this process also highlighted differences in how the three bibliometric databases represented SDG-related research.\nAn illustrative example using SDG4 (Quality Education) revealed significant omissions. The LLM responses, reflecting the underlying database classifications, inadequately addressed numerous geographical areas, including most African countries (with the exception of South Africa), other developing nations, Least Developed Countries (LDCs), and Small Island Developing States (SIDS). Similarly, critical groups of actors were systematically overlooked, such as vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Many crucial thematic focuses pertinent to SDG4 were also underrepresented or entirely missing from the LLM outputs. These included vocational training, scholarships, the creation of safe and inclusive learning environments, education for sustainable lifestyles, human rights education, the promotion of peace and non-violence, global citizenship, the appreciation of cultural diversity, and even fundamental aspects like free primary and secondary education and tertiary education.\nExtending these observations across all five selected SDGs, several patterns emerged. Regarding locations, LDCs received scant attention, with Sub-Saharan Africa being mentioned primarily in the context of SDG8. The United States held an “undoubted monopoly” in terms of mentions, followed by South Africa and China, and then the UK and Australia. In the realm of metrics and data, the LLMs frequently recalled specific surveys like the Demographic and Health Surveys (DHS) and World Values Survey (WVS) as data sources. Various indicators, benchmarks, and research methodologies—spanning theoretical, empirical, and thematic analyses, as well as market dynamics and macroeconomics—were also identified, with semantic networks formed after fine-tuning indicating recurrent survey data.\nA consistent and concerning finding related to actors: discriminated and vulnerable categories were systematically overlooked across all analysed SDGs. Even when prompts specifically targeted these groups for different SDGs, the LLMs failed to generate substantial, macro-level responses. In terms of thematic focuses, many SDG-specific sensitive topics, such as human trafficking, human exploitation, and migration, were notably absent. Furthermore, the analysis discerned database-specific tendencies. Across three different SDGs, Web of Science’s classified literature leaned towards a more theoretical approach. Conversely, both Scopus and OpenAlex appeared to favour and represent more empirical research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "href": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "title": "7  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "7.8 Synthesis and Limitations",
    "text": "7.8 Synthesis and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s findings highlight a critical issue: employing Large Language Models as an analytical instrument, mediating between the SDG classifications provided by bibliometric databases and their interpretation by policymakers, uncovers a systematic deficiency in the underlying data. Specifically, scientific publications, as classified under various SDGs, frequently overlook the most disadvantaged categories of individuals, the poorest nations, and numerous underrepresented topics that are, in fact, explicit focuses of the SDG targets themselves. In stark contrast, the classified literature demonstrates considerable attention towards economic superpowers and rapidly developing countries. These results unequivocally show how an ostensibly objective, science-informed practice such as the bibliometric classification of SDGs can wield a decisive influence on perceived research landscapes and priorities.\nResearchers also acknowledged several inherent methodological limitations. Large Language Models exhibit high sensitivity to a range of factors. These include the specific model architecture chosen, although DistilGPT2 was selected for its suitability to the task. The nature of the training data is also paramount; this was partly addressed by utilising three distinct databases, which provided varied training corpora for the LLMs. Furthermore, hyper-parameters, general model parameters, and the chosen decoding strategy all significantly influence LLM behaviour. The impact of decoding strategy was partially mitigated by employing three different recognised methods (top-k, nucleus sampling, and contrastive search). Finally, whilst the study employed a general framework, the use of more developed or specialised LLM architectures could potentially reveal different or more nuanced outcomes. Despite efforts to account for variations in training data and decoding strategies, these elements remain influential variables in LLM performance and output.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "",
    "text": "Overview\nResearchers investigate the complexities of parsing footnotes within law and humanities scholarship, a domain poorly served by conventional bibliometric databases. Their work addresses the critical need for accurate citation data to construct citation graphs. These graphs prove invaluable for tracing patterns in knowledge production, reconstructing influences, and measuring the reception of ideas in intellectual history. The project identifies significant deficiencies in existing data sources such as Web of Science, Scopus, and even OpenAlex, particularly concerning non-English, pre-digital, and non-“A-journal” publications. A primary challenge stems from the intricate nature of humanities footnotes—often termed “footnotes from hell”—which contain extensive commentary and messy, embedded references that traditional machine learning tools, such as those based on conditional random forests, struggle to process effectively.\nTo overcome these limitations, the research explores the potential of Large Language Models (LLMs) and Vision Language Models (VLMs) for reference extraction. Recognising the paramount importance of result trustworthiness, the team embarked on creating a high-quality gold standard dataset. This dataset comprises over 1,100 footnotes from 25 articles spanning various languages (French, German, Spanish, Italian, Portuguese) and a significant historical period (1958-2018), meticulously encoded in TEI XML. This standard facilitates interoperability, allows for contextual markup beyond simple reference management, and enables comparison with existing tools like Grobid.\nFurthermore, the researchers developed Llamore, a lightweight Python package designed for reference extraction from text or PDFs using LLMs/VLMs. Llamore supports both open and closed models and provides an evaluation framework based on the F1 score, incorporating a sophisticated alignment process for comparing extracted references against gold standard data. Initial results demonstrate Llamore, using Gemini 2.0 Flash, significantly outperforms Grobid on their specialised humanities dataset. Nevertheless, Grobid remains competitive on datasets it was trained for, such as the PLOS 1000 biomedical dataset. Future work aims to expand the training data, enhance evaluation metrics, and add support for more nuanced citation analysis, including citation context and resolution of abbreviations like op cit.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "href": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities",
    "text": "8.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities\n\n\n\nSlide 01\n\n\nResearchers embark upon the challenge of parsing footnotes within law and humanities scholarship, a task with which current Large Language Models (LLMs) and other algorithms often struggle. Their primary objective involves generating the specific data required to construct comprehensive citation graphs. Such graphs offer powerful tools for intellectual historians, enabling the discovery of patterns and intricate relationships within the production of knowledge. Moreover, they facilitate the reconstruction of scholarly influences and allow for the measurement of how published ideas are received over time. An illustrative application involves tracking shifts in the most-cited authors, exemplified by an interactive web application analysing the Journal of Law and Society between 1994 and 2003.\nAn extremely poor coverage of historical Social Sciences and Humanities (SSH) material by established bibliometric datasources significantly impedes this research. Prominent databases like Web of Science, Scopus, and even the more accessible OpenAlex, prove largely inadequate for this domain, as they simply do not contain the requisite data. Compounding this issue, Web of Science and Scopus prove prohibitively expensive and operate under highly restrictive licences, creating dependencies undesirable for open scholarly inquiry. Whilst OpenAlex offers an open-access alternative, its coverage for the specialised content needed—particularly non-“A-journals,” pre-digital publications, and non-English language works—remains insufficient. For instance, data for the Zeitschrift für Rechtssoziologie, a German journal for law and society, reveals a stark lack of citation information prior to the 2000s in both Dimensions and OpenAlex.\nSeveral factors contribute to this poor coverage. Primarily, commercial interest in humanities scholarship pales in comparison to that for STEM fields, medicine, and economics, which dominate these large bibliometric databases. Furthermore, these platforms typically prioritise the “impact factor” as a metric for science evaluation, a concern quite distinct from the nuanced inquiries of intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "href": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools",
    "text": "8.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools\n\n\n\nSlide 04\n\n\nBeyond database limitations, researchers identify the inherent complexity of humanities footnotes—aptly termed “footnotes from hell”—as a core challenge. These footnotes frequently feature extensive commentary and disordered data, all embedded within a significant amount of textual “noise,” as examples of German and English academic texts illustrate. Consequently, creating accurate training data for these intricate structures becomes an arduous task. Traditional annotation methods demand a laborious process of manually identifying and tagging various bibliographic elements, such as author, title, and publication date, often within specialised software interfaces.\nFurthermore, existing tools, predominantly reliant on Conditional Random Forests and similar machine learning approaches, prove incapable of effectively handling such complex footnotes. Their performance significantly degrades when confronted with this type of data. For instance, performance metrics for the ExCite tool, detailed by Boulanger and Iurshina (2022), demonstrate variable extraction and segmentation accuracy across different training datasets, highlighting the difficulties with footnoted material. The challenges are multifaceted, encompassing varying citation styles, the complexities of multilingual terminology, and the pervasive use of ellipses, abbreviations (like idem or derselbe), and cross-references. Ambiguities, such as discerning whether an initial numeral signifies a volume or a page number, can perplex even human readers. Misleading capitalisation and the appearance of personal names within titles, which are then erroneously identified as authors, further complicate automated extraction. Language models may also struggle with specialised terminology with which they are unacquainted.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "href": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation",
    "text": "8.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation\n\n\n\nSlide 10\n\n\nScientists now explore Large Language Models (LLMs) as a promising avenue for tackling reference extraction. Early experiments conducted in 2022 with models like text-davinci-003 already indicated the considerable power of LLMs to extract references from disordered textual data. Newer models, including Vision Language Models (VLMs) capable of directly processing PDF documents, hold the promise of even greater efficacy. Researchers investigate various methods, such as prompt engineering, Retrieval Augmented Generation (RAG), and finetuning, to harness these capabilities.\nNevertheless, a critical question looms: can one trust the results generated by these models? The potential for error, exemplified by a widely reported incident of a lawyer misusing ChatGPT in federal court, underscores this concern. A guiding principle for the research, therefore, necessitates avoiding attempts to solve problems for which no validation data exists. This requires developing a robust testing and evaluation solution. Such a solution must rest upon three pillars:\n\na high-quality Gold Standard dataset\na flexible framework that can readily adapt to the fast-moving landscape of AI technology\nsolid testing and evaluation algorithms capable of producing comparable and reliable metrics",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "href": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.4 Crafting a High-Quality Gold Standard: A TEI XML Approach",
    "text": "8.4 Crafting a High-Quality Gold Standard: A TEI XML Approach\n\n\n\nSlide 10\n\n\nAndreas Wagner detailed the team’s efforts to compile a high-quality dataset suitable for both training and evaluation, opting for TEI XML encoding. This choice, whilst perhaps less common in contemporary machine learning circles, stands as the preeminent standard within text-based humanities and digital editorics. Several compelling reasons underpin this decision. TEI XML provides a well-established, comprehensively specified standard for text interchange, surpassing the capabilities of purely bibliographical standards like CSL or BibTeX by covering a broader range of textual phenomena. Crucially, it extends beyond mere reference management to include citations, cross-references, and other forms of contextual markup, which can prove invaluable for tasks such as classifying citation intention. Furthermore, adopting TEI allows researchers to tap into a wealth of existing text collections and corpora from digital editorics projects, many of which publish their source data in this format, sometimes including detailed reference encodings.\nAnother significant advantage of TEI XML lies in the extensive tooling available. Grobid, a prominent tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. Utilising the same data format enables direct performance comparisons with Grobid, facilitates the sharing of training data with the Grobid team and others, and allows the project to leverage Grobid’s existing training resources.\nThe dataset currently under development draws from open-access journals. It involves the meticulous encoding of over 1,100 footnotes extracted from 25 articles, encompassing a diverse range of languages—French, German, Spanish, Italian, and Portuguese—and spanning a considerable period from 1958 to 2018. This collection anticipates yielding over 1,600 individual references; importantly, multiple references to the same work are encoded separately to capture the context of each occurrence. This endeavour remains a work in progress, having adapted its strategy midway to focus on Open Access journals and to incorporate PDFs alongside text, reference strings, and parsed TEI structures. Despite its strengths, TEI XML is no panacea; conceptual challenges, such as distinguishing pointers from references, and technical complexities, like handling constrained elements versus elliptic material, persist. These considerations lead to a fundamental question: how precisely should “performance” be defined and measured in this context?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment",
    "text": "8.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment\n\n\n\nSlide 14\n\n\nDavid Carreto Fidalgo introduced Llamore, an acronym for Large Language Models for Reference Extraction. Engineers developed this Python package to perform two primary functions: firstly, to extract citation data from raw input text or PDF documents utilising (multimodal) LLMs, and secondly, to evaluate the performance of this extraction process. Llamore processes textual or PDF inputs and outputs references formatted as TEI XML. When provided with gold standard references, it generates an F1 score as an evaluation metric.\nTwo principal objectives guided Llamore’s creation. It needed to be lightweight, containing fewer than 2000 lines of code and functioning as an interface to a user’s chosen model rather than embedding models itself. Concurrently, compatibility with both open and closed-source LLMs and VLMs formed a key design consideration. Users can install Llamore via pip. For extraction, one defines an extractor based on the desired model (e.g., GeminiExtractor, OpenAIExtractor). Notably, the OpenAIExtractor ensures broad compatibility with open model serving frameworks like Ollama and VLLM, which typically offer OpenAI-compatible API endpoints. The chosen extractor then processes a PDF or a raw text string, returning references that can be exported to an XML file in TEI biblStruct format. For evaluation, the F1 class is imported and used to compute a macro-average F1 score by comparing the extracted references against gold standard references; users can specify parameters like Levenshtein distance for matching.\nThe evaluation hinges on the F1 score, a well-established metric for structured data comparison, deriving from precision (matches divided by predicted elements) and recall (matches divided by gold elements). An F1 score of 1 signifies perfect extraction, whilst 0 indicates no matches. A crucial aspect of evaluation involves aligning the set of extracted references with the set of gold references. Llamore tackles this by formulating it as an Unbalanced Assignment Problem, employing a solver from the SciPy library. This process involves calculating F1 scores for every possible pairing of extracted and gold references, constructing a matrix of these scores, and then identifying the assignment that maximises the total F1 score under the constraint of unique pairings. This sophisticated alignment ensures accurate macro-averaging, with missing or hallucinated references appropriately penalised with an F1 score of zero. This alignment methodology strongly resembles recent work by Baka and colleagues.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "href": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "title": "8  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "8.6 Comparative Performance, Key Insights, and Future Directions",
    "text": "8.6 Comparative Performance, Key Insights, and Future Directions\n\n\n\nSlide 20\n\n\nTo assess Llamore’s efficacy, researchers conducted comparative performance evaluations. On the PLOS 1000 dataset, comprising 1000 PDFs from the biomedical domain, Llamore (utilising Gemini 2.0 Flash) achieved an F1 score (macro average, exact match) of 0.62, performing on par with Grobid’s score of 0.61. This result is notable given that Grobid was trained on portions of this type of biomedical literature. However, a stark contrast emerged during evaluation on the team’s custom humanities dataset. Here, Grobid’s F1 score plummeted to 0.14, indicating significant difficulty in extracting references. In contrast, Llamore achieved an F1 score of 0.45, demonstrating substantially better, indeed threefold improved, performance on this challenging, footnoted material.\nThese findings lead to several key takeaways. Grobid remains a preferable option for literature similar to its training data, primarily because it operates much faster and consumes fewer resources. Nevertheless, for the complex, footnoted literature characteristic of the humanities, experiments with Llamore paired with Gemini models reveal a significant performance advantage. One must note that these current performance metrics pertain to pure reference extraction and do not yet encompass more nuanced analyses such as citation context or cross-referencing.\nLooking ahead, the team plans to expand their efforts by producing more training data and further refining test metrics. A significant focus will augment Llamore’s capabilities to support more sophisticated analyses. This includes identifying citations in their context (e.g., determining if a citation is approving or critical), resolving abbreviations like op cit., extracting specific pages cited, and accurately counting multiple citations to the same work.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "9  Science Dynamics and AI",
    "section": "",
    "text": "Overview\nResearchers at DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, a prominent research institute, have pioneered an innovative AI-driven solution. This system directly addresses the pervasive challenge of information overload within scientific research. Their primary objective centred on crafting a system that enables users to ‘chat with papers’ from a carefully curated collection, specifically focusing on articles from the method-data-analysis (mda) journal.\nThis ambitious endeavour harnesses sophisticated data processing pipelines, collectively termed EverythingData. These pipelines encompass meticulous term extraction, precise embedding generation, and the efficient utilisation of a Qdrant vector store. Central to this architecture is the Ghostwriter interface, which elegantly facilitates natural language interaction with the processed documents. The underlying methodology draws heavily upon Retrieval Augmented Generation (RAG), seamlessly integrating vector spaces with knowledge graphs, notably Wikidata, to significantly enhance contextual understanding and factual accuracy. This approach, inspired by concepts such as GraphRAG, employs a local, one-billion-parameter Large Language Model (LLM) as both a reasoning engine and an intuitive interface; Gemma3 specifically handles translation tasks.\nThe system’s architecture meticulously splits papers into identifiable blocks, links entities to knowledge graphs for robust grounding, and inherently supports multilingual queries. Initial tests, conducted on a collection of 100 mda articles, compellingly demonstrate the system’s capacity to deliver factual, referenced answers. Crucially, it explicitly states when information is absent, thereby preventing hallucinations, and readily permits iterative query refinement. Moreover, this work proposes a novel methodology for creating LLM benchmarks by decoupling knowledge into Wikidata identifiers, thus paving the way for more sustainable and verifiable AI in scientific knowledge production.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "href": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "title": "9  Science Dynamics and AI",
    "section": "9.1 Context: Science Dynamics, Information Overload, and AI",
    "text": "9.1 Context: Science Dynamics, Information Overload, and AI\n\n\n\nSlide 01\n\n\nThis exploration into AI’s pivotal role in science dynamics stems from a collaborative endeavour between DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, a research-active archive. Whilst DANS primarily focuses on data archiving, GESIS also conducts extensive research. The initiative originated from the profound experimentation of Slava Tikhonov, a senior research engineer at DANS, who has meticulously developed intricate data processing pipelines—complex systems aptly characterised by Arno Simons as a ‘tangle of interwoven components’.\nModern sciences evolve with remarkable rapidity and increasing differentiation. This trajectory, however, poses a significant challenge: how can researchers effectively review, evaluate, and select pertinent information from an ever-expanding corpus? Consequently, scholars confront a veritable ‘flood of information’. The ability to locate and comprehend existing knowledge forms a fundamental precondition for creating new insights, whether by individuals or broader academic communities. A central question thus emerges: can machines, particularly the latest AI technologies that have paradoxically contributed to this information proliferation, also assist in the knowledge production process itself? This chapter investigates this crucial question through the lens of Information Retrieval, aiming to elucidate complex AI solutions comprehensively via a practical use case.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "href": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "title": "9  Science Dynamics and AI",
    "section": "9.2 Research Objectives and System Introduction: Ghostwriter and EverythingData",
    "text": "9.2 Research Objectives and System Introduction: Ghostwriter and EverythingData\n\n\n\nSlide 02\n\n\nInvestigators pursued a specific research question: could they construct an AI solution enabling users to interact conversationally with a selected collection of academic papers? To address this, they developed a system comprising two primary components, internally designated Ghostwriter and EverythingData. Ghostwriter functions as the user-facing interface, whilst EverythingData encompasses the intricate array of backend processes.\nThis chapter introduces foundational concepts such as Information Retrieval, human-machine interaction, and the Retrieval-Augmented Generation (RAG) technique in generative AI. Subsequently, it details the compelling use case involving the method-data-analysis (mda) journal. A core segment elucidates the workflow underpinning this ‘local’ or ‘tailored’ AI solution, providing illustrations of both front-end and back-end operations, before concluding with a summary and outlook.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "title": "9  Science Dynamics and AI",
    "section": "9.3 The Ghostwriter Interface: A Novel Information Retrieval Approach",
    "text": "9.3 The Ghostwriter Interface: A Novel Information Retrieval Approach\n\n\n\nSlide 03\n\n\nResearchers conceptualise the Ghostwriter system as a novel interface for Information Retrieval. Its design philosophy draws inspiration from Slava Tikhonov’s insightful metaphors, which distinguish between two modes of interaction: ‘talking to the librarian’ and ‘talking to the expert’. The ‘librarian’ symbolises engagement with structured data, knowledge organisation systems, and pre-existing classifications. Conversely, the ‘expert’ represents interaction through natural language.\nCrucially, the Ghostwriter interface enables users to converse with both these symbolic entities simultaneously. This capability rests upon a local Large Language Model (LLM) operating on a target data collection, which is further embedded within a network of supplementary data interpretation sources accessible via APIs. This innovative approach seeks to overcome the classic Information Retrieval challenge, where users often require prior knowledge of a database’s schema and typical values to formulate effective queries and obtain optimal results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "href": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "title": "9  Science Dynamics and AI",
    "section": "9.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG",
    "text": "9.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG\n\n\n\nSlide 04\n\n\nThe system’s development firmly situates itself within the broader scientific discourse of Retrieval Augmented Generation (RAG). For a comprehensive introduction to this topic, particularly the integration of knowledge graphs, readers are highly encouraged to consult Philip Rattliff’s seminal paper, ‘GenAI Knowledge Graph The GraphRAG Manifesto: Adding Knowledge to GenAI’ (Neo4j, 11 July 2024). Arno Simons also merits acknowledgement for his significant contributions in this domain, particularly concerning the RAG ‘tool box’.\nThis RAG approach fundamentally comprises three main ingredients. Firstly, researchers construct a vector space from the content of data files, encoding them as embeddings that meticulously capture properties and their attributes; various Machine Learning algorithms and diverse LLMs compute these embeddings. Secondly, a graph forms a metadata layer, seamlessly integrating with diverse ontologies and controlled vocabularies, including those pertinent to responsible AI, and is expressed using the Croissant ML standard. The overarching vision, termed GraphRAG, aims to unify these graph and vector components within a single model. Developers plan to implement this ‘locally’, conceptualising it as a form of Distributed AI where the LLM serves as both an ‘interface’ between human and AI and a ‘reasoning engine’. In practice, the LLM connects to a ‘RAG library’ (representing the graph), navigates datasets, and consumes embeddings (the vectors) to provide essential context for its operations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "href": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "title": "9  Science Dynamics and AI",
    "section": "9.5 System Workflow: From Document Ingestion to Query Response",
    "text": "9.5 System Workflow: From Document Ingestion to Query Response\n\n\n\nSlide 05\n\n\nThe system processes information through a clearly defined workflow, commencing with a collection of input documents. For demonstration purposes, researchers utilised articles from the mda journal, scraping a small number, though any document collection can serve as input. These documents first enter the EverythingData backend, where a series of sophisticated operations transform them. Initially, the system ingests information into a vector store, employing Qdrant for this purpose. Subsequent operations include meticulous term extraction, the construction of precise embeddings, and various enrichments. Notably, selected terms become structured data within a graph, further enriched by linking to external resources such as Wikidata. This coupling with knowledge graphs proves crucial, as it significantly enhances the value of words, phrases, and embeddings by adding layers of contextualisation.\nAll processed data then populates a ‘vector space RAG-Graph’, which forms the core reasoning substrate. When a user poses a question in natural language via the Ghostwriter interface, this query directly interacts with the vector space RAG-Graph. In response, the system delivers not only a list of relevant documents, typical of conventional information retrieval systems, but also a generated summary or explanatory text that the underlying ‘machinery’ deems pertinent to the user’s question. An accompanying diagram from TheAidedge.io compellingly illustrates the comparative roles of vector and graph databases within RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "href": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "title": "9  Science Dynamics and AI",
    "section": "9.6 Implementation and Use Case: Interacting with MDA Journal Articles",
    "text": "9.6 Implementation and Use Case: Interacting with MDA Journal Articles\n\n\n\nSlide 07\n\n\nSlava Tikhonov, a senior research engineer at DANS, meticulously detailed the system’s implementation, drawing upon his early engagement with Large Language Models (LLMs) since testing GPT-2 in 2020. His methodology involves deconstructing the LLM training process into smaller, adaptable components. This approach yields a versatile system applicable not only to academic papers but also to diverse web content; for instance, it can interact with spreadsheets, enabling users to query specific values and receive factual, non-hallucinated responses derived exclusively from the spreadsheet’s data. For complex queries, the system employs a one-billion-parameter LLM, significantly enhanced by integrated knowledge graphs.\nThe primary use case centres on the ‘mda methods, data, analyses’ journal, a GESIS publication. Engineers ingested papers from this journal into the Ghostwriter tool, thereby creating a distinct collection. A core design principle dictates that the system must not rely on any general knowledge pre-loaded into the LLM; instead, it must answer questions using only factual information present within the specified papers. If the requested information is absent, the system transparently states ‘I don’t know’, thereby avoiding speculation. For testing, developers utilised a collection of 100 articles scraped from the mda website. The Ghostwriter instance for these mda papers remains accessible at https://gesis.now.museum. The GESIS ‘Ask Questions’ interface, presented as part of the broader ecosystem, allows users to add new content collections via various means, including single webpages, website crawlers, or RSS feeds.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "href": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "title": "9  Science Dynamics and AI",
    "section": "9.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement",
    "text": "9.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement\n\n\n\nSlide 10\n\n\nThe Ghostwriter system compellingly demonstrates its core functionalities through practical examples. When a user queries, for instance, ‘explain male breadwinner model to me’, the system furnishes a detailed explanation and, crucially, provides precise references to the source documents. This implementation actively prevents hallucination by meticulously locating information within the ingested texts. Engineers achieve this accuracy by splitting each paper into small, identifiable blocks, each assigned a unique identifier. LLM-based techniques then intelligently connect and retrieve these blocks, whilst weights and other methods further refine the process. Furthermore, knowledge graphs assist in predicting which specific text segments are most likely to provide a relevant answer to a given question.\nShould a query be refined—for instance, to ‘explain how data was collected on male breadwinner model’—and the information proves absent from the current corpus, the system responds transparently: ‘According to the provided text, there is no direct information about how data was collected on the male breadwinner model.’ This honesty represents a key feature. The system also supports iterative improvement; an ‘add paper’ button enables users to integrate new documents they discover externally. Subsequently, if the same question is posed, the system can readily utilise this newly incorporated information. For all queries, the interface displays source papers, including their titles, direct links (e.g., to mda.gesis.org), and relevance scores, although not all listed documents may contain the exact query terms in their full text.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "title": "9  Science Dynamics and AI",
    "section": "9.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities",
    "text": "9.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities\n\n\n\nSlide 11\n\n\nSeveral advanced backend mechanisms empower the Ghostwriter system, particularly in entity management and multilingual support. An entity extraction pipeline annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby effectively bridging vector spaces and knowledge graphs. These entities then link to richer knowledge graph representations, with Wikidata serving as a prime example; this linkage proves vital for establishing ground truth. The system also offers immediate multilinguality, enabling seamless interaction even when query and document languages differ. Finally, the LLM processes the retrieved, relevant text pieces to generate a coherent summary or ‘explanatory text’.\nDelving deeper into fact extraction, a user’s query undergoes mapping to a graph representation, and its constituent strings are meticulously annotated with ‘facts’. For instance, terms such as ‘gender roles’ or ‘male breadwinner model’ connect to concepts like ‘societal expectations’ or ‘economic systems’. This process relies on a Knowledge Organisation System (KOS) that can be iteratively applied to reveal progressively deeper semantic layers beneath a term. Instead of relying on free-text strings, the system links entities to Wikidata, thereby obtaining unique identifiers. These identifiers intrinsically connect to multilingual translations and a wealth of properties, allowing, for example, the term ‘male’ to resolve to its specific Wikidata entry (e.g., Q12308941), with LLM embeddings providing similarity scores for disambiguation.\nMultilingual capability is robustly implemented. The system identifies the core concept of a query, such as ‘male breadwinner model’ (represented as ‘bread winner mo’). An LLM, specifically Gemma3, then generates translations of this core concept into a multitude of languages, including Czech, Danish, German, and Japanese, amongst others. These translations effectively broaden the search parameters for the primary LLM.\nThis sophisticated approach underpins a broader vision for Knowledge Organisation Systems. By converting concepts into Wikidata identifiers, knowledge becomes decoupled from the specifics of individual questions or papers. Such abstracted knowledge can be stored independently of any single LLM, thereby fostering model agnosticism. This decoupling also facilitates a novel benchmarking methodology: different LLMs, even those yet to be developed, can be evaluated by their ability to return the same set of identifiers for identical conceptual queries. Any deviations would signal potential issues with a model’s suitability for certain tasks. Collaborations with industry leaders like Google and Meta aim to establish this KOS-centric methodology as a sustainable and foundational element for future scientific endeavours, positioning KOS as a cornerstone of future knowledge management.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "",
    "text": "Overview\nPhilosophical inquiry, a discipline demanding exceptional linguistic and semantic precision, increasingly explores the application of Retrieval Augmented Generation (RAG) systems. Standard Large Language Models (LLMs), however, pose significant limitations for rigorous philosophical research. These include restricted access to complete textual sources, a propensity for hallucination, an inability to learn texts verbatim, and constrained context windows.\nRAG architecture directly addresses these issues. By integrating a curated data corpus, a sophisticated retrieval mechanism, and prompt augmentation with retrieved text, RAG enables direct text access, manages extensive corpora, and crucially, facilitates robust source attribution. Its potential applications in philosophy are broad, encompassing both didactic and research uses. In didactics, RAG systems could allow students to interactively explore complex texts, such as Locke’s Essay concerning Human Understanding. For research, applications range from efficient fact retrieval from handbooks and the exploration of previously unexamined corpora, to the identification of specific passages for close reading, and ultimately, the potential to answer nuanced research questions.\nTo investigate these promising applications, researchers constructed an example RAG system, utilising the Stanford Encyclopedia of Philosophy (SEP) as its primary data source. They meticulously scraped the SEP’s content, converting it into markdown format. Initial development, however, revealed surprisingly poor performance from basic RAG configurations, prompting a qualitative study into optimal system architectures. This comprehensive study necessitated extensive refinement of various components. Researchers meticulously tweaked model choices, including generative LLMs like gpt-4o-mini and embedding models, alongside numerous hyperparameters such as top-k, token limits, temperature, chunk size, and overlap. They also explored algorithmic enhancements, notably reranking mechanisms. Evaluating the unstructured text answers, which frequently articulated complex philosophical propositions, proved particularly challenging, underscoring the critical need for robust evaluation standards.\nCrucial findings emerged regarding chunking strategies: these profoundly impact system performance. For the highly systematised SEP, treating entire main sections—averaging 3,000 words—as individual retrievable documents yielded demonstrably superior results. This outcome proved counter-intuitive, as these sections considerably exceeded the embedding model’s typical input length of approximately 500 words. Furthermore, reranking retrieved documents using a generative LLM to assess relevance substantially enhanced performance. This process, which scores documents based on informativeness and the length of relevant passages, significantly improved the quality of generated answers, albeit at an increased computational cost. Whilst RAG systems adeptly integrate verbatim corpora, mitigate hallucinations, and provide citations, they demand meticulous, corpus-specific tuning. A significant challenge, however, became apparent: RAG systems may underperform on broad overview questions. Their inherent focus on locally retrieved information can inadvertently obscure the larger conceptual landscape. This observation points towards a compelling need for more flexible, perhaps agentic, RAG systems, capable of discerning and adapting to diverse question types.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "href": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation",
    "text": "10.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation\n\n\n\nSlide 01\n\n\nPhilosophical inquiry frequently grapples with intricate questions. Consider, for instance, elucidating Aristotle’s theory of matter within the Physics, or tracing the evolution of Einstein’s concept of locality from his early works on relativity to his 1948 paper addressing quantum mechanics and ‘Wirklichkeit’. Whilst standard Large Language Models (LLMs) like ChatGPT can generate superficially plausible and differentiated responses to such queries, they exhibit significant limitations when applied to rigorous philosophical research.\nA primary constraint involves access to textual sources. LLMs typically lack dynamic access to the full text of scholarly works, even if those texts formed part of their training data. Consequently, requests for specific quotations from chapters or papers may lead to hallucinations or an admission of inability. Even when online search capabilities are activated, copyright restrictions can prevent the reproduction of material.\nFurthermore, the fundamental training mechanisms of LLMs are engineered to avoid mere parroting of texts. Instead, they learn generalisable statistical patterns of language production, with explicit mechanisms preventing verbatim memorisation. This contrasts sharply with the needs of philosophical research, which hinges on meticulous engagement with original source materials and their precise, fine-grained formulations. The limited context window of current LLMs—for instance, ChatGPT-4o’s 128,000 tokens—also poses a significant hurdle when dealing with extensive philosophical corpora.\nTo surmount these challenges, researchers propose Retrieval Augmented Generation (RAG) systems. A RAG system’s architecture typically involves a curated data source, such as the complete corpus of Aristotle’s or Einstein’s writings. From this source, documents are retrieved using methods like semantic search, hybrid approaches, or traditional keyword search. These retrieved documents, or relevant chunks thereof, then augment the user’s original prompt before processing by the LLM. This setup directly tackles the problem of text access, provides a mechanism for managing large context sizes by focusing on relevant segments, and crucially, facilitates attribution by enabling the system to cite the sources for its generated claims, much like the numbered citations seen in tools such as Perplexity AI.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "href": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.2 Applications of RAG Systems in Philosophical Scholarship",
    "text": "10.2 Applications of RAG Systems in Philosophical Scholarship\n\n\n\nSlide 09\n\n\nRAG systems offer a promising avenue for interacting with philosophical corpora, distinguished by their capacity to integrate detailed domain knowledge and rely on verbatim textual evidence. This capability opens several valuable applications within philosophical scholarship, broadly categorised into didactic and research uses.\nIn the realm of didactics, RAG systems can transform how students engage with challenging philosophical texts. For instance, students approaching Locke’s Essay concerning Human Understanding can benefit immensely from the ability to pose repeated questions. This iterative process proves highly instructive, allowing them to start with broad inquiries, such as “What is Locke’s general idea?”, and progressively delve into more specific aspects, like “What is his idea in epistemology?” or “What is his theory of matter?”. Such interactions foster a deeper, more nuanced understanding of the material.\nBeyond educational settings, RAG systems hold significant potential for research. They can streamline the process of looking up facts in handbooks, a task that traditionally involved manually searching physical volumes for information, perhaps for a footnote. RAG offers a more efficient method, potentially with greater reliability than relying on the unverified outputs of standard LLMs. Furthermore, researchers can employ RAG systems to explore unexamined corpora; once digitised, collections of unpublished manuscripts or less-studied texts can be “chatted with” to gain an overview and deeper insights into their content. Another key research application is the identification of specific passages relevant to a particular research question, thereby facilitating focused close reading. Ultimately, the aspiration is that RAG systems might, at some point, become capable of providing detailed answers to at least certain components of complex philosophical research questions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "href": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project",
    "text": "10.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project\n\n\n\nSlide 09\n\n\nResearchers embarked on a project to construct an example RAG system, selecting the Stanford Encyclopedia of Philosophy (SEP)—a widely respected online handbook—as the primary data source. The initial step involved scraping the SEP’s content and converting it into markdown format. Originally, the ambition was to develop a directly useful tool for the philosophical community. However, early attempts to implement a standard textbook RAG system, comprising distinct retrieval and generation components, produced answers of surprisingly poor quality; indeed, these initial outputs were often inferior to those obtainable from a standalone LLM like ChatGPT.\nThis experience prompted a significant shift in focus. The project evolved into a qualitative study aimed at determining the optimal configuration for RAG systems tailored to the specific demands of philosophical texts. Achieving improved performance necessitated a meticulous process of refining numerous aspects of the system. This involved careful selection of both the generative LLM and the embedding model responsible for understanding text semantics. Extensive hyperparameter tuning became essential, covering parameters such as top-k (the number of documents retrieved), maximum input and output token lengths, the temperature or top-p settings influencing the creativity of the generated text, and the strategies for chunk size and overlap in document segmentation. Beyond parameter adjustments, researchers explored more complex algorithmic solutions, such as implementing reranking mechanisms to mitigate problems like semantic mismatch between the query and retrieved documents.\nThe methodology for enhancing the system was predominantly one of trial and error, guided by theoretical insights into how RAG components interact. A significant hurdle encountered throughout this process was the evaluation of the system’s output. Philosophical RAG systems generate answers in free, unstructured text, often articulating complex propositions rather than simple atomic facts (unlike, for example, a historical query seeking Wittgenstein’s last place of living, which expects a city name). Consequently, robust evaluation standards are paramount to assess whether these generated propositions accurately convey the intended philosophical concepts and facts, a non-trivial task.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.4 SEP RAG System: Interface and Functionality Details",
    "text": "10.4 SEP RAG System: Interface and Functionality Details\n\n\n\nSlide 09\n\n\nThe developed Stanford Encyclopedia of Philosophy RAG (SEP RAG) system features a user-facing frontend and a backend constructed with Python, amounting to a few thousand lines of code. The frontend interface provides users with considerable control over the generation process. Within its input section, users can configure several key parameters: they can select the generative model (with gpt-4o-mini shown as an example), view the chosen model’s maximum prompt token limit (e.g., 128,000 tokens), and set a specific prompt token limit for the RAG system’s input (e.g., 15,000 tokens). Additionally, users can define a persona for the LLM—for instance, “You are an expert philosopher. You answer meticulously and precisely”—and specify the number of texts to retrieve for context (e.g., 15). A dedicated field allows for the input of a philosophical question, such as “What is priority monism?”, followed by a “Generate answer” button to initiate the process.\nUpon generation, the system presents its output in a structured manner. Notably, it offers a comparative view, displaying the answer from a standalone LLM (serving as a benchmark) alongside the answer produced by the SEP RAG system. This side-by-side presentation facilitates a more effective assessment of the RAG system’s contribution. Furthermore, the output includes a detailed list of the texts retrieved from the SEP. This list specifies the names of the articles and the particular section headings that the system identified as relevant. Crucially, it also indicates which of these retrieved texts were ultimately included in the augmented prompt passed to the LLM and which, if any, were truncated due to the imposed prompt length limitations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.5 Optimising Document Chunking for Philosophical Corpora",
    "text": "10.5 Optimising Document Chunking for Philosophical Corpora\n\n\n\nSlide 15\n\n\nResearchers dedicated particular attention to optimising the hyperparameter of chunk size, which dictates how documents are segmented for retrieval and processing. They explored several distinct options for this segmentation. One approach involved a fixed number of words or tokens, for example, 500, a method often favoured in computer science for its straightforward implementation. Another considered using natural paragraph breaks as delimiters. A third strategy focused on segmenting the source material by its inherent sections, potentially at various levels of the document hierarchy.\nThrough experimentation with the Stanford Encyclopedia of Philosophy, a clear finding emerged: the most effective results were achieved when entire main sections of SEP articles were treated as the individual “documents” for retrieval. This outcome was somewhat surprising because the average length of these main sections—around 3,000 words—considerably surpassed the input limit of the embedding model, which could process only a little over 500 words at a time.\nThe proposed explanation for this counterintuitive success rests on the specific nature of the SEP. It is a highly systematised and meticulously structured encyclopedic work. Within such a well-organised corpus, the initial portion of a main section (the first 500 words or so that the embedding model can ingest) likely contains enough concentrated information to represent the semantic core of the entire section adequately. However, it is important to note a caveat: this successful strategy of using large, section-level chunks may not be universally applicable. Its efficacy is probably tied to the SEP’s unique characteristics and might not translate effectively to more heterogeneous textual collections or corpora that lack such a high degree of internal systematisation and clear structural demarcation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "href": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.6 Implementing Reranking to Enhance Retrieval Relevance",
    "text": "10.6 Implementing Reranking to Enhance Retrieval Relevance\n\n\n\nSlide 18\n\n\nResearchers identified that an initial retrieval process, even one based on semantic similarity, can sometimes include documents that are not genuinely relevant to the user’s specific question—these are known as false positives. To address this limitation, they incorporated an additional step: reranking. The primary aim of reranking is to re-evaluate and reorder the initially retrieved set of documents, arranging them according to their true relevance to the posed query.\nThe implemented solution involves leveraging a generative Large Language Model (gLLM) to perform this relevance assessment. This choice stems from the understanding that gLLMs exhibit more sophisticated semantic differentiation capabilities than embedding models can offer on their own. Consequently, a gLLM can provide a more nuanced and accurate judgement of how well each retrieved text pertains to the question. During the reranking process, the gLLM scores each candidate document based on specific categories, notably its informativeness concerning the query and the length of the relevant passage contained within it. These individual scores are then aggregated into a “Total Score,” which quantifies the overall relevance of each document.\nThe introduction of this reranking stage proved highly effective. Evaluations demonstrated that it leads to very good results, significantly enhancing the quality and relevance of the documents ultimately used to generate the answer. However, this improvement comes at a cost: the reranking step, by invoking a powerful gLLM for each retrieved document, substantially multiplies the computational resources required, which in turn can increase the monetary expense of operating the RAG system.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "href": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "title": "10  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "10.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy",
    "text": "10.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy\n\n\n\nSlide 18\n\n\nRetrieval Augmented Generation systems present several distinct advantages for philosophical scholarship. They can seamlessly integrate verbatim corpora, ensuring that answers are grounded in authentic textual evidence, and can effectively incorporate domain-specific and specialised knowledge. These capabilities lead to the generation of more detailed answers and, crucially, a dramatic reduction in the incidence of hallucinations. Furthermore, the ability of RAG systems to cite the relevant documents underpinning their responses directly supports scientific rigour and verifiability, making them, in principle, well-suited for assisting in scholarly tasks.\nNevertheless, several points of caution warrant consideration. RAG systems are not “plug-and-play” solutions; they inherently demand extensive and continuous tweaking to achieve optimal performance. The ideal settings for hyperparameters and model choices are not universal but are instead highly contingent upon the specific characteristics of the corpus in use and the nature of the questions typically posed to the system. Rigorous evaluation of RAG outputs is paramount. This requires establishing a representative set of test questions along with clearly defined expected or ideal answers. Such evaluation processes become particularly challenging when working with unexplored or novel corpora, and the active involvement of domain experts—in this case, philosophers—is indispensable for any meaningful assessment of quality and accuracy.\nResearchers also identified specific challenges and limitations. A significant issue arises if the retrieval mechanism fails to locate any relevant documents; in such instances, the quality of the generated answer tends to decrease substantially, often necessitating adjustments to the user’s prompt. An intriguing, somewhat counterintuitive finding was that RAG systems can sometimes produce worse results for broad, widely discussed overview questions, such as “What are the central arguments against scientific realism?”. The hypothesised reason for this phenomenon is that the RAG system’s operational prompt directs it to focus intently on the local information contained within the retrieved texts. This localised focus, whilst beneficial for specific queries, can inadvertently distract from or fail to adequately synthesise the broader, more encompassing perspective required to answer overview questions comprehensively.\nLooking ahead, these observations underscore the need for more flexible RAG systems. Future developments may involve systems capable of discerning between different types of questions and adapting their strategies accordingly, potentially moving in the direction of more sophisticated “agentic” RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "",
    "text": "Overview\nThis chapter presents ongoing research, undertaken jointly with Mike Schneider from the University of Missouri, exploring how computational methods and social network analysis can address fundamental questions in the philosophy of science. The investigation centres on the field of quantum gravity, examining whether its research landscape exemplifies “plural pursuit”—distinct, concurrent instances of normal science aimed at a common problem-solving goal.\nResearchers initiated a bottom-up reconstruction of the quantum gravity research landscape. This involved a linguistic analysis of 228,748 abstracts and titles from Inspire HEP, employing the Bertopic pipeline for embedding and unsupervised clustering into 611 fine-grained topics. Simultaneously, a social network analysis of a co-authorship graph, encompassing 30,000 physicists, identified 819 communities through community detection methods.\nTo address the inherent scale-dependency of topics and communities, the researchers implemented hierarchical clustering for both. They applied Ward agglomerative clustering for topics and hierarchical stochastic block modelling for communities. An adaptive topic coarse-graining strategy, based on the Minimum Description Length criterion, further refined the initial 611 topics to a more manageable 50, thereby balancing model fit with complexity to illuminate the field’s social structure.\nThis bottom-up reconstruction was then rigorously confronted with a top-down approach, derived from physicists’ own intuitions. These insights were gathered via a survey of founding members of the International Society for Quantum Gravity. A Support Vector Machine (SVM) classifier, trained on hand-coded labels and text embeddings (all-MiniLM-L6-v2), subsequently predicted paper categorisation according to physicists’ identified approaches, such as string theory, supergravity, and holography.\nFindings suggest that whilst some bottom-up derived topics align well with physicists’ categorisations, others—notably supergravity and string theory—appear merged within the socio-epistemic structure. This reflects an evolution in the field not always captured by historical or purely conceptual distinctions. Ultimately, this work demonstrates that computational methods can effectively revisit and challenge long-standing philosophical intuitions concerning scientific structures like paradigms and communities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.1 Conceptualising Plural Pursuit in Quantum Gravity",
    "text": "11.1 Conceptualising Plural Pursuit in Quantum Gravity\n\n\n\nSlide 01\n\n\nResearchers initiated an inquiry into the structure of scientific fields, specifically focusing on a long-standing challenge in fundamental physics: the formulation of a quantum theory of gravity. This ambitious endeavour seeks to reconcile our understanding of phenomena at very small scales with those at very large scales. The field of quantum gravity currently presents a multitude of attempted solutions, with string theory standing as the most prominent amongst others, including supergravity, loop quantum gravity (encompassing spin foams), causal set theory, and asymptotic safety.\nTo characterise this situation of multiple, simultaneous research efforts, the investigators, in collaboration with philosopher Mike Schneider, introduce the notion of “plural pursuit”. They define plural pursuit as the existence of distinct yet concurrent instances of normal science, all dedicated to a common problem-solving goal—in this case, the unification of quantum mechanics and general relativity. Furthermore, each such instance of normal science articulates through a specific social community intertwined with an intellectual disciplinary matrix. This conceptualisation draws upon established frameworks in the philosophy of science, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’s research programmes.\nConsequently, a central empirical question arises: does quantum gravity research indeed constitute an instance of plural pursuit? Answering this involves determining whether the field comprises independent communities, each pursuing different paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.2 Bottom-Up Reconstruction: Linguistic and Social Networks",
    "text": "11.2 Bottom-Up Reconstruction: Linguistic and Social Networks\n\n\n\nSlide 03\n\n\nTo empirically investigate the structure of quantum gravity research, scientists embarked on a bottom-up reconstruction of its landscape. This reconstruction aimed to delineate not only the intellectual and linguistic fabric of the field but also its underlying social structure. For this purpose, they gathered a substantial corpus comprising 228,748 abstracts and titles from theoretical physics publications listed on the Inspire HEP database.\nThe analytical process proceeded in two main stages. Firstly, a linguistic analysis sought to map the intellectual structure. Researchers employed the Bertopic pipeline, initially spatialising the documents into an embedding space (L.1). Subsequently, they performed unsupervised clustering on this space (L.2), yielding a highly fine-grained partition of 611 distinct topics. This level of detail was deemed necessary to capture niche research approaches within quantum gravity, some of which might encompass only around one hundred papers. Based on this classification, researchers then assigned each physicist a specialty (σ), defined as the most prevalent topic appearing across their individual publications (L.3).\nIn parallel, a social network analysis scrutinised the co-authorship graph of the field. In this graph, physicists represent nodes, and co-authorship relationships form the edges. Applying community detection methods to this network, which included approximately 30,000 physicists, researchers identified around 819 distinct communities (S.1). This dual analysis thus provided two distinct ways of partitioning the scientists: one based on their topical specialisations and another based on their collaborative communities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.3 Addressing Scale in Plural Pursuit",
    "text": "11.3 Addressing Scale in Plural Pursuit\n\n\n\nSlide 06\n\n\nInvestigators conceptualise plural pursuit, in terms of their computational constructs, as an intuitive one-to-one mapping between social communities and intellectual topics. If such a mapping existed perfectly, a correlation matrix plotting communities against topics would appear block-diagonal. This would signify that each community specialises exclusively in a distinct topical domain, thereby indicating a clear division of intellectual labour.\nHowever, when researchers directly correlated their initial fine-grained partitions of 819 communities and 611 topics, the resulting matrix—visualised using a measure related to normalised pointwise mutual information, npmic,k—proved exceedingly complex and difficult to interpret. Several factors contribute to this intricacy. Firstly, the level of fine-graining in the topic partition is somewhat arbitrary; a broad research programme like string theory, for instance, might be fragmented across numerous fine-grained topics. Secondly, multiple, distinct communities can simultaneously pursue large research programmes, influenced by various micro-social dynamics.\nMore fundamentally, both computational notions of “topic” and “community” are scale-dependent. This implies that literature and social networks can be partitioned at different levels of granularity. This technical issue mirrors a conceptual reality: research programmes themselves are often nested. For instance, one can hierarchically categorise string theory into families like Superstring Theory (further branching into Type II and Heterotic theories, with sub-branches such as Type IIA, Type IIB, Heterotic SO(32), and Heterotic E8 × E8), Bosonic String Theory, and Type I string theory. Consequently, to robustly identify instances of plural pursuit, one must confront and resolve the ambiguity introduced by these multiple, interacting scales.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "href": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining",
    "text": "11.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining\n\n\n\nSlide 09\n\n\nTo navigate the complexities of scale, researchers proposed a hierarchical reconstruction of the quantum gravity landscape. For topics, they implemented Ward agglomerative clustering, starting with the initial 611 fine-grained topics and iteratively merging them based on an objective function, visualised as a dendrogram. For communities, they employed hierarchical stochastic block modelling (drawing on Peixoto, 2014), which inherently learns a multi-level partition of the co-authorship network into increasingly coarse-grained communities. These hierarchical structures permit observation of the system—comprising physicists, their specialties, and their community affiliations—at various scales. However, the choice of which specific scale to analyse for topics and communities remains initially arbitrary, with different choices leading to different correlation patterns and interpretations.\nAn adaptive topic coarse-graining strategy was developed to address this challenge. The underlying idea is to simplify the detailed topic structure by merging topics, provided such merging does not discard information crucial for understanding the social organisation of the field. Many subtle linguistic distinctions, whilst conceptually valid, might not influence how scientists collaborate. This strategy relies on the Minimum Description Length (MDL) criterion, which seeks a partition σ that minimises the sum of two terms: one representing the negative log-likelihood of the social graph G given the partition σ (model fit), and another representing the negative log-probability of the partition σ itself (model complexity). This balances the explanatory power of the topic partition regarding social structure against the desire for a less complex, more parsimonious partition. Applying this, the initial ~600 topics were refined to just 50. Notably, this process preserved some small-scale linguistic topics deemed important for social structure, whilst amalgamating others into broader categories.\nWith these 50 adaptively coarse-grained topics, investigators re-examined the correlation matrix, attempting to match each topic to community structures across different hierarchical levels. Some very broad topics, such as one encompassing general quantum field theory and quantum gravity aspects, appeared ubiquitous and not tied to any specific community. In contrast, the string theory topic demonstrated a strong correspondence with a community structure at the third level of the community hierarchy. Other research programmes, like loop quantum gravity, seemed to align with communities at much finer-grained levels. These findings suggest a complex interplay rather than a simple, clear-cut instance of plural pursuit; nested structures were evident, such as a smaller community focused on holography existing within the larger string theory community. This indicates an entanglement of different scales and a lack of straightforward division of intellectual labour.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.5 Confronting Bottom-Up with Physicists’ Intuitions",
    "text": "11.5 Confronting Bottom-Up with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nResearchers then sought to confront their bottom-up reconstruction of the quantum gravity landscape with the intuitions of physicists working within the field. They conducted a survey targeting the founding members of the International Society for Quantum Gravity, asking them to list the quantum gravity approaches they perceived as structuring the overall research landscape. Although consensus was not universal, this exercise yielded a consolidated list of approaches, including asymptotic safety, causal sets, string theory, supergravity, and holography, amongst many others. For subsequent detailed analysis, the investigators focused on string theory, supergravity, and holography, particularly because some physicists expressed uncertainty about whether these should be considered entirely separate. Indeed, some argued that supergravity and aspects of holography are fundamentally rooted in string theory, despite historical and certain conceptual distinctions.\nTo operationalise this top-down perspective, they trained a Support Vector Machine (SVM) classifier. The classifier’s task was to predict the categorisation of papers into these physicist-defined approaches, using text embeddings derived from titles and abstracts (via the all-MiniLM-L6-v2 model) and trained on a set of hand-coded labels. The resulting supervised, top-down classification was then compared against the 50 coarse-grained bottom-up topics using a correlation heatmap.\nThis comparison revealed that certain physicist-defined approaches aligned well with the emergent bottom-up topics, especially those that were well-defined and conceptually autonomous. Conversely, approaches that were more phenomenological or lacked a fully developed conceptual framework showed poorer correspondence. A significant observation concerned a large “string theory” cluster identified through the bottom-up, scale-aware analysis; this cluster appeared to encompass both supergravity and string theory. This finding resonated with feedback from the survey, exemplified by one physicist who noted the substantial overlap in personnel working on supergravity and string theory, questioning whether the communities could be meaningfully separated. This suggests that the bottom-up methodology, by filtering out linguistic nuances that lack significant correlates in the social structure, can reveal how fields evolve socio-epistemically, sometimes merging entities that were once more distinct. The initial, finer-grained linguistic clustering, however, does correctly capture these underlying conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "title": "11  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "11.6 Conclusions and Philosophical Implications",
    "text": "11.6 Conclusions and Philosophical Implications\n\n\n\nSlide 18\n\n\nThis investigation yields several important conclusions regarding the structure and analysis of scientific fields. Firstly, socio-epistemic systems, such as research fields, can and should be observed at multiple scales. Consequently, fundamental concepts like “communities” and “disciplinary matrices” are inherently scale-dependent. Secondly, the task of identifying configurations of plural pursuit—which ideally manifest as a one-to-one mapping between distinct communities and their unique intellectual substrates—requires a careful matching of these social and intellectual structures across relevant scales.\nSpecifically for the domain of quantum gravity, the bottom-up reconstruction of its research landscape offers a valuable tool. It can serve to either confirm or prompt a re-assessment of certain intuitions that physicists themselves hold about the organisation and evolution of their field. More broadly, this work underscores a significant methodological point: the advent of increasingly powerful computational methods provides new avenues to rigorously revisit, challenge, or refine philosophical insights that have, for extended periods, relied primarily on intuition. This applies particularly to understanding complex structures like scientific paradigms and research communities.\nIn essence, the researchers propose a powerful synergy, suggesting that, to paraphrase Clausewitz, computation can be viewed as the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "",
    "text": "Overview\nResearchers Francis Lareau, from the Université de Sherbrooke and the Université du Québec à Montréal, alongside Christophe Malaterre, also from the Université du Québec à Montréal, conducted a comprehensive comparative study. Their investigation sought to determine whether applying topic modelling to titles or abstracts suffices for scientific literature, or if full-text analysis remains indispensable, particularly within the history, philosophy, and sociology of science. This inquiry directly addresses the substantial resources full-text processing demands.\nThe study meticulously constituted a corpus of scientific articles, subsequently identifying their title, abstract, and full-text sections. Researchers then applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these three textual levels. Following this, they rigorously analysed and compared the six resulting topic models.\nThis comparison employed both qualitative methods, drawing upon a pre-existing detailed analysis of an astrobiology corpus, and quantitative measures. These quantitative metrics included the Adjusted Rand Index, topic diversity, joint recall, and Coherence CV. The findings aim to guide researchers in selecting appropriate text levels for topic modelling, aligning their choices with specific objectives and resource constraints by highlighting performance variations across different models and textual structures.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "href": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels",
    "text": "12.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels\n\n\n\nSlide 01\n\n\nInvestigators initiated an inquiry to ascertain the most effective textual level—titles, abstracts, or full-texts—for applying topic modelling techniques to scientific literature. This area holds particular relevance for the history, philosophy, and sociology of science. Topic modelling has, indeed, emerged as a crucial instrument for dissecting substantial volumes of scholarly publications.\nThis powerful technique enables diverse analytical tasks, such as identifying research trends and paradigm shifts, uncovering thematic substructures and their interrelations, and tracing the evolution of scientific terminology. Observations from existing literature, however, reveal a varied application of topic modelling across these different textual components.\nThis background prompts a central research question: can analyses restricted to titles or abstracts yield sufficient insights, or does comprehensive full-text analysis remain essential? The considerable resources demanded for obtaining, preprocessing, and analysing complete full-text corpora lend urgency to this question.\nTo address this, investigators first assembled a corpus of scientific articles. Subsequently, they meticulously identified the title, abstract, and full-text sections for each document. Two distinct topic modelling methodologies, Latent Dirichlet Allocation (LDA) and BERTopic, were then applied to each of these three textual levels. Finally, the six generated topic models underwent rigorous qualitative and quantitative comparison to evaluate their respective performances.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.2 Methodology: Topic Modelling Approaches – LDA and BERTopic",
    "text": "12.2 Methodology: Topic Modelling Approaches – LDA and BERTopic\n\n\n\nSlide 05\n\n\nInvestigators employed two distinct topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches operate on the premise that documents can be translated into numerical vectors. This transformation allows topics to be identified through the analysis of repetitions that highlight linguistic regularities. Machine learning techniques then automate the detection of these underlying patterns.\nLatent Dirichlet Allocation, a well-established statistical method, constructs simple vector representations by counting word occurrences within documents. Within this framework, topics manifest as latent variables governed by Dirichlet’s probability distribution. A key advantage of LDA is its capacity to handle extensive texts, rendering it applicable to titles, abstracts, and full-text documents alike.\nConversely, BERTopic offers a more recent, modular alternative. This approach leverages vector representations derived from Large Language Models, with BERT (Bidirectional Encoder Representations from Transformers) serving as its foundational model. In BERTopic, topics emerge as clusters of similar documents.\nWhilst earlier iterations of BERTopic faced limitations with long texts, this study incorporated new embedding techniques. These advancements enable the processing of substantial textual inputs, up to approximately 131,000 tokens, thereby extending BERTopic’s utility to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "href": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus",
    "text": "12.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus\n\n\n\nSlide 07\n\n\nFor the qualitative comparison, researchers utilised material from a prior in-depth topic analysis of an astrobiology corpus, detailed in Malaterre & Lareau (2023). Following a thorough evaluation process, they selected an existing Latent Dirichlet Allocation (LDA) full-text model, which featured 25 distinct topics, as a reference. Each of these 25 topics had undergone meticulous analysis, examining its most representative words and associated documents, leading to the generation of a descriptive label for each topic using pertinent keywords.\nSubsequently, the interrelations between these topics were quantified. Researchers calculated the mutual correlation based on how topics appeared together within documents. A community detection algorithm then processed these correlations, successfully identifying four overarching thematic clusters. These clusters received designations using letters (A, B, C, D) and distinct colours (red, green, yellow, and blue) for clarity.\nThe study presented these findings visually, employing a graph that illustrated the correlations between the 25 topics, complete with their assigned labels and colour-coded cluster memberships. In this graphical representation, the thickness of the lines signified the strength of the correlation between connected topics, whilst the size of the circles indicated the overall prevalence of each topic throughout the entire document collection. In essence, this pre-existing, detailed analysis provided a robust qualitative foundation against which the six topic models generated in the current investigation could be systematically compared.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.4 Methodology: Quantitative Analysis Metrics",
    "text": "12.4 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nResearchers employed four distinct metrics for the quantitative analysis to compare the topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings produced by the models, with a crucial correction for agreements that might occur by chance. An ARI value of zero typically signifies a random clustering.\nSecondly, topic diversity was assessed. This metric quantifies the proportion of distinct top words that characterise the topics within a given topic model, indicating whether different topics are described by unique sets of terms. Thirdly, joint recall provided a measure of how well the top words collectively represent the documents classified under each topic. Specifically, it evaluates the average document-topic recall, considering the relationship between a topic’s top words and its associated documents.\nFinally, coherence, specifically Coherence CV, was measured. This metric aims to determine if the top words constituting a topic are semantically related and form a meaningful group. Its calculation involves averaging the cosine relative distance between the top words within each topic.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.5 Results: Adjusted Rand Index Analysis of Model Similarity",
    "text": "12.5 Results: Adjusted Rand Index Analysis of Model Similarity\n\n\n\nSlide 09\n\n\nThe application of the Adjusted Rand Index (ARI) across all six topic models revealed varying degrees of similarity between them. As a reminder, an ARI score of zero signifies that the agreement between two clusterings is no better than random. The results, often visualised as a heatmap, indicated that the Latent Dirichlet Allocation (LDA) model applied to titles (LDA Title) was the most distinct. It showed the lowest similarity to the other models, with ARI values generally falling below 0.2.\nIn contrast, all other models demonstrated a better overall match with one another, achieving ARI values consistently above 0.2. Notably, the BERTopic models exhibited a stronger internal coherence; they tended to align more closely with each other, yielding ARI values that surpassed 0.35. Within this group, the BERTopic model applied to abstracts (BERTopic Abstract) emerged as a somewhat central figure, as it corresponded well with nearly every other model, the only significant exception being the divergent LDA Title model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.6 Results: Qualitative Comparison of LDA Models",
    "text": "12.6 Results: Qualitative Comparison of LDA Models\n\n\n\nSlide 09\n\n\nA more detailed qualitative analysis focused on the Latent Dirichlet Allocation (LDA) models. When comparing the LDA full-text model with the LDA abstract model (Table A), researchers observed a good overall fit. This conclusion arose because topics from one model generally found a corresponding match in the other, evidenced by a high proportion of shared documents, which formed a noticeable reddish diagonal in the suitably organised heatmap.\nNevertheless, some transformations occurred: three topics present in the full-text LDA model disappeared in the abstract model, whilst another three full-text topics split into multiple, more granular topics within the abstract representation. Conversely, three entirely new topics emerged in the LDA abstract model, and three other abstract topics appeared to be the result of mergers from the full-text model. An additional observation was the presence of one small class, or topic, in the LDA abstract model, encompassing fewer than 50 documents.\nThe comparison between the LDA full-text model and the LDA title model (Table B) revealed a starkly different picture. Here, the fit was poor, indicating substantial reorganisation of thematic structures. Numerous topics from the full-text model disappeared when moving to the title-based model, and concurrently, many new topics emerged that were specific to the LDA title analysis. The heatmap for this comparison displayed a profusion of dark vertical and horizontal lines, visually underscoring the extensive restructuring of topics.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.7 Results: Qualitative Comparison Involving BERTopic Models",
    "text": "12.7 Results: Qualitative Comparison Involving BERTopic Models\n\n\n\nSlide 11\n\n\nInvestigators then examined the BERTopic models in comparison. When contrasting the BERTopic full-text model with the original LDA full-text model (Table C), they found an average overall fit. From the perspective of the LDA full-text topics, eight disappeared in the BERTopic full-text representation, and six were split into more granular topics. Conversely, five new topics emerged within the BERTopic full-text model, and one topic appeared to be the result of a merger. This model, however, presented class size issues: specifically, four small classes and one extremely large class.\nNext, comparing the BERTopic abstract model against the LDA abstract model (Table D), researchers noted a relatively good overall fit. In this transition, four topics from the LDA abstract model disappeared, whilst six were split. The BERTopic abstract model introduced two new topics and featured four topics that resulted from mergers. Importantly, the class sizes in this BERTopic abstract model were generally balanced.\nFinally, the comparison between the BERTopic title model and the LDA title model (Table E) indicated an average fit. Seven topics from the LDA title model were absent in the BERTopic title model, and one LDA title topic was split. The BERTopic title model, in turn, presented seven new topics and one topic formed by a merger. This model also exhibited class size concerns, with three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.8 Results: LDA Top-Words Qualitative Analysis",
    "text": "12.8 Results: LDA Top-Words Qualitative Analysis\n\n\n\nSlide 13\n\n\nA qualitative assessment of the top words defining topics within the Latent Dirichlet Allocation (LDA) models revealed that, generally, the topics were relatively well-formed across the full-text, abstract, and title variations. Investigators observed instances of robust topics that maintained a strong correspondence across all three LDA models. A notable example was the topic labelled “A-Radiation spore” in the LDA full-text model, which aligned closely with semantically similar topics in both the LDA abstract model (characterised by top words such as “radiation,” “spore,” and “space”) and the LDA title model (with top words including “space,” “simulated,” and “spore”).\nFurthermore, some topics identified in the full-text model underwent splitting, fragmenting into several distinct topics within the abstract and title models. For instance, the “A-Life civilization” topic from the full-text analysis split, and one of its resultant components in the abstract model cohered into a general theme concerning research and astrobiology; this particular split was deemed logical. Another full-text topic, “B-Chemistry,” also fragmented, though its resulting divisions proved more challenging to interpret readily without deeper investigation.\nConversely, the analysis also identified instances of topic merging. Certain topics from the full-text model consolidated into new, more encompassing topics in the other LDA models. For example, the distinct full-text topics “B-Amino-acid” and “B-Protein-gene-rna” merged within the LDA abstract model. This fusion created a broader, more generalised topic, a development considered to be a sensible thematic consolidation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.9 Results: BERTopic Top-Words Qualitative Analysis",
    "text": "12.9 Results: BERTopic Top-Words Qualitative Analysis\n\n\n\nSlide 14\n\n\nContinuing the top-words assessment with the three BERTopic models (full-text, abstract, and title), researchers again found that the topics were, on the whole, relatively well-formed when compared against the LDA full-text baseline. The previously identified robust topic, “A-Radiation spore,” demonstrated its stability by maintaining good correspondence across all BERTopic model variations as well.\nThe topic “A-Life-civilization,” also from the LDA full-text model, showed relative stability when analysed with BERTopic across the different text levels. However, it did undergo some degree of splitting here and there. These divisions typically resulted in the formation of narrower, more specific topics pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic from the LDA full-text model, when subjected to BERTopic analysis across the full-text, abstract, and title inputs, also tended to split. This fragmentation consistently led to the emergence of more narrowly focused chemical themes.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.10 Results: Quantitative Analysis – Coherence (CV)",
    "text": "12.10 Results: Quantitative Analysis – Coherence (CV)\n\n\n\nSlide 15\n\n\nResearchers then presented the performance metrics for all six models, considering a range of topic numbers from 5 to 50. Beginning with coherence (specifically Coherence CV), which assesses the semantic relatedness of a topic’s top words, several patterns emerged. The analysis revealed that models based on titles consistently yielded the poorest coherence scores.\nComparing text levels, abstract-based models generally demonstrated superior coherence to their full-text counterparts. When contrasting the modelling techniques, BERTopic typically outperformed Latent Dirichlet Allocation (LDA) in terms of coherence for both abstract and title inputs. However, this advantage for BERTopic tended to lessen as the specified number of topics for the models rose. Across all configurations, the BERTopic Abstract model clearly emerged as the top performer for this particular metric.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.11 Results: Quantitative Analysis – Topic Diversity",
    "text": "12.11 Results: Quantitative Analysis – Topic Diversity\n\n\n\nSlide 16\n\n\nThe analysis of topic diversity, which measures the extent to which topics are described by distinct sets of words, showed a general trend: diversity tended to decrease as the number of topics in the models increased. Models constructed from titles offered better diversity scores when compared to their abstract or full-text equivalents.\nRegarding the modelling techniques, BERTopic consistently achieved higher diversity scores than Latent Dirichlet Allocation (LDA). The BERTopic Title model emerged as the winner for this metric, although the BERTopic Full-text model followed very closely in performance.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.12 Results: Quantitative Analysis – Joint Recall",
    "text": "12.12 Results: Quantitative Analysis – Joint Recall\n\n\n\nSlide 17\n\n\nJoint recall, a metric that evaluates how effectively the top words of a topic collectively represent all documents classified within that topic, yielded further insights. Models based on titles demonstrated the poorest performance in terms of joint recall. Conversely, full-text models generally outperformed their abstract and title-based counterparts on this measure.\nWhen comparing the two primary modelling techniques, Latent Dirichlet Allocation (LDA) tended to achieve better joint recall than BERTopic. The LDA Full-text model and the BERTopic Full-text model emerged as the top performers for joint recall, with the BERTopic Abstract model also demonstrating strong results, following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.13 Results: Summary of Model Performance",
    "text": "12.13 Results: Summary of Model Performance\n\n\n\nSlide 17\n\n\nTo provide a consolidated view, researchers assembled the various results into a summary table. This table depicted the performance of each of the six models—LDA Full-text, LDA Abstract, LDA Title, BERTopic Full-text, BERTopic Abstract, and BERTopic Title—across several assessment categories: overall fit, top-words quality, coherence, diversity, and joint recall. Performance levels were visually represented using circles, where a fully black circle indicated the highest score and a white circle denoted the lowest.\nIt is crucial to recognise that these results do not point to a single, universally superior model. The optimal choice invariably depends on the specific research objectives and needs of the investigator. For instance, if the primary aim involves discovering major thematic trends, and the precise classification of every single document is not paramount, then metrics like poor recall or the presence of a large class of unassigned documents might not present critical drawbacks.\nConversely, if the objective demands that all identified topics comprehensively cover the maximum number of relevant documents, then certain models become less suitable. Specifically, researchers do not recommend BERTopic Full-text and BERTopic Title for such tasks, as they both tended to produce large groups of unclassified documents; BERTopic Title also suffered from poor recall. The LDA Title model is likewise not advised for this scenario, given its generally weak performance across almost all assessment criteria.\nIn light of these findings, the researchers generally recommend performing topic modelling on either abstracts or full-texts, using either LDA or BERTopic. This recommendation comes with the important proviso that the chosen combination does not lead to significant misclassification of documents pertinent to the topics of interest.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "title": "12  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "12.14 Discussion and Conclusion: Implications and Future Directions",
    "text": "12.14 Discussion and Conclusion: Implications and Future Directions\n\n\n\nSlide 17\n\n\nThe research culminated in several key findings and pointed towards future avenues of exploration. Firstly, title-based models generally exhibited poor performance. A plausible explanation for this lies in the inherent lack of informational content within titles alone, which can consequently lead to the misclassification of documents. Nevertheless, it is noteworthy that even the BERTopic Title model managed to identify a number of meaningful topics, suggesting that the utility of titles is not entirely negligible. This highlights a potential need to strike a balance between achieving well-defined topics and ensuring adequate document coverage for each topic.\nSecondly, full-text models presented their own set of challenges. With Latent Dirichlet Allocation (LDA) applied to full-texts, topics sometimes appeared more loosely defined and broader in their thematic scope. Furthermore, such models occasionally identified transverse topics—for instance, those related to methodology—which might be secondary to the primary research themes of interest. BERTopic, when applied to full-texts, sometimes produced topics that were overly narrow. This specificity could lead to poor document coverage and contribute to problems with class size, such as the emergence of extremely large, undifferentiated clusters of documents.\nThirdly, abstract-based models demonstrated commendable performance. The results derived from abstracts showed consistency between both LDA and BERTopic approaches. Moreover, these abstract models aligned well with the LDA full-text model, indicating that abstracts often provide a balanced and effective summary of information suitable for topic modelling.\nA fourth significant observation concerned the robustness of topics. Overall, the study found that very similar thematic structures emerged across the diverse range of models and text levels analysed. This consistency opens possibilities for employing meta-analytic techniques to pinpoint the most robust and consistently identified topics. Furthermore, the relative distances or similarities between models (such as those measured by the Adjusted Rand Index) could potentially be used to identify an optimal or most central model. In this particular study, the BERTopic Abstract model appeared to fulfil such a role, performing strongly across various metrics.\nLastly, the findings prompt consideration of new modelling approaches. Researchers hypothesise that it might be feasible to develop novel models, or refine existing ones, by explicitly leveraging the structural information inherent in scientific articles—that is, the distinct characteristics of full-texts, abstracts, and titles. Such an approach could potentially lead to the extraction of an even more meaningful and nuanced set of topics or defining top-words.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "13  Time-Aware Language Models",
    "section": "",
    "text": "Overview\nResearchers at the Max Planck Institute of Geoanthropology propose a novel architecture, termed the “Time Transformer”, designed to imbue Large Language Models (LLMs) with explicit time-awareness. This work addresses a critical limitation: current LLMs possess only an implicit, statistically derived understanding of time, hindering their capacity to accurately process temporally evolving information, particularly for historical analysis. The core innovation involves augmenting standard Transformer models by incorporating a dedicated temporal dimension into token embeddings, thereby explicitly encoding the utterance time for each token.\nA proof-of-concept study utilised a small generative LLM, trained on a corpus of Met Office weather reports spanning 2018 to 2024. For this experiment, the temporal dimension represented the min-max normalised day of the year. The Time Transformer demonstrated its capability to learn and reproduce both synthetically introduced temporal drifts in language patterns—such as synonym replacement and alterations in word co-occurrence—and naturally occurring seasonal variations within the weather data. Key components of this investigation comprised the baseline vanilla Transformer model, its Time Transformer modification, and the curated dataset of weather reports. Development and training relied on standard LLM frameworks and an HPC cluster equipped with NVIDIA A100 GPUs. Potential applications extend beyond historical analysis to include the creation of foundation models for diverse time-sensitive tasks, enabling interactions with specific temporal contexts, and possibly modelling other contextual metadata dimensions like geography or genre. Nevertheless, this approach necessitates training models from scratch, presents challenges in curating temporal metadata, and raises questions regarding the feasibility of fine-tuning existing models.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "href": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "title": "13  Time-Aware Language Models",
    "section": "13.1 The Temporal Deficit in Current Language Models",
    "text": "13.1 The Temporal Deficit in Current Language Models\n\n\n\nSlide 01\n\n\nJochen Büttner, from the Max Planck Institute of Geoanthropology, introduced a foundational concept aimed at enhancing language models. His presentation formalised an idea with potential applications in historical analysis (HPSS), though the speaker acknowledged its basic nature and solicited information regarding any pre-existing similar work.\nResearchers argued that current Large Language Models operate with merely an implicit comprehension of time, a comprehension statistically distilled from the vast quantities of text encountered during training. Whilst these models demonstrate a considerable, albeit indirect, grasp of temporal concepts, explicit time-awareness promises significant benefits, particularly for historical analysis and potentially broader applications. Consider, for instance, two statements: “The primary architectures for processing text through NNs are LSTMs,” accurate around 2017, and “The primary architectures for processing text through NNs are Transformers,” pertinent circa 2025. Humans effortlessly resolve the apparent contradiction by understanding the different temporal contexts. However, within an LLM’s training data, which lacks explicit temporal markers, these statements directly compete, compelling the model towards an unavoidable error in at least one instance.\nConsequently, during inference, an LLM prompted with “The primary architectures for processing text through NNs are” will likely predict “Transformers,” influenced by an inherent recency bias from its training. Eliciting an older truth, such as “LSTMs,” often necessitates careful prompt engineering—perhaps by adding “In 2017” or altering verb tenses—a process researchers describe as somewhat haphazard. The central objective, therefore, involves engineering explicitly time-aware LLMs, empowering them to learn and reproduce evolving patterns within training data as a direct function of time.\nFormally, standard LLMs estimate the probability of a subsequent token given a sequence of preceding tokens, denoted p(xn | x1, …, xn-1). In reality, this probability remains non-static; it dynamically changes with time, correctly represented as p(xn | x1, …, xn-1, t). For instance, the likelihood of “Transformers” completing the aforementioned sentence in 2017 was effectively zero. One can express the probability of an entire token sequence uttered at a specific time t as the product of these conditional probabilities: p(x1, x2, …, xn | t) = Πk=1 to n p(xk | x1, …, xk-1, t). Current models can only mirror temporal shifts in these underlying distributions through in-context learning during inference, a less direct mechanism.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "title": "13  Time-Aware Language Models",
    "section": "13.2 Architecting Time-Awareness: The Time Transformer",
    "text": "13.2 Architecting Time-Awareness: The Time Transformer\n\n\n\nSlide 13\n\n\nAddressing the challenge of modelling the time-dependent probability distribution p(xn | x1, …, xn-1, t) necessitated a novel approach. One existing strategy, time slicing, involves training distinct models for separate temporal segments, assuming distributions remain relatively static within each slice. However, this technique proves exceptionally data-inefficient.\nConsequently, researchers conceived the “Time Transformer”, an architecture distinguished by its elegant simplicity. Standard Natural Language Processing tasks commence by transforming words or tokens into vectorial representations—embeddings—which models refine during training. The Time Transformer innovates by appending an additional dimension to these latent semantic token features, specifically encoding the token’s origin time. Thus, every token in a sequence, uttered at a particular time, receives this explicit temporal information. For instance, the representation for “cat” would subtly differ in this dimension depending on whether it was uttered recently or several years prior.\nOne can formalise this time-aware embedding as E(x, t) = {e1(x), e2(x), …, ed-1(x), φ(t)}, where φ(t) represents the encoded time. The Transformer model then processes a sequence of these augmented embeddings, [E(x1, t), E(x2, t), …, E(xn-1, t)], to predict the time-conditioned probability pθ(xn | x1, …, xn-1, t). The training objective remains the minimisation of the negative log likelihood across the dataset: minθ - Σi=1 to N Σk=1 to n(i) log pθ(xk(i) | x1(i), …, xk-1(i), t(i)). Through this mechanism, temporal information directly ‘injects’ into every token’s representation, enabling the model to learn precisely how significantly the time dimension influences each individual token.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "href": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "title": "13  Time-Aware Language Models",
    "section": "13.3 Empirical Validation: Experimental Design and Implementation",
    "text": "13.3 Empirical Validation: Experimental Design and Implementation\n\n\n\nSlide 10\n\n\nTo validate the Time Transformer concept, researchers required a dataset characterised by a limited vocabulary and simple, repetitive language, thereby facilitating the training of a small generative LLM. Met Office weather reports from the UK’s National Meteorological Service, accessible via their digital archive (https://digital.nmla.metoffice.gov.uk/), fulfilled these criteria admirably. Researchers scraped data spanning 2018 to 2024, yielding approximately 2,500 reports, each comprising around 150-200 words. They also noted an alternative dataset, TinyStories. Preprocessing involved extracting daily reports from monthly PDFs and applying a straightforward tokenisation strategy: no sub-word units, and a disregard for case and interpunctuation. This yielded a modest vocabulary of 3,395 unique words.\nResearchers first constructed a baseline ‘vanilla’ Transformer model. This decoder-only architecture comprised an embedding layer, positional encoding, and dropout, followed by four decoder blocks—each containing multi-head attention (with eight heads), residual connections with layer normalisation, and a feed-forward network—culminating in a final dense layer for output probability distribution. This relatively small model, with 39 million parameters (150MB), contrasts sharply with models such as GPT-4 (1.8 trillion parameters). Training occurred on an HPC cluster in Munich, utilising two NVIDIA A100 GPUs, achieving a rapid 11 seconds per epoch owing to the dataset’s and model’s compactness. The associated code is available on GitHub (j-buettner/time_transformer), though primarily serving as a learning tool. This vanilla model demonstrated proficiency in replicating the language of the weather reports.\nTransitioning to the Time Transformer involved a minimal architectural adjustment. Instead of a standard embedding, researchers incorporated time data by reserving one dimension within the, for example, 512-dimensional latent semantic space for a temporal signal. They concatenated this time value with the token’s semantic embedding before positional encoding. Specifically, a non-trainable, min-max normalised day of the year (calculated as (day of year - 1) / 364) served as the time embedding, a choice made to exploit natural seasonal variations in weather patterns. Researchers acknowledged that alternative methods for encoding time could also be employed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "href": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "title": "13  Time-Aware Language Models",
    "section": "13.4 Learning Temporal Dynamics: Experimental Outcomes",
    "text": "13.4 Learning Temporal Dynamics: Experimental Outcomes\n\n\n\nSlide 16\n\n\nThe primary inquiry guiding these experiments sought to determine whether the Time Transformer could efficiently learn temporal drift within the underlying data distribution. A first experiment, termed “synonymic succession,” involved injecting a synthetic temporal drift. Researchers implemented a time-dependent replacement of the word “rain” with “liquid sunshine,” where the probability of replacement followed a sigmoid function across the days of the year—commencing at zero and culminating at one by year’s end. By generating a weather prediction for each day and analysing the monthly frequencies of these terms, they found the model accurately reproduced this injected pattern: “rain” predominated early in the year, whilst “liquid sunshine” emerged towards the end, with a clear mid-year transition, all subject to expected statistical fluctuations.\nBeyond synthetic changes, the model also captured naturally occurring seasonal patterns, such as the increased frequency of terms like “snow” and “sleet” in winter months, and “hot” or “warm” in summer. However, researchers viewed these as simpler instances of temporal influence, primarily affecting word frequencies. To explore a more complex scenario, a second experiment focused on altering a co-occurrence pattern, which they described as the “fixation of a collocation.” Here, they synthetically replaced instances of “rain” not immediately followed by “and” with “rain and snow” in a time-dependent manner. This aimed to render “rain and snow” an obligatory pairing by the year’s end, akin to how “bread and butter” functions as a fixed phrase. Again, analysis of daily predictions across the year confirmed the model’s success: towards the year’s end, predictions almost exclusively featured “rain and snow,” whilst earlier in the year, “rain” could appear alone—though “rain and snow” also occurred, reflecting genuine meteorological conditions for periods like January.\nInvestigations into the model’s internal workings, specifically its attention mechanisms (using techniques alluded to as ‘excite’), revealed that certain attention heads had specialised in capturing these temporal dependencies. For instance, the attention paid from “snow” back to “rain” (when generating “rain and snow”) varied appropriately with the time of year. Furthermore, early-year co-occurrences of “rain and snow” often correctly conditioned on contextual cues like “cold system,” underscoring the model’s ability to learn nuanced patterns. These findings collectively provided a proof of concept: Transformer-based LLMs can indeed be rendered efficiently time-aware through the simple addition of a temporal dimension to their token embeddings.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "href": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "title": "13  Time-Aware Language Models",
    "section": "13.5 Broader Implications and Future Trajectories",
    "text": "13.5 Broader Implications and Future Trajectories\n\n\n\nSlide 21\n\n\nThe successful proof of concept for the Time Transformer opens several avenues for application and further research. A foundational Time Transformer could provide a robust basis for numerous downstream tasks reliant on historical data. Furthermore, an instruction-tuned version might enable users to interact with information as it existed at a specific point in time, potentially even enhancing present-focused interactions by providing a richer temporal context. This architectural principle could, moreover, extend to model dependencies on other metadata dimensions, such as geographical origin or textual genre.\nRegarding future work, researchers identified several promising directions. Benchmarking the Time Transformer against approaches that treat time as an explicit token within the input sequence would prove valuable. Another important investigation involves testing whether the inclusion of an explicit temporal dimension enhances training efficiency; the hypothesis posits that it could aid the model in more readily deciphering complex temporal patterns that are otherwise only implicitly cued.\nNevertheless, translating this concept into widespread practical application faces notable challenges. The architectural modification—the addition of a temporal dimension to embeddings—raises questions about the feasibility and efficiency of fine-tuning existing pre-trained models; indeed, it may necessitate training new models from scratch. This, in turn, implies significant computational costs for any application beyond the small-scale demonstration. A crucial shift from current practices involves the loss of metadata-free self-supervised learning; the Time Transformer requires meticulous data curation to assign a temporal marker to every token sequence. For historians, accurately determining the ‘generation date’ of textual material can prove complex, involving considerations of original utterance, reprints, and publication lags.\nAs a concluding reflection, the presenter suggested that a more modest, targeted encoder model, akin to BERT, built upon the same time-aware principle, might offer a pragmatic path for specific tasks that do not require full generative capabilities. Such a model could focus on learning relevant temporal patterns without the overhead of modelling all linguistic intricacies. Collaboration on exploring these targeted applications is welcomed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#overview",
    "href": "chapter_ai-nepi_018.html#overview",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Researchers presented a two-part investigation into leveraging Large Language Models (LLMs) for enhancing metadata and conducting diachronic analyses of chemical knowledge within historical scientific texts. The first part detailed the application of LLMs to improve metadata for a diachronic corpus, specifically focusing on categorising articles by scientific discipline, assigning semantic tags (topics), and generating abstractive summaries. The second part presented a case study analysing the evolution of the chemical space over time across different disciplines, aiming to identify periods of heightened interdisciplinarity and knowledge transfer. This work utilised the Philosophical Transactions of the Royal Society of London, a corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and almost 300 million tokens. For metadata enrichment, the team employed Llama 3, particularly the Hermes-2-Pro-Llama-3-8B model, fine-tuned for structured output. For the diachronic analysis of chemical terms, ChemDataExtractor, a Python module, identified chemical substances, and Kullback-Leibler Divergence (KLD) measured changes in their usage over time, both within and between disciplines such as chemistry, biology, and physics. Key findings include the successful application of LLMs for article categorisation, the identification of distinct evolutionary patterns in chemical terminology across disciplines, and the detection of knowledge transfer instances between scientific fields.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "href": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.1 Project Objectives and Structure",
    "text": "14.1 Project Objectives and Structure\n\n\n\nSlide 01\n\n\n    Researchers outlined a project, presented by Diego Alves and Sergey and developed in collaboration with LLM expert Badr Abdullah, titled \"Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.\" The investigation divides into two distinct parts. Part one explores the application of Large Language Models (LLMs) to enhance the metadata associated with historical texts, particularly within a diachronic corpus. This enhancement focuses on categorising articles by scientific discipline, assigning relevant semantic tags or topics, and creating abstractive summaries. Subsequently, part two delves into a case study analysing the evolution of the chemical space across different disciplines over time. This analysis specifically seeks to identify periods marked by significant interdisciplinarity and instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "href": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.2 Data Source: The Philosophical Transactions of the Royal Society",
    "text": "14.2 Data Source: The Philosophical Transactions of the Royal Society\n\n\n\nSlide 01\n\n\n    The project investigates how scientific English evolved over time to become an optimised medium for expert-to-expert communication, concurrently analysing phenomena such as knowledge transfer and the identification of influential papers and authors. For this purpose, researchers utilised the Philosophical Transactions of the Royal Society of London. First published in 1665, this journal holds the distinction of being the oldest scientific journal in continuous publication. It played a pivotal role in the development of scientific communication, notably establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge, and it continues to be a highly respected publication today.\n\n    Throughout its history, the journal has featured numerous influential contributions. For instance, in the 17th century, Isaac Newton published his \"New Theory about Light and Colours\" (1672). The 18th century saw Benjamin Franklin's account of \"The 'Philadelphia Experiment'\" concerning the electrical kite. Later, in the 19th century, James Clerk Maxwell detailed his \"Dynamical Theory of the Electromagnetic Field\" (1865). Beyond these seminal works, the corpus also contains more curious papers, such as Monsieur Autour's speculations on the inhabitants of the Earth and Moon. However, the current research refrains from fact-checking or assessing the scientific validity of these historical papers, focusing instead on other analytical dimensions.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "href": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata",
    "text": "14.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata\n\n\n\nSlide 08\n\n\n    Researchers employed the Royal Society Corpus (RSC) version 6.0 full for their analysis. This extensive collection covers over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 individual texts and amounting to almost 300 million tokens. The corpus already possesses some encoded metadata, including author names, century, year, and volume information.\n\n    A previous study attempted to define research disciplines and classify papers within this corpus using Latent Dirichlet Allocation (LDA) topic modelling. The output from this LDA analysis, however, revealed a mixture of disciplines, sub-disciplines, and even types of texts, such as \"Observation\" and \"Reporting,\" rather than purely thematic categories. A visual representation of this earlier classification showed a hierarchical structure with categories like \"LifeScience1,\" \"LifeScience2,\" alongside \"Chemistry\" and \"Physics,\" highlighting the need for a more refined categorisation approach.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation",
    "text": "14.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation\n\n\n\nSlide 10\n\n\n    The first part of the project focused on employing Large Language Models (LLMs) to enhance existing metadata and generate new metadata types for the historical texts. Researchers aimed to leverage LLMs for several information management and knowledge organisation tasks. These included text clean-up, summarisation, improved information extraction, categorisation, and enhanced access and retrieval capabilities, with the potential to feed structured information into knowledge graphs.\n\n    To illustrate the process, an example article titled \"A Spot in one of the Belts of Jupiter\" was considered. Such historical texts often present syntactic complexities characteristic of older writing styles. For this article, the LLM was tasked with providing a hierarchical categorisation (e.g., Discipline: Astronomy, Sub-discipline: Planetary Science), a list of relevant index terms (e.g., Astronomy, Jupiter, Telescopes), and a concise \"Too Long; Didn't Read\" (TL;DR) summary. An example summary provided was: \"The author reports observing a spot in one of Jupiter's belts using a 12-foot telescope. The spot was found to move from east to west within two hours, indicating movement on the planet's surface.\"",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "href": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.5 LLM Selection and Prompt Engineering for Metadata Generation",
    "text": "14.5 LLM Selection and Prompt Engineering for Metadata Generation\n\n\n\nSlide 13\n\n\n    Researchers selected Llama 3 for their metadata enrichment tasks, a new release in the Llama LLM family offering 8 billion (8B) and 70 billion (70B) parameter versions, with a 400B parameter model currently in training. This model, accessible via Hugging Face, reportedly offers significant improvements over predecessors like Mistral and Llama 2. The team specifically utilised instruction-tuned versions, deemed suitable for their objectives, and employed Hermes-2-Pro-Llama-3-8B, a variant fine-tuned for generating structured output, particularly JSON and YAML.\n\n    A detailed system prompt guided the LLM. The prompt first established a role: \"Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.\" It then defined the objective: to read, analyse, and organise the corpus to create a structured database facilitating research. The input was described as OCR-extracted text snippets with existing metadata.\n\n    Four main tasks were specified. Task A involved reading and analysing the article to suggest an alternative, more content-reflective title. Task B required writing a concise 3-4 sentence TL;DR summary in simple language, suitable for a high school student. Task C mandated the identification of exactly five main topics, conceptualised as Wikipedia-style keywords for scientific sub-fields. Finally, Task D involved identifying a primary scientific discipline from a predefined list of nine (Physics, Chemistry, Environmental  Earth Sciences, Astronomy, Biology  Life Sciences, Medicine  Health Sciences, Mathematics  Statistics, Engineering  Technology, Social Sciences  Humanities) and a corresponding second-level sub-discipline, which the LLM could freely define but could not be one of the primary disciplines.\n\n    An example input provided to the LLM included metadata for Isaac Newton's 1672 letter, which has a very long original title, along with a text snippet. The desired output was specified in YAML format, exemplified by a revised title (\"A New Theory of Light and Colours\"), five topics (e.g., \"Optics,\" \"Refraction\"), a TL;DR summary, and the categorisation (\"Physics,\" \"Optics  Light\"). The prompt concluded by strictly requiring valid YAML output.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "href": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.6 LLM Output Validation and Discipline Distribution Analysis",
    "text": "14.6 LLM Output Validation and Discipline Distribution Analysis\n\n\n\nSlide 15\n\n\n    Initial sanity checks on the LLM's output revealed promising results. An impressive 99.81% of the generated outputs (17,486 out of 17,520) conformed to a valid YAML structure, with only a minor 0.19% failing this validation. Regarding the prediction of scientific disciplines, 94% fell within the predefined set of nine categories. However, some hallucinations and errors occurred; for instance, the LLM occasionally produced minor variations like \"Earth Sciences\" instead of the specified \"Environmental  Earth Sciences,\" or invented entirely novel categories such as \"Music.\" In other cases, it mistakenly included the numerical index of a discipline as part of its name or classified sub-disciplines like \"Neurology\" or \"Zoology\" as primary disciplines. Despite these anomalies, the researchers concluded that the majority of papers received correct discipline assignments.\n\n    An analysis of the distribution of scientific disciplines over time, based on the LLM's categorisation, revealed distinct trends. Up until the end of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. A notable peak in chemical articles emerged in the late 18th century, coinciding with the chemical revolution, after which chemistry established itself as a principal pillar of the Royal Society's publications. Progressing into the 19th and 20th centuries, three main pillars—Biology, Physics, and Chemistry—became dominant.\n\n    Furthermore, researchers visualised the TL;DR summaries using t-SNE projections. This technique demonstrated how different disciplines clustered in the semantic space. Chemistry appeared centrally, with significant overlap with Physics and Biology. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters. This static visualisation hinted at the potential for diachronic analysis, enabling the observation of shifts and evolving overlaps between disciplines over time. These LLM-derived discipline categorisations subsequently informed the diachronic analysis of the chemical space.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "href": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.7 Part II: Diachronic Analysis of the Chemical Space Methodology",
    "text": "14.7 Part II: Diachronic Analysis of the Chemical Space Methodology\n\n\n\nSlide 13\n\n\n    The second part of the research concentrated on a diachronic analysis of the chemical space, focusing on three disciplines most prevalent in the corpus: Chemistry, Biology, and Physics. To achieve this, researchers first needed to extract chemical terms from the texts. They employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initially, applying ChemDataExtractor to the full text of articles produced a significant amount of noise. Consequently, a refined two-pass approach was adopted: the tool was first run on the texts, and then re-applied to the list of substances generated in the initial pass, which effectively reduced the noisy output.\n\n    For analysing the evolution of this chemical space, the team utilised Kullback-Leibler Divergence (KLD). KLD, or relative entropy, serves to detect changes across different situational contexts. It quantifies the number of additional bits required to encode a given dataset (A) when using a non-optimal model based on a different dataset (B). Higher KLD values indicate greater dissimilarity between the datasets, whilst lower values suggest similarity.\n\n    Researchers applied KLD in two distinct ways. Firstly, they conducted an intra-discipline diachronic analysis by comparing a future period (dataset A) with a past period (dataset B) within each of the three disciplines (Chemistry, Physics, and Biology) independently. This involved a sliding window technique: for a given central year, chemical term frequencies from a 20-year window preceding it were compared against those from a 20-year window succeeding it. The central year was then advanced by five years, and the comparison repeated, allowing them to trace the evolution of the chemical space within each discipline over the entire timeline. Secondly, they performed an inter-discipline comparative analysis, making pairwise comparisons of the chemical space in Chemistry versus Physics, and Chemistry versus Biology, based on 50-year segments of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.8 KLD Analysis Results: Intra-Disciplinary Evolution",
    "text": "14.8 KLD Analysis Results: Intra-Disciplinary Evolution\n\n\n\nSlide 21\n\n\n    The Kullback-Leibler Divergence (KLD) analysis per discipline revealed that the evolutionary trend of the chemical space was quite similar across Chemistry, Biology, and Physics. Peaks and troughs in KLD values, indicating periods of significant change or stability respectively, occurred at roughly the same times for all three fields. Notably, towards the end of the analysed timeline, the KLD plots tended to flatten, and the overall KLD values decreased, suggesting less variation in chemical terminology between successive future and past periods.\n\n    Researchers then focused on a prominent KLD peak observed in the late 18th century (approximately 1740-1810). The KLD methodology permitted a closer examination of the specific chemical substances contributing most to this divergence. During the sub-period of 1776-1816, in both Biology and Physics, one or two particular elements exhibited extremely high KLD values, indicating they were primary drivers of change in the chemical lexicon of those fields. Despite these drivers, the analysis showed that largely the same set of chemical elements featured prominently across Chemistry, Biology, and Physics during this era.\n\n    This pattern contrasted significantly with observations from a later period, specifically the second half of the 19th century (approximately 1850-1900). Here, the KLD graphs for Biology and Physics were much more populated with influential chemical substances, and the individual contributions of these elements to the overall KLD were more uniform. A differentiation in the types of substances also became apparent: Biology's chemical lexicon began to evolve distinctly towards terms associated with biochemistry. Simultaneously, Chemistry and Physics showed an increasing focus on noble gases and radioactive elements, many of which were discovered towards the end of the 19th century.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer",
    "text": "14.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer\n\n\n\nSlide 24\n\n\n    Pairwise comparisons using Kullback-Leibler Divergence (KLD) further illuminated the distinct chemical focuses of the disciplines, particularly evident in word clouds representing the second half of the 20th century. When comparing Chemistry and Biology, the word cloud for Biology featured a greater prominence of substances related to biochemical processes within living organisms. In contrast, Chemistry's word cloud highlighted substances associated with organic chemistry, such as hydrocarbons and benzene. A comparison between Chemistry and Physics revealed that Physics's chemical lexicon was characterized by a higher frequency of metals—including rare earth metals, semi-metals, and radioactive metals—alongside noble gases. These interdisciplinary comparisons effectively identified thematic divergences in chemical terminology.\n\n    Crucially, this pairwise KLD analysis also enabled the detection of instances termed \"knowledge transfer.\" This phenomenon describes situations where a chemical element, initially ranked as highly distinctive of Chemistry in an earlier period, subsequently becomes more characteristic of either Biology or Physics in a later period. For example, when comparing Chemistry and Physics, the element tin was found to be distinctive of Chemistry during the first half of the 18th century but shifted to become more distinctive of Physics in the second half of that century. Similar shifts for other elements were observed in the early 20th century. A comparable pattern emerged in the Chemistry versus Biology comparison during the 20th century, where elements becoming distinctive of Biology were, once again, frequently related to biochemical processes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "title": "14  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "14.10 Conclusion and Future Work",
    "text": "14.10 Conclusion and Future Work\n\n\n\nSlide 21\n\n\n    In conclusion, the researchers successfully employed a Large Language Model (LLM) to improve article categorisation and topic modelling for texts within their historical corpus. Building upon these LLM-generated classifications, they conducted a diachronic analysis of the chemical space across three key disciplines—Chemistry, Biology, and Physics—and performed interdisciplinary comparisons of this chemical space.\n\n    Looking ahead, several avenues for future work present themselves. For the LLM-focused component (Part I), plans include testing alternative LLMs and undertaking a formal evaluation of the results obtained from the Llama 3 model. Regarding the analysis of the chemical space (Part II), researchers intend to pursue a more fine-grained interdisciplinary analysis, potentially by experimenting with different sizes for diachronic sliding windows and varying comparison timeframes. Furthermore, they aim to expand the analysis to include additional disciplines, such as comparing Chemistry with Medicine, and to explore tracing the evolution of the chemical space using the concept of surprisal.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers are exploring the computational analysis of semantic change, focusing on the intricate modelling of diverse contextual factors and their dynamic interplay. This investigation forms an integral part of the Cascade project, a prestigious Marie Curie doctoral network, with significant contributions from PhD student Sofía Aguilar. Previous work meticulously modelled different context types in isolation; the current objective, however, seeks to synthesise these approaches to illuminate their complex interactions.\nThe chemical revolution, specifically the profound conceptual shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a compelling pilot study. Linguists involved in this endeavour examine how language adapts to real-world transformations, drawing upon established register theory and principles of rational communication. The study aims to detect periods of linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately uncover the linguistic mechanisms and communicative drivers underpinning these transformations. A novel framework, employing Graph Convolutional Networks (GCNs), is proposed to model language dynamics by treating context as a central signal, thereby aiming to overcome limitations of existing methods in capturing the nuanced interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot",
    "text": "15.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot\n\n\n\nSlide 01\n\n\nWithin the Cascade project, a Marie Curie doctoral network, researchers are rigorously investigating the computational analysis of semantic change. PhD student Sofía Aguilar spearheads efforts to model context comprehensively, meticulously examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more holistic understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing extensively upon the Royal Society Corpus (RSC). This pivotal historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation richly documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the iconic painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors: situational (where), temporal (when), experiential (what), interpersonal (who), textual (how), and causal (why).\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers inherent variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.2 Detecting Temporal Shifts through Kullback-Leibler Divergence",
    "text": "15.2 Detecting Temporal Shifts through Kullback-Leibler Divergence\n\n\n\nSlide 04\n\n\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal substantially higher divergence due to profound linguistic evolution.\nResearchers Degaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. This technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time (e.g., from 1725 to 1845) reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 precisely coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can meticulously track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "href": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.3 Paradigmatic Change and Scientific Influence: Cascade Models",
    "text": "15.3 Paradigmatic Change and Scientific Influence: Cascade Models\n\n\n\nSlide 08\n\n\nBeyond temporal detection, this investigation delves into paradigmatic context and the dynamics of conceptual change, referencing seminal work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand precisely who spearheads and propagates these linguistic and conceptual shifts, researchers Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that elegantly illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "href": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal",
    "text": "15.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal\n\n\n\nSlide 10\n\n\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with valuable contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is directly proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, researchers meticulously examine shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, vividly visualised through graphs plotting surprisal against year, robustly supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community utilising that form.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics",
    "text": "15.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics\n\n\n\nSlide 12\n\n\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the profound understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. The proposed framework positions context as a central signal for modelling language dynamics, with Graph Convolutional Networks (GCNs) identified as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four distinct stages:\n\nData Sampling: This initial stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: Researchers begin by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, network size is refined using community detection algorithms, such as that proposed by Riolo and Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering profound insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "title": "15  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "15.6 Reflections: Limitations and Future Directions",
    "text": "15.6 Reflections: Limitations and Future Directions\n\n\n\nSlide 16\n\n\nThis research acknowledges several profound questions that delineate its current limitations and chart compelling future directions. A primary concern involves the very nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context truly integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "",
    "text": "Overview\nResearchers meticulously investigate the processes and impacts of science funding, moving beyond conventional bibliometric analyses of publications and grants. This inquiry focuses upon the National Human Genome Research Institute (NHGRI), renowned for its pioneering contributions to biology, particularly through the Human Genome Project. An interdisciplinary team, encompassing historians, physicists, ethicists, computer scientists, and former NHGRI leadership, analyses an extensive internal archive. This repository contains over two million pages of documents from the NHGRI, including meeting notes, handwritten correspondence, presentations, spreadsheets, and emails. Growing by 5% annually through continuous digitisation, this archive offers unparalleled insight into the internal workings of a major funding agency.\nMethodologically, the project employs advanced computational techniques to process and analyse these born-physical and born-digital artefacts. These techniques include developing a custom-built handwriting removal model using a U-Net architecture. This process enhances the accuracy of Optical Character Recognition (OCR) for printed text whilst enabling a distinct processing pipeline solely for handwriting recognition. Multimodal models, harnessing visual, textual, and structural modalities, facilitate sophisticated tasks such as entity extraction and synthetic data generation to train classifiers. Robust systems for entity and Personally Identifiable Information (PII) recognition meticulously mask and disambiguate sensitive data, including names, organisations, email addresses, locations, and identification numbers, achieving high F1 scores with relatively small fine-tuned datasets.\nA pivotal aspect of the research involved reconstructing and analysing the NHGRI’s internal and external email correspondence network. By extracting entities from thousands of scanned paper emails and linking them, researchers successfully recreated an intricate web of communication from the Human Genome Project era. This reconstructed network encompasses 62,511 distinct email conversations originating from 5,414 individual scanned emails. Network analysis, including stochastic block modelling and brokerage role analysis, uncovered leadership structures, such as an informal ‘Kitchen Cabinet’ within the International HapMap Project. It also revealed patterns of information flow, suggesting NHGRI leadership frequently adopted a consultative, rather than gatekeeping, posture.\nFurthermore, the research explored funding decisions by computationally modelling the factors influencing organism selection for genome sequencing post-Human Genome Project. This model integrated biological features (e.g., genome size, distance to model organisms), project characteristics (e.g., team size, submission history, gender equity, internal proposal origin), applicant reputational metrics (e.g., H-index, community size, network centrality), and linguistic characteristics of proposals (e.g., argumentation, repetitiveness). The findings suggest that all these feature categories collectively predicted funding success, with reputational factors like H-index and community size demonstrating a Matthew Effect.\nThe project’s broader objective centres on rendering such born-physical archives Findable, Accessible, Interoperable, and Reusable (FAIR). This involves applying these methodologies to other extensive archives, including federal court records and seismological data. Crucially, this endeavour underscores the imperative of preserving these rich historical data sources and developing sophisticated tools to extract knowledge, inform policy, enhance data accessibility, and address pressing scientific questions. The consortium actively seeks testers, partners, and users for its ‘Born Physical, Studied Digitally’ initiative, which receives support from the NIH, NVIDIA, and the NSF.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "href": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.1 Limitations of Current Science Funding Analysis",
    "text": "16.1 Limitations of Current Science Funding Analysis\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds support research endeavours, expecting them to yield societal benefits such as informed policy, clinical advancements, and new technologies. Scholars in the science of science predominantly study this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Indeed, analyses of these sources, including bibliometrics, have offered valuable insights into diverse aspects of science, including its long-term impact, the evolution of team sizes, the emergence of interdisciplinary fields, and the career trajectories of scientists.\nNevertheless, relying solely on the scientific article presents an incomplete, even skewed, representation of the complex scientific process. To assume that bibliometrics fully encapsulates the essence of science constitutes an oversimplification. A deeper understanding necessitates investigating the processes that precede publication, moving beyond the flawed picture painted by articles alone. Such an approach could illuminate critical questions: Does scientific inquiry shape funding priorities, or do funding agendas dictate the direction of science? Within the innovation pipeline, from initial ideation to eventual long-term impact, where do innovations flourish, diffuse across domains, or ultimately falter?\nNotably, the focus on published articles means that failed projects, which could offer significant learning opportunities, often remain unexamined. Beyond direct financial support, funders may also contribute through public data provision, community engagement initiatives, technology development, and cooperative agreements. All these factors influence both the creation of knowledge and the scholars involved. A dynamic interplay also exists between grant funding and technological development.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.2 The Human Genome Project: A Paradigm of ‘Big Science’",
    "text": "16.2 The Human Genome Project: A Paradigm of ‘Big Science’\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a landmark example of ‘big science’ in biology, analogous to large-scale projects in fields like particle physics. This colossal undertaking brought together tens of countries and thousands of researchers with the shared goal of sequencing the entire human genome. Its significance extends across multiple dimensions. Firstly, the HGP captured public interest to an extent previously unseen for a biological research programme, shifting focus from laboratory-based studies of organisms like Drosophila and C. elegans to a grand human-centric endeavour. Secondly, its impact resonates profoundly today; a vast majority of modern biological research, particularly omics methodologies, would be unfeasible without the reference genome it produced. Indeed, the HGP effectively established genomics as a distinct scientific discipline.\nFurthermore, the project pioneered new data-sharing practices, now widely adopted, and forged a powerful synergy between computational science and biology. Two principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, crucially for this study, the National Human Genome Research Institute (NHGRI), which served as the HGP division within the US National Institutes of Health (NIH). Dr Francis Collins, then Director of the NHGRI and later Director of the NIH, played a pivotal leadership role.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "href": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.3 NHGRI: An Innovative Funding Agency",
    "text": "16.3 NHGRI: An Innovative Funding Agency\n\n\n\nSlide 04\n\n\nAnalysis reveals the National Human Genome Research Institute (NHGRI) as a particularly innovative funding body within the National Institutes of Health (NIH). Several bibliometric indicators support this assessment when comparing NHGRI to other NIH institutes. For instance, NHGRI-funded research accounts for a larger share of manuscripts in the top 5% most cited publications. Moreover, its output demonstrates substantial long-term citation impact (measured after ten years) and generates significant citations from patents, indicating translation into clinical applications. The research funded by NHGRI also exhibits high ‘disruption’ scores, suggesting it often pioneers new directions.\nWhilst these metrics establish NHGRI’s innovative capacity, the underlying reasons for this success remain less understood. Consequently, an interdisciplinary research team has assembled to investigate the processes and practices that enable NHGRI to lead innovation. This team comprises experts from diverse fields, including history, physics, ethics, and computer science, and notably includes Dr Francis Collins, a former Director of both NHGRI and NIH. Their collective aim is to unravel the factors contributing to the rise of genomics, identify potential failure points and innovation spillovers, and understand the collaborative mechanisms between funding agencies and academic scientists that foster scientific advancement.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.4 The NHGRI Archive: A Rich and Complex Source",
    "text": "16.4 The NHGRI Archive: A Rich and Complex Source\n\n\n\nSlide 07\n\n\nOwing to the recognised historical importance of the Human Genome Project, the NHGRI meticulously preserved a substantial collection of its internal documentation, spanning from the 1980s and 1990s into subsequent years. This internal archive constitutes a rich repository, containing diverse materials such as the daily meeting notes of scientists coordinating the genome project, handwritten annotations from correspondence, conference agendas, formal presentations, detailed spreadsheets, and newspaper clippings chronicling the period. Additionally, it includes various internal forms, research proposals, and extensive email communications.\nThe sheer volume of this collection, currently exceeding two million pages and expanding by approximately 5% each year due to continuous digitisation, presents a formidable challenge. Effectively analysing such a vast and complex born-physical and born-digital artefact at scale necessitates innovative approaches, forming the core of the research team’s methodological development.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "href": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.5 Internal Archives Versus Public Data",
    "text": "16.5 Internal Archives Versus Public Data\n\n\n\nSlide 09\n\n\nThe internal documents housed within the NHGRI archive possess characteristics and content fundamentally distinct from publicly accessible information, such as Requests for Applications (RFAs) and peer-reviewed publications. Whilst scholars can readily access RFAs and publications through databases like PubMed or NIH RePORTER, the internal records offer a different perspective. Visualisations, such as t-SNE plots, demonstrate that these internal materials form distinct clusters, separate from the clusters representing public RFAs and publications.\nThese internal records provide detailed accounts of numerous large-scale genomic projects initiated and funded by NHGRI. Examples include the Ethical, Legal, and Social Implications (ELSI) Program’s LSAC, modENCODE, eMERGE, ENCODE, the foundational Human Genome Project itself, PAGE, the International HapMap Project, H3Africa, and the NHGRI-EBI GWAS Catalog. Many of these initiatives represented substantial investments, often amounting to tens or hundreds of millions of dollars, and mobilised thousands of researchers globally. Their collective purpose was to develop crucial resources for the genomics community, thereby catalysing the advancement of the entire field.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.6 Computational Methodologies for Archive Processing",
    "text": "16.6 Computational Methodologies for Archive Processing\n\n\n\nSlide 10\n\n\nResearchers developed specialised computational methods to manage the born-physical archive, a significant portion of which contains handwritten material. The use of AI for handwriting analysis presents not only technical hurdles but also ethical considerations regarding the unknown nature of handwritten content. To address this, the team trained a custom-built handwriting model, employing a U-Net architecture, specifically to identify and remove handwritten portions from documents. This process offers a dual benefit: it enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text and simultaneously enables the creation of a distinct processing pipeline dedicated solely to handwriting recognition.\nBeyond handwriting, the project leverages advances in multimodal models, drawing from the document intelligence research community. These models ingeniously combine visual information (the document image), textual content, and layout structure. Such an integrated approach supports diverse tasks, including sophisticated entity extraction. Furthermore, it facilitates the generation of synthetic documents, which serve as valuable training data for developing and refining new classification algorithms, thereby improving the system’s analytical capabilities over time.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "href": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.7 Entity and PII Recognition",
    "text": "16.7 Entity and PII Recognition\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous handling of sensitive information. The documents contain genuine Personally Identifiable Information (PII), including details of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles today. Consequently, the researchers implemented robust methods for entity and PII recognition. Their system effectively identifies, masks, and disambiguates such sensitive data throughout the vast collection.\nThe performance of these recognition models proves strong, as evidenced by F1 scores that improve with increased fine-tuning data for various entity types. These include persons (PERSON), organisations (ORG), email addresses (EMAIL), locations (LOC), and identification numbers (IDNUM). This careful approach ensures privacy and ethical compliance whilst enabling scholarly analysis of the archive’s content.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "href": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.8 Reconstructing the NHGRI Correspondence Network",
    "text": "16.8 Reconstructing the NHGRI Correspondence Network\n\n\n\nSlide 13\n\n\nResearchers conducted a detailed case study involving the reconstruction of email networks from the NHGRI archive. By extracting entities from thousands of scanned paper copies of emails and linking them, they successfully recreated the intricate web of correspondence that occurred during the Human Genome Project era. This reconstructed network encompasses 62,511 distinct email conversations originating from 5,414 individual scanned emails. Each node in the visualised network typically represents an individual involved in these communications, with affiliations to NIH (National Institutes of Health) or external entities (such as other funding agencies, companies, or universities) clearly delineated.\nApplying network analysis techniques, such as stochastic block modelling for community detection, yielded significant insights, particularly concerning the coordination of large-scale initiatives like the International HapMap Project. The HapMap Project, a major genomics endeavour following the HGP, focused on human genetic variation and laid the groundwork for genome-wide association studies (GWAS). Managing such a complex project, involving numerous universities and agencies, relied on formal structures like a steering committee. However, the computational analysis, performed in an unsupervised manner, revealed a hitherto undocumented informal leadership circle, dubbed the ‘Kitchen Cabinet’. This group, referencing a term from the Nixonian political era, apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "href": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.9 Information Brokerage and Leadership Dynamics",
    "text": "16.9 Information Brokerage and Leadership Dynamics\n\n\n\nSlide 16\n\n\nFurther investigation into the NHGRI’s operational dynamics focused on comparing the communication patterns of the informal ‘Kitchen Cabinet’ with those of the formal Steering Committee. Utilising brokerage role analysis, which assesses how individuals or groups mediate information flow within a network, researchers identified distinct behavioural patterns. This analysis categorises nodes based on their interaction styles, such as ‘consultant’ (receiving information and disseminating it back within their own group) or ‘gatekeeper’ (receiving information but not sharing it back with the originating group).\nThe findings indicate that the ‘Kitchen Cabinet’ primarily functioned in a consultant capacity, a pattern that distinguished it from other formal leadership structures active at the time. Notably, individuals like Francis Collins appeared to play significant consultant roles within this informal group. This suggests a leadership style at NHGRI that favoured consultation and open information exchange rather than restrictive gatekeeping, potentially contributing to the agency’s collaborative successes.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.10 Modelling Funding Decisions for Genome Sequencing",
    "text": "16.10 Modelling Funding Decisions for Genome Sequencing\n\n\n\nSlide 13\n\n\nThe research extended to analysing the funding agency’s decision-making processes through portfolio analysis, specifically modelling the choices made when selecting non-human organisms for genome sequencing following the completion of the Human Genome Project. Funding agencies like NHGRI faced complex decisions in allocating resources amongst numerous proposals from different organismal research communities, each advocating for their chosen species (e.g., various primates). To understand these decisions, scientists developed a machine learning model designed to recapitulate the actual funding outcomes.\nThis computational model incorporated a diverse array of features. Biological characteristics, such as the proposed organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (Area Under Curve, AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, measures of gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors submitting the proposal, the overall size of the relevant research community, the proposers’ centrality within the NHGRI network, and the breadth of community support, showed strong predictive power (AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, also contributed to the model’s accuracy (AUC: 0.85 ± 0.04).\nCrucially, when all these feature sets were combined, the model achieved a high level of performance in predicting funding decisions (AUC: 0.94 ± 0.03). This indicates that a multifaceted approach, considering biological, project-related, reputational, and linguistic factors, is necessary to understand the complexities of such funding allocations.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "href": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.11 Interpreting Funding Decisions: The Matthew Effect",
    "text": "16.11 Interpreting Funding Decisions: The Matthew Effect\n\n\n\nSlide 15\n\n\nTo delve deeper into the factors driving funding decisions, researchers employed feature interpretability techniques. These methods help to elucidate how individual features within the computational model influence the predicted outcome, thereby identifying characteristics associated with a higher or lower probability of receiving funding. One significant observation from this analysis is the presence of a ‘Matthew Effect’, a phenomenon where initial advantages tend to accumulate further advantages.\nSpecifically, the analysis revealed that proposals submitted by authors with a higher maximum H-index were more likely to secure funding. Similarly, organisms supported by a larger, more established research community also had a greater chance of being selected for sequencing. This pattern aligns with the strategic objectives of funding agencies, which often prioritise projects perceived as having a higher potential for significant downstream impact and eventual clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "href": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.12 Broader Applications: From Archives to Knowledge",
    "text": "16.12 Broader Applications: From Archives to Knowledge\n\n\n\nSlide 16\n\n\nThe methodologies and insights gained from the NHGRI archive project extend to a broader vision of leveraging born-physical archives through computational analysis. This specific study serves as a compelling example of how such historical data, when processed with advanced tools, can yield valuable knowledge. The research consortium collaborates with various partners and utilises diverse data sources beyond the NHGRI, including federal court records from the United States and seismological data (seismograms) from the EarthScope Consortium.\nA generalised workflow underpins these efforts. It begins with the acquisition of data and metadata from these archives. Subsequently, a sophisticated knowledge creation pipeline applies a series of computational processes. These include:\n\npage stream segmentation to delineate document structures\nhandwriting extraction\nentity disambiguation to resolve identities\nlayout modelling to understand document formats\ndocument categorisation\nfurther entity recognition\nredaction of personal information for privacy\nmodelling of decisions or processes captured in the records\n\nThe ultimate aim of this comprehensive approach is to transform raw archival data into actionable insights that can address pressing scientific questions, inform policy-making, and significantly enhance the accessibility of these rich historical resources. This transformation relies on robust algorithms and a well-developed cyberinfrastructure.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "href": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "title": "16  Beyond Traditional Views of Science Funding",
    "section": "16.13 Preservation, Collaboration, and NHGRI’s Significance",
    "text": "16.13 Preservation, Collaboration, and NHGRI’s Significance\n\n\n\nSlide 17\n\n\nA significant challenge remains in the preservation of born-physical data, much of which currently resides in vulnerable conditions, such as shipping containers susceptible to damage and neglect. The imperative to safeguard these invaluable historical records for future scholarly and scientific inquiry cannot be overstated. Recognising this, the ‘Born Physical, Studied Digitally’ consortium actively seeks collaboration, inviting testers, partners, and users to engage with their tools and methodologies. This initiative receives support from prominent organisations including the National Institutes of Health (specifically NHGRI), NVIDIA, and the National Science Foundation (NSF).\nThe speaker also highlighted a pertinent contemporary issue: recent proposals within the United States to dissolve the NHGRI. This underscores the critical need to appreciate the agency’s historical contributions. Evidence suggests NHGRI stands as one of the most innovative funding agencies in the annals of science. Consequently, the rich data contained within its archives promise to unlock answers to numerous significant scientific questions, reinforcing the importance of its continued study and preservation.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nResearchers Malte Vogl, Raphael Schlattmann, and Alex Kaye confront the formidable challenge of extracting structured knowledge from historically significant, yet computationally intractable, unstructured biographical sources. Their pioneering work introduces a novel method to transform such materials—typically printed books or dictionaries—into readily queryable knowledge graphs.\nThis innovative approach transcends the conventional reliance on pre-structured datasets, prevalent within History and Philosophy of Science (HPSS). Instead, it strategically employs Large Language Models (LLMs) not as infallible oracles, but as integral components within a meticulously engineered pipeline. The primary objective involves imposing rigorous structure upon unstructured data, such as Polish and German biographical dictionaries, thereby facilitating complex historical inquiries. These investigations might, for instance, explore the formation of intellectual networks or trace the evolution of professional connections over time.\nThe team expertly utilises tools like Neo4j for graph representation, having developed a sophisticated multi-stage pipeline that incorporates crucial human oversight to ensure both accuracy and relevance. Ultimately, this methodology enables the construction of controllable knowledge graphs, where entities—individuals, locations, organisations, and publications—become distinct nodes, and their interconnections, meticulously derived from textual evidence, form the relational edges.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "href": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs",
    "text": "17.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs\n\n\n\nSlide 01\n\n\nResearchers confront a significant challenge in historical studies: many invaluable biographical sources persist in unstructured textual formats, such as printed dictionaries and compendia. Whilst existing History and Philosophy of Science (HPSS) datasets often comprise structured information—for instance, publication databases or email archives—these unstructured materials, despite their rich detail, resist computational analysis. Consequently, this project pioneers a methodology to systematically impose structure upon such data, specifically targeting biographical entries. The core idea leverages Large Language Models (LLMs), not as standalone solutions, but as crucial components within a larger, controllable pipeline engineered to construct knowledge graphs.\nHistorically, tools like Get Grass facilitated access to printed materials for digitisation. The current approach, however, aims for a deeper level of structuration. It conceptualises biographical information as a knowledge graph: a network where entities—individuals, geographical locations, countries, published works, or organisations—constitute the nodes. The relationships between these entities, as described in the source texts, form the connecting edges. Such a graph, once constructed, permits complex, structured queries. For instance, one might investigate how professional networks evolved within a specific discipline during a particular era, or trace the contacts an individual established over their career.\nThe team employs LLMs selectively, focusing on their utility for specific tasks within a broader information processing chain, rather than pursuing an elusive ‘perfect’ model. Source materials include Polish biographical collections and German-language resources such as the handbook Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. An entry from the latter, for Alexander Abusch, might detail his role as Minister für Kultur alongside birth and death dates. An extraction pipeline processes these text-based entries, often derived from scanned documents, transforming them into a visual and queryable graph, potentially managed in a system like Neo4j. This process thus converts isolated, albeit information-dense, textual accounts into a connected, structured dataset ripe for scholarly exploration.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "href": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.2 Illustrative Extraction: Structuring Polish Biographical Entries",
    "text": "17.2 Illustrative Extraction: Structuring Polish Biographical Entries\n\n\n\nSlide 06\n\n\nTo illustrate the extraction methodology, consider a Polish biographical entry for Bartsch Henryk, an evangelical priest (ks. ewang.). The text records his birth on 12th December 1832 in Wladyslawowo, situated within the Konin district. Furthermore, it details his extensive travels to Italy (Włochy), Greece (Grecja), the Holy Land (Ziemia Święta), and Egypt (Egipt). His scholarly contributions encompass several publications: Wspomnienia z podróży do Jerozolimy i Kairo (Warsaw, 1873), Listy z podróży po Grecji i Sycylji (Warsaw, 1874), and Z teki podróżniczej, szkice dawne i nowe oryginalne i tłumaczone (Warsaw, 1883). The entry itself cites Bystron’s Wielka Encyklopedja as a source.\nFrom this concise textual snippet, the system extracts key entities and their relationships. ‘Bartsch Henryk’ is identified as a PERSON, his ROLE as ‘ks. ewang.’, his birthdate as ‘12. XII. 1832’ (a DATE), and ‘Wladyslawowo’ along with the countries he visited as LOCATIONs. Relationships such as ‘born on’, ‘born in’, ‘located in’, and ‘travelled to’ link these entities. Consequently, this process yields a set of structured triples: (Bartsch Henryk, is a, ks. ewang.), (Bartsch Henryk, travelled to, Włochy), and so forth, effectively translating narrative information into a machine-readable format suitable for knowledge graph construction.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction",
    "text": "17.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction\n\n\n\nSlide 08\n\n\nEngineers have designed a sophisticated two-stage pipeline to transform raw textual data into structured knowledge graphs. This architecture systematically processes information, integrating both automated techniques and human expertise. The first stage, ‘Ontology-agnostic Open Information Extraction’ (OIE), focuses on identifying factual statements. It commences by loading and chunking the source data, then proceeds to OIE extraction, validates these extractions, and standardises them against benchmarks. A critical quality control checkpoint determines the sufficiency of the OIE output; insufficient output triggers a manual correction loop for a sample of triples.\nSubsequently, the second stage, ‘Ontology-driven Knowledge Graph (KG) building’, refines and structures this information. This stage commences by formulating Competency Questions (CQs) that define the KG’s desired analytical capabilities. Based on these CQs, developers create an ontology and define SHACL (Shapes Constraint Language) shapes for validation. The pipeline then maps the extracted triples to this ontology, disambiguates entities (potentially linking them to Wikidata IDs), represents the data using RDF-star, and subsequently performs SHACL validation. Similar to Stage 1, a quality control loop allows for manual correction of CQs, the ontology, or SHACL shapes if necessary. Both stages integrate layers for LLM interaction and crucial ‘Human in the Loop’ interventions, ensuring robust and reliable outcomes.\nSeveral core principles underpin this pipeline’s design. It is fundamentally research-driven and data-oriented; thus, ontology development directly addresses specific research questions and aligns with the data’s realistic provisions. The human-in-the-loop paradigm strategically combines LLM automation for efficiency with expert oversight at all critical junctures, balancing scale with quality. Transparency is paramount; researchers design each step for verifiability. Furthermore, task decomposition breaks the complex process into manageable, sequential units—notably performing OIE before aligning with an ontology. Finally, modularity ensures the system can adapt, allowing for the integration of improved models or components as technology evolves.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.4 Stage 1 Workflow: Open Information Extraction in Detail",
    "text": "17.4 Stage 1 Workflow: Open Information Extraction in Detail\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, Open Information Extraction (OIE), meticulously processes raw biographical texts to identify factual assertions. It commences by loading and chunking data: the system ingests pre-processed files containing biographical narratives and segments them into manageable units. This step yields semi-structured data, often organised tabularly, with each row corresponding to a text chunk and including identifiers such as name, role, the biographical snippet itself, and a unique chunk ID.\nFollowing this preparation, OIE extraction commences. For each chunk—comprising, for example, a person’s name such as ‘Havemann, Robert’ and a segment of their biography like ‘… 1935 Prom. mit … an der Univ. Berlin…’—the system attempts to extract factual statements. This process generates raw Subject-Predicate-Object (SPO) triples, enriched with pertinent metadata such as an associated timeframe and a confidence score indicating the extraction’s reliability.\nSubsequently, OIE validation scrutinises these raw triples. The original text chunk and its corresponding extracted triples serve as input. Human experts, or potentially other specialised LLMs, then assess the accuracy and relevance of each triple against the source text. This critical review produces a set of validated SPO triples. Finally, the OIE Standard step evaluates the overall quality of this extraction phase. Researchers compare the validated triples against a ‘Gold Standard’—a reference set of triples meticulously created or verified by domain experts. This comparison yields key performance indicators such as F1-score, Precision, and Recall, offering a quantitative measure of the OIE process’s success.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail",
    "text": "17.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail\n\n\n\nSlide 12\n\n\nThe second stage of the pipeline focuses on building the knowledge graph in an ontology-driven manner, ensuring the final structure aligns precisely with research objectives. This stage commences by formulating Competency Questions (CQs). Drawing upon a sample of validated triples from Stage 1, experts define the specific analytical questions the knowledge graph must be capable of answering. This process yields a set of manually refined CQs that guide subsequent development.\nNext, developers create the ontology. Using the CQs and the sample of validated triples as inputs, they design a formal ontology. This ontology specifies the classes of entities (e.g., Person, Organisation, Event), the properties these entities can possess, and the types of relationships that can exist between them, all meticulously tailored to address the CQs. This process yields a comprehensive Ontology Definition.\nWith the ontology established, ontology mapping commences. The pipeline processes the validated triples from Stage 1, mapping their constituent subjects, predicates, and objects to the corresponding classes and properties within the defined ontology. This step transforms the relatively raw triples into conceptual RDF (Resource Description Framework) statements. Finally, disambiguation and Wikidata ID linking refine the graph. This crucial step involves resolving ambiguities in entity references—for instance, ensuring that different individuals sharing the same name are correctly distinguished. The system links entities to external identifiers, such as Wikidata IDs, where feasible. This phase also incorporates RDF-star statement generation, allowing annotations or contextual information to attach directly to individual triples, thereby enriching the graph’s expressive power. This process yields a set of disambiguated triples and RDF-star statements, ready for SHACL validation and subsequent querying.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "href": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations",
    "text": "17.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations\n\n\n\nSlide 15\n\n\nResearchers applied the knowledge graph extraction pipeline to a set of significant Polish historical sources: three complementary compilations by Stanisław Zieliński. These encompass Mały słownik pionierów polskich kolonjalnych i morskich (1933), a biographical dictionary of colonial and maritime pioneers; Bibljografja czasopism polskich zagranicą, 1830-1934 (1935), a bibliography of Polish periodicals published abroad; and Wybitne czyny Polaków na obczyźnie (1935), a record of notable Polish achievements internationally.\nThe structured data extracted from these volumes enables the exploration of several nuanced research questions. For instance, the knowledge graph facilitates the identification of individuals or communities whose pivotal roles in developing ideas and practices might have been obscured by dominant historical narratives. It allows for an analysis of shifts in migration patterns, such as those occurring before and after the January Uprising of 1863. Furthermore, investigators can examine the function of specific journals, determining if they served as ‘boundary objects’ linking diverse intellectual or professional circles, or which publications proved most central to the communities of practice amongst Polish migrants.\nInitial analysis of the Zieliński data yielded a substantial social network graph containing 3,598 nodes and 5,443 edges. Visualisations of this network distinguish editors (coloured green) from other individuals (coloured pink), offering a preliminary glimpse into the relational structures embedded within these historical texts.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "href": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”",
    "text": "17.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”\n\n\n\nSlide 17\n\n\nAnother significant application of the extraction methodology involves the German biographical lexicon, Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. This extensive work, first published in the 1980s by the Bundesstiftung zur Aufarbeitung der SED-Diktatur and subsequently digitised in the 2000s, profiles approximately 4,000 prominent East German figures from diverse fields including politics, science, culture, and sports. Containing entries for individuals such as Gustav Hertz and Robert Havemann, it serves as an indispensable resource for researchers and journalists seeking to understand the complex legacy of the German Democratic Republic.\nThe structured data derived from this lexicon enables quantitative analyses of historical patterns. One such analysis, visually presented as a scatter plot, investigates the relationship between state award recipients, high-ranking positions, and affiliation with the Socialist Unity Party (SED). The plot maps individuals based on their award status (e.g., Karl-Marx-Orden, Nationalpreis der DDR) against their rate of holding high positions and their SED affiliation rate. Comparative statistics reveal striking correlations: for instance, 95.0% of the 38 Karl-Marx-Orden recipients were SED members, compared to only 38.5% of the 1,056 individuals in the sample without this award. Recipients of the Karl-Marx-Orden also held a significantly higher share of high positions (65.8%) compared to those with no awards (28.0%), and an average birth year of 1905.9, substantially earlier than the 1923.0 average for non-recipients. Further detailed breakdowns show that 100% of Karl-Marx-Orden recipients who held Politbüro positions were members, highlighting the award’s strong ties to the highest echelons of power.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "title": "17  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "17.8 Achievements, Current Challenges, and Future Trajectories",
    "text": "17.8 Achievements, Current Challenges, and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project has successfully demonstrated a significant advancement: the transformation of isolated biographical entries into a resource capable of supporting complex structural queries. This marks a crucial step towards unlocking the rich, latent information within historical texts. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques to ensure greater accuracy and in enhancing benchmarking methodologies to rigorously assess pipeline performance.\nLooking ahead, researchers will immediately complete and finalise the current pipeline’s development. Subsequently, a systematic comparison of its outputs against alternative pipelines and existing software packages will provide valuable performance context. A key objective involves scaling the entire process to analyse full datasets, thereby enabling more comprehensive historical investigations.\nBeyond these immediate goals, future ambitions include fine-tuning the pipeline for highly specific research use cases. The team also plans to explore the potential of GraphRAG (Graph Retrieval Augmented Generation), which could allow users to query the knowledge graphs using natural language. Furthermore, the team expresses interest in constructing multilayered networks, possibly employing frameworks such as ModelSEN (Multilayer Social-Epistemic Networks), to facilitate even deeper structural analyses of the complex relationships within the historical data. Interested parties may contact Raphael Schlattmann, Alex Kaye, or Malte Vogl via their provided email addresses.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "18  References",
    "section": "",
    "text": "19 References\n{.unlisted}",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html",
    "href": "ai-nepi_001_chapter.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "",
    "text": "Overview\nThis workshop convenes researchers to explore the burgeoning applications of Large Language Models (LLMs) within the history, philosophy, and sociology of science (HPSS). Scheduled from 2nd to 4th April 2025 at TU Berlin and online, the event aims to foster discussion and collaboration on novel AI-assisted methodologies. The gathering features keynote addresses by distinguished academics Iryna Gurevych, Pierluigi Cassotti, and Nina Tahmasebi, alongside contributions from a diverse group of participants. The central theme revolves around harnessing the capabilities of LLMs to analyse complex scientific texts, understand conceptual developments, and investigate the dynamics of knowledge creation. Further details and registration information are available via the workshop website: https://www.tu.berlin/hps-mod-sci/workshop-llms-for-hpss.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-genesis-of-the-workshop",
    "href": "ai-nepi_001_chapter.html#the-genesis-of-the-workshop",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.1 The Genesis of the Workshop",
    "text": "2.1 The Genesis of the Workshop\nThe conception of this workshop arose from two distinct yet complementary wellsprings of research and intellectual curiosity. One origin lies within our project, Network Epistemology in Practice (NEPI). Within this framework, Arno Simons pioneered the training of one of the initial large language models specifically on physics texts, a domain of particular interest to the project. His proposal to discuss such work in a broader academic forum found ready support. Michael Zichert, also a member of the project team, readily agreed that this was a worthwhile endeavour and a pertinent topic, given his own work employing LLMs to analyse conceptual issues in physics.\nA second impetus came from Gerd Graßhoff, a cooperation partner of the NEPI project and a long-standing acquaintance. For many years, Professor Graßhoff has advocated the application of artificial intelligence in the history and philosophy of science, particularly for analysing scientific discovery processes. He independently conceived a similar idea for a workshop focused on novel AI-assisted methods for HPSS. Consequently, a decision to join forces culminated in the organisation of this event. The workshop is a collaborative effort by Gerd Graßhoff, Arno Simons, Michael Zichert, and myself, Adrian Wüthrich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-nepi-project-a-nexus-for-llm-application",
    "href": "ai-nepi_001_chapter.html#the-nepi-project-a-nexus-for-llm-application",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 The NEPI Project: A Nexus for LLM Application",
    "text": "2.2 The NEPI Project: A Nexus for LLM Application\nThe European Research Council (ERC) funds the “Network Epistemology in Practice” (NEPI) project, which serves as a significant contextual background for this workshop. Researchers in the NEPI project investigate the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The primary objective is to achieve a deeper understanding of how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual methodological approach. On one hand, network analysis techniques illuminate the communication structures within the Atlas collaboration. On the other hand, semantic tools, including large language models, are utilised to trace the flow of ideas through these intricate network structures. This latter application, exploring how LLMs can reveal patterns of conceptual development and transmission, represents a particularly compelling area of current research and directly informs the themes of this workshop. Beyond this specific focus, the workshop aims to showcase a wide array of other innovative LLM applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#workshop-modalities-and-acknowledgements",
    "href": "ai-nepi_001_chapter.html#workshop-modalities-and-acknowledgements",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.3 Workshop Modalities and Acknowledgements",
    "text": "2.3 Workshop Modalities and Acknowledgements\nThe smooth execution of this event relies on careful planning and the invaluable contributions of several individuals.\n\n2.3.1 Participant Engagement and Conduct\nTo ensure a comprehensive record and wider dissemination of the discussions, all sessions are being recorded. Participants provided consent for this during registration. A camera is directed towards the speaker, complemented by four roving microphones and an iPhone serving as a backup audio recorder. Subject to presenters’ approval, videos of the talks, including the audio of ensuing discussions (though only video of the presenter, not the audience), will later be uploaded to our YouTube channel. Should any participant require further information or wish to discuss their consent, they are encouraged to approach the organisers.\nGiven the considerable number of attendees and the relatively constrained time for presentations and subsequent dialogue, all participants are kindly requested to keep their questions and comments concise and to the point. Following each presentation, approximately four questions or comments will be collected, after which the presenter can respond collectively. This approach aims to optimise time and facilitate a broader range of interactions. For discussions extending beyond the sessions, an Etherpad (or Cryptpad) is available; a QR code provides access. This platform allows for continued engagement, where presenters can also read and respond to queries. Additionally, attendees—both online and in person—may use the Zoom chat function to post questions or comments at any time during the sessions.\n\n\n2.3.2 Networking and Logistics\nBeyond the formal presentations, the workshop programme incorporates ample opportunities for informal networking. Lunch breaks, coffee breaks, a modest reception, and a workshop dinner (for which seating is unfortunately limited to those who received confirmation) are designed to facilitate interaction amongst researchers and fellows. Coffee and refreshments are served in the main workshop area. Lunch and the reception will take place in room H2051. To reach this location, proceed down the hall to the very last staircase of the building on the other side, descend one floor, and the room will be nearby. We can also proceed there together after today’s final presentation.\n\n\n2.3.3 Gratitude\nThis event would not have been possible without the dedicated assistance of several individuals. Particular thanks are due to Svenja Goetz, Lea Stengel, and Julia Kim, who helped to think through various aspects and managed numerous administrative and organisational tasks. We also extend our gratitude to Oliver Ziegler and his Unicam team for their expertise in recording the keynotes and for their crucial support in setting up the technical infrastructure for all sessions, ensuring a smooth experience for both in-person and Zoom participants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "href": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.4 Introducing the Keynote Speakers",
    "text": "2.4 Introducing the Keynote Speakers\nBefore commencing with the academic programme, it is a pleasure to introduce our distinguished keynote speakers.\n\n2.4.1 Pierluigi Cassotti and Nina Tahmasebi: Semantic Change and Humanities\nThe first keynote address will be delivered shortly by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi is the Principal Investigator of the “Change is Key!” research programme in Gothenburg, and Pierluigi Cassotti is a researcher within this project. They are widely recognised for their significant contributions to the field of semantic change detection. Their work encompasses not only technical aspects, such as the creation of benchmarks, but also addresses broader issues, including the application of data science methods to questions within the humanities. This expertise makes them an ideal pairing to inaugurate our workshop discussions.\n\n\n\nKeynote speakers Pierluigi Cassotti and Nina Tahmasebi, experts in large-scale text analysis for cultural and societal change\n\n\n\n\n2.4.2 Iryna Gurevych: Elevating NLP to the Cross-Document Level\nOur second keynote speaker, Iryna Gurevych, will present her talk tomorrow late afternoon. Professor Gurevych heads the Ubiquitous Knowledge Processing Lab at the Technical University Darmstadt. Her extensive research focuses on information extraction, semantic text processing, and machine learning. Furthermore, her work involves significant applications of Natural Language Processing to the social sciences and humanities. This interdisciplinary focus aligns perfectly with the aims of this workshop.\n\n\n\nKeynote speaker Iryna Gurevych, focusing on intertextual NLP applications\n\n\nWith these introductions made, we are now ready for the first keynote. Please join me in welcoming Nina Tahmasebi and Pierluigi Cassotti. The floor is yours. And a warm welcome to all participants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html",
    "href": "ai-nepi_003_chapter.html",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "",
    "text": "4 Overview\nThis chapter offers an exploration into the domain of Large Language Models (LLMs). It commences with a primer on the foundational architectures underpinning these sophisticated tools, notably the Transformer model and its influential derivatives, BERT and GPT. Subsequently, the discussion navigates towards the practical applications of LLMs within the interdisciplinary field of History and Philosophy of Science and Science Studies (HPSS). The chapter culminates in a series of critical reflections, considering the unique challenges, the imperative for developing LLM literacy, and the importance of methodological integrity as HPSS researchers engage with these rapidly evolving technologies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "href": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "4.1 A Primer on Large Language Models",
    "text": "4.1 A Primer on Large Language Models\nThe advent of Large Language Models represents a significant milestone in artificial intelligence, fundamentally altering how machines process and generate human language. At the core of this revolution lies the Transformer architecture, a novel design that has spurred the development of powerful and versatile language models.\n\n4.1.1 The Transformer Architecture: A Foundation for LLMs\nThe Transformer architecture, introduced by researchers in 2017 (Vaswani2017?), forms the bedrock of contemporary LLMs. Originally conceived for language translation tasks, such as converting German to English, its innovative design has proven remarkably adaptable. The model comprises two primary, interconnected streams: the encoder and the decoder.\n Figure 1: The Transformer model architecture, as detailed by (Vaswani2017?).\nThe process begins when the encoder receives an input sentence. It meticulously converts the words into numerical representations, which then undergo several layers of processing. These processed numerical data are subsequently passed to the decoder. The decoder, in turn, generates the output sentence word by word. Crucially, each word it produces is fed back into the model, informing the prediction of the subsequent word, a loop that continues until the entire translated sentence materialises.\nWithin these numerical processing units, multiple layers work to progressively contextualise word embeddings. A key distinction between the encoder and decoder lies in their operational dynamics. The encoder reads the entire input sentence simultaneously. This holistic approach allows each word to interact with every other word, thereby enabling the model to construct a comprehensive, full-context representation of the sentence’s meaning. Conversely, the decoder operates sequentially. When generating an output, such as an English translation, the English words can only consider their predecessors; they cannot look into the future. This constraint is logical, as the model’s task is to predict the next word in the sequence.\n\n\n4.1.2 From Transformers to Pre-trained Language Models\nAlmost immediately following the publication of the Transformer paper, researchers began to re-engineer its encoder and decoder streams independently. This innovative path led to the creation of pre-trained language models. These models signify a shift away from systems designed purely for translation. Instead, they cultivate a profound understanding or generative capability of language itself. With subsequent fine-tuning, which can range from minor adjustments to more extensive additional training, these pre-trained models can be adeptly tailored for a wide array of Natural Language Processing (NLP) tasks.\n\n\n4.1.3 Encoder-Based Models: The Rise of BERT\nThe encoder component of the Transformer architecture was ingeniously adapted to form the basis of models such as BERT, which stands for Bidirectional Encoder Representations from Transformers (Devlin2018?). The BERT family of models continues to be highly influential in the field of NLP.\n Figure 2: Conceptual representation of BERT, highlighting its encoder-based nature.\nBERT’s distinctive strength lies in its operational mechanism: each word in the input stream can attend to, or ‘listen to’, every other word in both forward and backward directions. This bidirectionality allows the model to produce a full-context understanding of the entire input at once. The original terminology, “bidirectional encoder-based representations from Transformers,” whilst descriptive, has somewhat faded in common parlance, yet the model’s core principles endure.\n\n\n4.1.4 Decoder-Based Models: The Generative Power of GPT\nConversely, the decoder stream of the Transformer architecture provides the foundation for Generative Pre-trained Transformer (GPT) models (Radford2018?). These models are the driving force behind widely recognised systems such as ChatGPT.\nGPT models possess a different structural characteristic: they process text sequentially, with each token only able to attend to preceding tokens. This ‘unidirectional’ or ‘autoregressive’ nature, whilst limiting their view to the past, endows them with a powerful capability for generating new text. This contrasts with BERT-like models, whose primary strength is not text generation but deep language understanding. Consequently, these two families of models serve distinct purposes: BERT-like models excel at comprehending sentences coherently, whilst GPT-like models specialise in producing language.\nBeyond these foundational encoder-only and decoder-only models, the landscape also includes hybrid architectures that combine encoder and decoder components. Furthermore, sophisticated techniques exist for using decoders in ways that allow them to perform tasks more akin to encoders, blurring the lines between these categories. Models such as XLM, which builds upon XLNet, exemplify such advanced applications, though a detailed exploration of these nuances extends beyond the scope of this primer. The core distinction between generative models that produce language and full-context models that understand language remains a key takeaway.\n\n\n4.1.5 Adapting Foundational Models: Pre-training and Fine-tuning Strategies\nAdapting these powerful foundational models to specific domains, such as scientific language, involves several key strategies. The initial stage, known as pre-training, is where the model first encounters and learns from vast quantities of text. This learning occurs either by predicting the next token in a sequence, typical for GPT models, or by predicting randomly masked words within a sentence, a hallmark of BERT’s training. Such pre-training from scratch demands immense computational power and data resources, often placing it beyond the reach of individual research groups.\n Figure 3: Comparative overview of BERT and GPT architectures, emphasising their distinct processing flows.\nA more accessible approach is continued pre-training. Here, an already pre-trained model, such as a general-purpose BERT model, undergoes further training on a specialised corpus, for instance, a collection of physics texts. This allows the model to adapt its learned representations to the nuances of the specific domain.\nBeyond pre-training, researchers can leverage these models by adding supplementary layers on top of the pre-trained base. These new layers can then be trained for specific downstream tasks, such as classifying text for sentiment or identifying named entities. Prompt-based learning offers another avenue for adaptation, though its intricacies are not detailed here.\nA crucial technique for enhancing model capabilities is contrastive learning. Whilst LLMs generate rich contextualised word embeddings, creating equally effective embeddings for entire sentences or documents requires additional steps. Contrastive learning provides a key method to achieve this. Sentence-BERT, for example, employs this technique to map sentences into a dense vector space where semantic similarity corresponds to proximity. This allows documents or sentences to be represented in the same type of embedding space as individual words, facilitating tasks like semantic search and clustering. The development of robust sentence and document embeddings remains an active area of research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#harnessing-large-language-models-in-hpss-research",
    "href": "ai-nepi_003_chapter.html#harnessing-large-language-models-in-hpss-research",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "4.2 Harnessing Large Language Models in HPSS Research",
    "text": "4.2 Harnessing Large Language Models in HPSS Research\nThe capabilities of LLMs extend beyond general language tasks, offering promising avenues for research in the History and Philosophy of Science and Science Studies (HPSS). Adapting these models to the specific textual data and analytical questions prevalent in HPSS requires careful consideration of various techniques and an awareness of the evolving landscape of scientific LLMs.\n\n4.2.1 Domain Adaptation through Retrieval Augmented Generation (RAG)\nOne prominent method for tailoring LLMs to specific domains without extensive retraining is Retrieval Augmented Generation (RAG). This approach creates a pipeline that enhances the model’s knowledge base with domain-specific information at the point of inference.\n Figure 4: The RAG pipeline, combining retrieval mechanisms with generative models for domain-specific responses.\nA RAG system typically involves at least two models, or a model coupled with a retrieval mechanism, operating in concert. For example, when a user poses a query, such as “What are LLMs?”, a retrieval component—often powered by a BERT-like model proficient in semantic similarity—searches a database of relevant documents (e.g., HPSS scholarly articles). This component identifies and retrieves passages most similar to the query. These retrieved passages are then incorporated into the prompt provided to a generative LLM. The generative model uses this augmented context to formulate a more informed and domain-specific answer. This dynamic integration of external knowledge is a feature in many contemporary LLM applications, including instances where ChatGPT searches the internet to answer queries.\nIt is important to recognise that many advanced LLM applications, including reasoning models and “agents,” are not monolithic LLMs. Rather, they are complex systems comprising multiple LLMs, traditional algorithms, and various other tools working together. Understanding these distinctions—different architectures, varied fine-tuning strategies, the critical difference between word versus sentence embeddings, and the levels of abstraction involved—is fundamental for effectively applying LLMs in research.\n\n\n4.2.2 A Survey of LLM Applications in HPSS\nCurrent investigations are actively exploring the utility of LLMs as research tools within HPSS. A preliminary survey of existing applications reveals several emerging categories where these models demonstrate considerable potential (Ho2024?).\n Figure 5: Overview of LLM applications in HPSS research.\nThese categories include:\n\nDealing with data and sources:\n\nParsing and extracting structured information from texts, such as publication types, acknowledgements, and citations.\nInteracting with source materials through functionalities like automated summarisation or conversational interfaces built using RAG-type systems.\n\nKnowledge structures:\n\nExtracting specific entities relevant to HPSS, for example, scientific instruments, celestial bodies, or chemical compounds.\nMapping complex relationships, such as disciplinary connections, the contours of interdisciplinary fields, or the dynamics of science-policy discourses.\n\nKnowledge dynamics:\n\nTracing the conceptual histories of key terms, for instance, analysing the evolution of “theory” in Digital Humanities or the terms “virtual” and “Planck” in physics.\nIdentifying markers of novelty, such as detecting breakthrough papers or tracking the emergence of new technologies within a corpus.\n\nKnowledge practices:\n\nReconstructing arguments within texts by identifying premises, conclusions, and causal claims.\nAnalysing citation contexts to understand the purpose, sentiment, or function of a citation.\nConducting discourse analysis to identify linguistic features like hedge sentences, specialised jargon, or instances of boundary work.\n\n\n\n\n4.2.3 Emerging Trends and Persistent Concerns in HPSS LLM Adoption\nThe adoption of LLMs within HPSS reflects an accelerating interest. Research utilising these models is increasingly appearing not only in computationally focused journals like Scientometrics and JASIST but also in traditional HPSS journals that have historically been less engaged with computational methods. This broadening appeal suggests that the enhanced semantic capabilities of modern LLMs are attracting qualitative researchers, historians, and philosophers.\n Figure 6: Trends and concerns in the application of LLMs within HPSS.\nThe degree of customisation in these applications varies considerably. Some researchers undertake sophisticated architectural modifications or custom pre-training efforts. Others perform custom fine-tuning on existing models, whilst some may use off-the-shelf tools like ChatGPT for specific tasks.\nDespite the enthusiasm, several recurring concerns temper the adoption of LLMs in HPSS:\n\nComputational Resources: The significant computational power required for training and, in some cases, running large models remains a barrier.\nOpaqueness: The “black box” nature of many LLMs, where the internal decision-making processes are not easily interpretable, poses challenges for scholarly rigour and validation.\nData Scarcity: A lack of large, high-quality, digitised datasets specific to HPSS research questions can hinder model training and adaptation.\nBenchmark Deficiencies: The absence of standardised benchmarks tailored for HPSS tasks makes it difficult to compare model performance and progress.\nModel Trade-offs: Researchers must navigate the trade-offs between different model types (e.g., BERT-like for understanding versus GPT-like for generation), as no single model is universally optimal. The most effective model is always contingent upon the specific research purpose and available data.\n\nNevertheless, a positive trend towards greater accessibility is evident. Tools like BERTopic, for instance, have simplified complex tasks like topic modelling, largely due to excellent developer support and user-friendly interfaces, making advanced computational methods more approachable for a wider range of HPSS scholars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#critical-reflections-and-future-directions-for-llms-in-hpss",
    "href": "ai-nepi_003_chapter.html#critical-reflections-and-future-directions-for-llms-in-hpss",
    "title": "3  Large Language Models: Foundations, HPSS Applications, and Reflections",
    "section": "4.3 Critical Reflections and Future Directions for LLMs in HPSS",
    "text": "4.3 Critical Reflections and Future Directions for LLMs in HPSS\nAs Large Language Models become increasingly integrated into the research toolkit, it is imperative for the HPSS community to engage in critical reflection. This involves acknowledging discipline-specific challenges, fostering LLM literacy, upholding methodological integrity, and addressing the rapidly evolving nature of these technologies.\n Figure 7: Critical reflections for the HPSS community concerning LLM adoption.\n\n4.3.1 Navigating HPSS-Specific Challenges\nThe application of LLMs in HPSS is not without its unique hurdles. Firstly, the historical evolution of concepts and language presents a significant complication. LLMs are predominantly trained on contemporary text, which means they may misinterpret or lack understanding of archaic language, historical terminologies, or shifts in conceptual meaning over time. Researchers must develop strategies to address this, perhaps through targeted fine-tuning on historical corpora or by employing critical interpretive frameworks when analysing model outputs.\nSecondly, HPSS scholarship often embodies a reconstructive and critical perspective. Historians and philosophers of science frequently aim to “read between the lines,” understand the situated context of historical actors, analyse social implications, and identify subtle discursive strategies such as boundary work. Standard LLMs are not inherently designed for such nuanced, interpretative readings. Innovative approaches are therefore needed to guide models towards performing these more sophisticated analytical tasks that are central to HPSS inquiry.\nThirdly, practical data-related issues persist. HPSS researchers often contend with sparse data, particularly for niche historical topics. Source materials may exist in multiple languages, involve old scripts or non-standard orthography, and suffer from a lack of comprehensive digitalisation, all of which pose challenges for LLM processing.\n\n\n4.3.2 Cultivating LLM Literacy within the HPSS Community\nTo effectively navigate these complexities and harness the potential of LLMs, fostering a robust LLM literacy within the HPSS community is paramount. This extends beyond merely using the tools; it involves a deeper engagement with their underlying principles and implications.\nResearchers should familiarise themselves with the fundamental concepts of LLMs, Natural Language Processing (NLP), and Deep Learning (DL), encompassing both the theoretical underpinnings and the practical capabilities and limitations of available tools. Understanding which model architectures, training regimes, and fine-tuning strategies are most appropriate for specific HPSS research problems is essential. This may, for some, involve acquiring or enhancing coding skills, although the trend towards more intuitive natural language interfaces for interacting with models may gradually lower this barrier.\nFurthermore, the development and sharing of domain-specific datasets and benchmarks tailored to HPSS research questions would significantly advance the field. Without a solid foundation of LLM literacy, there is a risk of misinterpreting model outputs, being swayed by superficially impressive visualisations without a critical understanding of their generation, or inappropriately applying tools to research questions.\n\n\n4.3.3 Upholding Methodological Integrity whilst Embracing Innovation\nA crucial aspect of integrating LLMs into HPSS research is the commitment to maintaining established scholarly methodologies and epistemic standards. Whilst these new tools offer powerful analytical capabilities, they must serve, not supplant, the core research objectives of the discipline.\nThe challenge lies in effectively translating HPSS research problems into tractable NLP tasks—such as classification, generation, or summarisation—without allowing the technical specificities of the NLP task to hijack or distort the original scholarly inquiry. The goal is to use LLMs to answer HPSS questions, not merely to find HPSS data upon which to apply LLM techniques.\nSimultaneously, LLMs present exciting new opportunities for bridging traditional qualitative and quantitative approaches within HPSS. They can enable the analysis of textual data at scales previously unimaginable, potentially revealing patterns and connections that complement close reading and interpretative scholarship. This could foster greater interdisciplinary collaboration and methodological pluralism.\nIt is also valuable for the HPSS community to reflect on its own intellectual pre-history concerning computational methods. For instance, techniques like co-word analysis, developed in the 1980s by scholars such as Michel Callon and Arie Rip, emerged from specific theoretical commitments within science and technology studies (STS) and sought to map the socio-cognitive dynamics of scientific fields. Acknowledging this lineage can inform contemporary engagements with LLMs, encouraging a theoretically grounded and critically reflective application of these new tools.\n\n\n4.3.4 Addressing Emergent Questions and the Evolving Landscape\nThe field of LLMs is characterised by exceptionally rapid development, prompting ongoing questions and requiring continuous adaptation from the research community.\nThe issue of model ‘hallucinations’—the propensity of generative models to produce plausible-sounding but factually incorrect or nonsensical information—remains a pertinent concern. Whilst the accuracy of leading models is improving, the outputs of generative LLMs always necessitate careful critical scrutiny and fact-checking, particularly when employed in scholarly contexts.\nThe expanding capacity for multilingual support in LLMs opens new avenues for comparative and cross-linguistic HPSS research. However, selecting or training appropriate models for specific multilingual tasks demands careful consideration of data availability, linguistic nuances, and the specific research goals. The principle remains: the choice of model must align with the research problem and the nature of the available data.\nIndeed, the very term ‘Large Language Model’ may soon prove insufficient, as models increasingly demonstrate multimodal capabilities, processing and generating not only text but also images, sound, and other forms of data. This rapid evolution, where the definition of ‘large’ itself is a constantly moving target with model parameter counts escalating dramatically, underscores the need for researchers to maintain an awareness of ongoing developments.\nThe emergence of ‘agents’—more autonomous systems that can combine LLMs with other tools to perform complex, multi-step tasks—signals a potential new frontier. These agents could transform research practices, assist in data analysis, or even participate in scholarly communication in novel ways. Intriguingly, theoretical frameworks developed within HPSS and STS, such as Actor-Network Theory (ANT), offer robust conceptual language for understanding and analysing these evolving socio-technical assemblages, where human and non-human actors interact and co-construct knowledge.\nUltimately, a core question for the HPSS community is whether, and how, LLMs can contribute to solving longstanding intellectual challenges within the discipline. Early applications, such as the use of contextualised word embeddings to trace the nuanced evolution of concepts like ‘Planck’ through scientific literature over time, suggest significant potential. By carefully mapping word senses and their changing prominence, researchers can gain new insights into conceptual dynamics. Such capabilities offer pathways to address core problems in HPSS that were previously intractable through manual methods alone, promising a richer and more detailed understanding of the history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html",
    "href": "ai-nepi_004_chapter.html",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "",
    "text": "Overview\nOpenAlex Mapper facilitates the exploration of scholarly literature by projecting search queries from the OpenAlex database onto a pre-computed base-map of scientific articles. The underlying workflow encompasses three principal stages. Initially, an embedding model, Specter 2 (Singh2022?), undergoes fine-tuning to enhance its ability to distinguish between academic disciplines. Subsequently, a base-map materialises from a random sample of 300,000 articles drawn from OpenAlex; their abstracts are embedded and then projected into a two-dimensional space using Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?). This process yields both the visual base-map and a trained UMAP model. Finally, for an individual user query, the system retrieves relevant records from OpenAlex, embeds their abstracts using the fine-tuned Specter 2 model, and projects these new embeddings onto the existing base-map via the trained UMAP model. The outcome is an interactive map, accessible online, which visually situates the user’s query results within the broader landscape of science.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#introduction",
    "href": "ai-nepi_004_chapter.html#introduction",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThe proliferation of scholarly literature presents both immense opportunities and significant challenges for researchers seeking to understand the structure and dynamics of science. Navigating this vast corpus to identify connections, trace influences, and map intellectual terrains requires innovative tools. OpenAlex Mapper, a project developed by Maximilian Noichl (Utrecht University) and Andrea Loettgers, with contributions from Taya Knuuttila (University of Vienna) under an ERC grant focused on Possible Life, offers such a solution. This chapter introduces OpenAlex Mapper, detailing its technical architecture, demonstrating its interactive capabilities, and exploring its applications within the History and Philosophy of Science and Scholarship (HPSS). The tool aims to bridge the gap between detailed qualitative analysis and large-scale quantitative perspectives on scientific knowledge.\n\n\n\nIntroducing OpenAlex Mapper, a tool for visualising connections within scholarly literature.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#the-openalex-mapper-a-technical-architecture",
    "href": "ai-nepi_004_chapter.html#the-openalex-mapper-a-technical-architecture",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.2 The OpenAlex Mapper: A Technical Architecture",
    "text": "4.2 The OpenAlex Mapper: A Technical Architecture\nThe power of OpenAlex Mapper resides in its carefully constructed technical pipeline, which transforms textual data from scholarly articles into spatial representations on an interactive map. This process unfolds in three principal phases: enhancing a language model for disciplinary specificity, constructing a foundational map of science, and dynamically projecting user-specific queries onto this map.\n\n4.2.1 Fine-tuning the Embedding Model\nAt the core of OpenAlex Mapper lies a sophisticated language model, Specter 2 (Singh2022?). Researchers initiated the process by fine-tuning this model. The primary objective of this fine-tuning was to improve the model’s ability to recognise and respect disciplinary boundaries. By training the model on a dataset of articles with closely related disciplinary backgrounds, its capacity to distinguish subtle yet significant differences between fields of study was enhanced. This step ensures that the subsequent mapping process reflects genuine thematic and disciplinary distinctions within the scholarly literature. Visualisations of the embedding space before and after fine-tuning, often produced using UMAP, typically illustrate a clearer separation of disciplinary clusters post-tuning.\n\n\n4.2.2 Base-map Preparation\nWith a discipline-aware embedding model established, the next phase focuses on creating a base-map representing a broad overview of science. For this, the system draws upon the OpenAlex database, a large and openly accessible repository of scholarly material. A random sample of 300,000 article abstracts constitutes the base data. Each abstract in this sample is then converted into a high-dimensional vector representation—an embedding—using the fine-tuned Specter 2 model.\nTo visualise these complex embeddings, engineers employ Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?), a powerful dimensionality reduction technique. UMAP projects the high-dimensional abstract embeddings into a two-dimensional space, arranging them such that similar articles appear closer together. This projection forms the static base-map. Importantly, the UMAP algorithm, once trained on this base data, is preserved. This trained UMAP model retains the knowledge of how to position articles within this specific two-dimensional representation of science.\n\n\n4.2.3 Processing Individual User Queries\nThe true dynamism of OpenAlex Mapper emerges when users introduce their own research interests. The system allows users to input arbitrary queries directly to the OpenAlex database, typically by providing a URL generated from an OpenAlex search. Upon receiving a query, OpenAlex Mapper downloads the relevant scholarly records, often facilitated by tools like PyAlex.\nThese newly acquired records, specifically their abstracts, then undergo the same embedding process as the base-map data, utilising the fine-tuned Specter 2 model. The crucial step follows: the trained UMAP model, developed during the base-map preparation, projects these new embeddings into the existing two-dimensional map. This ensures that the queried articles are positioned coherently relative to the initial 300,000 articles, as if they had been part of the original layout process. The outcome is an interactive map, accessible online and available for download via the data-mapplot library, where the user’s query results are highlighted against the backdrop of the broader scientific landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#navigating-the-scholarly-landscape-using-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#navigating-the-scholarly-landscape-using-openalex-mapper",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.3 Navigating the Scholarly Landscape: Using OpenAlex Mapper",
    "text": "4.3 Navigating the Scholarly Landscape: Using OpenAlex Mapper\nInteracting with OpenAlex Mapper is designed to be an intuitive process, enabling researchers to explore interdisciplinary connections and the contextual position of specific topics or publication sets. The tool is accessible online, and users can follow a straightforward procedure to generate custom maps.\nThe process begins with a visit to the OpenAlex website. Here, users can leverage the full search capabilities of the OpenAlex platform to identify a corpus of literature relevant to their interests. This might involve searching for papers using a particular model (e.g., the Kuramoto model), publications from a specific institution within a given year (e.g., Utrecht University papers from 2019), or articles citing a seminal work (e.g., Wittgenstein’s Philosophical Investigations). Once the desired search results are obtained, the user copies the URL generated by OpenAlex for that specific query.\n\n\n\nThe OpenAlex Mapper user interface, showing input fields for OpenAlex search URLs and sample settings.\n\n\nThis URL is then pasted into the designated input field within the OpenAlex Mapper interface. Users can adjust several settings, such as sample size, particularly for large result sets, as the embedding process can be computationally intensive. Options may also exist to configure aspects of the final visualisation, for instance, colouring points by publication date or displaying citation links between mapped papers.\nUpon clicking “Run Query”, OpenAlex Mapper initiates its backend processes. It downloads the records corresponding to the OpenAlex search URL, extracts the abstracts, and then embeds them using the fine-tuned language model. Subsequently, the trained UMAP model projects these new embeddings onto the base-map. After a processing period, the interactive visualisation appears, displaying the queried papers as distinct points on the map, often highlighted for clarity. Users can then zoom, pan, and hover over points to retrieve metadata, thereby facilitating an exploratory analysis of how their topic of interest relates to the wider scholarly domain.\n\n\n\nAn example of OpenAlex Mapper processing a query for “scale free networks”, displaying the results on the interactive map.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#illuminating-hpss-applications-and-insights",
    "href": "ai-nepi_004_chapter.html#illuminating-hpss-applications-and-insights",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.4 Illuminating HPSS: Applications and Insights",
    "text": "4.4 Illuminating HPSS: Applications and Insights\nOpenAlex Mapper offers a potent instrument for addressing enduring questions within the History and Philosophy of Science and Scholarship (HPSS). It particularly aids in tackling challenges related to the generalisability of findings from small samples or specific case studies and in validating qualitative insights against broader, large-scale trends in contemporary science. The tool facilitates heuristic investigations rooted in quantitative methods, yet always allows for a return to the textual sources.\n\n\n\nA visualisation of interconnected academic fields, representative of maps generated by OpenAlex Mapper.\n\n\n\n4.4.1 Tracing Model Templates Across Disciplines\nOne significant application lies in the investigation of model templates. In philosophy of science, model templates conceptualise how models with very similar structures emerge and find utility across diverse scientific disciplines. OpenAlex Mapper can help visualise where specific model templates—for instance, the Hopfield model, originally developed in one context and later transported to others—have genuinely established a presence and continued usage. By mapping papers related to such templates, researchers can observe their distribution across the scientific landscape, identifying areas of concentration and unexpected applications, thereby understanding how these templates might structure science in ways orthogonal to traditional disciplinary boundaries.\n\n\n4.4.2 Mapping Conceptual Terrains\nThe tool also proves valuable for mapping the landscape of scientific concepts. For example, one can explore the distribution and interplay of concepts like ‘phase transition’ versus ‘emergence’. By querying OpenAlex for papers discussing these concepts and projecting them onto the base-map, their disciplinary loci and areas of overlap or distinction become apparent. This capability allows researchers to broaden conceptual analysis into interdisciplinary contexts, overcoming the difficulties often associated with obtaining and integrating disparate datasets from various fields.\n\n\n4.4.3 Analysing the Distribution of Scientific Methods\nFurthermore, OpenAlex Mapper can illuminate the distribution and adoption of specific scientific methods across disciplines. Consider, for example, ongoing debates in the philosophy of science regarding the role of machine learning techniques versus classical statistical methods. Researchers used OpenAlex Mapper to compare the prevalence of a machine learning technique, the random forest model, with a somewhat analogous classical method, logistic regression. Plotting papers that employ these methods reveals distinguishable patterns of disciplinary preference. Such visualisations prompt deeper philosophical questions: why, for instance, might neuroscientists favour random forests while researchers in thematically proximate fields like psychiatry or mental health research continue to rely more on logistic regressions? Exploring these distributions can uncover underlying epistemological commitments or practical constraints shaping methodological choices.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#considerations-and-future-prospects",
    "href": "ai-nepi_004_chapter.html#considerations-and-future-prospects",
    "title": "4  The Workflow and Applications of OpenAlex Mapper",
    "section": "4.5 Considerations and Future Prospects",
    "text": "4.5 Considerations and Future Prospects\nSeveral considerations inform the current capabilities and future development of OpenAlex Mapper, ensuring a nuanced understanding of its outputs. These qualifications are crucial for interpreting the generated maps and for guiding further refinements of the tool.\n\n\n\nA summary of qualifications regarding OpenAlex Mapper, including data dependencies and methodological considerations.\n\n\nA primary factor is the OpenAlex database itself. Whilst OpenAlex offers a vast and commendably open collection of scholarly metadata, its completeness and the quality of its data are not perfect. The accuracy and comprehensiveness of the underlying data naturally influence the insights derivable from the maps. Data quality is, however, reasonable, especially when compared to other available large-scale bibliographic databases.\nThe current iteration of the embedding model within OpenAlex Mapper is trained exclusively on English-language texts. This inherently limits the scope of analysis, particularly for research areas with significant non-English literature or for historical periods where English was not the dominant scholarly language. However, as the tool often focuses on more recent scientific history, this limitation may be less acute for certain applications. Future developments could incorporate multilingual models to broaden coverage, although high-quality, science-trained multilingual models remain an area of active research.\nEffectiveness of the embedding step also depends on the availability of abstracts or informative titles within the source records. Papers lacking these textual elements cannot be effectively processed and mapped, potentially leading to omissions in the visualisation.\nFinally, the Uniform Manifold Approximation and Projection (UMAP) algorithm, central to the dimensionality reduction and visualisation, possesses inherent characteristics that users must acknowledge. UMAP is a stochastic algorithm, meaning that each run can produce slightly different layouts, even with identical input data. The maps generated are thus one realisation amongst many possibilities. More fundamentally, projecting extremely high-dimensional data (such as Specter 2 embeddings, which have 768 dimensions) into just two dimensions inevitably involves trade-offs. The algorithm must prioritise certain relationships and necessarily distorts others; not all high-dimensional proximities can be perfectly preserved in the lower-dimensional representation. Understanding these trade-offs is key to a sound interpretation of the visual patterns.\nFurther details on the technical aspects and ongoing research are available in a working paper titled “Philosophy at Scale: Introducing OpenAlex Mapper” (Noichl2023working?).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Applications of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#conclusion",
    "href": "ai-nepi_004_chapter.html#conclusion",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nOpenAlex Mapper represents a novel approach to navigating and understanding the complex, interconnected web of scientific knowledge. By combining advanced language models with powerful dimensionality reduction techniques, it provides researchers, particularly within HPSS, with an interactive means to explore the distribution of concepts, methods, and intellectual traditions across diverse disciplinary landscapes. Its capacity to ground large-scale visualisations in specific textual sources offers a promising avenue for integrating qualitative insights with quantitative analyses, thereby enriching our understanding of how science operates and evolves. Despite certain limitations inherent in its data sources and algorithms, the tool furnishes a dynamic platform for heuristic investigation and the generation of new research questions about the structure and dynamics of scholarly communication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html",
    "href": "ai-nepi_005_chapter.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "6 Overview\nThis chapter navigates the complexities of genre classification within historical medical periodicals, a core component of the ActDisease project. It commences with an introduction to the ActDisease project itself, detailing its objectives, the dataset of patient organisation magazines, and the inherent challenges encountered during dataset digitisation. Subsequently, the focus shifts to genre classification experiments. This section explores the motivation behind classifying genres in this unique corpus, delves into zero-shot and few-shot classification methodologies, and presents experiments involving few-shot prompting with the Llama-3.1 8b Instruct model. The chapter culminates in a conclusion that synthesises the findings and discusses their implications for historical research and computational linguistics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#about-actdisease",
    "href": "ai-nepi_005_chapter.html#about-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "6.1 About ActDisease",
    "text": "6.1 About ActDisease\nThe ActDisease project, funded by the European Research Council (ERC), embarks on a historical investigation into the pivotal role of patient organisations in shaping modern medicine across Europe. Central to this endeavour are the periodicals published by these organisations, which serve as the primary source material. These documents, originating from England, Germany, France, and Great Britain, offer a rich tapestry of information reflecting the evolving landscape of disease understanding and patient advocacy.\n\n\n\nTitle page: Genre Classification for Historical Medical Periodicals, ActDisease Project.\n\n\n\n6.1.1 About the Project\nActDisease, an acronym for “Acting out Disease – How Patient Organisations Shaped Modern Medicine,” is an ERC-funded research initiative. Its fundamental purpose lies in studying how patient organisations actively contributed to the conceptualisation of diseases, the lived experiences of illness, and the trajectory of medical practices throughout the 20th century in Europe. The project specifically concentrates on ten European patient organisations spanning Sweden, Germany, France, and Great Britain, covering a period from approximately 1890 to 1990. The principal source materials are the periodicals, predominantly magazines, issued by these patient organisations.\n\n\n\nOverview of the ActDisease project, highlighting its ERC funding and focus on patient organisation periodicals. The image on the right depicts Heligoland, Germany, the founding place of the Hay Fever Association in 1897.\n\n\n\n\n6.1.2 Dataset Description\nThe ActDisease dataset comprises a recently digitised private collection of patient organisation magazines. This collection encompasses materials from Germany (covering Allergy/Asthma, Diabetes, Multiple Sclerosis), Sweden (Allergy/Asthma, Diabetes, Lung Diseases), France (Diabetes, Rheumatism/Paralysis), and the United Kingdom (Diabetes, Rheumatism). In total, the dataset amounts to 96,186 pages. The table below summarises the magazines by country, disease, page count, and year coverage. Exploration of these materials reveals a diverse array of text types, which, interestingly, exhibit similarity across all magazines.\n\n\n\nTable summarising the ActDisease dataset by country, disease, number of magazines, page count, and year coverage, alongside example magazine covers.\n\n\n\n\n6.1.3 Dataset Digitisation Challenges\nThe digitisation process primarily involved Optical Character Recognition (OCR) using ABBYY FineReader Server 14. This software performed well on most common layouts and fonts. Nevertheless, significant challenges persist for OCR concerning complex layouts, slanted text, rare fonts, and variations in scan or photo quality. Consequently, remaining issues include OCR errors, particularly in German and French texts, and disrupted reading order.\nResearchers conducted experiments on post-OCR correction of German texts employing instruction-tuned generative models (DanilovaAangenendt2025?). Notably, OCR errors appear frequently in creative texts, such as advertisements, humour pages, and poems. The OCR model demonstrated proficiency in recognising common layouts and fonts; however, complex layouts and disrupted reading order continue to pose difficulties, prompting experiments to address these issues.\n\n\n\nExamples of digitisation challenges, including complex layouts and varied text types from historical periodicals.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "href": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "6.2 Genre Classification Experiments",
    "text": "6.2 Genre Classification Experiments\nThe inherent diversity of texts within the patient organisation periodicals necessitated a robust method for distinguishing between different kinds of content. This led to the exploration of genre classification as a means to enhance the analytical depth of the research.\n\n6.2.1 Motivation for Genre Classification\nInitial examinations of the materials confirmed a wide variety of text types, consistently present across all magazines. A single page might juxtapose an administrative report, an advertisement, and a humour section. Conventional analytical methods, such as yearly and decade-based topic models and term counts, do not adequately account for this heterogeneity and are likely biased towards the most frequent text types.\n\n\n\nSlide highlighting the challenges posed by diverse text types co-occurring on the same page in historical magazines.\n\n\nConsequently, genre emerged as a useful conceptual tool. In Language Technology, a genre is often defined as a class of documents sharing a communicative purpose (Petrenz2004?; Kessler1997?)—a definition well-suited to this project’s needs. Genre classification advances the key objective of exploring the material from multiple perspectives to formulate historical arguments. It facilitates the study of communicative strategies over time (Broersma2010?) across different countries, diseases, and publications. Furthermore, it enables a more fine-grained analysis of term distributions and topic models within specific genre groups.\n \nThe ActDisease data manifests a variety of genres. Examples include poetry, academic reports (such as studies on the pancreas), legal documents like deeds of covenant, advertisements (for instance, for chocolate aimed at diabetics), instructive messages like recipes or medical advice, reports from patient organisations detailing meetings and activities, and narratives recounting patients’ lives.\n\n\n\nExamples illustrating the variety of genres found within the ActDisease dataset, such as patient experiences, advertisements, and instructive texts.\n\n\n\n\n6.2.2 Zero-Shot and Few-Shot Classification\nGiven the scarcity of annotated data, investigations into zero-shot and few-shot learning approaches became paramount. For zero-shot learning, key research questions (RQs) arose:\n\nRQ1: Can genre labels from available public datasets be efficiently mapped to the project’s custom labels?\nRQ2: How does performance vary across different datasets and models?\n\nFor few-shot learning, the pertinent questions were:\n\nRQ3: How does performance change with different training set sizes across various models?\nRQ4: Does prior fine-tuning on the full dataset considerably enhance performance?\n\nThese explorations are detailed in (DanilovaSoderfeldt2025?).\n \nProject historians supervised the definition of genre labels, aiming for utility in content separation within the materials and general applicability for similar datasets. The table below outlines the genres and their brief definitions. For instance, the ‘Academic’ genre serves to convey information from the scientific medical community to the magazine’s audience, whilst ‘Administrative’ texts aim to report on and inform about patient organisation events and activities. Other common genres include advertisements, guidance, fiction, legal texts, news, non-fiction prose, and question-answer sections from the periodicals.\n\n\n\nTable defining the genres used in the classification experiments, such as Academic, Administrative, Advertisement, and Guide.\n\n\nThe annotation unit selected was the paragraph, specifically ABBYY-generated paragraphs merged by font pattern (type, size, bold, italic) within a page. Annotators sampled texts from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal”, focusing on the first and mid-year issues for each year. The annotation team comprised four historians and two computational linguists, all native or proficient in Swedish and German. Each paragraph received two annotations, achieving an average inter-annotator agreement of 0.95 Krippendorff’s alpha, indicating a high level of consistency.\n \nAn example of the annotation file structure reveals columns corresponding to genres, where annotators made hard assignments—a challenging but successfully executed task.\n\n\n\nExample screenshot of the annotation file used, showing paragraph metadata and columns for genre assignment.\n\n\nFor the experiments, the team first split the data into training and held-out sets, the latter constituting approximately 30% of the annotated data. For few-shot experiments, they divided the held-out set equally and balanced it by label. Legal and news genres were excluded from these particular experiments due to insufficient training data. The entire test set served for zero-shot experiments. Analysis of genre distribution across languages and genres in the training and held-out samples highlighted a strong imbalance in advertisement and non-fictional prose across languages.\n \nZero-shot experiments necessitated external datasets. Researchers utilised modern datasets from previous work on automatic web genre classification: the Corpus of Online Registers of English (CORE) and the Functional Text Dimensions (FTD) dataset, both annotated at the document level. Additionally, they used a sample from Universal Dependencies (UDM), which contains sentence-level annotations in multiple languages. Two annotators performed genre mapping independently. For the final mapping, only assignments with full agreement were chosen. For some ActDisease genres, no suitable labels existed in the available external datasets.\n \nThe pipeline for training data creation involved mapping, followed by preprocessing, chunking, and sampling in several configurations based on language family and label levels (both custom and original). Multilingual encoders, successfully employed in prior automatic genre classification work, formed the basis of these experiments. Models included XLM-RoBERTa (Conneau2020?), mBERT (Devlin2019?), and, of particular interest, historical mBERT (hmBERT) (Schweter2022?), which is pretrained on historical multilingual newspapers and includes the languages relevant to this study. BERT-like models have seen extensive use in previous web register and genre classification (LepekhinSharoff2022?; KuzmanLjubesic2023?; LaippalaEtAl2023?). XLM-RoBERTa stands as a state-of-the-art web genre classifier (KuzmanEtAl2023?). hmBERT’s pretraining on a large corpus of multilingual historical newspapers makes it highly relevant, whilst mBERT provides a comparison point for hmBERT, as it is not directly comparable with XLM-RoBERTa.\n\n\n\nList of multilingual encoder models used in the experiments: XLM-RoBERTa, mBERT, and historical mBERT.\n\n\nFine-tuning these models across all configurations yielded 48 distinct models. The metrics presented subsequently are averaged across these configurations.\n\n\n\nDiagram illustrating the creation of 48 fine-tuned models from four training set configurations (FTD, CORE, UDM, Merged) and three base models (XLM-RoBERTa, mBERT, hmBERT).\n\n\n\n6.2.2.1 Zero-Shot Learning Evaluation\n \nGiven that label sets do not perfectly overlap between datasets, the evaluation involved examining each individual genre and analysing confusion matrices to avert potential biases. As a baseline, a state-of-the-art web genre classifier served, considering only the most similar labels.\nOverall, fine-tuning on the Functional Text Dimensions (FTD) dataset, using the project’s specific mapping, yielded better performance. In most configurations, systematic bias was absent, and per-genre metrics proved quite good. Interestingly, on certain datasets, some models handled specific genres much more effectively than others on average. This was evident for XLM-RoBERTa, which predicted Question-Answer (QA) genres substantially better than other models when fine-tuned on UDM. Conversely, hmBERT fine-tuned on UDM showed superior performance for Administrative texts, and CORE-based models excelled at predicting Legal texts.\n\n\n\nOverview of zero-shot learning results, highlighting the better performance of models fine-tuned on FTD and specific model-dataset strengths for certain genres.\n\n\nThe confusion matrices for selected configurations illustrate this behaviour. For instance, hmBERT fine-tuned on UDM demonstrates strong performance for the ‘administrative’ genre. XLM-RoBERTa fine-tuned on CORE shows good results for ‘academic’ and ‘legal’ genres. When XLM-RoBERTa is fine-tuned on UDM, it performs well on ‘QA’. Finally, XLM-RoBERTa fine-tuned on FTD shows good classification of ‘legal’ texts.\n\n\n\nConfusion matrices for various model configurations in zero-shot learning, highlighting specific genre prediction strengths. (Top-left: hmbert_UDM, Top-right: xlmr_CORE, Bottom-left: xlmr_UDM, Bottom-right: xlmr_FTD)\n\n\nDetailed average F1 scores per category, averaged across data configurations, further substantiate these observations. Highlighted values in the table indicate performance that is not a result of systematic biases towards those categories. Models fine-tuned on FTD generally show strong F1 scores for the ‘legal’ genre. The hmBERT model fine-tuned on UDM shows a notable F1 score for ‘administrative’, and XLM-RoBERTa fine-tuned on UDM performs well for ‘QA’.\n \n\n\n6.2.2.2 Few-Shot Learning Evaluation\n \nTurning to few-shot learning, analysis reveals how models performed with varying data sizes, both with and without prior Masked Language Model (MLM) fine-tuning on the entire ActDisease dataset (indicated by -mlm). Prior MLM fine-tuning clearly offers an advantage. This pre-training boosted the performance for the historical model (hmBERT-mlm, the orange line in the graph), which even outperformed all other models by a small margin, particularly as dataset size increased. The F1 score generally keeps increasing with the number of training instances, though it remains below 0.8 even with 1182 instances.\n\n\n\nLine graph showing few-shot learning performance (F1 score vs. dataset size) for various models with and without MLM fine-tuning.\n\n\nExamination of detailed scores indicates that hmBERT-MLM’s superior performance stems largely from its ability, unlike other models, to maintain the distinction between fiction and non-fiction prose even with the full dataset size. Other models, especially XLM-RoBERTa, exhibit a drastic drop in performance for these genres under such conditions.\n\n\n\nTable of few-shot learning per-category F1 scores and overall metrics for different models (XLMR, XLMR-MLM, hmBERT, hmBERT-MLM, mBERT, mBERT-MLM) at training sizes of 500 and 1182 instances.\n\n\nThe confusion matrix for XLM-RoBERTa with prior MLM fine-tuning on the whole dataset (XLM-Roberta-MLM) shows that non-fiction prose is frequently over-predicted for fiction. Both genres, within the ActDisease data, contain narratives about patient experiences. All genres are confined to a specific domain: patient organisation magazines focused on diabetes. Both fictional and (auto)biographical texts frequently revolve around the experiences of diabetes patients and are thus likely to share themes and narrative structures. This suggests that with the full data size, these genres may become more similar, and more data might be necessary to improve performance differentiation.\n\n\n\nConfusion matrix for XLM-Roberta-MLM classification results with the full-sized training dataset, illustrating the confusion between fiction and non-fiction prose.\n\n\n\n\n\n6.2.3 Few-Shot Prompting Llama-3.1 8b Instruct\nDue to insufficient data for comprehensive instruction tuning at this stage, researchers performed few-shot prompting of Llama-3.1 8b Instruct, a popular multilingual generative model with open weights. The prompt structure included genre definitions and two to three carefully selected examples for each genre.\n\n\n\nExample of the prompt structure used for few-shot prompting of Llama-3.1 8b Instruct, including genre definitions and example placeholders.\n\n\nThe results indicate that Llama-3.1 8b Instruct handles certain labels fairly well, achieving a notable F1-score for ‘legal’ texts (0.84). However, only two or three examples proved insufficient to adequately represent more nuanced or diverse genres such as non-fictional prose, advertisement, and administrative texts, where performance was more modest. The overall macro average F1-score was 0.59.\n\n\n\nResults of few-shot prompting Llama-3.1 8b Instruct, including a table of F1-scores per genre and a confusion matrix.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#conclusion",
    "href": "ai-nepi_005_chapter.html#conclusion",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "6.3 Conclusion",
    "text": "6.3 Conclusion\nPopular magazines, rich in diverse genres that reflect evolving communicative strategies, represent a promising yet challenging source for historical research, particularly in the history of science. Accounting for these genres is crucial for the accurate and detailed interpretation of text mining results.\nGenre classification techniques can render these complex sources more accessible for text mining. In scenarios with no training data, leveraging available modern datasets for general-purpose categories or employing few-shot instruction of capable generative models are viable options. However, when some annotated data is available, few-shot learning of multilingual encoders like XLM-RoBERTa or, particularly, historical multilingual BERT (hmBERT) with prior MLM fine-tuning, emerges as a highly effective approach. For this project, hmBERT-MLM yielded the most significant gains.\n\n\n\nSummary of conclusions regarding the challenges of genre diversity in popular magazines and the effectiveness of different genre classification approaches.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html",
    "href": "ai-nepi_006_chapter.html",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "",
    "text": "7 Overview\nThe VERITRACE web application represents an early, or ‘alpha’, iteration of a sophisticated tool designed for scholarly research. This version, not yet publicly accessible, requires considerable development to realise its full potential; it currently serves more as a promise of future capabilities. Central to its ongoing development is the testing of a BERT-based Large Language Model, specifically LaBSE (Language-agnostic BERT Sentence Embedding), to generate vector embeddings. These embeddings aim to represent every passage within the project’s textual corpus, although initial assessments suggest this model may not ultimately prove sufficient for the complex demands of the research. It is important to recognise that static screenshots, such as those presented herein, offer a very limited substitute for direct interaction with the dynamic web application.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#introduction-to-the-veritrace-project",
    "href": "ai-nepi_006_chapter.html#introduction-to-the-veritrace-project",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.1 Introduction to the VERITRACE Project",
    "text": "7.1 Introduction to the VERITRACE Project\nThe VERITRACE project, a five-year initiative funded by an ERC Starting Grant, embarks on an ambitious journey to trace the intellectual currents flowing from the ‘ancient wisdom’ tradition into the burgeoning natural philosophy and science of the early modern period. This tradition manifests in a diverse collection of works, including texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for historians of chemistry, the Corpus Hermeticum.\n\n\n\nVERITRACE Project Title Slide\n\n\nResearchers have assembled a core corpus of 140 works that typify this ancient wisdom tradition, forming the basis for close textual analysis. Whilst the historical record confirms the influence of these ideas—Newton, for instance, read the Sibylline Oracles, and Kepler was familiar with the Corpus Hermeticum—VERITRACE seeks to delve deeper. The project aims to uncover a much broader and more intricate network of texts and authors who engaged with this tradition. Many of these works, often penned by lesser-known figures, constitute what one scholar has termed ‘the great unread’, having seldom been the primary focus of historical inquiry due to their sheer volume and relative obscurity. Illuminating these connections is a central objective of the project.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#computational-approaches-in-historical-research",
    "href": "ai-nepi_006_chapter.html#computational-approaches-in-historical-research",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.2 Computational Approaches in Historical Research",
    "text": "7.2 Computational Approaches in Historical Research\nTo address its research questions, the VERITRACE project pioneers large-scale, multilingual exploration within the domain of History, Philosophy, and Sociology of Science (HPSS). The team develops tools for keyword searching across its vast corpus. Furthermore, a significant component involves identifying instances of textual reuse. This encompasses both direct, lexical quotations—which may or may not be explicitly cited—and more indirect forms of influence, such as paraphrasing or subtle allusions that contemporary readers would have recognised as originating from specific sources, like the Corpus Hermeticum.\n\n\n\nVERITRACE Project Team and Goals\n\n\nEffectively, the project endeavours to construct an ‘early modern plagiarism detector’, capable of navigating a substantial multilingual collection of texts. Through this computational lens, researchers anticipate uncovering previously ignored networks of texts, passages, themes, topics, and authors. Such discoveries hold the potential to reveal new patterns and shed fresh light on the intellectual history and philosophy of science during this transformative period.\n\n\n\nComputational HPSS Aims for the VERITRACE Project",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-corpus-a-large-diverse-multilingual-data-set",
    "href": "ai-nepi_006_chapter.html#the-corpus-a-large-diverse-multilingual-data-set",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.3 The Corpus: A Large, Diverse Multilingual Data Set",
    "text": "7.3 The Corpus: A Large, Diverse Multilingual Data Set\nThe foundation of VERITRACE’s analytical work rests upon a large, diverse, and multilingual data set, focusing exclusively on printed books rather than handwritten manuscripts. This corpus draws from three primary data sources and encompasses texts in at least six different languages, with publication dates spanning nearly two centuries, from 1540 to 1728—a period concluding shortly after Newton’s death.\nThe principal data sources include:\n\nEarly English Books Online (EEBO)\nGallica, providing access to digitised materials from the French National Library\nThe Bavarian State Library, which constitutes the largest single contributor to the corpus\n\nCollectively, these sources provide approximately 430,000 texts. To analyse this extensive collection, the project employs an array of state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, amongst others.\n\n\n\nLarge, Diverse Multilingual Data Set for VERITRACE",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "href": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.4 Core Challenges and the Role of Large Language Models",
    "text": "7.4 Core Challenges and the Role of Large Language Models\nThis ambitious undertaking naturally encounters several core challenges. The variable quality of Optical Character Recognition (OCR) in the texts, provided directly by libraries in raw formats (such as XML, HOCR, or even HTML files) without ground truth page images, presents a fundamental hurdle affecting all downstream processing. Additionally, the complexities of early modern typography and semantics, compounded by the multilingual nature of the corpus (spanning at least six languages), demand sophisticated handling. The sheer volume of data—hundreds of thousands of texts printed across Europe over two centuries—also poses significant logistical and computational challenges.\nLarge Language Models (LLMs) play a crucial role in addressing some of these complexities. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, acting as ‘judges’ in this process. However, the primary focus of this discussion lies with the encoder side: the utilisation of BERT-based LLMs to generate embeddings. These embeddings encode the semantic meaning of sentences or short passages (groups of sentences) within the textual corpus, forming the bedrock for advanced textual analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#a-brief-look-at-llms-as-judges-for-metadata-enrichment",
    "href": "ai-nepi_006_chapter.html#a-brief-look-at-llms-as-judges-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.5 A Brief Look at LLMs as Judges for Metadata Enrichment",
    "text": "7.5 A Brief Look at LLMs as Judges for Metadata Enrichment\nOne application of LLMs within VERITRACE involves their use as ‘judges’ to enrich bibliographic metadata. The basic motivation stems from the desire to map records from high-quality sources like the Universal Short Title Catalogue (USTC) onto the project’s own records. Successful mapping creates enriched metadata that is less likely to require manual cleaning. Whilst some mapping can be automated using external identifiers, many records lack such links, and the project’s data often requires cleaning before effective matching can occur.\n\n\n\nCase Study: LLMs as Judges to Enrich VERITRACE Metadata\n\n\nTo tackle this, researchers employ a panel, or ‘bench’, of LLMs. These models operate under extensive prompt guidelines to compare pairs of bibliographic records—a task exceedingly tedious for human team members, who previously faced reviewing tens of thousands of such pairs. The LLMs determine whether records match and provide reasoning for their decisions.\n\n\n\nAttempt 1: Matching Gallica and USTC Records using Fuzzy Matching and LLM Review\n\n\nThis endeavour remains a work in progress. A major current challenge is the propensity of the open-source models (such as Llama and its variants) to produce hallucinations in their output. Attempts to mitigate this by requesting more structured output often lead to more generic and less helpful responses, particularly in the reasoning provided. Achieving a balance between structured output and insightful reasoning is an ongoing refinement. Despite these hurdles, the potential for LLMs to save considerable time in metadata enrichment remains significant, and further development continues.\n\n\n\nPrompt Guidelines and an Example of LLM Output for Metadata Matching",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-veritrace-web-application-an-initial-glimpse",
    "href": "ai-nepi_006_chapter.html#the-veritrace-web-application-an-initial-glimpse",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.6 The VERITRACE Web Application: An Initial Glimpse",
    "text": "7.6 The VERITRACE Web Application: An Initial Glimpse\nThe VERITRACE web application, a new development shared publicly for the first time here, represents the project’s primary interface for textual exploration and analysis. It is crucial to understand that this is an ‘alpha’ version, not yet available for public use and currently residing only on a local machine. It requires substantial further work and should be viewed as an indication of the project’s aspirations rather than a finished product.\n\n\n\nVERITRACE Web Application Overview and Current Status\n\n\nCurrently, the team is testing a BERT-based LLM, LaBSE (Language-agnostic BERT Sentence Embedding), to generate the vector embeddings that underpin semantic analysis within the application. However, early indications suggest that this particular model may not be sufficiently robust for the nuanced requirements of the historical texts in the VERITRACE corpus. The screenshots presented offer only a static snapshot; the true utility of such a tool can only be appreciated through interactive use.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-intricate-data-processing-pipeline",
    "href": "ai-nepi_006_chapter.html#the-intricate-data-processing-pipeline",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.7 The Intricate Data Processing Pipeline",
    "text": "7.7 The Intricate Data Processing Pipeline\nTransforming raw textual data from its original formats (XML, HOCR, HTML files) into a structured and searchable Elasticsearch database, which serves as the backend for the VERITRACE web application, involves a considerable amount of preparatory work. This is not a simple push-button operation; rather, it entails a complex, 15-stage data processing pipeline.\n\n\n\nVERITRACE Data Processing Pipeline Dashboard and Stages\n\n\nEach stage in this pipeline—from extracting text into files, generating character position mappings, segmenting documents, to assessing OCR quality—demands careful optimisation. Numerous background processes must execute flawlessly to prepare the data for scholarly analysis. The generation of vector embeddings, crucial for semantic matching, occurs towards the end of this intricate pipeline. Each step could itself be the subject of extensive refinement.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#navigating-the-features-of-the-veritrace-web-application",
    "href": "ai-nepi_006_chapter.html#navigating-the-features-of-the-veritrace-web-application",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.8 Navigating the Features of the VERITRACE Web Application",
    "text": "7.8 Navigating the Features of the VERITRACE Web Application\nThe VERITRACE web application organises its functionalities into roughly five main sections, designed to facilitate comprehensive exploration and analysis of the corpus.\n\n7.8.1 Explore Section: Corpus Insights and Metadata\nThe ‘Explore’ section provides users with statistical overviews of the corpus. For instance, current metadata records, drawn directly from a MongoDB database, number 427,395, describing the entirety of the collected books. This area allows users to gain a broad understanding of the corpus’s composition.\n\n\n\nVERITRACE Explore Section Interface with Corpus Statistics\n\n\nWithin this section, the ‘Metadata Explorer’ enables users to browse and filter documents based on their rich metadata attributes. One notable feature is the detailed language information. Language identification algorithms run on every text, analysing segments as small as approximately 50 characters. This granularity is vital because many early modern texts are multilingual, often containing sections in Greek or other languages alongside the primary Latin, for example. The system can identify such instances, noting, for example, that a text comprises 15% Greek and 85% Latin, classifying it as substantively multilingual.\nFurthermore, the application attempts to assess OCR quality on a page-by-page basis. This is a challenging task without access to ground truth page images, relying instead on analysis of the raw text. Nevertheless, providing some level of quality assessment for each page, rather than a single score for an entire book, is a key objective.\n\n\n7.8.2 Search Section: From Keywords to Complex Queries\nMost scholars will likely gravitate towards the ‘Search’ section for initial investigations. This feature supports standard keyword searches. A prototype, operating on just 132 files from the total corpus, already demonstrates the scale: a search for “Hermes” yields 22 documents with 332 total matches, and the index for this small subset alone occupies 15 gigabytes. Extrapolating to the full corpus of over 400,000 texts suggests the final index will reach terabyte scales.\n\n\n\nVERITRACE Search Section Interface with Example Queries\n\n\nBeyond basic keywords, the underlying Elasticsearch engine empowers much more sophisticated queries. Users can perform field-specific searches, such as locating all books by Kepler that also contain the keyword “Hermes”. The system also supports complex Boolean queries with ‘AND’ and ‘OR’ operators, nested queries, and proximity searches. The latter allows for nuanced investigations, for example, finding all texts where “Hermes” and “Plato” are mentioned within ten words of each other.\n\n\n7.8.3 Analyse Section: Future Capabilities\nThe ‘Analyse’ section of the website is currently under development but will house advanced analytical tools. Planned features include:\n\nTopic modelling to discover thematic structures within the corpus or selected document sets.\nLatent Semantic Analysis (LSA) for exploring document similarity.\nDiachronic analysis to visualise linguistic and conceptual shifts over time. Insights from the wider research community actively inform the development of these analytical capabilities.\n\n\n\n\nVERITRACE Analyse Section (Future Implementation)\n\n\n\n\n7.8.4 Read Section: Accessing Digital Facsimiles\nRecognising that scholars often need to consult original textual sources, the ‘Read’ section provides access to PDF facsimiles of every text in the corpus. An integrated Mirador viewer allows users to read these documents directly within the web application, much like interacting with digital collections on library websites. This feature ensures that close reading of the source material can occur alongside computational analysis, with relevant metadata readily available.\n\n\n\nVERITRACE Read Section with Integrated Mirador Viewer",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#unveiling-textual-connections-the-match-functionality",
    "href": "ai-nepi_006_chapter.html#unveiling-textual-connections-the-match-functionality",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.9 Unveiling Textual Connections: The Match Functionality",
    "text": "7.9 Unveiling Textual Connections: The Match Functionality\nThe ‘Match’ section of the VERITRACE web application is central to the project’s aim of identifying textual reuse and influence. Users can specify a query text and a comparison text (or set of texts). The system supports comparisons between single documents, multiple documents (e.g., all of Kepler’s works in the database), and even a single text against the entire corpus. The latter, whilst a powerful goal, presents considerable computational challenges regarding processing time for the user.\n\n\n\nVERITRACE Match Interface for Comparing Documents, Illustrated with Newton’s Opticks\n\n\nA key design principle is to expose underlying parameters to the user. Text matching is not a one-size-fits-all process; numerous parameters can and must be tweaked to achieve optimal results for different research questions. Users can, if they wish, adjust settings such as the minimum similarity score to observe how results change.\n\n7.9.1 Types and Modes of Matching\nThe application offers several approaches to textual comparison:\n\nLexical Matching: This method uses keyword matching to find passages with similar vocabulary. It is effective for identifying direct textual reuse within the same language but struggles with translations or paraphrases.\nSemantic Matching: Employing vector embeddings, this approach seeks conceptually similar passages, even if they share little common vocabulary. This is particularly vital for a multilingual corpus where ideas may be expressed differently across languages.\nHybrid Matching: This combines lexical and semantic techniques, allowing for weighted contributions from each to identify a broader range of connections.\n\nUsers can also select a ‘Matching Mode’:\n\nStandard: Offers a balance between performance and accuracy.\nComprehensive: Aims for maximum recall of potential matches, though this is computationally more intensive and slower.\nSelective (Faster): Prioritises higher precision, potentially yielding fewer results but executing more quickly.\n\n\n\n7.9.2 Sanity Checks: Testing the Matching Algorithms\nTo evaluate the matching capabilities, researchers performed several ‘sanity checks’ using Isaac Newton’s Opticks, specifically comparing the Latin edition of 1719 with the English edition of 1718.\n\nLexical Match Across Languages (Latin vs. English Opticks): When performing a lexical match between these two texts in different languages, the expectation is to find no significant matches. Using the ‘Standard’ mode, the system indeed found no matches, aligning with this expectation. Interestingly, switching to ‘Comprehensive’ mode did yield three matches, all in English. This suggests that the Latin edition likely contains some English text (perhaps in a preface or notes), which the more sensitive comprehensive mode correctly identified. This outcome serves as a useful validation of the different modes’ behaviours.\n\n\n\nSanity Check 1: Results of a Lexical Match Across Languages (Newton’s Opticks)\n\n\nLexical Match of a Text to Itself (English Opticks vs. English Opticks): When a text is lexically matched against itself, a high degree of similarity is expected. The interface displays a match summary, including a quality score, average similarity of matches, and a coverage score (indicating how much of the text is involved in matches). The system also provides automatic highlighting of the query passage on the left and the comparison passage on the right, along with the similarity score for each pair. In this self-comparison, the results demonstrated the expected high similarity.\n\n\n\nSanity Check 2: Summary Statistics for a Text Lexically Matched to Itself (Newton’s Opticks)\n\n\nSemantic Match Between a Text and Its Translation (Latin vs. English Opticks): For a semantic match between a text and its translation, one would anticipate finding numerous conceptually similar passages, even though their vocabularies differ. The results of this test appeared reasonable. Despite OCR imperfections, the system identified passages discussing similar concepts, such as ‘colours’, across the two language versions. This demonstrates the potential of semantic matching to bridge linguistic divides.\n\n\n\nSanity Check 3: Setting up a Semantic Match Between Newton’s Opticks (Latin) and its English Translation\n\n\nThe detailed comparison view allows users to explore these semantic connections side-by-side.\n\n\n\nSemantic Matches Displayed: Latin Optice Query Passage on Left, English Opticks on the Right\n\n\nHowever, analysis of the summary statistics for this semantic match reveals areas for further investigation. Whilst the quality score (average similarity of matches) appears reasonable and high, the coverage score might seem low. This could indicate limitations in the current embedding model (LaBSE) or, alternatively, reflect genuine substantive differences between the Latin and English editions, as the Latin edition is reportedly longer. Other queries using the LaBSE model have suggested it may be inadequate for the specific domain of historical texts, potentially suffering from ‘out-of-domain model collapse’.\n\n\n\nSummary Statistics for the Semantic Match Between Newton’s Opticks (Latin vs. English)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#issues-on-the-horizon-and-future-directions",
    "href": "ai-nepi_006_chapter.html#issues-on-the-horizon-and-future-directions",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "7.10 Issues on the Horizon and Future Directions",
    "text": "7.10 Issues on the Horizon and Future Directions\nAs the VERITRACE project progresses, several critical issues and future directions require careful consideration to achieve its ambitious goals.\n\n\n\nKey Issues and Future Considerations for the VERITRACE Project\n\n\nThe choice of an appropriate vector embedding model is paramount. LaBSE, selected initially for its speed and relatively modest storage requirements, appears insufficient for the nuanced demands of the historical corpus. Researchers are exploring alternatives such as XLM-Roberta, intfloat multilingual-e5-large, or various historical mBERT models. Each of these presents its own trade-offs between accuracy, storage footprint, and inference time. A fundamental question is whether to persist with pre-trained models or to invest in fine-tuning a base model specifically on the VERITRACE historical corpus, given its distinct characteristics.\nThe phenomenon of semantic meaning changing over time poses another significant challenge. How can a single model effectively capture and compare concepts expressed in texts published centuries apart (e.g., from 1540 versus 1700) and across different languages? Ensuring that texts from disparate periods and linguistic backgrounds are meaningfully represented within the same vector space is a complex problem.\nPoor-quality OCR continues to be a pervasive issue, affecting all downstream processes. If the raw text is flawed, accurate segmentation into sentences and passages becomes difficult, undermining the foundation for reliable analysis. Re-OCRing the entire corpus of 430,000 texts is not feasible. Potential strategies include selectively re-OCRing only the texts with very poor quality or investing time in finding existing higher-quality digital versions of these works.\nFinally, scaling and performance will increasingly become critical concerns. The current prototype, operating on only 132 texts, already sees query times of around 15 seconds for certain operations. Extrapolating these performance characteristics to the full corpus of 430,000 texts highlights the immense computational and optimisation challenges that lie ahead.\nThe VERITRACE team actively seeks advice and collaboration from the wider research community to address these multifaceted issues and to realise the full potential of computational methods in exploring the rich tapestry of early modern intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html",
    "href": "ai-nepi_007_chapter.html",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nUnderstanding the internal workings of complex machine learning models, particularly Large Language Models (LLMs), presents a significant challenge. A primary goal in explainable Artificial Intelligence (AI) involves identifying the input features most salient to a model’s predictions and, crucially, how these features interact. Initial approaches often focus on first-order effects, pinpointing individual features—such as specific pixels in an image or words in a text—that contribute to a classifier’s output. For instance, in analysing historical documents, a model might learn to associate the presence of numerical data with the classification of a page as a table. This chapter explores the evolution of interpretability techniques, moving from these foundational methods towards more nuanced understandings of feature interactions and their application in generating scientific insights, especially within the humanities.\nA brief introduction to explainable AI (XAI) clarifies what the machine learning community generally considers an explanation. This foundation is essential for appreciating the subsequent discussion on transparency, applications, and scientific insights derived from LLMs in humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "ai-nepi_007_chapter.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI (XAI) 1.0: Feature Attributions",
    "text": "7.1 Explainable AI (XAI) 1.0: Feature Attributions\nMachine learning historically concentrated on visual data, primarily images. Only in the last decade or so has the field turned more substantial attention towards language, although earlier work certainly exists. The significant shifts in language-focused AI are relatively recent. To comprehend the decision-making processes of ‘black box’ machine learning models, researchers typically examined classification tasks. Given an input, such as an image containing a specific object, a model might yield a correct prediction. However, users often remain unaware of the basis for this classification.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’.\n\n\nThe domain of explainable AI has dedicated considerable research, spanning roughly a decade, to understanding and tracing the origins of model predictions. In the context of image classification, this often involves generating a heatmap. Such a heatmap highlights the pixels most responsible for a given prediction. For example, it could visually demonstrate why a model recognised a rooster in an image.\n\n\n\nDiagram illustrating a black box AI system classifying an image of a rooster. Input x (rooster image) goes into a Black Box AI System, resulting in prediction f(x) (Rooster). Citation: Samek et al. (2017).\n\n\nBeyond simply understanding individual predictions, the broader imperative for explainability serves several critical functions. Firstly, it allows for the verification of predictions, ensuring that the model operates on a reasonable basis. Secondly, explainability aids in correcting errors and understanding how models make mistakes. Thirdly, it can reveal surprising solutions or patterns, thereby contributing to learning about the underlying problem itself. Increasingly, explainability is also vital for ensuring compliance with regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-shift-towards-generative-ai",
    "href": "ai-nepi_007_chapter.html#the-shift-towards-generative-ai",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.2 The Shift Towards Generative AI",
    "text": "7.2 The Shift Towards Generative AI\nThe landscape of AI has undergone a transformation with the advent of Generative AI (GenAI). Previously, the standard scenario involved classification models. Now, models possess multifaceted capabilities: they can classify, find similar images, generate new images, and answer questions on a vast array of topics. This versatility makes it considerably more challenging to ground an LLM’s prediction or answer in specific input features.\n\n\n\nDiagram contrasting classification models with generative AI. An input image of a rooster feeds into a Black Box AI System, which can now produce varied outputs like ‘rooster’, ‘similar images’, ‘generate examples’, ‘Q&A’. Text highlights ‘beyond heatmaps’, ‘feature interactions’, ‘mechanistic view’. Today’s foundation models are described as multi-task and world models. Citation: Samek et al. (2017).\n\n\nConsequently, there is a need to move beyond heatmap representations. Future interpretability methods must consider feature interactions and adopt more mechanistic perspectives. Today’s foundation models function not merely as task-specific tools but as models of the world, capable of reflecting societal nuances and the evolution of textual features over time. This capacity makes them particularly interesting for humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#why-scrutiny-of-models-remains-essential",
    "href": "ai-nepi_007_chapter.html#why-scrutiny-of-models-remains-essential",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.3 Why Scrutiny of Models Remains Essential",
    "text": "7.3 Why Scrutiny of Models Remains Essential\nAI models, despite their advancements, can and do make surprising errors. Two well-known examples illustrate this fallibility. In one instance, a standard object classifier identified a boat based on the surrounding water—a correlated, easier-to-detect textural feature—rather than the boat itself (Lapuschkin2019?). More recently, LLMs have demonstrated mistakes in multi-step planning tasks. For example, when tasked with the Tower of Hanoi puzzle and asked to predict the next move, an LLM might incorrectly attempt to move the largest, inaccessible disk, thereby failing to grasp the problem’s physical constraints (MondalWebb2024?).\n\n\n\nSlide titled ‘Models can make mistakes’. Left: Object detection example showing a sailboat and a heatmap indicating water as a key feature (Lapuschkin et al., Nat Commun ’19). Right: Multi-step planning example showing an LLM making an incorrect move in the Tower of Hanoi game (Mondal & Webb et al., arxiv ’24).\n\n\nAlthough more recent reasoning models may perform better, these examples, observed in relatively standard models, underscore the continued need for robust interpretability methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#advancing-interpretability-xai-2.0-and-structured-approaches",
    "href": "ai-nepi_007_chapter.html#advancing-interpretability-xai-2.0-and-structured-approaches",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.4 Advancing Interpretability: XAI 2.0 and Structured Approaches",
    "text": "7.4 Advancing Interpretability: XAI 2.0 and Structured Approaches\nTo address the limitations of earlier methods, particularly in the context of complex models like LLMs, the field is progressing towards XAI 2.0, emphasising structured interpretability. This involves moving beyond simple heatmaps to explore more sophisticated relationships within the data and the model.\n\n\n\nSlide titled ‘XAI 2.0’ with subtitle ‘Structured Interpretability’.\n\n\n\n7.4.1 First-Order Explanations\nFirst-order explanations, often visualised as heatmaps, remain useful for understanding classifiers. Researchers applied these to a table classifier designed to distinguish subgroups within historical data, specifically historical tables. To verify that the classifier operated meaningfully, heatmaps helped confirm that predictions were based on relevant features. Indeed, the model correctly focused on numerical content, a reasonable proxy for identifying a numerical table.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ with a diagram of four circles (x1 highlighted) and ‘classifier predictions’ with images of historical tables.\n\n\n\n\n7.4.2 Second-Order and Higher-Order Interactions\nBeyond individual features, pairwise relationships, or second-order features, offer deeper insights. When examining the similarity between embeddings of two items (e.g., images or tables), explaining the resulting similarity score benefits from an interaction-based representation. This approach can reveal, for instance, that the perceived similarity between two historical tables stems from interactions between specific corresponding digits, confirming the model’s intended function.\nFurther advancements explore higher-order interactions, particularly within graph structures. In contexts such as citation networks or networks of books, where entities are classified, subgraphs or sets of features (feature walks) often become relevant collectively. Identifying these complex interactions can lead to more sophisticated insights into model behaviour, approaching a circuit-level understanding.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’ showing three columns: ‘first-order’ (classifier predictions), ‘second-order’ (pairwise relationships), and ‘higher-order’ (graph structure), each with a diagram and example images.\n\n\nThese evolving interpretability methods provide a richer understanding of how models arrive at their conclusions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "href": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.5 Applications in Language and the Humanities",
    "text": "7.5 Applications in Language and the Humanities\nThe principles of structured interpretability find potent applications in analysing language and informing research in the humanities.\n\n7.5.1 First-Order Attributions in LLMs\n\n\n\nSlide titled ‘First-Order Attributions in LLMs’.\n\n\nA standard application for first-order attributions involves sentiment prediction, often using movie reviews. After training a model on such data, researchers can employ heatmap-like techniques, adapted for transformers, to determine which parts of a review contribute most to its sentiment classification. Such analyses can uncover biases; for example, reviews containing male Western names (e.g., Lee, Barry, Raphael, Cohen brothers) might be more likely classified as positive, whilst those with foreign-sounding names (e.g., Saddam, Castro, Chan) might skew negative. Explainable AI proves highly effective in detecting these fine-grained biases within models.\nInvestigations into long-range dependencies in LLMs offer another avenue for first-order attributions. When an LLM generates a summary for a long text (e.g., a Wikipedia article with up to an 8,000-token context window), one can analyse where the information in the summary originates. Studies, such as those by (Jafari2024?), reveal that models tend to prioritise information presented later in the context. While they can access information from the beginning of a long context, they do so much less frequently. This tendency is important to recognise: an LLM-generated summary may not be balanced, often favouring more recently presented data.\n\n\n\nSlide titled ‘Ex 1b: First-Order Attributions for Long-Range Dependencies in LLMs’. Setup: Generating text summaries for long inputs. Results: Example text showing token dependencies, with later context often prioritised. Citation: Jafari et al., MambaLRP (NeurIPS ’24).\n\n\n\n\n7.5.2 Second and Higher-Order Interactions in Text\n\n\n\nSlide titled ‘Second & Higher-Order Interactions in Text’.\n\n\nExamining second and higher-order interactions provides further clarity. Consider a pair of sentences, such as “A cat I really like. It is a great cat.” A model like Sentence-BERT can produce an embedding and a similarity score. However, the reason for this specific score often remains obscure. Second-order explanations can generate interaction scores between tokens, revealing why the model deemed the sentences similar. Frequently, these explanations highlight noun-matching strategies (synonyms or identical nouns), noun-verb pairings, and interactions involving separator tokens.\nThese findings suggest that models, when forced to compress extensive information, may ultimately rely on relatively simplistic strategies—a “bag of token types” approach—rather than intricate linguistic understanding. This insight is valuable for those embedding data and computing rankings based on similarity.\n\n\n7.5.3 Graph Neural Networks for Structured Language Predictions\nGraph Neural Networks (GNNs) offer a framework for encoding structural information, and their operations can be conceptualised in ways similar to LLMs, where attention mechanisms dictate message passing between tokens. This parallel allows for the application of GNN-based interpretability methods to language.\n\n\n\nSlide titled ‘Graph Neural Networks for Structured Predictions’, showing an input graph and a model process diagram with interaction and prediction stages.\n\n\nAttributions in GNNs can manifest as “walks,” or interactions of features. Standard first-order explanations may fail to capture the complexity of language, such as negation. For instance, in the sentence “First, I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen,” a simple explanation might assign a high positive score due to “like,” missing the negation. Higher-order explanation methods, however, can correctly identify the negative sentiment of the first clause (“First I didn’t like the boring pictures”) whilst appropriately evaluating the positive sentiment of the second clause, thus capturing the hierarchical structure more accurately (Schnake2022?). Natural language’s hierarchical nature lends itself well to graph structures, making GNNs (or LLMs trained on sentiment tasks with walk extraction) suitable for such analyses.\n\n\n\nSlide titled ‘Ex 3: Interaction of nodes learns complex language structure’. Setup: Hierarchical structure in language suited to graphs. Example sentence with highlighted positive/negative parts, comparing standard (BoW) and higher-order explanations. Citation: Schnake et al. (TPAMI ’22).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#ai-based-scientific-insights-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.6 AI-based Scientific Insights in the Humanities",
    "text": "7.6 AI-based Scientific Insights in the Humanities\nThe application of AI, particularly explainable AI, extends into generating novel scientific insights within the humanities, moving beyond mere technical analysis to active knowledge discovery.\n\n\n\nSlide titled ‘B. AI-based Scientific Insights in the Humanities’.\n\n\n\n7.6.1 Extracting Visual Definitions from Corpora\nAn initial foray into this area involved using heatmap-based approaches to analyse a corpus of mathematical instruments. Researchers developed a classifier to categorise images as, for example, a “machine” or a “mathematical instrument.” In collaboration with historians, including Matteo Valleriani and Jochen Büttner, they scrutinised these visual definitions to determine if AI could offer more objective categorisation criteria (Valleriani2019?). This process requires close collaboration with domain experts to ensure the meaningfulness of AI-derived definitions. For instance, the analysis revealed that fine-grained scales on mathematical instruments were highly relevant features for the model’s decisions (ElHajjEberle2023?).\n\n\n\nSlide titled ‘Ex 4: Extracting visual definitions from corpora’. Left: Examples of historical mathematical instruments and machines. Right: Class-specific heatmap explanations showing relevant visual features for ‘math. instrument’, ‘machine’, etc. Citation: El-Hajj & Eberle+ (Int J Digit Humanities ’23).\n\n\n\n\n7.6.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA more extensive project focused on numerical tables from the Sphaera corpus, comprising early modern texts from 1472 to 1650 (Valleriani2019?; Eberle2024a?). Historians sought an automated method for matching tables with similar semantics, a task previously unfeasible at scale due to the sheer volume and complexity of the data.\n\n\n\nSlide titled ‘Ex 5: Corpus-level analysis of early modern astronomical tables’, showing examples of historical astronomical tables from the Sphaera Corpus and Sacrobosco Table Corpus.\n\n\nThis collaboration led to the development of a workflow designed to aid historians in gaining insights at scale. The concept of the “XAI-Historian” emerged—an historian equipped with AI and XAI tools to discover case studies and engage in more data-driven hypothesis generation. Instead of applying large foundation models directly to this out-of-domain data (which proved ineffective), researchers trained a smaller, specialised model capable of detecting numerical bigrams (pairs of adjacent numbers) within the tables.\n\n\n\nSlide titled ‘Ex 5: Historical insights at scale: xAI-Historian’. Setup: Historical tables as carriers of knowledge; Sacrobosco Corpus; ML challenges. Diagram shows workflow: data collections -&gt; atomization-recomposition (input table, bigram map, histograms) -&gt; corpus-level analysis (embeddings, data similarity). Citation: Eberle et al. (Sci Adv ’24).\n\n\nThe reliability of this bigram model was verified by checking if it consistently detected the same bigrams in corresponding inputs (e.g., “38” and “38” in two related tables). This verification step engendered trust in the model’s decisions, allowing its effective use for larger-scale analysis (Eberle2022?; Eberle2024b?).\n\n\n\nSlide titled ‘Verifying modeling and features using XAI and Historians’. Setup: Tables represented by bag of bigrams. Results: Scatter plot of table embeddings. Bigram Model: Images of tables with corresponding bigrams highlighted. Expert Ground Truth: Tables and charts showing histogram correlation and cluster classification purity. Citations: Eberle et al. (TPAMI ’22), Eberle et al. (Sci Adv ’24).\n\n\n\n\n7.6.3 Investigating Innovation Spread with Cluster Entropy\nArmed with these tools, researchers conducted case studies. One such study employed cluster entropy to investigate the spread of innovation across Europe during the early modern period. Publishing locations (cities) each produced a certain “programme” of printed table types. Some cities exhibited diverse outputs, whilst others focused on reprinting existing materials. Analysing this at scale was previously intractable.\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’, showing a map of Europe with Sphaera publication cities. Left side details setup and results with table clusters for Lisbon, Venice, Wittenberg. Citation: Eberle et al. (Sci Adv ’24).\n\n\nA clustering approach, based on the representations derived from the bigram model, quantified the diversity of each city’s print programme using entropy. A low entropy score indicated repetitive content, whereas a high entropy score signified a more diverse output. This analysis identified Frankfurt and Wittenberg as having particularly low entropy scores. Frankfurt am Main was already known as a centre for reprinting editions. The case of Wittenberg proved more intriguing: political control by Protestant reformers actively limited the print programme, dictating the curriculum to be published. This AI-driven analysis detected this historical anomaly, which aligned with existing historical knowledge and intuition (Eberle2024b?).\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Bar chart showing H(p) - H(pmax) for various cities, with Frankfurt/Main and Wittenberg highlighted. Explanations: Frankfurt (reprinting centre), Wittenberg (political control). Citation: Eberle et al. (Sci Adv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "ai-nepi_007_chapter.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.7 Conclusion: AI-based Methods for the Humanities",
    "text": "7.7 Conclusion: AI-based Methods for the Humanities\nHumanities and Digital Humanities (DH) researchers have largely concentrated on digitising source material. However, the automated analysis of these corpora is far from trivial, often hindered by data heterogeneity and a scarcity of labels. Multimodality presents further complexities.\n\n\n\nSlide titled ‘Conclusion - AI-based methods for the Humanities’ with bullet points summarizing challenges and opportunities.\n\n\nMachine learning, coupled with XAI, offers the potential to scale humanities research and cultivate novel research directions. Foundation Models and LLMs, through prompting, can assist with intermediate tasks such as labelling, data curation, and error correction. Nevertheless, for more complex research questions, their utility remains limited, particularly when dealing with low-resource data—a significant roadblock due to scaling laws. Furthermore, out-of-domain transfer, especially for historical and small-scale datasets, requires thorough evaluation. Current LLMs are primarily trained and aligned for natural language tasks and code generation, which may not directly translate to the nuanced requirements of many humanities research endeavours. Continued development and careful application of these AI tools, in close collaboration with domain experts, will be crucial for unlocking their full potential in the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html",
    "href": "ai-nepi_008_chapter.html",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nCurrent advancements in large language models (LLMs) present both opportunities and significant challenges for their application in the nuanced domains of history, philosophy, and sociology of science (HPSS). Whilst these models offer powerful text processing capabilities, they inherently lack critical faculties essential for scholarly rigour. This chapter explores the deficiencies of existing LLMs and proposes a framework centred on validation and curated scholarly resources to harness AI for deeper, more reliable insights into scientific inquiry and its historical development. The central argument posits that to truly model science, we require systems that move beyond pattern recognition towards genuine epistemic agency.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#navigating-the-deficiencies-of-contemporary-language-models",
    "href": "ai-nepi_008_chapter.html#navigating-the-deficiencies-of-contemporary-language-models",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Navigating the Deficiencies of Contemporary Language Models",
    "text": "8.1 Navigating the Deficiencies of Contemporary Language Models\nLarge Language Models, in their current state, exhibit several fundamental shortcomings when considered for sophisticated scholarly tasks. A primary concern involves their propensity for “hallucination”—generating plausible yet incorrect information. Effectively, an adversarial mechanism to counter such fabrications remains absent. Furthermore, the embedding vectors that underpin LLM operations do not equate to the meanings of expressions in a human, contextual sense. Consequently, there is a risk that these models might formulate statements that sound coherent but are, in fact, false.\n\n\n\nModelling Science: LLM for HPSS\n\n\nA significant issue arises from the tendency of LLMs to repeat information disseminated across internet media without critical assessment. Such repetition does not constitute knowledge, nor does it align with the objectives for LLM contributions in academic research. Instead of merely echoing prevalent narratives, a system for scholarly use should actively seek what is best justified. Moreover, current models possess no inherent capacity to devise plans for scientific inquiry or to strategise research. These limitations are not trivial; they represent core functionalities that today’s technologies show no immediate promise of achieving. This raises a crucial question: how can we cultivate hope for more epistemically sound AI?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#championing-validation-in-scientific-inquiry",
    "href": "ai-nepi_008_chapter.html#championing-validation-in-scientific-inquiry",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 Championing Validation in Scientific Inquiry",
    "text": "8.2 Championing Validation in Scientific Inquiry\nAddressing the identified gaps in LLM capabilities necessitates a focus on validation. Indeed, validation emerges as a critical, perhaps paramount, requirement. Validation, in this context, involves furnishing reasons and arguments that support or contest the truth of a proposition. It extends to providing evidence for or against a proposition’s veracity and offering justifications for or against pursuing particular actions.\n\n\n\nKey elements missing from current AI models for science\n\n\n\n8.2.1 Defining Computational Epistemology\nTo systematically address the challenge of validation, a new discipline becomes essential. “Computational Epistemology” is proposed as this new subject, dedicated to developing the methods and methodologies required to integrate robust validation processes into AI systems. This endeavour aims to cultivate epistemic agency within these systems. Such agency would empower them to identify propositions that transcend mere sentences, discern arguments within diverse texts and historical sources, and recognise the intentions, plans, and actions of individuals as documented in historical records.\n\n\n\nThe central role of validation and epistemic agency",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#implementing-validated-ai-assisted-research",
    "href": "ai-nepi_008_chapter.html#implementing-validated-ai-assisted-research",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 Implementing Validated AI-Assisted Research",
    "text": "8.3 Implementing Validated AI-Assisted Research\nThe practical application of these principles can transform how researchers conduct historical inquiries. Consider a working environment designed for such purposes, where AI assists in navigating complex historical questions.\n\n8.3.1 A Case Study: The Sanssouci Waterworks\nAn illustrative example involves the construction of the water fountain at Sanssouci castle, a project undertaken by Frederick the Great. A long-standing debate in the history of science concerns the involvement of Leonhard Euler, one of the 18th century’s foremost mathematicians. Specifically, questions persist regarding whether his participation contributed to the project’s ultimate failure—one of the most significant construction failures of that era—or if responsibility lay elsewhere. Historians of science continue to grapple with assigning accountability.\nWithin an AI-enhanced research environment, an investigator can pose a precise query, such as: “Reconstruct which persons carried out which work on the water fountain.” The objective is to receive a validated, qualified answer, one grounded in proven evidence and demonstrably correct, rather than mere hearsay. The system, in response, can generate a detailed list of individuals involved, their specific contributions, timelines, remuneration, and outcomes, potentially uncovering information about figures previously unknown to the researcher. This interface, exemplified by a tool named ‘Cursor’ employing an AI agent (perhaps named ‘Bernoulli’), facilitates such structured inquiry. However, a primary difficulty emerges: effective research demands more than analysing a single PDF document. It requires the capability to search across all available sources, a task for which simple token-based indexing proves insufficient.\n\n\n\nAI-assisted inquiry into historical sources regarding Sanssouci",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#erecting-a-scholarly-foundation-for-ai",
    "href": "ai-nepi_008_chapter.html#erecting-a-scholarly-foundation-for-ai",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Erecting a Scholarly Foundation for AI",
    "text": "8.4 Erecting a Scholarly Foundation for AI\nTo overcome these challenges and enable meaningful AI-driven research, several core components are indispensable. These components form the bedrock upon which reliable, validated inquiries can be built.\n\n8.4.1 The Role of Curated Scholarly Editions\nFirstly, a scholarly curated editorial board, which has meticulously worked on primary sources, provides an essential foundation. For instance, the Opera Omnia of Leonhard Euler, comprising 86 volumes, represents approximately 120 years of dedicated scholarly effort by numerous academics. This comprehensive collection, completed only recently, includes all of Euler’s 866 publications and his entire correspondence. Such rigorously edited collections, supplemented by the work of other scholars, are fundamental.\n\n\n\nThe necessity of curated scholarly content as a data foundation\n\n\nThese curated collections serve as a crucial substitute for the often-unreliable information landscape from which LLMs typically draw. They form the basis of a structured database of content items. This database should encompass elements such as:\n\nA chronology of actions.\nA record of expressions communicated between individuals.\nA timeline of terminological and linguistic usage by specific persons.\nA history of the tools and material objects employed by historical figures.\n\nCrucially, all entries within this database must be validated by primary sources, creating a detailed inventory of historically proven activities for which a reliable record exists.\n\n\n\nExamples of curated scholarly works like Euler’s Opera Omnia\n\n\n\n\n8.4.2 Advancing Beyond Embeddings: The Scholarium Registry\nOnce such detailed and validated records are established, they can be interrogated using advanced multimodal AI models. Current findings suggest that models like Gemini 2.5, capable of integrating information from both text and images, are particularly well-suited to meet the complex requirements of these tasks.\nThis structured, curated content forms what can be termed a ‘Scholarium’—a registry of knowledge that moves beyond the limitations of opaque embedding vectors. This registry catalogues:\n\nPersonal actions, including communication acts (letters, publications, reports).\nStatements, encompassing implications, arguments, and inquiries.\nThe use of language, terminology, concepts, and relations.\nThe application of models, methods, tools, and devices.\nThe utilisation of data, information, evidence, and sources.\n\nAccess to this rich, structured information can be facilitated through Application Programming Interfaces (APIs), such as the Model Context Protocol (MCP) API, enabling sophisticated AI-driven analysis.\n\n\n\nThe Scholarium concept: a registry of curated scholarly information and the Model Context Protocol",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#fostering-an-open-ecosystem-for-knowledge",
    "href": "ai-nepi_008_chapter.html#fostering-an-open-ecosystem-for-knowledge",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Fostering an Open Ecosystem for Knowledge",
    "text": "8.5 Fostering an Open Ecosystem for Knowledge\nThe development and maintenance of such a sophisticated research infrastructure depend on robust support systems and a commitment to open principles.\n\n8.5.1 Ensuring Long-Term Access with FAIR Repositories\nA critical component is a long-term, Findable, Accessible, Interoperable, and Reusable (FAIR) repository for storing and publishing the curated data. Zenodo, hosted by CERN in Geneva, offers a suitable platform, ensuring the persistence and accessibility of these valuable datasets for many years. This commitment to FAIR principles is vital for the scholarly community.\n\n\n\nZenodo as a FAIR infrastructure for long-term data preservation\n\n\n\n\n8.5.2 Collaborative Technical Support and Open Principles\nTechnical support for this ecosystem is also paramount. A startup, OpenScienceTechnology, provides the necessary expertise to run the infrastructure. This includes managing an MCP API server, which allows artificial intelligence models worldwide to access the curated data via a standardised API. This entire endeavour rests upon a foundation of open collaboration. The core principles guiding this initiative are:\n\nOpen Source\nOpen Access\nOpen Data\n\nIncluding the MCP API Server\n\nOpen Collaboration\n\nThese principles ensure transparency, accessibility, and community involvement, fostering a sustainable and evolving research environment.\n\n\n\nPrinciples of OpenScienceTechnology supporting the research infrastructure",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#towards-a-future-of-validated-scientific-discovery",
    "href": "ai-nepi_008_chapter.html#towards-a-future-of-validated-scientific-discovery",
    "title": "8  Modelling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Towards a Future of Validated Scientific Discovery",
    "text": "8.6 Towards a Future of Validated Scientific Discovery\nThe journey towards integrating AI meaningfully into the history, philosophy, and sociology of science requires a paradigm shift. Rather than relying on the probabilistic outputs of general-purpose LLMs, the focus must turn to building systems grounded in validated, curated knowledge. By establishing robust scholarly registries, leveraging FAIR infrastructure, and fostering open collaboration, we can develop AI tools that act as genuine partners in discovery. This approach promises not only to enhance research efficiency but also to deepen our understanding of science itself, moving towards a future where AI contributes to generating reliable, justifiable, and insightful knowledge.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#overview",
    "href": "ai-nepi_001_chapter.html#overview",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "",
    "text": "Workshop Announcement: Large Language Models for the History, Philosophy and Sociology of Science",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#network-epistemology-in-practice-a-nexus-for-llm-application",
    "href": "ai-nepi_001_chapter.html#network-epistemology-in-practice-a-nexus-for-llm-application",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction",
    "section": "2.2 Network Epistemology in Practice: A Nexus for LLM Application",
    "text": "2.2 Network Epistemology in Practice: A Nexus for LLM Application\nFunding for this workshop stems from the European Research Council (ERC) grant supporting the Network Epistemology in Practice (NEPI) project. A primary objective of NEPI involves studying the internal communication of the Atlas collaboration at CERN, the renowned particle physics laboratory. Through this investigation, we seek to better understand how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual methodological approach. On one hand, network analysis allows us to map and comprehend the communication structure within this vast collaboration. On the other hand, semantic tools are utilised to trace how ideas flow and evolve within these network structures. It is precisely here that the application of LLMs becomes crucial. This particular application—using LLMs to discern patterns of conceptual development and transmission in scientific discourse—represents an area of intense personal interest, though the workshop promises to unveil a multitude of other fascinating applications.\n\n\n\nAdrian Wüthrich discussing the NEPI project and LLM applications",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science: A Workshop Introduction</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#overview",
    "href": "ai-nepi_003_chapter.html#overview",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "",
    "text": "Today’s Menu slide outlining the chapter’s structure: Primer on LLMs, Applications in HPSS, and Reflections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "href": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.2 Adapting and Specialising Language Models",
    "text": "3.2 Adapting and Specialising Language Models\nThe proliferation of scientific language models, as surveyed by Ho and colleagues (Ho2024?), underscores the diverse efforts to tailor these technologies for specific research domains.\n\n\n\nA landscape diagram of Scientific Large Language Models, showing a timeline and categorisation of various models by type (Encoders, Decoders, Enc-Dec) and indicating open-source versus closed-source status.\n\n\nAdapting these powerful pre-trained models to specific scientific language or tasks involves several strategies.\n\n3.2.1 Strategies for Domain Adaptation\nInitial pre-training, where a model first encounters language, demands substantial computational resources and data. During this phase, models learn language by predicting the next token (as in GPT models) or by predicting randomly masked words within a sentence (as in BERT models). For many research groups, undertaking full pre-training from scratch is infeasible.\nA more accessible approach involves continued pre-training. Researchers can take an existing pre-trained model, such as a general BERT model, and continue its training on a specialised corpus, for instance, a collection of physics texts. This allows the model to adapt its parameters to the nuances of that specific domain.\nAlternatively, one can use pre-trained models as feature extractors. By adding a few extra layers on top of a pre-trained model, researchers can train these new layers for specific downstream tasks, such as sentiment classification or named entity recognition.\nContrastive learning offers another key method, particularly for generating sentence or document embeddings. While word embeddings capture semantic relationships between words, many applications require representations for entire sentences or documents. SentenceBERT, for example, employs contrastive learning to fine-tune BERT-like models to produce meaningful sentence embeddings, placing semantically similar sentences close together in the embedding space. This technique is vital for tasks requiring semantic similarity assessment at a level beyond individual words.\n\n\n3.2.2 Retrieval Augmented Generation (RAG)\nRetrieval Augmented Generation (RAG) has emerged as a significant technique for adapting LLMs to specific domains or tasks, often without requiring extensive model retraining. RAG systems typically involve multiple models acting in concert.\n\n\n\nDiagram illustrating the Retrieval Augmented Generation (RAG) process, showing a ‘Retrieval’ phase (querying documents, pooling) and a ‘Generation’ phase (using retrieved context with a generative model).\n\n\nIn a RAG pipeline, a user query (e.g., “What are LLMs?”) is first encoded, often by a BERT-like model, into a sentence embedding. This embedding is then used to search a database of relevant documents, retrieving passages most similar to the query. These retrieved passages provide specific context. The pipeline then integrates this retrieved information into the prompt supplied to a generative model (like GPT). The generative model uses this augmented context to produce a more informed and domain-specific answer. Many contemporary applications, including some functionalities within ChatGPT that involve searching the internet, utilise RAG principles. Reasoning models and the increasingly discussed ‘agents’ are also typically not single LLMs but rather complex systems of LLMs combined with various other tools and data sources.\n\n\n3.2.3 Key Distinctions to Remember\nTo navigate the LLM landscape effectively, several core distinctions warrant reiteration. These include the fundamental differences between encoder, decoder, and encoder-decoder architectures. Grasping various fine-tuning strategies is also essential. Furthermore, understanding the distinction between word embeddings and sentence (or document) embeddings is crucial, as they serve different analytical purposes. Finally, appreciating the different levels of abstraction at which these models operate—from token processing to document-level understanding—helps in selecting and applying them appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "href": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.3 Applications of LLMs in HPSS Research",
    "text": "3.3 Applications of LLMs in HPSS Research\nA nascent but growing body of work explores the utility of LLMs as tools within History and Philosophy of Science and Science Studies (HPSS) research. Preliminary surveys reveal several emerging categories of application.\n\n\n\nSlide listing applications of LLMs in HPSS, categorised into: Dealing with data and sources, Knowledge structures, Knowledge dynamics, and Knowledge practices.\n\n\n\n3.3.1 Categorising HPSS Applications\nResearchers are employing LLMs for a variety of tasks:\n\nDealing with data and sources: This includes parsing and extracting structured information from texts, such as publication types, acknowledgements, or citations. Interacting with sources through summarisation or RAG-type ‘chatting with your documents’ also falls into this category.\nAnalysing knowledge structures: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds. They also aid in mapping complex relationships, such as those between disciplines, interdisciplinary fields, or science-policy discourses.\nInvestigating knowledge dynamics: Conceptual histories of terms (e.g., “theory” in Digital Humanities, or “virtual” and “Planck” in physics) can be traced using LLM-derived embeddings. Identifying novelty, such as breakthrough papers or emerging technologies, represents another application.\nExamining knowledge practices: LLMs can support argument reconstruction by identifying premises and conclusions or causal relationships. Citation context analysis, an established HPSS method, can be enhanced to determine the purpose or sentiment of citations. Discourse analysis, focusing on elements like hedge sentences, jargon, or boundary work, also benefits from these tools.\n\n\n\n3.3.2 Observed Trends and Recurring Concerns\nThe application of LLMs in HPSS exhibits several notable trends and prompts recurring concerns amongst researchers.\n\n\n\nSlide summarising trends and concerns in LLM use for HPSS: accelerating interest, varying customisation, repeating concerns (resources, opaqueness, data, benchmarks, model trade-offs), and a trend toward accessibility.\n\n\nAn accelerating interest in LLMs is evident, with studies appearing not only in information science journals like Scientometrics and JASIST but also increasingly in journals traditionally less focused on computational methods. This suggests that the semantic capabilities of LLMs are attracting qualitative researchers and philosophers.\nThe degree of customisation varies widely. Some researchers develop new architectures or undertake custom pre-training, whilst others fine-tune existing models or use off-the-shelf tools like ChatGPT.\nSeveral concerns consistently surface. The substantial computational resources required for training and, in some cases, running large models pose a significant barrier. The ‘opaqueness’ or lack of interpretability of some models remains a challenge. A scarcity of suitable training data and domain-specific benchmarks for HPSS tasks is frequently noted. Researchers also grapple with trade-offs between different model types (e.g., BERT-like versus GPT-like). The potential for generative models to ‘hallucinate’ or produce plausible but incorrect information is another significant concern, although this issue is gradually improving with newer models and techniques like RAG.\nDespite these challenges, a trend towards greater accessibility is apparent. Tools like BERTopic, which simplifies topic modelling, are gaining popularity due to their ease of use and robust maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "href": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.4 Critical Reflections and Future Pathways",
    "text": "3.4 Critical Reflections and Future Pathways\nEngaging with LLMs in HPSS necessitates careful consideration of specific disciplinary challenges, a commitment to building LLM literacy, and adherence to core HPSS methodologies.\n\n\n\nSlide outlining key reflections: Acknowledging HPSS-specific challenges, Building LLM literacy, and Staying true to HPSS methodologies.\n\n\n\n3.4.1 Acknowledging HPSS-Specific Challenges\nSeveral challenges are particular to HPSS contexts. The historical evolution of concepts and language is crucial; LLMs are typically trained on modern language, which may lead to biases or misinterpretations when applied to historical texts. HPSS often adopts a reconstructive, critical, and reflective perspective, seeking to read between the lines and understand texts within their situated socio-historical contexts, including subtle discursive strategies. Current LLMs are not inherently trained for this type of nuanced reading. Furthermore, HPSS research frequently contends with sparse data, multiple languages, archaic scripts, and incompletely digitised archives.\n\n\n3.4.2 The Imperative of LLM Literacy\nTo address these challenges effectively, HPSS researchers must cultivate LLM literacy. This involves familiarising themselves with the underlying principles of LLMs, NLP, and Deep Learning—encompassing both the tools and their theoretical underpinnings. It requires learning to identify the most appropriate model architectures and training regimes for specific HPSS research questions and data. The terminology itself is in flux; the term “LLM” may become less adequate as models become increasingly multimodal, incorporating images, sound, and other data types. The definition of “large” in “Large Language Model” also shifts rapidly with technological advancements. Developing shared datasets and benchmarks tailored to HPSS problems is another vital aspect of building collective literacy and capability. For tasks involving multilinguality, understanding which models are suitable or whether custom training is feasible given available resources becomes paramount.\n\n\n3.4.3 Upholding HPSS Methodological Integrity\nWhilst embracing new tools, it is essential to remain true to established HPSS methodologies. HPSS research problems must be thoughtfully translated into NLP tasks (e.g., classification, generation, summarisation) without allowing the technical task to overshadow or distort the original research question. Simultaneously, LLMs offer new opportunities for bridging qualitative and quantitative approaches, potentially fostering richer, mixed-methods research designs.\nLLMs may offer novel ways to address core HPSS questions. For instance, contextualised word embeddings can track the evolving meanings of concepts like “Planck” across different contexts (Max Planck the person, Planck institutes, the Planck satellite, Planck length), revealing shifts in scientific discourse over time. There is potential, though requiring careful exploration, to use LLMs to investigate complex phenomena such as paradigm shifts and incommensurability.\nFinally, HPSS can reflect on its own pre-history concerning some concepts now central to LLMs. For example, co-word analysis, developed in the 1980s by science studies scholars like Michel Callon and Arie Rip, shares intellectual roots with current embedding-based approaches to mapping knowledge landscapes.\n\n\n3.4.4 The Evolving Landscape: Agents and Beyond\nThe field of language modelling is developing at a rapid pace. The rise of ‘agents’—systems where LLMs interact with other tools and data sources to perform complex tasks autonomously—signals a further evolution. Interestingly, some of the language used by computer scientists to describe these emerging agentic systems echoes concepts from Actor-Network Theory (ANT) and other STS frameworks, suggesting that HPSS theories may offer valuable conceptual tools for understanding and critically engaging with these technological advancements. The journey requires continuous learning, critical assessment, and a commitment to harnessing these powerful models responsibly in the pursuit of insightful HPSS research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.1 Introduction to OpenAlex Mapper",
    "text": "4.1 Introduction to OpenAlex Mapper\nResearchers at Utrecht University and the University of Vienna developed OpenAlex Mapper, a tool designed to navigate and analyse the vast expanse of scholarly communication. Maximilian Noichl, in collaboration with Andrea Loettgers and Taya Knuuttila, spearheaded this initiative, which received support from an ERC grant focused on ‘Possible Life’. The tool offers an interactive platform for investigating interdisciplinary connections and the topical distribution of research. Users can access the slides accompanying this work and interact with the tool itself via the developer’s website (maxnoichl.eu/talk), allowing for a direct engagement with its capabilities. This chapter elucidates the technical underpinnings of OpenAlex Mapper, demonstrates its practical application, and explores its potential contributions to research in the History, Philosophy, and Sociology of Science (HPSS).\n\n\n\nA visual representation of interconnected academic fields, illustrating the concept behind OpenAlex Mapper.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.2 Technical Architecture of OpenAlex Mapper",
    "text": "4.2 Technical Architecture of OpenAlex Mapper\nThe operational framework of OpenAlex Mapper integrates several sophisticated computational techniques to generate its insightful visualisations. Its construction involved a multi-step process, beginning with model refinement and culminating in a system for dynamic query projection.\n\n\n\nDiagram illustrating the three-stage workflow of OpenAlex Mapper: Finetuning the embedding model, Base-map preparation, and Individual user-query processing.\n\n\n\n4.2.1 Fine-tuning the Embedding Model\nThe initial phase concentrated on refining a language model to better capture the nuances of disciplinary distinctions. Investigators selected Specter 2 (Singh2022?), a language model adept at generating embeddings for scientific documents. They fine-tuned this model using a dataset of articles from closely related disciplinary backgrounds. This procedure trained the model to distinguish more effectively between these proximate fields, thereby improving its sensitivity to disciplinary boundaries. Visualisations produced through UMAP dimensionality reduction during this training process confirmed the enhanced separation of disciplines. These adjustments to the language model were incremental, rather than a complete retraining, yet crucial for the subsequent mapping accuracy.\n\n\n4.2.2 Base-map Preparation\nFollowing the model refinement, attention turned towards constructing the foundational map of scientific literature. For this, researchers utilised the OpenAlex database, a comprehensive and openly accessible repository of scholarly material that surpasses Web of Science and Scopus in its inclusiveness and ease of batch querying. From OpenAlex, they randomly sampled 300,000 article abstracts. These abstracts were then processed using the fine-tuned Specter 2 model to generate high-dimensional embeddings.\nSubsequently, Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?) served to reduce these embeddings from their high-dimensional space to a two-dimensional representation. This projection formed the ‘base-map’, a visual landscape of scientific research. Crucially, the UMAP model trained during this stage was preserved for later use.\n\n\n4.2.3 Processing Individual User Queries\nOpenAlex Mapper empowers users to explore this base-map with their own research questions. An individual can submit an arbitrary OpenAlex search query, typically as a URL. The tool then downloads the corresponding records from OpenAlex, often employing PyAlex for this task. It proceeds to embed the abstracts of these retrieved documents using the same fine-tuned Specter 2 model employed for the base-map.\nThe core of the dynamic mapping lies in the next step: the newly generated embeddings are projected into the two-dimensional space using the previously trained UMAP model. This ensures that the queried articles are positioned on the map as if they had been part of the original layout process. UMAP’s architecture facilitates this projection of new data onto an existing embedding. The final output is an interactive map, available online and for download via data-mapplot, where the user’s query results appear highlighted against the backdrop of the broader scientific landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "href": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.3 Utilising OpenAlex Mapper: A Practical Guide",
    "text": "4.3 Utilising OpenAlex Mapper: A Practical Guide\nEngaging with OpenAlex Mapper is a relatively straightforward process, designed to be accessible for researchers seeking to explore interdisciplinary connections. The tool is available online at https://m7n-openalex-mapper.hf.space.\n\n\n\nThe user interface of OpenAlex Mapper, showing input fields for OpenAlex search URLs and settings for sample size and plot customisation.\n\n\nThe primary workflow involves these steps:\n\nNavigate to the OpenAlex website (openalex.org).\nConduct a search for a topic, author, institution, or any other entity of interest, utilising the full search capabilities of OpenAlex. For instance, one might search for papers discussing “scale-free network models”, articles published by a specific university in a given year, or publications citing a particular seminal work.\nOnce the search results appear, copy the URL from the browser’s address bar. This URL encapsulates the precise query.\nReturn to the OpenAlex Mapper interface and paste this URL into the designated “OpenAlex-search URL” input field.\nAdjust sample settings if necessary. Given that embedding abstracts can be computationally intensive, particularly for large result sets, users can opt to reduce the sample size. Options include selecting the first ‘n’ samples or other sampling methods.\nConfigure plot settings, such as choosing to colour points by publication date or displaying the citation graph over the map.\nClick the “Run Query” button.\n\nBehind the scenes, OpenAlex Mapper then downloads the specified records from OpenAlex. It embeds the abstracts of these documents and projects them onto the base-map. After a processing period, the interactive visualisation appears, allowing users to explore where their query results cluster and how they relate to different regions of the scientific map.\n\n\n\nThe OpenAlex website displaying search results for “scale free networks”, from which a URL can be copied for use in OpenAlex Mapper.\n\n\nThe tool provides immediate visual feedback on the distribution of the queried literature. For example, a search for “coriander” might reveal its presence not only in expected fields like botany or food science but also in unexpected areas such as epidemiology or public health, prompting further investigation into these connections. Similarly, mapping publications on “scale-free network models” can illustrate their prevalence and application across diverse scientific domains.\n\n\n\nThe OpenAlex Mapper interface processing a query for “scale free networks”, with a progress bar indicating the embedding stage.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper offers a valuable methodological addition to the toolkit of HPSS researchers. It aims to help bridge the gap between detailed, qualitative case studies and the broader, large-scale dynamics of contemporary science. Many insights in HPSS derive from close-up views of specific scientific episodes, often based on close reading of texts, interaction with scientists, or ethnographic methods. Whilst these studies provide rich understanding, generalising their findings or validating them in the context of global, rapidly evolving “big science” presents a considerable challenge.\nThe tool assists in addressing questions about the reach, influence, and contextual embedding of scientific ideas, models, and methods. For instance, one might ask: Where did the Hopfield Model, developed in a specific context, truly gain traction and find lasting application across the sciences? OpenAlex Mapper allows for such heuristic, qualitative investigations to be supported and guided by quantitative, large-scale data analysis, whilst always enabling a return to the underlying textual sources.\n\n4.4.1 Investigating Model Templates\nResearchers initially developed OpenAlex Mapper with the study of ‘model templates’ in mind. In the philosophy of science, model templates refer to abstract structural forms of models that recur across diverse scientific disciplines, potentially structuring scientific inquiry in ways orthogonal to traditional disciplinary boundaries. Using the tool, investigators can map the occurrence of specific model templates—such as percolation models, network models, or agent-based models—revealing their distinct, sometimes non-continuous, footprints across the scientific landscape. This visualisation can illuminate how similar formalisms are adopted and adapted in varied epistemic contexts.\n\n\n4.4.2 Mapping Conceptual Landscapes\nThe tool also proves useful for exploring the distribution and interrelation of scientific concepts. For example, one can map the concept of ‘phase transition’ and contrast its disciplinary spread with that of ‘emergence’. Whilst conceptual mapping is an established practice, OpenAlex Mapper extends this capability to broad interdisciplinary contexts, overcoming common difficulties associated with acquiring and harmonising diverse datasets. It allows researchers to visualise how concepts travel, transform, and are contested across different fields.\n\n\n4.4.3 Analysing Methodological Distributions\nA further application lies in examining the distribution of specific research methods. Consider the ongoing debate in philosophy of science regarding the role of machine learning techniques versus classical statistical methods in scientific discovery. OpenAlex Mapper can map the usage of a machine learning technique like the random forest model against a more traditional method such as logistic regression. Observing distinct patterns in their disciplinary uptake—for instance, why neuroscientists might favour random forests whilst psychiatric researchers predominantly use logistic regressions for thematically similar problems—can generate new philosophical questions about methodological choice, epistemic justification, and disciplinary cultures.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "href": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.5 Methodological Considerations and Limitations",
    "text": "4.5 Methodological Considerations and Limitations\nWhilst OpenAlex Mapper offers powerful analytical capabilities, several qualifications warrant attention.\n\n\n\nA slide summarising key qualifications and limitations of the OpenAlex Mapper tool and its underlying data.\n\n\nA list of these considerations includes:\n\nData Source Imperfections: The utility of the tool is intrinsically linked to the comprehensiveness and accuracy of the OpenAlex database. Although data quality is generally reasonable, and comparable to other major bibliographic databases, it is not flawless. Users must remain mindful of potential biases or gaps in the underlying data.\nLanguage Limitations: Currently, the fine-tuned Specter 2 model employed by OpenAlex Mapper processes only English-language texts. This inherently limits the scope of analysis, particularly for research published in other languages or for historical periods where English was less dominant in scientific communication. Future developments could incorporate multilingual models to address this, although high-quality, science-trained multilingual models are not yet widely available.\nDependency on Abstracts and Titles: The embedding process relies on the availability of abstracts or, at a minimum, informative titles. Sources lacking such textual information cannot be effectively processed or mapped, potentially excluding certain types of publications or older literature.\nUMAP Algorithm Characteristics: The UMAP algorithm, central to the dimensionality reduction and visualisation, possesses certain characteristics that influence the output.\n\nStochasticity: UMAP is a stochastic algorithm, meaning that each run can produce slightly different layouts. The generated map represents one realisation amongst many possibilities.\nDimensionality Trade-offs: Projecting high-dimensional data (such as the 768 dimensions of Specter embeddings) into a mere two dimensions inevitably involves trade-offs and potential distortions. The algorithm must prioritise certain relationships, which can lead to some misalignments or compressions of the true semantic distances between documents.\n\n\nA working paper, “Philosophy at Scale: Introducing OpenAlex Mapper,” provides a more detailed exposition of the technical aspects and further discusses these methodological points.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  }
]