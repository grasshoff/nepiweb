---
title: "Unveiling the Dynamics of Science: Insights from the NHGRI Archives"
author: "[Presenter Name]"
date: "[Date of Presentation]"
bibliography: bibliography.bib
format:
  html:
    toc: true
    number-sections: true
    theme: sandstone
    code-fold: false
    link-citations: true
---
## Overview {.unnumbered}
The National Human Genome Research Institute (NHGRI) archive presents a rich repository of information, yet its intricate structure poses considerable analytical challenges. This collection encompasses a diverse array of materials, offering a granular view into the workings of a major scientific funding agency and the large-scale research programmes it supported. Within the archive, one finds formal presentations detailing legal frameworks for genomics and draft agendas from pivotal strategy meetings, such as those for the International HapMap Project. Alongside these, spreadsheets meticulously record project timetables and the projected distribution of genetic markers. The archive also preserves contemporary accounts through newspaper clippings that captured the public and regional significance of genomic endeavours. Furthermore, administrative insight comes from documents like research proposal review forms. The collection is further enriched by less formal, yet equally valuable, materials including handwritten notes from meetings, detailed research proposals submitted by organisations like Applied Biosystems, and internal communications such as emails and memos, complete with annotations that reveal the day-to-day coordination and decision-making processes. This variety underscores the archive's potential for deep analysis, whilst simultaneously highlighting the complexity inherent in processing and interpreting such heterogeneous data.

## Beyond Bibliometrics: Seeking a Deeper Understanding of Science

Scientific articles and their associated bibliometrics offer valuable, albeit partial, insights into the mechanisms of science. Analyses of publication data have illuminated aspects ranging from the long-term impact of scientific discoveries and the dynamics of team sizes to the emergence of interdisciplinary fields and the career trajectories of scientists. Nevertheless, to equate the scientific enterprise solely with its published outputs represents an oversimplification of its inherent complexity. The scientific article, as a final product, often presents a skewed and incomplete picture. A more comprehensive understanding necessitates looking beyond these outputs to explore the intricate processes that underpin scientific advancement.

![Diagram illustrating the conventional view of science funding flowing from federal agencies through grants to publications and investigators, ultimately benefiting the public.](images/ai-nepi_020_slide_03.jpg)

Examining the scientific process itself, rather than focusing exclusively on its products, opens avenues to address fundamental questions. For instance, such an approach allows investigation into the interplay between scientific direction and funding allocation: does scientific inquiry shape funding priorities, or do funding mechanisms dictate the course of science? Delving into the innovation pipeline, from initial ideation through to long-term impact, can reveal where innovation flourishes, where it disseminates effectively, and, crucially, where it falters. Published articles seldom document failed projects, yet understanding these failures is vital for a complete picture of scientific endeavour. Whilst a full exploration of these complex questions extends beyond a brief overview, current research endeavours aim to shed light on these issues, offering initial insights into how such conclusions can be drawn.

![Diagram detailing the flow from federal funding agencies to grants, leading to knowledge and scholars, and ultimately public benefit, with references to studies on impact, team size, and collaborations.](images/ai-nepi_020_slide_04.jpg)

## The Human Genome Project: A Paradigm of "Big Science" in Biology

The pursuit of understanding large-scale scientific endeavours, such as "big science" in particle physics, finds a compelling parallel in the realm of biology, particularly with the Human Genome Project (HGP). Many will recall this era, when numerous countries and thousands of researchers collaborated in an unprecedented biological undertaking: to sequence the entire human genome. The HGP's significance is multifaceted. Firstly, it captured public attention on a scale previously unseen for a biological project, moving beyond the traditional laboratory focus on model organisms like *Drosophila* and *C. elegans*. Secondly, its impact resonates profoundly today; a vast swathe of modern biological research, especially omics methodologies, relies heavily on the reference genome established by the HGP. Indeed, the very field of genomics owes its genesis to this monumental achievement. The project also pioneered new data-sharing practices, now widely adopted, and forged a powerful, history-making synergy between computational science and biology.

Two principal organisations spearheaded this global effort: the Wellcome Trust in the United Kingdom and, crucially for this discussion, the National Human Genome Research Institute (NHGRI), the arm of the U.S. National Institutes of Health (NIH) dedicated to the HGP.

![Diagram illustrating how federal funding agencies influence knowledge, scholars, and public outcomes, with feedback loops indicating how these outcomes inform future agency actions. Questions about the science-funding narrative and innovation pathways are posed.](images/ai-nepi_020_slide_05.jpg)

The NHGRI, in particular, stands out as one of the most innovative funding bodies within the NIH, and consequently, within the broader biomedical research community. This innovation manifests in several ways: a high proportion of its funded publications rank amongst the top 5% most cited; its research demonstrates significant citation impact within a decade; it yields considerable clinical applications derived from patents; and the research it supports is often characterised as disruptive. While NHGRI's innovative capacity is evident, the underlying reasons for this success remain less understood. To uncover these drivers of innovation, an examination of the agency's processes becomes essential. What specific strategies and operational methods enable NHGRI to foster such impactful scientific advancements?

### An Interdisciplinary Approach to Uncovering Innovation

Addressing these questions requires a collaborative, interdisciplinary effort. A dedicated team, comprising engineers, historians, physicists, ethicists, and computer scientists—including figures central to the HGP and NHGRI leadership—has convened. Their collective aim is to understand the factors that catalysed the rise of genomics, identify potential failure modes in the scientific process, pinpoint where innovation spills over into new domains, and explore how funding agencies can more effectively collaborate with academic scholars and scientists to enhance scientific outcomes.

![Image depicting leaders at a podium during the Human Genome Project announcement, alongside Nature and Science magazine covers celebrating the achievement.](images/ai-nepi_020_slide_06.jpg)

This collaborative investigation brings together diverse expertise to analyse the complex interplay of factors shaping scientific progress.

![Group photo of the interdisciplinary research team involved in studying the NHGRI archive and the history of genomics.](images/ai-nepi_020_slide_07.jpg)

## The NHGRI Archive: A Window into Scientific Process

A crucial resource for this investigation is the internal archive of the NHGRI. Owing to the historic significance of the Human Genome Project, the agency preserved a substantial collection of internal forms and documentation spanning the 1980s, 1990s, and beyond. This archive offers an unparalleled glimpse into the daily operations and decision-making processes of a major scientific initiative. It contains, for example, the meeting notes of scientists coordinating the genome project, handwritten annotations on correspondence related to agendas and conferences, formal presentations, data-rich spreadsheets, and newspaper clippings that chronicle this pivotal period.

Comprising over two million pages and expanding annually by approximately 5% through ongoing digitisation efforts, this archive represents a vast "born-digital" and digitised physical artefact. The sheer scale and heterogeneity of this collection present a significant challenge: how can a research team effectively study such a complex dataset at scale? This challenge necessitates the development and application of advanced analytical methodologies.

![Composite image showcasing the diverse document types within the NHGRI archive, including presentations, spreadsheets, newspapers, forms, meeting notes, handwritten documents, proposals, and emails.](images/ai-nepi_020_slide_09.jpg)

The contents of these internal documents differ fundamentally from publicly available materials such as Requests for Applications (RFAs) and peer-reviewed publications, which scholars typically access through databases like PubMed or NIH RePORTER. While public documents outline formal calls and disseminate final results, the internal archive reveals the nuanced, often informal, processes behind them. These internal records detail the numerous genomic projects funded by the NHGRI—endeavours often involving tens or hundreds of millions of dollars and the coordinated efforts of thousands of researchers. Collectively, these projects generated the foundational resources for the genomics community, fuelling the ascent of the entire field.

## Advanced Methodologies for Archive Analysis

Analysing such a vast and varied archive necessitates sophisticated computational approaches. Whilst a comprehensive account of all methodologies employed is beyond this scope, certain key techniques merit attention.

### Addressing Handwritten Content

A significant challenge within this born-digital and digitised physical archive is the prevalence of handwritten material. Handwriting presents difficulties for automated analysis, not only from a technical standpoint but also due to ethical considerations surrounding the interpretation of potentially ambiguous content. To address this, researchers developed a bespoke handwriting model. This model serves a dual purpose: it can remove handwriting to improve the accuracy of Optical Character Recognition (OCR) on printed text within documents, and it facilitates a separate, dedicated pipeline for handwriting recognition itself.

![Example of a document containing extensive handwritten notes, illustrating the challenges for automated processing.](images/ai-nepi_020_slide_11.jpg)

### Leveraging Multimodal Models and Synthetic Data

Beyond handwriting, the research leverages advances from the document intelligence community, particularly multimodal models. These models ingeniously combine visual information (the appearance of the document), textual content, and layout structure. This integrated approach enables a range of analytical tasks. One key application is sophisticated entity extraction, identifying and categorising important named entities within the documents. Furthermore, these models empower researchers to generate synthetic documents. Such synthetic data provides valuable training material for developing and refining new classification algorithms, enhancing the analytical capabilities applied to the archive.

![Diagram illustrating a U-Net architecture, likely used for image segmentation tasks such as separating handwriting from printed text, alongside examples of handwritten notes.](images/ai-nepi_020_slide_12.jpg)

The adaptability of multimodal models allows for their application to diverse analytical objectives. By creating a joint embedding space for text, image, and layout features, supervised by layout information and trained using masked autoencoder objectives, these systems can perform tasks such as entity extraction via text decoders and synthetic data generation via vision decoders. This approach, drawing on recent developments in the field [@Huang2022; @Zhang2022], significantly expands the toolkit for interrogating complex document archives.

![Diagram illustrating a multimodal model architecture that integrates text, image, and layout inputs for tasks like entity extraction and synthetic data generation.](images/ai-nepi_020_slide_13.jpg)

## Conclusion: Towards a Nuanced History of Science

The systematic analysis of rich, internal archives like that of the NHGRI, powered by advanced computational techniques, promises to transform our understanding of how science truly operates. By moving beyond the published record and delving into the processes, decisions, collaborations, and even the setbacks that characterise major scientific undertakings, researchers can construct a more nuanced and comprehensive history of science. This endeavour not only illuminates past achievements, such as the Human Genome Project, but also offers valuable lessons for optimising future scientific funding, collaboration, and innovation. The continued development and application of AI-driven document analysis tools will undoubtedly be pivotal in unlocking the full potential of such historical records.
