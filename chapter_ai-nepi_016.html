<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Francis Lareau &amp; Christophe Malaterre">
<meta name="dcterms.date" content="2025-06-21">

<title>14&nbsp; A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_017.html" rel="next">
<link href="./chapter_ai-nepi_015.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-c2d8198b7f72dec16de60f0cb3fab69f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a0afd4a9b901cc50d8ed64d4ec5e2aec.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_016.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper presentation slide</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and Understanding LLMs &amp; AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI, Computational Epistemology, and Open Science Infrastructure</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI-NEPI: Citation extraction from scholarly publications in the humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context for Language Variation and Change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#topic-modelling-in-hpss" id="toc-topic-modelling-in-hpss" class="nav-link" data-scroll-target="#topic-modelling-in-hpss"><span class="header-section-number">14.1</span> Topic Modelling in HPSS</a></li>
  <li><a href="#research-question-and-methodology" id="toc-research-question-and-methodology" class="nav-link" data-scroll-target="#research-question-and-methodology"><span class="header-section-number">14.2</span> Research Question and Methodology</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">14.3</span> Methodology</a></li>
  <li><a href="#comparing-lda-and-bertopic" id="toc-comparing-lda-and-bertopic" class="nav-link" data-scroll-target="#comparing-lda-and-bertopic"><span class="header-section-number">14.4</span> Comparing <em>LDA</em> and <em>BERTopic</em></a></li>
  <li><a href="#framework-for-qualitative-comparison" id="toc-framework-for-qualitative-comparison" class="nav-link" data-scroll-target="#framework-for-qualitative-comparison"><span class="header-section-number">14.5</span> Framework for Qualitative Comparison</a></li>
  <li><a href="#quantitative-analysis-metrics" id="toc-quantitative-analysis-metrics" class="nav-link" data-scroll-target="#quantitative-analysis-metrics"><span class="header-section-number">14.6</span> Quantitative Analysis Metrics</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">14.7</span> Results</a></li>
  <li><a href="#model-similarity-via-adjusted-rand-index" id="toc-model-similarity-via-adjusted-rand-index" class="nav-link" data-scroll-target="#model-similarity-via-adjusted-rand-index"><span class="header-section-number">14.8</span> Model Similarity via Adjusted RAND Index</a></li>
  <li><a href="#lda-performance-across-text-types" id="toc-lda-performance-across-text-types" class="nav-link" data-scroll-target="#lda-performance-across-text-types"><span class="header-section-number">14.9</span> <em>LDA</em> Performance Across Text Types</a></li>
  <li><a href="#bertopic-performance-across-text-types" id="toc-bertopic-performance-across-text-types" class="nav-link" data-scroll-target="#bertopic-performance-across-text-types"><span class="header-section-number">14.10</span> <em>BERTopic</em> Performance Across Text Types</a></li>
  <li><a href="#qualitative-analysis-of-lda-top-words" id="toc-qualitative-analysis-of-lda-top-words" class="nav-link" data-scroll-target="#qualitative-analysis-of-lda-top-words"><span class="header-section-number">14.11</span> Qualitative Analysis of <em>LDA</em> Top-Words</a></li>
  <li><a href="#comparative-topic-formation" id="toc-comparative-topic-formation" class="nav-link" data-scroll-target="#comparative-topic-formation"><span class="header-section-number">14.12</span> Comparative Topic Formation</a></li>
  <li><a href="#quantitative-results-topic-coherence" id="toc-quantitative-results-topic-coherence" class="nav-link" data-scroll-target="#quantitative-results-topic-coherence"><span class="header-section-number">14.13</span> Quantitative Results: Topic Coherence</a></li>
  <li><a href="#quantitative-results-topic-diversity" id="toc-quantitative-results-topic-diversity" class="nav-link" data-scroll-target="#quantitative-results-topic-diversity"><span class="header-section-number">14.14</span> Quantitative Results: Topic Diversity</a></li>
  <li><a href="#quantitative-results-joint-recall" id="toc-quantitative-results-joint-recall" class="nav-link" data-scroll-target="#quantitative-results-joint-recall"><span class="header-section-number">14.15</span> Quantitative Results: Joint Recall</a></li>
  <li><a href="#summary-matrix-of-model-performance" id="toc-summary-matrix-of-model-performance" class="nav-link" data-scroll-target="#summary-matrix-of-model-performance"><span class="header-section-number">14.16</span> Summary Matrix of Model Performance</a></li>
  <li><a href="#overall-performance-summary" id="toc-overall-performance-summary" class="nav-link" data-scroll-target="#overall-performance-summary"><span class="header-section-number">14.17</span> Overall Performance Summary</a></li>
  <li><a href="#discussion-and-conclusions" id="toc-discussion-and-conclusions" class="nav-link" data-scroll-target="#discussion-and-conclusions"><span class="header-section-number">14.18</span> Discussion and Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Francis Lareau &amp; Christophe Malaterre <a href="mailto:francislareau@hotmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Université du Québec à Montréal; Université de Sherbrooke; CIRST
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter investigates a central question in computational text analysis: whether robust topic modelling necessitates full-text documents, or if titles and abstracts provide sufficient data. The authors undertake a rigorous comparative analysis of two prominent techniques, <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) and <em>BERTopic</em>. They applied these models to a corpus on Astrobiology, systematically segmenting the data into three distinct types: full-text documents, abstracts alone, and titles alone.</p>
<p>Their evaluation framework is twofold. A qualitative analysis explores thematic clustering and the coherence of top-words, whilst a comprehensive quantitative analysis employs four key metrics. These include the Adjusted Rand Index (ARI) to measure model similarity, Topic Diversity to assess the uniqueness of topics, Joint Recall to evaluate content coverage, and Coherence CV to gauge the interpretability of the generated topics.</p>
<p>The findings reveal a nuanced trade-off between the models and data types. <em>BERTopic</em>, for instance, excels in generating diverse topics, particularly from titles. Conversely, <em>LDA</em> models trained on full-text achieve the highest joint recall, indicating superior content coverage. The results suggest that the optimal choice of model and input data depends entirely on the specific analytical goals of the researcher, whether they prioritise thematic diversity, content coverage, or topic coherence.</p>
</section>
<section id="topic-modelling-in-hpss" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="topic-modelling-in-hpss"><span class="header-section-number">14.1</span> Topic Modelling in HPSS</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>Topic modelling has established itself as a significant analytical tool within the domains of History, Philosophy, and Sociology of Science (HPSS). Its utility is demonstrated across a range of applications that enhance scholarly inquiry. Within these fields, scholars employ this technique to identify influential authors and papers, trace the conceptual evolution of scientific ideas over time, and map the intellectual structure of entire disciplines.</p>
<p>Furthermore, topic modelling enables the discovery of previously hidden thematic connections in large corpora, the analysis of long-term trends in scientific discourse, and the comparison of distinct research programmes. This capacity for large-scale analysis also makes it an invaluable resource for conducting comprehensive literature reviews.</p>
</section>
<section id="research-question-and-methodology" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="research-question-and-methodology"><span class="header-section-number">14.2</span> Research Question and Methodology</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The authors’ investigation centres on a fundamental research question: does robust topic modelling depend on the analysis of full-text documents, or can comparable results be achieved using only titles or abstracts?</p>
<p>To answer this, the team implements a formal comparative methodology. This framework systematically evaluates two distinct topic modelling approaches—the probabilistic <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) and the transformer-based <em>BERTopic</em>. They apply each model to three different granularities of text data: complete full-text documents, abstracts, and titles. Subsequently, the outputs from these combinations are assessed through both qualitative and quantitative analysis, providing a multi-faceted evaluation of their performance.</p>
</section>
<section id="methodology" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="methodology"><span class="header-section-number">14.3</span> Methodology</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>The investigation proceeds with a detailed examination of the methodologies employed to compare the two topic modelling techniques.</p>
</section>
<section id="comparing-lda-and-bertopic" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="comparing-lda-and-bertopic"><span class="header-section-number">14.4</span> Comparing <em>LDA</em> and <em>BERTopic</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>At their core, both <em>Latent Dirichlet Allocation</em> and <em>BERTopic</em> share common postulates; they generally rely on a bag-of-words representation and conceptualise topics as distinct distributions over a vocabulary. Nevertheless, their underlying mechanisms differ significantly. <em>LDA</em> is a generative probabilistic model that assumes each document is a mixture of various topics.</p>
<p>In contrast, <em>BERTopic</em> leverages modern transformer models to create contextual word and sentence embeddings. It then applies clustering algorithms to these rich semantic representations to identify topics. This allows it to capture nuances of meaning that frequency-based models like <em>LDA</em> may miss.</p>
</section>
<section id="framework-for-qualitative-comparison" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="framework-for-qualitative-comparison"><span class="header-section-number">14.5</span> Framework for Qualitative Comparison</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The authors established a clear framework for the qualitative comparison of the models, using a specialised Astrobiology corpus as their primary dataset. Within this framework, they configured an <em>LDA</em> model to generate 25 distinct topics from the corpus.</p>
<p>Following this initial modelling, the team further organised these 25 topics through a clustering process into four high-level thematic groups. To visualise the interplay and connections between these themes, the analysts created a correlation graph, mapping the relationships between the identified topic clusters.</p>
</section>
<section id="quantitative-analysis-metrics" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="quantitative-analysis-metrics"><span class="header-section-number">14.6</span> Quantitative Analysis Metrics</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>The quantitative evaluation relies on four distinct metrics, each chosen to assess a specific aspect of model performance. The Adjusted Rand Index (ARI) measures the similarity between the clustering structures produced by different topic models. Topic Diversity calculates the percentage of unique words present in the top terms across all topics, providing a measure of model redundancy.</p>
<p>Joint Recall is used to evaluate how effectively a model trained on a text subset, such as abstracts, can retrieve the topics found in the corresponding full-text model. Finally, Coherence CV assesses the human interpretability of a topic by computing the semantic similarity of its most prominent words.</p>
</section>
<section id="results" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="results"><span class="header-section-number">14.7</span> Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>The subsequent sections present the empirical results derived from the authors’ comprehensive qualitative and quantitative analyses.</p>
</section>
<section id="model-similarity-via-adjusted-rand-index" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="model-similarity-via-adjusted-rand-index"><span class="header-section-number">14.8</span> Model Similarity via Adjusted RAND Index</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>The authors used the Adjusted RAND Index (ARI) to quantify the similarity between the outputs of different models. Their results, presented in a matrix, show that models of the same family—such as <em>LDA</em> models trained on abstracts versus titles—exhibit greater similarity to one another than they do to models from the other family, like <em>BERTopic</em>.</p>
<p>This finding indicates that the choice of algorithm (<em>LDA</em> vs.&nbsp;<em>BERTopic</em>) has a more profound impact on the resulting topic structure than the choice of input text. Furthermore, the analysis reveals that models trained on abstracts more closely resemble their full-text counterparts than models trained on titles do, suggesting abstracts retain more of the core thematic structure.</p>
</section>
<section id="lda-performance-across-text-types" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="lda-performance-across-text-types"><span class="header-section-number">14.9</span> <em>LDA</em> Performance Across Text Types</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>An analysis of <em>Latent Dirichlet Allocation</em> (<em>LDA</em>) performance across different text granularities reveals notable variations. Using heatmaps to visualise topic distributions, the authors compared models trained on full-text documents against those trained on only abstracts or titles.</p>
<p>Their results indicate that whilst some thematic correspondence exists, the topic structures generated from abstracts and titles frequently diverge from those derived from the full text. This suggests that relying on shorter text segments can lead to a different, and potentially less complete, thematic representation of the corpus when using <em>LDA</em>.</p>
</section>
<section id="bertopic-performance-across-text-types" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="bertopic-performance-across-text-types"><span class="header-section-number">14.10</span> <em>BERTopic</em> Performance Across Text Types</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>The performance of <em>BERTopic</em> also varies significantly depending on the input text. Visualised through three distinct matrices, the team’s analysis shows that the <em>BERTopic</em> model trained on full-text documents tends to produce a high number of small and highly specific topics.</p>
<p>In contrast, when trained on abstracts, the model yields topics that are more stable and clearly defined. Training on titles, however, results in the formation of broader and more generalised thematic categories, demonstrating how the input data’s scope directly influences the granularity of the output.</p>
</section>
<section id="qualitative-analysis-of-lda-top-words" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="qualitative-analysis-of-lda-top-words"><span class="header-section-number">14.11</span> Qualitative Analysis of <em>LDA</em> Top-Words</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>A qualitative comparison of the top-words generated by <em>LDA</em> models highlights the impact of text granularity on topic interpretability. By examining the lists of top-words from models trained on full-text, abstracts, and titles, the authors observed clear divergences in topic coherence and thematic focus.</p>
<p>For instance, a distinct topic related to ‘life detection’ might appear clearly in both the full-text and abstract-based models. However, in the model trained solely on titles, the same theme could become less coherent, potentially merging with other, more general topics and losing its specific meaning.</p>
</section>
<section id="comparative-topic-formation" class="level2" data-number="14.12">
<h2 data-number="14.12" class="anchored" data-anchor-id="comparative-topic-formation"><span class="header-section-number">14.12</span> Comparative Topic Formation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Contrasting the behaviour of <em>LDA</em> and <em>BERTopic</em> reveals fundamental differences in how they construct topics from text. The authors’ analysis shows that a single, broad topic identified by an <em>LDA</em> model can often be resolved into several more specific and nuanced topics by <em>BERTopic</em>, a phenomenon known as topic splitting.</p>
<p>Conversely, <em>BERTopic</em>’s ability to discern fine-grained semantic distinctions may result in multiple related topics that <em>LDA</em>, with its focus on word co-occurrence, merges into a single, more generalised category. These patterns of splitting and merging underscore the distinct operational logics of the two algorithms.</p>
</section>
<section id="quantitative-results-topic-coherence" class="level2" data-number="14.13">
<h2 data-number="14.13" class="anchored" data-anchor-id="quantitative-results-topic-coherence"><span class="header-section-number">14.13</span> Quantitative Results: Topic Coherence</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>The quantitative analysis of topic coherence, measured using the CV score, produced nuanced results. When comparing <em>BERTopic</em> and <em>LDA</em> across titles, abstracts, and full-text data, no single model or text type demonstrated consistent superiority.</p>
<p>Instead, topic coherence appears to be highly dependent on the specific model configuration. The number of topics a user specifies is a particularly influential variable, with coherence scores for both <em>LDA</em> and <em>BERTopic</em> fluctuating significantly as this parameter changes.</p>
</section>
<section id="quantitative-results-topic-diversity" class="level2" data-number="14.14">
<h2 data-number="14.14" class="anchored" data-anchor-id="quantitative-results-topic-diversity"><span class="header-section-number">14.14</span> Quantitative Results: Topic Diversity</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>In the evaluation of topic diversity, a clear pattern emerged. The authors found that <em>BERTopic</em> models consistently outperform their <em>LDA</em> counterparts, generating topic sets with less word overlap.</p>
<p>Notably, the peak diversity scores were achieved when <em>BERTopic</em> was trained on titles alone. This finding suggests that for research goals where maximising the variety of distinct themes is paramount, the combination of the <em>BERTopic</em> algorithm and title-only data provides a highly effective strategy.</p>
</section>
<section id="quantitative-results-joint-recall" class="level2" data-number="14.15">
<h2 data-number="14.15" class="anchored" data-anchor-id="quantitative-results-joint-recall"><span class="header-section-number">14.15</span> Quantitative Results: Joint Recall</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>The analysis of joint recall, which measures how well a model captures the themes of the entire document, yields an unambiguous result. Models trained on full-text data consistently achieve the highest recall scores.</p>
<p>Specifically, the <em>LDA</em> model applied to full-text documents registered the top performance. This outcome demonstrates that for applications where comprehensive thematic coverage is the primary objective, there is no substitute for analysing the complete text of the documents.</p>
</section>
<section id="summary-matrix-of-model-performance" class="level2" data-number="14.16">
<h2 data-number="14.16" class="anchored" data-anchor-id="summary-matrix-of-model-performance"><span class="header-section-number">14.16</span> Summary Matrix of Model Performance</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>A summary matrix provides a consolidated overview of the comparative analysis. It systematically contrasts the performance of <em>LDA</em> and <em>BERTopic</em> when applied to full-text, abstract, and title data. Using a range of evaluation metrics, the matrix employs a simple visual key—filled circles—to indicate which combination of model and data input excels for each specific measure, allowing for a quick, at-a-glance assessment of the relative strengths of each approach.</p>
</section>
<section id="overall-performance-summary" class="level2" data-number="14.17">
<h2 data-number="14.17" class="anchored" data-anchor-id="overall-performance-summary"><span class="header-section-number">14.17</span> Overall Performance Summary</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>The final performance summary synthesises the findings across all evaluation criteria, including overall fit, top-word quality, coherence, diversity, and joint recall. This overview uses filled circles to denote strong performance and red crosses to flag identified weaknesses.</p>
<p>The <em>LDA</em> model trained on full-text, for example, is highlighted for its excellent joint recall and overall fit but shows limitations in topic diversity. Conversely, the <em>BERTopic</em> model trained on titles excels in producing diverse topics but at the cost of lower content coverage. This clearly illustrates a fundamental trade-off: methods that maximise topic diversity often do so at the expense of comprehensive recall, and vice versa.</p>
</section>
<section id="discussion-and-conclusions" class="level2" data-number="14.18">
<h2 data-number="14.18" class="anchored" data-anchor-id="discussion-and-conclusions"><span class="header-section-number">14.18</span> Discussion and Conclusions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_016_slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>In conclusion, the authors synthesise the distinct performance characteristics of <em>LDA</em> and <em>BERTopic</em> when applied to different sections of scholarly documents. As their similarity matrix demonstrated, the choice of algorithm is a more powerful determinant of the final topic structure than the granularity of the input text.</p>
<p>Scholars seeking the most comprehensive thematic coverage should favour full-text analysis, particularly with <em>LDA</em>, which excels in joint recall. However, for projects prioritising the discovery of a wide and diverse range of topics, <em>BERTopic</em> applied to titles proves to be a superior strategy. Ultimately, the study confirms that there is no single best approach; the optimal combination of model and data is entirely contingent on the specific objectives of the research.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_015.html" class="pagination-link" aria-label="Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_017.html" class="pagination-link" aria-label="Making Transformer-Based LLMs Time-Aware: A Proof of Concept">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>