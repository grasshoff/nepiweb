---
title: "Is it sufficient to apply topic modelling to titles or abstracts, or is full-text analysis necessary?"
author: "[Placeholder Author Name(s)]"
date: "[Placeholder Date]"
bibliography: bibliography.bib
format:
  html:
    toc: true
    number-sections: true
    theme: sandstone
    code-fold: false
    link-citations: true
---

## Overview {.unnumbered}
This chapter investigates a critical question in computational text analysis: whether applying topic modelling to the titles or abstracts of scientific articles yields sufficient insight, or if the analysis of full texts is indispensable. Addressing this question is pressing, given the significant resources required to obtain, preprocess, and analyse extensive full-text corpora. The study systematically explores this issue by first constituting a corpus of scientific articles. Subsequently, it identifies the distinct title, abstract, and full-text sections within these articles. Two prominent topic modelling approaches, Latent Dirichlet Allocation (LDA) and BERTopic, are then applied to each of these three textual components. Finally, the resulting six topic models undergo rigorous qualitative and quantitative comparison to elucidate the optimal approach for extracting meaningful thematic structures.

## Introduction

Topic modelling has emerged as a powerful computational tool for uncovering thematic structures within large collections of documents. Its utility spans numerous disciplines, offering insights into diverse textual data.

### Context: Topic Modelling in Scholarly Research

Within the History, Philosophy, and Sociology of Science (HPSS), topic modelling provides a valuable lens through which to examine the evolution and structure of scientific knowledge. Researchers employ these techniques for various analytical purposes. For instance, topic models can aid in identifying research trends and paradigm shifts, as explored by [@GriffithsSteyvers2004]. They also facilitate the identification of themes' substructures and their interrelations [@BleiLafferty2007]. Furthermore, analysing the evolution of scientific vocabulary becomes feasible through such methods [@ChavalariasCointet2013]. Topic modelling can also contribute to uncovering hidden biases within scientific discourse [@SugimotoEtAl2013] and to studying the sociology of scientific communities [@GerowEtAl2018]. Its applications extend to analysing the interdisciplinary nature of disciplines [@HyeyoungEtAl2022] and enhancing the historiography of science [@Mimno2012] and art [@Browman2023].

![Applications of topic modelling in HPSS.](images/ai-nepi_016_slide_07.jpg)

### The Central Research Question and Approach

A pressing concern for researchers employing topic modelling is the choice of textual material. Specifically, the central question this study addresses is whether it is sufficient to apply topic modelling to titles or abstracts alone, or if full-text analysis is truly necessary. This consideration gains importance due to the substantial resources—time, computational power, and access—involved in acquiring, preparing, and scrutinising full-text corpora.

To investigate this, a multi-stage methodology was adopted. Initially, a corpus of scientific articles was constituted. From each article, the title, abstract, and full-text sections were carefully identified and separated. Subsequently, two distinct topic modelling approaches, Latent Dirichlet Allocation (LDA) and BERTopic, were applied to each of these three text segments (titles, abstracts, full texts). This process generated six unique topic models. The final stage involved a comprehensive analysis and comparison of these six models, employing both qualitative and quantitative measures to evaluate their respective strengths and weaknesses. The overall workflow is depicted below.

![Flowchart illustrating the research methodology, from scientific corpus to qualitative and quantitative analysis.](images/ai-nepi_016_slide_01.jpg)

## Methodology

The methodological framework of this study encompasses corpus preparation, the application of selected topic modelling techniques, and a structured approach for comparing the resultant models.

![Methodology title slide.](images/ai-nepi_016_slide_04.jpg)

### Corpus Constitution and Preprocessing

To address the central research question, the initial step involved constituting a corpus of scientific articles. Within this corpus, researchers meticulously identified and isolated the title, abstract, and full-text sections of each document. This segmentation allowed for independent topic modelling on each type of textual data.

### Topic Modelling Techniques: LDA and BERTopic

Two distinct topic modelling techniques were selected for this comparative analysis: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches operate on the premise that documents can be represented by numerical vectors and that topics emerge from linguistic regularities, such as word repetitions, within the text. Machine learning algorithms then automatically detect these regularities.

![Comparison of LDA and BERTopic approaches.](images/ai-nepi_016_slide_09.jpg)

#### Latent Dirichlet Allocation (LDA)

LDA represents a classical statistical method in topic modelling. It employs a traditional vector representation based on word counts within documents (bag-of-words). In the LDA framework, topics are conceptualised as latent variables that adhere to a Dirichlet distribution. A key advantage of LDA for this study is its capacity to handle long texts, making it suitable for application to titles, abstracts, and full texts alike.

#### BERTopic

In contrast, BERTopic is a more recent, modular approach to topic modelling. It leverages Large Language Model (LLM)-based vector representations, originally utilising BERT (Bidirectional Encoder Representations from Transformers), which inspires its name. Within BERTopic, topics correspond to clusters of documents identified through their topological densities in the embedding space. Historically, BERTopic did not readily handle long texts. However, recent advancements in embedding techniques, capable of processing sequences up to approximately 131,000 tokens, have enabled its application to full-text data in this study.

### Framework for Comparison

The six topic models generated—LDA applied to titles, abstracts, and full texts, and BERTopic applied to the same three text segments—were systematically analysed and compared. This comparison employed both qualitative and quantitative methodologies to provide a comprehensive assessment of their performance and characteristics.

## Reference Corpus for Qualitative Analysis

For the qualitative comparison, this study utilised an existing in-depth topic analysis of an Astrobiology corpus, previously detailed by [@MalaterreLareau2023]. This reference work involved an LDA full-text model comprising 25 distinct topics.

Researchers meticulously analysed these 25 topics, examining their most representative words and documents to generate a descriptive label for each. The interrelations between these topics were then explored by calculating their mutual correlations, based on topic co-occurrence within documents. Subsequently, a community detection algorithm identified four overarching thematic clusters. These clusters are denoted by letters (A, B, C, D) and distinct colours (red, green, yellow, blue respectively).

The visual representation below displays these results as a network graph. Nodes in the graph represent the 25 topics, labelled and coloured according to their cluster. The thickness of the lines connecting nodes indicates the strength of the correlation between topics, whilst the size of the circles signifies the overall prevalence of each topic across all documents in the corpus. This established analysis provides a robust foundation for qualitatively comparing the six new topic models generated in the current investigation.

![Network graph of 25 topics from an Astrobiology corpus, clustered and correlated.](images/ai-nepi_016_slide_11.jpg)

## Results and Analysis

The comparative analysis of the six topic models yielded several insights into their similarities, differences, and overall performance. This section details these findings, drawing upon both quantitative metrics and qualitative assessments.

![Results title slide.](images/ai-nepi_016_slide_12.jpg)

### Inter-Model Similarity: Adjusted Rand Index

To evaluate the similarity between the six topic models, the Adjusted Rand Index (ARI) was employed. This metric assesses how similarly documents are clustered together across different models. An ARI score of zero indicates that the clustering agreement is no better than random.

The analysis revealed that the LDA model applied to titles (LDA-Title) was the most dissimilar from the other models, exhibiting ARI values generally below 0.2. Conversely, the remaining five models demonstrated a greater degree of concordance, with ARI values typically exceeding 0.2. Notably, the BERTopic models (BERTopic-Title, BERTopic-Abstract, BERTopic-FullText) showed stronger internal consistency, with ARI values amongst themselves often surpassing 0.35. The BERTopic model applied to abstracts (BERTopic-Abstract) appeared to be a relatively central model, correlating well with most other models, with the exception of the divergent LDA-Title model.

### Detailed Qualitative Comparisons

A more granular qualitative comparison focused on how topics from the reference LDA full-text model were represented or transformed in the other models.

#### LDA Model Variations: Full-Text, Abstracts, and Titles

Comparing the LDA full-text model with the LDA abstract model (Table A in the figure below) indicated a generally good fit. Many topics from the full-text model found a direct counterpart in the abstract model, evidenced by a high proportion of shared documents, forming a noticeable diagonal in the sorted comparison table. However, some transformations occurred: three full-text LDA topics did not appear in the abstract model, and three others were split. Conversely, three new topics emerged in the abstract model, and three others resulted from the merger of full-text topics. The abstract model also contained one small class with fewer than 50 documents.

In stark contrast, the comparison between the LDA full-text model and the LDA title model (Table B) revealed a poor fit. This was characterised by significant reorganisation, with many topics disappearing, splitting, or merging, indicating substantial divergence between the thematic structures derived from full texts and titles using LDA.

![Heatmap comparison of LDA Full-text topics versus LDA Abstract and LDA Title topics.](images/ai-nepi_016_slide_14.jpg)

#### BERTopic Model Variations versus LDA Full-Text

The comparison extended to BERTopic models against the reference LDA full-text model.

When comparing the LDA full-text model with the BERTopic full-text model (Table C in the figure below), an average fit was observed. Eight LDA full-text topics did not have clear counterparts and six appeared to be split within the BERTopic full-text model. From the BERTopic perspective, five new topics emerged, and one resulted from mergers. A notable characteristic of the BERTopic full-text model was the presence of four small classes (topics with few documents) and one very large class, suggesting some imbalance.

The LDA full-text model versus the BERTopic abstract model (Table D) showed a relatively good fit. In this comparison, four LDA full-text topics disappeared and six were split. The BERTopic abstract model introduced two new topics and four topics resulting from mergers. The class sizes in this model appeared more balanced.

Finally, comparing the LDA full-text model with the BERTopic title model (Table E) demonstrated an average fit. Seven LDA full-text topics disappeared and one was split. The BERTopic title model presented seven new topics and one merger. This model also exhibited three small classes and one large class.

![Heatmap comparisons of LDA Full-text topics versus BERTopic Full-text, Abstract, and Title topics.](images/ai-nepi_016_slide_15.jpg)

### Top-Word Analysis

Examining the top words associated with topics across different models provides further qualitative insight into their nature and stability.

#### LDA Top-Word Coherence and Transformations

Across the LDA models (Full-Text, Abstract, Title), topics were generally well-formed. Some topics demonstrated robustness, maintaining a strong correspondence across all three LDA models; "A-Radiation spore" exemplifies such a stable topic.

Instances of topic splitting were observed. For example, the full-text topic "A-Life civilization" split into more granular topics in the abstract and title models. This particular split appeared meaningful, potentially distinguishing general research themes from more specific astrobiology inquiries. Similarly, the full-text topic "B-Chemistry" also underwent splitting, although the rationale for this particular fragmentation was less immediately clear without deeper contextual analysis.

Topic merging also occurred. The full-text topics "B-Amino-acid" and "B-Protein-gene-RNA" tended to merge in the abstract and title models, forming a more general topic related to biomolecules, which is a coherent transformation. However, the splitting of "B-Chemistry" in the LDA title model proved somewhat difficult to interpret directly from top words alone.

![Table comparing top-words for selected topics across LDA Full-Text, LDA Abstract, and LDA Title models.](images/ai-nepi_016_slide_16.jpg)

#### BERTopic Top-Word Coherence and Transformations

A similar top-word assessment for the three BERTopic models (Full-Text, Abstract, Title) also revealed relatively well-formed topics. The robustness of the "A-Radiation spore" topic persisted across all BERTopic models. The "A-Life civilization" topic also showed relative stability, though with some splitting that led to narrower topics specifically focused on extraterrestrial life. Across the BERTopic models, the splitting of the "B-Chemistry" topic consistently resulted in more narrowly defined chemical themes.

![Table comparing top-words for selected topics across LDA Full-Text and BERTopic Full-Text, Abstract, and Title models.](images/ai-nepi_016_slide_18.jpg)

### Quantitative Performance Metrics Across Models

To complement the qualitative assessments, several quantitative metrics were calculated for all six models, varying the number of topics from 5 to 50. These metrics include coherence, diversity, and joint recall.

#### Topic Coherence

Topic coherence evaluates the semantic similarity of the top words within a topic, aiming to measure how meaningful a topic is. The analysis of coherence (CV metric) across different models and topic counts revealed several patterns. Models based on titles consistently offered the poorest coherence. Abstract-based models generally achieved better coherence than their full-text counterparts. Comparing the two algorithms, BERTopic models tended to exhibit superior coherence over LDA models, particularly for abstracts and titles. This advantage for BERTopic diminished somewhat as the number of topics increased. Overall, the BERTopic abstract model emerged as the strongest performer in terms of topic coherence.

![Line chart showing models' coherence (CV) by number of topics.](images/ai-nepi_016_slide_19.jpg)

#### Top-Word Diversity

Top-word diversity measures the uniqueness of words across different topics; higher diversity suggests that topics are more distinct. This metric generally tended to decrease as the number of topics increased for all models. Title-based models, interestingly, offered better top-word diversity compared to abstract or full-text models. Regarding the algorithms, BERTopic models again outperformed LDA models in terms of diversity. The BERTopic title model achieved the highest diversity scores, closely followed by the BERTopic full-text model.

![Line chart showing models' diversity by number of topics.](images/ai-nepi_016_slide_20.jpg)

#### Joint Recall

Joint recall assesses how well the top words of a topic collectively represent all documents classified under that topic. The results for joint recall showed that title-based models offered the worst performance. Full-text models performed better on this metric than their abstract and title counterparts. In this particular assessment, LDA models demonstrated slightly better joint recall than BERTopic models. The LDA full-text and BERTopic full-text models were the top performers, with the BERTopic abstract model following closely.

![Line chart showing models' micro joint recall by number of topics.](images/ai-nepi_016_slide_21.jpg)

### Synthesised Model Assessment

A summary of these varied assessments offers an overall perspective on model performance. The performance of each model across different criteria (overall fit, top-words, coherence, diversity, joint recall) can be visualised, for instance, using a system where a filled circle indicates high performance and an empty circle indicates lower performance.

It is crucial to recognise that different research objectives may prioritise different model characteristics. Consequently, no single model is universally "best". For instance, if the primary goal is the discovery of main topics without a stringent need for precise classification of every document, then issues like poor recall or class imbalance might be less critical. In such scenarios, BERTopic applied to full text performed well in identifying robust topics, despite some class imbalance. Even BERTopic applied to titles, whilst far from optimal on many metrics, did produce some meaningful topics found in other, more comprehensive models.

Conversely, the LDA title model is generally not recommended, as it performed poorly across almost all assessment criteria. The findings suggest that topic modelling on abstracts or full texts, using either LDA or BERTopic, offers more reliable results, provided that the chosen configuration does not lead to significant misclassification of documents relevant to the topics of interest.

![Summary table of model performance across various metrics.](images/ai-nepi_016_slide_22.jpg)

## Discussion

The findings from this comparative study stimulate further discussion on the nuances of applying topic modelling to different sections of scientific articles. Several key themes emerge from the analysis.

![Discussion points alongside a heatmap of Adjusted Rand Index values between models.](images/ai-nepi_016_slide_23.jpg)

### Performance of Title-Based Models

A consistent observation was the generally poor performance of models derived solely from titles. A plausible explanation lies in the inherent lack of informational richness in titles compared to abstracts or full texts. This brevity can lead to ambiguous or misleading signals for the topic modelling algorithms, potentially resulting in the misclassification of documents. Nevertheless, despite these limitations, even title-based models, particularly with BERTopic, were capable of identifying some meaningful thematic clusters that resonated with those found in more comprehensive models. This suggests that whilst titles alone may not be sufficient for robust topic modelling, they might still capture certain high-level thematic signals. The challenge lies in balancing the desire for well-defined topics with adequate document-topic coverage.

### Challenges with Full-Text Models

Full-text models, whilst offering the most comprehensive source of information, also present their own set of challenges. With LDA, topics derived from full texts can sometimes be overly broad or loosely defined. Furthermore, full-text analysis may surface "transverse" topics, such as those related to methodology or common research practices, which might obscure more domain-specific themes.

BERTopic, when applied to full texts, can sometimes generate topics that are excessively narrow. This hyper-specificity, while potentially insightful, can lead to poor document coverage for those topics and create problems with class size imbalance, where some topics are associated with very few documents and others with a disproportionately large number.

### Strengths of Abstract-Based Models

Models based on abstracts appeared to strike a favourable balance. Abstracts, designed as concise summaries, often contain a high density of relevant keywords and concepts. The results from abstract-based models, for both LDA and BERTopic, demonstrated consistency with each other and also with the LDA full-text model. The BERTopic abstract model, in particular, performed well across a range of metrics, including the Adjusted Rand Index where it showed good correspondence with other models (except LDA-Title).

### Topic Robustness Across Models

A significant finding is the general robustness of many topics across the different models and textual sources. Despite variations in algorithms and input data, core thematic structures frequently re-emerged. This consistency opens avenues for meta-analytic approaches. For example, one could systematically identify the most robust topics by observing which themes consistently appear across multiple models. Moreover, the relative "distance" or similarity (e.g., via ARI) between models could potentially guide the selection of an optimal or most central model. In this study, the BERTopic abstract model often occupied such a central position, aligning well with other models while also performing strongly on individual metrics.

### Future Directions for Topic Modelling

The exploration of different text structures (titles, abstracts, full texts) hints at possibilities for more sophisticated topic modelling approaches. Future research could investigate methods that explicitly leverage this structural information. For instance, models could be developed to differentially weight terms from different sections or to integrate information from titles, abstracts, and full texts in a hierarchical or multi-modal fashion to extract an even more meaningful and nuanced set of topics or top words.

## Conclusion

This investigation into the efficacy of topic modelling on titles, abstracts, and full texts of scientific articles has yielded several key insights.

Firstly, models relying solely on titles generally exhibit poor performance. The limited information content in titles can lead to inaccurate document classification, although some meaningful high-level topics can still emerge, particularly with BERTopic. A crucial aim is to achieve a balance between well-defined topics and comprehensive document coverage.

Secondly, full-text models, while information-rich, present challenges. LDA applied to full texts may produce overly broad topics or highlight secondary themes like methodology. BERTopic on full texts can result in topics that are too narrow, leading to issues with document coverage and class size imbalance.

Thirdly, abstract-based models often provide a strong compromise, effectively summarising key information. The thematic structures derived from abstracts showed consistency between LDA and BERTopic, and also aligned well with those from LDA full-text models.

Fourthly, a notable degree of topic robustness was observed across the various models and text segments. This consistency suggests the potential for meta-analytic techniques to identify core, stable themes. The BERTopic abstract model, in particular, demonstrated strong overall performance and centrality in its relationships with other models.

Finally, the distinct characteristics of topics derived from titles, abstracts, and full texts suggest an avenue for developing new models. Future approaches could explicitly incorporate this structural information from different article sections to refine the extraction of meaningful topics and representative keywords, potentially leading to more nuanced and accurate understandings of large textual corpora.
