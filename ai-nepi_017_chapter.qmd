---
title: "Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis"
author: "Jochen Büttner, Max Planck Institute of Geoanthropology"
date: "2024-07-26" # Inferred date, as none was provided in the source material
bibliography: bibliography.bib
format:
  html:
    toc: true
    number-sections: true
    theme: sandstone
    code-fold: false
    link-citations: true
---
## Overview {.unnumbered}
This chapter explores a novel approach to imbue large language models (LLMs) with an explicit understanding of time. Current models derive temporal understanding implicitly, through statistical patterns discovered within vast textual corpora. The work detailed herein proposes an architectural modification enabling LLMs to learn and reproduce changing textual patterns as a direct function of time. Such a capability promises particular benefits for historical analysis, though its applications may extend to other domains. A proof-of-concept generative LLM, developed to test this hypothesis, demonstrates the potential for explicit time-awareness, specifically its capacity to model temporal drift in language using a corpus of weather reports.

![Title slide for Time-Aware Language Models presentation.](images/ai-nepi_017_slide_01.jpg)

## The Implicit Nature of Time in Current LLMs

Large language models currently possess only an implicit grasp of time, an understanding gleaned statistically from the text upon which they train. This is not to suggest an absence of temporal comprehension; indeed, LLMs exhibit a considerable capacity to process and relate time-based information. Nevertheless, this capacity originates from subtle cues embedded within their training data, rather than from an explicit mechanism for temporal reasoning. Developing explicit time awareness in these models could significantly enhance their utility, most notably in historical analysis, but also across a broader spectrum of applications.

Consider, for instance, two sentences differing only in their final words, rendering them contradictory. Human readers intuitively resolve this contradiction by assuming the sentences originate from different temporal contexts. The first might reflect a state of affairs in 2017, whilst the second could represent the situation today or at some more recent juncture.

![Slide reiterating the project title and author.](images/ai-nepi_017_slide_02.jpg)

## Challenges of Implicit Time Representation

When such temporally distinct, contradictory sentences appear in an LLM's training data, the model lacks explicit information about their different origins. Consequently, these sentences effectively compete for the model's attention. The LLM must then decide which statement to prioritise, an arbitration that inevitably leads to imperfections in its learned representations, as favouring one means misrepresenting the other.

![A blank slide, likely used for emphasis during the presentation.](images/ai-nepi_017_slide_03.jpg)

Upon training, if such a model encounters an input sequence prompting next-token prediction, it may exhibit a recency bias. For example, it might predict "transformer" as the dominant architecture, even though "long short-term memories" (LSTMs) were prevalent at an earlier time and are also part of its training data. Extracting information about LSTMs would then necessitate altering the input context—perhaps by including the year "2017" or changing verb tenses. This process, akin to prompt engineering, involves a degree of uncertainty, as one attempts to intuit how the model has implicitly learned to understand time.

![Slide highlighting implicit vs. explicit time understanding in LLMs.](images/ai-nepi_017_slide_04.jpg)

## Towards Explicit Time-Awareness

There are scenarios demanding more robust temporal handling than current prompt engineering affords. Models require explicit time-awareness: the ability to learn and reproduce changing patterns in training data as a function of time. A straightforward method to achieve this forms the core of the investigations presented here, culminating in a proof-of-concept generative LLM. These generative models aim to produce or reproduce textual patterns observed in their input data, conditioned explicitly on time.

![Slide summarising key points about LLM time awareness and architectures.](images/ai-nepi_017_slide_05.jpg)

## Formalising Temporal Dependence in Language Models

Neural networks process text primarily through architectures such as LSTMs, prominent around 2017, and Transformers, which are central to contemporary models (the slide indicates a future context of 2025 for Transformers, perhaps referring to an evolving standard or specific variant).

![Slide contrasting LSTMs (2017) and Transformers (2025) as text processing architectures.](images/ai-nepi_017_slide_06.jpg)

Based on training data, LLMs estimate the probability distribution over the vocabulary for the next token, given a sequence of preceding tokens:
$p(x_n | x_1, \dots, x_{n-1})$.
However, in reality, the probability of a token, given its preceding context, is not static; it depends on time:
$p(x_n | x_1, \dots, x_{n-1}, t)$.
For an entire sequence of tokens uttered at time $t$, this becomes:
$p(x_1, x_2, \dots, x_n | t) = \prod_{k=1}^{n} p(x_k | x_1, x_2, \dots, x_{k-1}, t)$.
During inference, current LLMs can only reflect temporal drift in the distribution of underlying token sequences through in-context learning.

![Slide detailing probability distributions for next-token prediction, with and without time dependence.](images/ai-nepi_017_slide_07.jpg)

## The Time Transformer Architecture

A crucial question arises: how can one effectively model $p(x_n | x_1, \dots, x_{n-1}, t)$? One naive approach, time slicing—training separate models for distinct time periods—would be extremely inefficient in terms of data utilisation.

A more integrated solution involves the "Time Transformer". This architecture incorporates a temporal dimension directly into the latent semantic features of each token. The embedding for a token $x$ at time $t$ becomes $E(x, t) = [e_1(x), e_2(x), \dots, e_{d-1}(x), \phi(t)]$, where $\phi(t)$ represents the time feature. The sequence of these time-aware embeddings then feeds into the Transformer:
$[E(x_1, t), E(x_2, t), \dots, E(x_{n-1}, t)] \rightarrow \text{Transformer} \rightarrow p_\theta(x_n | x_1, \dots, x_{n-1}, t)$.

The training objective is to minimise the negative log-likelihood:
$\min_\theta - \sum_{i=1}^N \sum_{k=1}^{n^{(i)}} \log p_\theta(x_k^{(i)} | x_1^{(i)}, \dots, x_{k-1}^{(i)}, t^{(i)})$.
Through this mechanism, time is injected into every token's representation, allowing the model to learn the precise influence—strong or weak—of the temporal dimension on each token.

![Slide outlining the Time Transformer concept, including time-augmented embeddings and the training objective.](images/ai-nepi_017_slide_08.jpg)

## Experimental Validation

To assess the efficacy of this time-aware architecture, a series of experiments were conducted.

### Data

The chosen dataset comprises Met Office weather reports. This corpus features a limited vocabulary and simple, repetitive language, making it suitable for initial explorations. The Met Office, the UK's national meteorological service, issues daily weather reports, and past reports are accessible via their digital archive at [https://digital.nmla.metoffice.gov.uk/](https://digital.nmla.metoffice.gov.uk/). The selection of a constrained dataset allows for focused analysis, drawing some inspiration from work on small yet coherent language models, such as those explored in the "TinyStories" research.

![Slide describing the Met Office Weather reports dataset.](images/ai-nepi_017_slide_09.jpg)

### Vanilla Model Baseline

A baseline "vanilla" transformer model served as a point of comparison. This model employs a modest transformer decoder architecture, featuring four multi-head attention decoder blocks and comprising approximately 39 million parameters (around 150 MB). For context, this is considerably smaller than models like GPT-4, which reportedly has 1.8 trillion parameters across 120 layers. Training this vanilla model on two A100 GPUs required approximately 11 seconds per epoch. The code for this model is available at [https://github.com/j-buettner/time_transformer](https://github.com/j-buettner/time_transformer).

![Slide detailing the vanilla transformer model architecture, parameters, training, and accuracy.](images/ai-nepi_017_slide_11.jpg)

This vanilla model learns to reproduce the language of the weather reports effectively. For example, when provided with the seed sequence "During the night, a band", it generates coherent continuations such as: "of rain moved into Scotland, Northern Ireland and Northern England. Outbreaks of rain continued to move across Northern England and Wales. It stayed largely dry with clear spells and a few scattered showers in the North and West. Elsewhere there were plenty of clear spells and a few fog patches and overall it was a mild night across the South of the Uk ....".

![Slide showing the vanilla model's architecture and an example of its generated text.](images/ai-nepi_017_slide_13.jpg)

### Time Transformer Implementation

The Time Transformer architecture introduces a minimal adjustment to the vanilla model. Instead of a single input, it processes separate text input and time data. The text input passes through an embedding layer (e.g., $d_{model}=511$), whilst the time data passes through its own embedding layer (e.g., $d_{model}=1$). These two embeddings are then concatenated before proceeding through positional encoding and the subsequent decoder layers, which remain structurally similar to the vanilla model.

![Slide comparing the vanilla model architecture with the minimally adjusted Time Transformer architecture.](images/ai-nepi_017_slide_14.jpg)

In this implementation, the time dimension is non-trainable. It uses a min-max normalised day of the year as its input:
$\text{time embedding} = \frac{\text{day of year} - 1}{365 - 1}$.
This simple representation allows the model to associate linguistic patterns with specific times of the year.

![Slide detailing the non-trainable time embedding based on the day of the year.](images/ai-nepi_017_slide_15.jpg)

### Experiment 1: Synonymic Succession

The first experiment investigated the model's ability to learn synthetic temporal drift. This involved a time-dependent replacement in the training data: the word "rain" was gradually replaced by "liquid sunshine" as the year progressed. The objective was to ascertain if this time dependence would be reproduced in predicted token sequences, with one sequence generated for each day of the year.

![Slide outlining the first experiment: synonymic succession of "rain" with "liquid sunshine".](images/ai-nepi_017_slide_16.jpg)

Results indicate that the model successfully learned this pattern. A plot of the probability of the "rain" to "liquid sunshine" replacement throughout the year shows a sigmoid curve, mirroring the introduced synthetic drift. Furthermore, a monthly bar chart comparing occurrences of "rain" versus "liquid sunshine" in the generated text clearly illustrates the shift: "rain" dominates early in the year, whilst "liquid sunshine" becomes prevalent in later months.

![Slide showing results of the synonymic succession experiment: probability curve and monthly occurrences.](images/ai-nepi_017_slide_17.jpg)

### Capturing Seasonal Patterns

Beyond synthetic changes, the Time Transformer also demonstrated an ability to capture naturally occurring seasonal patterns in the weather report data. A monthly analysis of terms like "hot", "warm", "snow", "sleet", and "wintry" in the generated text reveals that the model correctly associates cold-weather terms with winter months and warm-weather terms with summer months.

![Slide displaying a bar chart of seasonal weather terms ("Hot" vs. "Snow") by month.](images/ai-nepi_017_slide_18.jpg)

### Experiment 2: Fixation of a Collocation

The second experiment explored the model's capacity to learn a change in co-occurrence patterns, specifically the "fixation of a collocation". A synthetic, time-dependent linguistic shift was introduced: instances of "rain" followed by any word except "and" were increasingly replaced by the fixed collocation "rain and snow" as the year progressed. This mirrors linguistic phenomena like "bread and butter", where a variable phrase becomes an obligatory one.

Analysis of the model's output confirmed its ability to learn this evolving collocation. A monthly comparison of "rain and snow" versus "rain only" occurrences in predicted sequences shows a clear trend towards the fixed collocation over time. For example, on Day 1, the model might predict "...heavy rain...", whereas on Day 363, it is more likely to predict "...heavy rain and snow...".

Furthermore, an examination of attention weights within the model offers insights into this learning process. A heatmap displaying attention from "snow" back to "rain" across different attention heads, comparing an early day (Day 1) with a late day (Day 363), reveals increased attention weights later in the year. This suggests that the model learns to strengthen the association between "rain" and "snow" as the collocation becomes more frequent.

![Slide detailing the second experiment: fixation of the collocation "rain and snow", with results including monthly occurrences and attention heatmaps.](images/ai-nepi_017_slide_19.jpg)

## Discussion, Applications, and Future Directions

The experiments provide a proof of concept: transformer-based LLMs can be efficiently made time-aware by adding a temporal dimension to the token embedding. This capability opens several avenues for applications and further research.

Potential applications include:

*   A foundational time transformer could serve as an excellent basis for various downstream tasks involving historical data.
*   An instruction-tuned time transformer might allow users to "talk to a specific time", querying historical states of information. By extension, this could also improve results in the common usage scenario of "talking to the present".
*   The same architectural approach could model the dependence of underlying token sequence distributions on other contextual or metadata dimensions, such as country or genre.

Potential next steps for research encompass:

*   Benchmarking this approach against an explicit time-token method, where time is treated as just another token in the input sequence.
*   Testing for potential increases in training efficiency due to the explicit handling of time.
*   Exploring other refinements and variations of the architecture.

However, several challenges attend the application of time-aware LLMs:

*   It remains unclear whether fine-tuning such models is possible and efficient, given the architectural change to incorporate time.
*   This approach moves away from the simplicity of purely metadata-free self-supervised learning, introducing the complexities of data curation, particularly the crucial task of accurately determining when a given token sequence was generated.
*   Consideration should be given to whether a modest, targeted encoder model might be more suitable for certain time-sensitive tasks.

![Slide summarising the proof of concept, applications, potential next steps, and challenges.](images/ai-nepi_017_slide_21.jpg)

These explorations into time-aware language models represent an initial step towards more nuanced and temporally grounded artificial intelligence. Further development in this area holds promise for enhancing our ability to analyse and understand time-varying information across diverse domains.