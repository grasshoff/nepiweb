<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-01-01">

<title>3&nbsp; Large Language Models for the History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">GENRE CLASSIFICATION FOR HISTORICAL MEDICAL PERIODICALS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">LLMs and Footnotes: Challenges in Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Possible applications of RAG systems in philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#presentation-agenda-and-objectives" id="toc-presentation-agenda-and-objectives" class="nav-link" data-scroll-target="#presentation-agenda-and-objectives"><span class="header-section-number">3.1</span> Presentation Agenda and Objectives</a></li>
  <li><a href="#the-transformer-architecture-foundations-of-large-language-models" id="toc-the-transformer-architecture-foundations-of-large-language-models" class="nav-link" data-scroll-target="#the-transformer-architecture-foundations-of-large-language-models"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of Large Language Models</a></li>
  <li><a href="#encoder-based-models-the-bert-architecture" id="toc-encoder-based-models-the-bert-architecture" class="nav-link" data-scroll-target="#encoder-based-models-the-bert-architecture"><span class="header-section-number">3.3</span> Encoder-Based Models: The BERT Architecture</a></li>
  <li><a href="#decoder-based-models-the-gpt-architecture" id="toc-decoder-based-models-the-gpt-architecture" class="nav-link" data-scroll-target="#decoder-based-models-the-gpt-architecture"><span class="header-section-number">3.4</span> Decoder-Based Models: The GPT Architecture</a></li>
  <li><a href="#evolution-and-categorisation-of-scientific-large-language-models" id="toc-evolution-and-categorisation-of-scientific-large-language-models" class="nav-link" data-scroll-target="#evolution-and-categorisation-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution and Categorisation of Scientific Large Language Models</a></li>
  <li><a href="#strategies-for-domain-and-task-adaptation-via-training" id="toc-strategies-for-domain-and-task-adaptation-via-training" class="nav-link" data-scroll-target="#strategies-for-domain-and-task-adaptation-via-training"><span class="header-section-number">3.6</span> Strategies for Domain and Task Adaptation via Training</a></li>
  <li><a href="#retrieval-augmented-generation-rag-for-domain-adaptation" id="toc-retrieval-augmented-generation-rag-for-domain-adaptation" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval-Augmented Generation (<em>RAG</em>) for Domain Adaptation</a></li>
  <li><a href="#fundamental-distinctions-in-large-language-model-application" id="toc-fundamental-distinctions-in-large-language-model-application" class="nav-link" data-scroll-target="#fundamental-distinctions-in-large-language-model-application"><span class="header-section-number">3.8</span> Fundamental Distinctions in Large Language Model Application</a></li>
  <li><a href="#applications-of-large-language-models-in-hpss-research" id="toc-applications-of-large-language-models-in-hpss-research" class="nav-link" data-scroll-target="#applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in HPSS Research</a></li>
  <li><a href="#trends-and-concerns-in-hpss-llm-adoption" id="toc-trends-and-concerns-in-hpss-llm-adoption" class="nav-link" data-scroll-target="#trends-and-concerns-in-hpss-llm-adoption"><span class="header-section-number">3.10</span> Trends and Concerns in HPSS LLM Adoption</a></li>
  <li><a href="#critical-reflections-on-large-language-models-in-hpss" id="toc-critical-reflections-on-large-language-models-in-hpss" class="nav-link" data-scroll-target="#critical-reflections-on-large-language-models-in-hpss"><span class="header-section-number">3.11</span> Critical Reflections on Large Language Models in HPSS</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            AI-NEPI Conference Participant
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This presentation meticulously details the application and implications of Large Language Models (<em>LLMs</em>) within the History, Philosophy, and Sociology of Science (<em>HPSS</em>). It commences by providing a foundational primer on <em>LLM</em> architectures, specifically elucidating the <em>Transformer</em> model, and differentiating between encoder-based (<em>BERT</em>) and decoder-based (<em>GPT</em>) systems. Subsequently, the report charts the evolution of scientific <em>LLMs</em>, categorising them by architecture and domain-specific applications.</p>
<p>A significant portion explores various adaptation strategies, including pre-training, continued pre-training, and the sophisticated Retrieval-Augmented Generation (<em>RAG</em>) pipeline. The authors highlight their utility in tailoring models to specific scientific contexts. The presentation then systematically outlines current and prospective <em>LLM</em> applications across <em>HPSS</em> research, encompassing data interaction, knowledge structure analysis, dynamics, and practices.</p>
<p>Crucially, the authors address the inherent challenges and concerns associated with <em>LLM</em> deployment in <em>HPSS</em>, such as computational demands, model opaqueness, data scarcity, and the critical need for <em>HPSS</em>-specific methodological alignment. The discussion further probes the evolving definition of “large” in <em>LLMs</em> and the emerging concept of multimodal agents, whilst underscoring <em>HPSS</em>’s historical contributions to the theoretical underpinnings of such advanced systems.</p>
</section>
<section id="presentation-agenda-and-objectives" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="presentation-agenda-and-objectives"><span class="header-section-number">3.1</span> Presentation Agenda and Objectives</h2>
<p>This presentation establishes a clear agenda for exploring the utility of Large Language Models (<em>LLMs</em>) within the History, Philosophy, and Sociology of Science (<em>HPSS</em>). Initially, it provides a concise primer on <em>LLMs</em>, detailing their fundamental principles and their specific adaptation for scientific domains. Subsequently, the discussion transitions to a summary of current applications observed within <em>HPSS</em> research. Finally, the presentation offers a series of critical reflections, intended to foster robust discussion throughout the workshop.</p>
</section>
<section id="the-transformer-architecture-foundations-of-large-language-models" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture-foundations-of-large-language-models"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of Large Language Models</h2>
<p>The <em>Transformer</em> architecture, a pivotal innovation from 2017, underpins virtually all contemporary Large Language Models. Its original designers crafted this model for language translation, facilitating conversions such as German to English. Structurally, it comprises two interconnected streams: an encoder on the left and a decoder on the right, linked centrally.</p>
<p>The process commences with input words, which the system encodes into numerical representations. These numbers then undergo intricate processing across multiple layers, progressively refining contextualised word embeddings. Subsequently, the system outputs words, with each generated word feeding back into the input stream to facilitate the prediction of the next element in the sequence.</p>
<p>Crucially, the encoder operates by processing the entire input sentence simultaneously, allowing each word to interact with every other word. This comprehensive interaction enables the model to construct a full, holistic representation of the sentence’s complete meaning. Conversely, the decoder stream functions unidirectionally; words can only access their predecessors, precluding any foresight into future elements. This design specifically enables the decoder to predict the subsequent word in a sequence, a fundamental capability for generative tasks.</p>
</section>
<section id="encoder-based-models-the-bert-architecture" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="encoder-based-models-the-bert-architecture"><span class="header-section-number">3.3</span> Encoder-Based Models: The BERT Architecture</h2>
<p>Immediately following the <em>Transformer</em>’s introduction, researchers began re-engineering its individual streams to develop pre-trained language models. This marked a significant shift from pure translation towards models adept at language comprehension. The encoder stream, for instance, evolved into models like <em>BERT</em> (Bidirectional Encoder Representations from <em>Transformers</em>), which remain highly prevalent.</p>
<p><em>BERT</em> operates on the principle that every word within the input stream can access and interact with all other words. This bidirectional attention mechanism enables the model to construct a comprehensive, full-context understanding of the entire input at once, proving highly effective for diverse Natural Language Processing tasks with minimal subsequent fine-tuning.</p>
</section>
<section id="decoder-based-models-the-gpt-architecture" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="decoder-based-models-the-gpt-architecture"><span class="header-section-number">3.4</span> Decoder-Based Models: The GPT Architecture</h2>
<p>Conversely, the decoder stream gave rise to the <em>GPT</em> (Generative Pre-trained <em>Transformers</em>) models, which now power widely used applications such as <em>ChatGPT</em>. These models, operating on the decoder-side architecture, possess a distinct structure: they can only attend to preceding words, precluding any future context. This unidirectional processing, however, confers a crucial capability—the generation of novel text. This generative capacity fundamentally differentiates <em>GPT</em> models from their <em>BERT</em> counterparts; whilst <em>BERT</em>-like models excel at coherent sentence understanding, they lack the inherent ability to produce new linguistic content.</p>
</section>
<section id="evolution-and-categorisation-of-scientific-large-language-models" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evolution-and-categorisation-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution and Categorisation of Scientific Large Language Models</h2>
<p>A fundamental distinction persists between generative models, exemplified by <em>GPT</em>, which primarily produce language, and full-context models, such as <em>BERT</em>, which excel at coherent sentence comprehension. Beyond these primary types, engineers have also developed architectures that combine both encoder and decoder components. Furthermore, advanced techniques enable decoders to operate in a manner more akin to encoders, as demonstrated by models like <em>XLM</em>, which builds upon <em>XLNet</em>.</p>
<p>An extensive overview of scientific Large Language Models from 2018 to 2024 reveals a burgeoning landscape. These models categorise primarily by their underlying architecture: encoders, which appear more frequently; encoder-decoders; decoders, which are less common; and other specialised configurations. Early influential models include <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>. The proliferation of these models extends across numerous scientific disciplines, with specific variants tailored for biomedicine, chemistry, material science, climate science, mathematics, physics, and social science, amongst others.</p>
</section>
<section id="strategies-for-domain-and-task-adaptation-via-training" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="strategies-for-domain-and-task-adaptation-via-training"><span class="header-section-number">3.6</span> Strategies for Domain and Task Adaptation via Training</h2>
<p>Adapting Large Language Models to specific scientific domains and tasks involves several training strategies. The initial phase, known as pre-training, enables models to acquire language proficiency, either by predicting the subsequent token, as seen in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This process, however, demands immense computational resources and extensive datasets, often exceeding the capacity of individual research endeavours.</p>
<p>A more accessible approach involves continued pre-training, where researchers leverage an already pre-trained model and further train it on domain-specific language; for instance, a <em>BERT</em> model might undergo additional training on physics texts. Whilst prompt-based methods also contribute to adaptation, the authors did not elaborate upon them in detail. Crucially, contrastive learning emerges as a pivotal method for generating sentence or document embeddings from existing word embeddings. This technique addresses the fundamental requirement of representing larger linguistic units within the same embedding space as individual words. <em>Sentence BERT</em> stands as a prominent example of this approach, and its implications may feature in forthcoming discussions.</p>
</section>
<section id="retrieval-augmented-generation-rag-for-domain-adaptation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval-Augmented Generation (<em>RAG</em>) for Domain Adaptation</h2>
<p>Retrieval-Augmented Generation (<em>RAG</em>) offers a sophisticated pipeline for adapting Large Language Models to specific scientific domains without necessitating direct model re-training. This architecture orchestrates the collaborative operation of at least two distinct models. The process initiates when a user poses a query, such as “What are <em>LLMs</em>?”. A <em>BERT</em>-type model then encodes this query into a sentence embedding, subsequently scoring its similarity against a comprehensive database of relevant documents.</p>
<p>The system retrieves the most pertinent passages from these documents, integrating these retrieved sentences directly into the prompt of a generative model, such as <em>ChatGPT</em>. Consequently, the generative model synthesises an answer, critically informed by this newly augmented context. This approach underscores a broader principle: contemporary reasoning models and agents transcend the confines of single <em>LLMs</em>, functioning instead as intricate systems that integrate <em>LLMs</em> with a diverse array of supplementary tools.</p>
</section>
<section id="fundamental-distinctions-in-large-language-model-application" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="fundamental-distinctions-in-large-language-model-application"><span class="header-section-number">3.8</span> Fundamental Distinctions in Large Language Model Application</h2>
<p>Understanding the application of Large Language Models necessitates grasping several fundamental distinctions:</p>
<ul>
<li><p>Firstly, models categorise by their underlying architecture and pre-training methodology, encompassing encoder-based systems like <em>BERT</em>, decoder-based systems such as <em>GPT</em>, and hybrid encoder-decoder configurations.</p></li>
<li><p>Secondly, various fine-tuning strategies exist, each tailored to specific tasks and datasets.</p></li>
<li><p>Thirdly, a critical difference lies between word embeddings, which provide contextualised representations of individual words, and sentence embeddings, which capture the meaning of entire sentences.</p></li>
<li><p>Finally, the level of abstraction in <em>LLM</em> deployment varies significantly, ranging from the use of individual <em>LLMs</em> to complex <em>LLM</em> pipelines, such as Retrieval-Augmented Generation, and sophisticated <em>LLM</em> agents, which integrate multiple models and tools to perform intricate tasks.</p></li>
</ul>
</section>
<section id="applications-of-large-language-models-in-hpss-research" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in HPSS Research</h2>
<p>A recent survey investigating the use of Large Language Models (<em>LLMs</em>) in History, Philosophy, and Sociology of Science (<em>HPSS</em>) research has identified four principal categories of application:</p>
<ul>
<li><p><em>Data and Sources</em>: <em>LLMs</em> facilitate the parsing and extraction of critical information, including publication types, acknowledgements, and citations. They also enable more interactive engagement with sources through summarisation and <em>RAG</em>-type conversational interfaces.</p></li>
<li><p><em>Knowledge Structures</em>: <em>LLMs</em> prove instrumental in entity extraction, identifying specific elements such as scientific instruments, celestial bodies, and chemicals. Furthermore, they assist in mapping complex relationships, including disciplinary boundaries, interdisciplinary fields, and science-policy discourses.</p></li>
<li><p><em>Knowledge Dynamics</em>: Researchers employ <em>LLMs</em> to trace conceptual histories, examining the evolution of terms like “theory” within Digital Humanities, or “virtual” and “Planck” in physics. These models also aid in novelty detection, pinpointing breakthrough papers and emerging technologies.</p></li>
<li><p><em>Knowledge Practices</em>: <em>LLMs</em> support argument reconstruction by identifying premises, conclusions, and causal links. They also enhance citation context analysis, a traditional <em>HPSS</em> method, by discerning the purpose and sentiment behind citations. Moreover, <em>LLMs</em> contribute to discourse analysis, enabling the detection of subtle linguistic features such as hedge sentences, specialised jargon, and instances of boundary work.</p></li>
</ul>
</section>
<section id="trends-and-concerns-in-hpss-llm-adoption" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="trends-and-concerns-in-hpss-llm-adoption"><span class="header-section-number">3.10</span> Trends and Concerns in HPSS LLM Adoption</h2>
<p>Observations reveal an accelerating interest in Large Language Models (<em>LLMs</em>) within the History, Philosophy, and Sociology of Science (<em>HPSS</em>). This trend is particularly evident in information science journals, such as <em>Scientometrics</em> and <em>JASIST</em>, and increasingly, in journals traditionally disinclined towards computational methods. The semantic power of these models now attracts qualitative researchers and philosophers, expanding their adoption.</p>
<p>The degree of <em>LLM</em> customisation within <em>HPSS</em> varies considerably, spanning a spectrum from straightforward, off-the-shelf use of tools like <em>ChatGPT</em> to the development of entirely new architectures and bespoke pre-training or fine-tuning regimes. Despite this burgeoning interest, several recurring concerns persist. Researchers frequently cite the overwhelming computational resources demanded by <em>LLMs</em>, their inherent opaqueness, and the pervasive lack of sufficient training data and standardised benchmarks. Furthermore, the necessity of navigating trade-offs between different model types, such as <em>BERT</em>-like and <em>GPT</em>-like architectures, presents a continuous challenge, as no single model universally fits all research purposes. Nevertheless, a discernible trend towards greater accessibility emerges, exemplified by tools like <em>BERTTopic</em>, which offers a user-friendly approach to topic modelling, largely due to diligent developer maintenance.</p>
</section>
<section id="critical-reflections-on-large-language-models-in-hpss" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="critical-reflections-on-large-language-models-in-hpss"><span class="header-section-number">3.11</span> Critical Reflections on Large Language Models in HPSS</h2>
<p>Critical engagement with Large Language Models (<em>LLMs</em>) in <em>HPSS</em> necessitates acknowledging several discipline-specific challenges. Foremost amongst these is the historical evolution of concepts and language; as <em>LLMs</em> typically undergo training on modern linguistic corpora, they may inadvertently introduce biases when applied to historical texts. This reality underscores the imperative either to train bespoke models or to adapt existing ones with acute awareness of these potential distortions.</p>
<p>Furthermore, <em>HPSS</em> inherently adopts a reconstructive and critically reflective perspective, demanding the ability to interpret texts beyond their surface meaning, to discern authorial context, and to identify subtle discursive strategies, such as boundary work. <em>LLMs</em> are not intrinsically equipped for such nuanced readings, thereby requiring innovative approaches to align their capabilities with <em>HPSS</em> methodologies. Compounding these issues are practical data limitations, including sparse datasets, the prevalence of multiple languages, the presence of old scripts, and a general lack of digitalisation.</p>
<p>Consequently, cultivating <em>LLM</em> literacy becomes paramount. This involves not only comprehending the tools of <em>LLMs</em>, Natural Language Processing (<em>NLP</em>), and Deep Learning (<em>DL</em>), but also grasping their underlying theoretical frameworks. Researchers must discern the most suitable architecture and training strategy for their specific <em>HPSS</em> problems and actively contribute to developing shared datasets and benchmarks. Whilst <em>NLP</em> is increasingly moving towards more natural language-based coding, retaining foundational coding skills remains crucial to prevent the uncritical adoption of off-the-shelf tools that might yield visually appealing but methodologically unsound results.</p>
<p>Ultimately, <em>HPSS</em> researchers must remain steadfast in their methodologies. This entails meticulously translating <em>HPSS</em> problems into <em>NLP</em> tasks without compromising the core methodological focus or allowing the technical demands of classification, generation, or summarisation to overshadow the overarching research purpose. Nevertheless, these models present novel opportunities for bridging qualitative and quantitative approaches, fostering interdisciplinary collaboration. Moreover, reflecting upon <em>HPSS</em>’s own pre-history in the development of such models, exemplified by co-word analysis pioneered by scholars like Callon and Rip in the 1980s, reveals a rich theoretical foundation, particularly within Actor-Network Theory, that offers valuable insights for contemporary <em>LLM</em> development.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>