<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jochen Büttner">
<meta name="dcterms.date" content="2025-01-01">

<title>15&nbsp; Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_018.html" rel="next">
<link href="./chapter_ai-nepi_016.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_017.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals, ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Computational Epistemology and Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#limitations-of-current-large-language-models-in-temporal-understanding" id="toc-limitations-of-current-large-language-models-in-temporal-understanding" class="nav-link" data-scroll-target="#limitations-of-current-large-language-models-in-temporal-understanding"><span class="header-section-number">15.1</span> Limitations of Current <em>Large Language Models</em> in Temporal Understanding</a></li>
  <li><a href="#the-challenge-of-temporal-drift-in-language-models" id="toc-the-challenge-of-temporal-drift-in-language-models" class="nav-link" data-scroll-target="#the-challenge-of-temporal-drift-in-language-models"><span class="header-section-number">15.2</span> The Challenge of Temporal Drift in Language Models</a></li>
  <li><a href="#formalising-time-dependent-probability-distributions" id="toc-formalising-time-dependent-probability-distributions" class="nav-link" data-scroll-target="#formalising-time-dependent-probability-distributions"><span class="header-section-number">15.3</span> Formalising Time-Dependent Probability Distributions</a></li>
  <li><a href="#introducing-the-time-transformer-architecture" id="toc-introducing-the-time-transformer-architecture" class="nav-link" data-scroll-target="#introducing-the-time-transformer-architecture"><span class="header-section-number">15.4</span> Introducing the <em>Time Transformer</em> Architecture</a></li>
  <li><a href="#experimental-validation-and-data-acquisition" id="toc-experimental-validation-and-data-acquisition" class="nav-link" data-scroll-target="#experimental-validation-and-data-acquisition"><span class="header-section-number">15.5</span> Experimental Validation and Data Acquisition</a></li>
  <li><a href="#vanilla-transformer-model-implementation" id="toc-vanilla-transformer-model-implementation" class="nav-link" data-scroll-target="#vanilla-transformer-model-implementation"><span class="header-section-number">15.6</span> Vanilla <em>Transformer</em> Model Implementation</a></li>
  <li><a href="#time-transformer-architectural-modification" id="toc-time-transformer-architectural-modification" class="nav-link" data-scroll-target="#time-transformer-architectural-modification"><span class="header-section-number">15.7</span> <em>Time Transformer</em> Architectural Modification</a></li>
  <li><a href="#experimental-validation-of-temporal-drift-learning" id="toc-experimental-validation-of-temporal-drift-learning" class="nav-link" data-scroll-target="#experimental-validation-of-temporal-drift-learning"><span class="header-section-number">15.8</span> Experimental Validation of Temporal Drift Learning</a></li>
  <li><a href="#advanced-temporal-pattern-learning-and-attention-analysis" id="toc-advanced-temporal-pattern-learning-and-attention-analysis" class="nav-link" data-scroll-target="#advanced-temporal-pattern-learning-and-attention-analysis"><span class="header-section-number">15.9</span> Advanced Temporal Pattern Learning and Attention Analysis</a></li>
  <li><a href="#proof-of-concept-and-future-implications" id="toc-proof-of-concept-and-future-implications" class="nav-link" data-scroll-target="#proof-of-concept-and-future-implications"><span class="header-section-number">15.10</span> Proof of Concept and Future Implications</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jochen Büttner <a href="mailto:buettner@gea.mpg.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Max Planck Institute of Geoanthropology
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter introduces a novel architectural modification for <em>Large Language Models</em> (<em>LLMs</em>), meticulously designed to imbue them with explicit time awareness. This enhancement promises to significantly improve their utility for historical analysis and a broader spectrum of applications. Current <em>LLMs</em> derive their understanding of time implicitly from statistical patterns within training data, a method that often proves insufficient when processing temporally sensitive information, such as contradictory statements from different eras. The proposed “<em>Time Transformer</em>” architecture directly addresses this limitation by injecting a temporal dimension into each token’s latent semantic representation.</p>
<p>To validate this concept, the authors employed a modest <em>Transformer</em> model, training it on a highly restricted, repetitive dataset: daily weather reports from the UK Met Office spanning 2018 to 2024. This dataset, comprising approximately 2,500 reports and a vocabulary of 3,395 words, facilitated rapid experimentation. The vanilla <em>Transformer</em>, featuring four decoder layers and 39 million parameters, successfully reproduced the language patterns characteristic of these weather reports.</p>
<p>The core innovation involves reserving a single dimension within the 512-dimensional token embedding for a non-trainable, min-max normalised “day of the year” value. This straightforward modification enables the model to learn and reproduce time-dependent patterns with remarkable efficacy. The authors demonstrated this through two key experiments: firstly, the model accurately reproduced a synthetically injected “rain” to “liquid sunshine” replacement pattern across the year; secondly, it learned a synthetic co-occurrence shift, where “rain” became obligatorily followed by “and snow” towards the year’s end. Further analysis of the attention heads revealed that specific heads specialised in processing this crucial temporal information.</p>
<p>Whilst proving the concept, the approach presents challenges for large-scale application. These primarily stem from the necessity of training models from scratch and the complex, labour-intensive process of accurately dating historical token sequences. Nevertheless, the research suggests potential for improved training efficiency and highlights the utility of such time-aware embeddings for targeted tasks, such as semantic shift detection in historical corpora.</p>
</section>
<section id="limitations-of-current-large-language-models-in-temporal-understanding" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="limitations-of-current-large-language-models-in-temporal-understanding"><span class="header-section-number">15.1</span> Limitations of Current <em>Large Language Models</em> in Temporal Understanding</h2>
<p>Current <em>Large Language Models</em> (<em>LLMs</em>) inherently possess merely an implicit understanding of time, which they derive statistically from the textual patterns observed during their extensive training. Whilst these models demonstrate a remarkable grasp of temporal concepts, their comprehension stems from the implicit cues embedded within the training data. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns within training data as a function of time, would undoubtedly enhance their application in historical analysis and, indeed, across a broader spectrum of domains.</p>
<p>Consider, for instance, two sentences that differ only in their final two words: “The primary architectures for processing text through NNs are <em>LSTMs</em>” and “The primary architectures for processing text through NNs are <em>Transformers</em>.” These statements are inherently contradictory, yet human readers instinctively recognise their temporal distinction. When such conflicting information appears within an <em>LLM</em>’s training data without explicit temporal context, these statements compete for the model’s attention. Consequently, the model cannot perfectly fulfil its objective, as a preference for one statement inevitably leads to an error regarding the other.</p>
<p>During inference, when presented with a sequence such as “The primary architectures for processing text through NNs are…”, an <em>LLM</em> typically exhibits a recency bias, often predicting “<em>Transformers</em>” due to its prevalence in more recent data. Manipulating the input context, for example by adding “In 2017” or altering verb tenses, constitutes a form of prompt engineering—an imprecise method of “fishing in the dark” to exploit the model’s implicit temporal understanding. A more robust approach necessitates explicit time awareness, enabling models to accurately reflect temporal shifts in information.</p>
</section>
<section id="the-challenge-of-temporal-drift-in-language-models" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="the-challenge-of-temporal-drift-in-language-models"><span class="header-section-number">15.2</span> The Challenge of Temporal Drift in Language Models</h2>
<p><em>Large Language Models</em> fundamentally operate by estimating the probability distribution over their vocabulary for the next token, given a preceding sequence of tokens. This process can be formally expressed as p(x_n | x_1, …, x_{n-1}). However, in the real world, the probability of a token appearing within a specific context is not static; rather, it demonstrably depends on time, a relationship represented as p(x_n | x_1, …, x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a particular time, t, follows the product of these conditional probabilities: p(x_1, x_2, …, x_n | t) = Π_{k=1}^n p(x_k | x_1, …, x_{k-1}, t).</p>
<p>Crucially, during their training, current <em>LLMs</em> largely treat these complex, time-dependent probability distributions as static. This simplification means that whilst they may implicitly capture some temporal nuances, their ability to reflect genuine temporal drift in the underlying token sequence distributions during inference is limited to in-context learning. This reliance on contextual cues, as previously illustrated, often proves insufficient for tasks requiring precise temporal understanding.</p>
</section>
<section id="formalising-time-dependent-probability-distributions" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="formalising-time-dependent-probability-distributions"><span class="header-section-number">15.3</span> Formalising Time-Dependent Probability Distributions</h2>
<p><em>Large Language Models</em> fundamentally estimate the probability distribution over their vocabulary for the next token, conditioned on a sequence of preceding tokens, formally expressed as p(x_n | x_1, …, x_{n-1}). Nevertheless, in practical applications, the probability of a token appearing within a given context is not static; instead, it inherently depends on time, a relationship denoted as p(x_n | x_1, …, x_{n-1}, t). This temporal dependency extends to entire sequences of tokens uttered at a specific time, t, where the probability is the product of individual conditional probabilities: p(x_1, x_2, …, x_n | t) = Π_{k=1}^n p(x_k | x_1, …, x_{k-1}, t).</p>
<p>Despite this inherent temporal variability in real-world data, current <em>LLMs</em> typically treat these probability distributions as static during their training phase. Consequently, during inference, these models can only reflect temporal drift in the underlying token sequence distributions through in-context learning. This limitation underscores the challenge of accurately modelling dynamic linguistic patterns without explicit temporal conditioning.</p>
</section>
<section id="introducing-the-time-transformer-architecture" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="introducing-the-time-transformer-architecture"><span class="header-section-number">15.4</span> Introducing the <em>Time Transformer</em> Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>To enhance the temporal understanding of <em>Large Language Models</em>, the authors recognised the necessity of explicitly modelling the time-dependent probability distribution, p(x_n | x_1, …, x_{n-1}, t). Traditional approaches, such as time slicing—training separate models for distinct temporal segments—prove exceptionally data inefficient. A more elegant solution, which the authors termed the “<em>Time Transformer</em>”, proposes a straightforward yet powerful modification: augmenting the latent semantic token features with an additional temporal dimension.</p>
<p>This architectural innovation involves extending the standard token embedding, E(x), to include a time component, φ(t), resulting in a new embedding function: E(x, t) = {e_1(x), e_2(x), …, e_{d-1}(x), φ(t)}. Consequently, the <em>Transformer</em> receives a sequence of time-dependent token embeddings, [E(x_1, t), E(x_2, t), …, E(x_{n-1}, t)], which then enables the model to output a truly time-dependent probability distribution, p_θ(x_n | x_1, …, x_{n-1}, t). Crucially, the fundamental training objective remains unchanged; the model continues to minimise the negative log likelihood across all sequences and partial sequences.</p>
<p>By directly injecting time into each token’s representation, the model gains the capacity to learn precisely how strongly or weakly this temporal dimension influences individual tokens. For certain words, the temporal context may hold little significance, whilst for others, it proves paramount. The inherent efficiency of the <em>Transformer</em> architecture then allows for the robust statistical processing of this newly integrated temporal information, facilitating a more nuanced and accurate understanding of language evolution.</p>
</section>
<section id="experimental-validation-and-data-acquisition" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="experimental-validation-and-data-acquisition"><span class="header-section-number">15.5</span> Experimental Validation and Data Acquisition</h2>
<p>To validate their proposed <em>Time Transformer</em> concept, the authors sought a dataset characterised by restricted language, a compact vocabulary, and repetitive patterns. The daily weather reports issued by the UK’s National Meteorological Service, the Met Office, proved an ideal candidate. These historical reports are readily accessible through their digital archive, located at <code>https://digital.nmla.metoffice.gov.uk/</code>. Another potential resource, “<em>TinyStories</em>”, was also identified as a suitable alternative for similar experimental contexts.</p>
<p>For the current study, the team systematically scraped data spanning from 2018 to 2024, yielding approximately 2,500 individual weather reports. This raw text then underwent a process of chunking and tokenisation. Notably, the processing eschewed sub-word tokenisation, whilst the authors deliberately disregarded both case and interpunctuation. This simplified approach resulted in a remarkably concise vocabulary, comprising merely 3,395 unique words across the entire seven-year dataset.</p>
</section>
<section id="vanilla-transformer-model-implementation" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="vanilla-transformer-model-implementation"><span class="header-section-number">15.6</span> Vanilla <em>Transformer</em> Model Implementation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>To establish a baseline and confirm the capacity for pattern learning, the authors developed a modest <em>Transformer</em> model. This architecture incorporates four multi-head attention decoder blocks, each meticulously designed. Within each decoder layer, an eight-head multi-head attention mechanism processes input, followed by a normalisation layer, a non-linear feed-forward network, and a subsequent normalisation. A final dense layer then undertakes the critical task of assigning probabilities to each token within the vocabulary.</p>
<p>This compact model comprises 39 million parameters, occupying approximately 150 MB of memory—a stark contrast to models like <em>GPT-4</em>, which command 1.8 trillion parameters distributed across 120 layers. Training occurred on two A100 GPUs within an HPC cluster in Munich, demonstrating remarkable efficiency with only 11 seconds required per epoch, a speed attributable to both the diminutive dataset and the model’s modest scale. The complete codebase for the model and its associated experiments is openly accessible via <code>https://github.com/j-buettner/time_transformer</code>. Crucially, the model proficiently learned to reproduce the distinct language of the weather reports. For instance, providing a seed sequence such as “During the night, a band…”, the model generated text virtually indistinguishable from authentic weather forecasts, confirming its successful acquisition of the underlying linguistic patterns.</p>
</section>
<section id="time-transformer-architectural-modification" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="time-transformer-architectural-modification"><span class="header-section-number">15.7</span> <em>Time Transformer</em> Architectural Modification</h2>
<p>The architectural modification required for the <em>Time Transformer</em> is remarkably minimal, representing a key advantage of the proposed approach. Within the 512-dimensional latent semantic space, the authors specifically reserved one dimension for encoding temporal information. This means the text input occupies 511 dimensions, whilst the time data is allocated a single dimension.</p>
<p>Crucially, this temporal dimension remains non-trainable. Instead, the authors populated it with a min-max normalised “day of the year” value, calculated as (day of year - 1) / (365 - 1). This particular choice was strategic, designed to exploit the inherent seasonal variations present within the weather report dataset, such as the prevalence of snow in winter or higher temperatures in summer. Nevertheless, the framework remains flexible, readily accommodating any other desired time embedding, offering adaptability for diverse temporal contexts.</p>
</section>
<section id="experimental-validation-of-temporal-drift-learning" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="experimental-validation-of-temporal-drift-learning"><span class="header-section-number">15.8</span> Experimental Validation of Temporal Drift Learning</h2>
<p>A central inquiry for the <em>Time Transformer</em> concerned its capacity to efficiently learn temporal drift within the underlying data distribution. The initial experiment, designated “synonymic succession,” directly addressed this by injecting a synthetic temporal drift into the training data. This involved a time-dependent replacement of the word “rain” with “liquid sunshine,” a substitution governed by a sigmoid function tied to the day of the year. The probability of this replacement progressively increased from zero at the year’s commencement to one by its conclusion.</p>
<p>To rigorously evaluate the model’s learning, the authors generated a distinct weather prediction for each day of the year. Subsequently, they meticulously counted the monthly frequencies of “rain” versus “liquid sunshine” occurrences within these generated sequences. The results unequivocally demonstrated that the model perfectly reproduced the synthetically introduced pattern. “Rain” appeared predominantly at the beginning of the year, whilst “liquid sunshine” dominated towards the end, with the transition occurring precisely in the middle of the year, exhibiting only minor, statistically expected variations. This outcome robustly confirmed the model’s ability to acquire and replicate time-dependent linguistic shifts.</p>
</section>
<section id="advanced-temporal-pattern-learning-and-attention-analysis" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="advanced-temporal-pattern-learning-and-attention-analysis"><span class="header-section-number">15.9</span> Advanced Temporal Pattern Learning and Attention Analysis</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>The second experiment advanced the investigation by focusing on altering a co-occurrence pattern, effectively demonstrating the “fixation of a collocation” from a variable to an obligatory relationship. The authors synthetically injected a time-dependent change: instances of “rain” not immediately followed by “and” were systematically replaced with “rain and snow.” This intervention, whilst altering a weather pattern from a meteorological perspective, fundamentally modified a linguistic collocation within the model’s domain, akin to fixing the phrase “bread and butter.”</p>
<p>Again, the authors assessed the model’s performance by generating one prediction for each day of the year. The results conclusively showed that the model successfully acquired this complex temporal pattern. Towards the year’s end, the generated text almost exclusively featured “raining and snowing,” whilst the beginning of the year saw a mix of “rain” and occasional “rain and snow.” Notably, these early occurrences of “rain and snow” were naturally conditioned on the presence of a “cold system,” indicating the model’s nuanced understanding. Further analysis of the attention heads within the <em>Transformer</em> architecture revealed that specific heads had specialised in processing this intricate temporal pattern, with the attention from “snow” to “rain” across different heads clearly illustrating the model’s learned temporal sensitivity.</p>
</section>
<section id="proof-of-concept-and-future-implications" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="proof-of-concept-and-future-implications"><span class="header-section-number">15.10</span> Proof of Concept and Future Implications</h2>
<p>The conducted experiments unequivocally establish a proof of concept: <em>Transformer</em>-based <em>Large Language Models</em> can indeed be rendered explicitly time-aware through the straightforward addition of a temporal dimension to their token embeddings. This fundamental innovation carries profound implications for a diverse array of applications, particularly within historical analysis.</p>
<p>Future research could profitably investigate whether this explicit temporal dimension might enhance training efficiency. By providing direct temporal cues, the model could potentially decipher underlying patterns more readily, circumventing the need to infer such information from implicit textual signals alone. Nevertheless, significant challenges impede the immediate, widespread application of this architecture.</p>
<p>Firstly, the approach necessitates training models from scratch, as existing, pre-trained <em>LLMs</em> cannot be simply fine-tuned to incorporate this novel temporal dimension. Consequently, the computational resources required for any serious, large-scale implementation remain substantial. Secondly, the method compromises the “metadata-free” elegance of traditional self-supervised learning. Each token sequence would demand a precise date assignment, a task fraught with complexity for historical documents, where distinguishing between original utterance dates and reprint dates, for instance, presents considerable curatorial hurdles.</p>
<p>As a pragmatic alternative, the authors suggest that the principle could be applied to develop a specialised embedding model, akin to a <em>BERT</em>-like embedder. Such a model, rather than being fully generative, would focus on targeted tasks, learning only the temporal patterns relevant to its specific domain. This approach would mitigate the computational demands and data curation complexities associated with training comprehensive generative models, whilst still leveraging the benefits of explicit time awareness.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_016.html" class="pagination-link" aria-label="Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_018.html" class="pagination-link" aria-label="Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>