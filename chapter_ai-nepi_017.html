<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jochen Büttner">
<meta name="dcterms.date" content="2025-06-21">

<title>15&nbsp; Time-aware large language models towards a novel architecture for historical analysis – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_018.html" rel="next">
<link href="./chapter_ai-nepi_016.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_017.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Validation is All You Need</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Study of LDA and BERTopic Performance Across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#temporal-limitations-in-current-llms" id="toc-temporal-limitations-in-current-llms" class="nav-link" data-scroll-target="#temporal-limitations-in-current-llms"><span class="header-section-number">15.1</span> Temporal Limitations in Current LLMs</a></li>
  <li><a href="#formalising-temporal-dependence" id="toc-formalising-temporal-dependence" class="nav-link" data-scroll-target="#formalising-temporal-dependence"><span class="header-section-number">15.2</span> Formalising Temporal Dependence</a></li>
  <li><a href="#modelling-temporal-drift" id="toc-modelling-temporal-drift" class="nav-link" data-scroll-target="#modelling-temporal-drift"><span class="header-section-number">15.3</span> Modelling Temporal Drift</a></li>
  <li><a href="#the-time-transformer-concept" id="toc-the-time-transformer-concept" class="nav-link" data-scroll-target="#the-time-transformer-concept"><span class="header-section-number">15.4</span> The <em>Time Transformer</em> Concept</a></li>
  <li><a href="#dataset-curation-and-processing" id="toc-dataset-curation-and-processing" class="nav-link" data-scroll-target="#dataset-curation-and-processing"><span class="header-section-number">15.5</span> Dataset Curation and Processing</a></li>
  <li><a href="#baseline-model-architecture" id="toc-baseline-model-architecture" class="nav-link" data-scroll-target="#baseline-model-architecture"><span class="header-section-number">15.6</span> Baseline Model Architecture</a></li>
  <li><a href="#integrating-the-temporal-dimension" id="toc-integrating-the-temporal-dimension" class="nav-link" data-scroll-target="#integrating-the-temporal-dimension"><span class="header-section-number">15.7</span> Integrating the Temporal Dimension</a></li>
  <li><a href="#dataset-suitability-for-time-aware-models" id="toc-dataset-suitability-for-time-aware-models" class="nav-link" data-scroll-target="#dataset-suitability-for-time-aware-models"><span class="header-section-number">15.8</span> Dataset Suitability for Time-Aware Models</a></li>
  <li><a href="#a-comparative-architecture" id="toc-a-comparative-architecture" class="nav-link" data-scroll-target="#a-comparative-architecture"><span class="header-section-number">15.9</span> A Comparative Architecture</a></li>
  <li><a href="#experiment-1-synonymic-succession" id="toc-experiment-1-synonymic-succession" class="nav-link" data-scroll-target="#experiment-1-synonymic-succession"><span class="header-section-number">15.10</span> Experiment 1: Synonymic Succession</a></li>
  <li><a href="#experiment-2-collocation-fixation" id="toc-experiment-2-collocation-fixation" class="nav-link" data-scroll-target="#experiment-2-collocation-fixation"><span class="header-section-number">15.11</span> Experiment 2: Collocation Fixation</a></li>
  <li><a href="#conclusions-and-future-directions" id="toc-conclusions-and-future-directions" class="nav-link" data-scroll-target="#conclusions-and-future-directions"><span class="header-section-number">15.12</span> Conclusions and Future Directions</a></li>
  <li><a href="#supplementary-resource" id="toc-supplementary-resource" class="nav-link" data-scroll-target="#supplementary-resource"><span class="header-section-number">15.13</span> Supplementary Resource</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jochen Büttner <a href="mailto:buettner@gea.mpg.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Max Planck Institute of Geoanthropology
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This presentation introduces a novel architectural approach for Large Language Models (LLMs), which the speaker terms the <em>Time Transformer</em>. The innovation directly addresses a fundamental limitation of current models, which derive only an implicit understanding of time from statistical patterns within their training data. The speaker highlights that whilst existing models demonstrate remarkable capabilities, their lack of explicit temporal conditioning can lead to inconsistencies when processing information that evolves, such as historical facts or linguistic trends.</p>
<p>The proposed <em>Time Transformer</em> integrates a dedicated temporal dimension directly into the token embeddings, thereby enabling the model to learn and reproduce changing linguistic patterns as a direct function of time. The authors validated this concept using a small generative LLM trained on a highly constrained dataset of Met Office weather reports. Their work demonstrates the model’s ability to capture and reproduce time-dependent linguistic shifts with high efficiency. The presentation explores the theoretical underpinnings of this approach, details the model architecture and data preparation, and presents two experiments demonstrating its efficacy in learning synthetic temporal drifts. Furthermore, it outlines potential applications, including historical analysis and instruction-tuned models, whilst acknowledging challenges related to fine-tuning and data curation.</p>
</section>
<section id="temporal-limitations-in-current-llms" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="temporal-limitations-in-current-llms"><span class="header-section-number">15.1</span> Temporal Limitations in Current LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>Current Large Language Models possess an inherently implicit understanding of time, derived statistically from the vast textual corpora used for their training. Whilst these models exhibit a profound grasp of temporal concepts, their comprehension stems from subtle cues embedded within the data rather than from explicit temporal conditioning. The authors contend that explicit time awareness would demonstrably enhance their utility, particularly within historical analysis and across a broader spectrum of applications.</p>
<p>Consider, for instance, two sentences that differ solely in their temporal context: ‘The primary architecture for processing text through Neural Networks is LSTM’ and ‘The primary architecture for processing text through Neural Networks is <em>Transformer</em>.’ Without explicit temporal information, these statements, representing different states of affairs in 2017 and 2025 respectively, directly contradict one another within an LLM’s training data. The model must then arbitrarily favour one, inevitably making an error regarding the other.</p>
<p>Furthermore, a discernible recency bias often influences LLM predictions, favouring more contemporary information. Current methods, such as prompt engineering, merely attempt to exploit the model’s implicit temporal understanding, a process the speaker likens to ‘fishing in the dark’ for desired outcomes. To overcome these limitations, the authors propose integrating time directly into the token embeddings of <em>Transformer</em>-based LLMs. This architectural modification aims to render LLMs explicitly time-aware, enabling them to learn and reproduce evolving linguistic patterns as a direct function of time.</p>
</section>
<section id="formalising-temporal-dependence" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="formalising-temporal-dependence"><span class="header-section-number">15.2</span> Formalising Temporal Dependence</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Fundamentally, Large Language Models operate by estimating the probability distribution over their vocabulary for the next token, conditioned on a sequence of preceding tokens. This process is mathematically represented as p(x_n | x_1, …, x_{n-1}). In the real world, however, the likelihood of a token appearing is not static; it is intrinsically dependent on time, thus becoming p(x_n | x_1, …, x_{n-1}, t).</p>
<p>Extending this principle, the joint probability for an entire sequence of tokens uttered at a particular time <em>t</em> is expressed as the product of these conditional probabilities: p(x_1, …, x_n | t) = ∏ p(x_k | x_1, …, x_{k-1}, t). Despite this inherent temporal variability, current LLM training processes frequently treat these probability distributions as static. Consequently, during inference, these models can only reflect temporal drift through in-context learning, a mechanism that relies on the immediate context provided rather than an explicit, integrated understanding of time.</p>
</section>
<section id="modelling-temporal-drift" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="modelling-temporal-drift"><span class="header-section-number">15.3</span> Modelling Temporal Drift</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>A significant challenge in current LLM training lies in the treatment of inherently time-dependent probability distributions as static. This simplification means that whilst the real-world likelihood of a token is a direct function of time—for instance, the probability of ‘transformer’ completing a sentence was effectively zero in 2017—LLMs primarily reflect such temporal drift only through in-context learning during inference.</p>
<p>To improve upon this, the authors sought more effective methods for modelling these dynamic, time-dependent probability distributions. Existing strategies, such as ‘time slicing’—where distinct models are trained for specific temporal segments—prove remarkably data-inefficient, as they assume static distributions within those slices. A more streamlined and integrated approach is therefore imperative.</p>
</section>
<section id="the-time-transformer-concept" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="the-time-transformer-concept"><span class="header-section-number">15.4</span> The <em>Time Transformer</em> Concept</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>To overcome the limitations of implicit temporal understanding, the authors propose an innovative solution termed the <em>Time Transformer</em>. This concept centres on a remarkably simple yet profound architectural adjustment: reserving a single dimension within the token embedding space specifically for time. This dedicated dimension explicitly conveys the utterance date for each token sequence, thereby providing direct temporal context.</p>
<p>The initial implementation employs a non-trainable, min-max normalised ‘day of the year’ as the time embedding. The team strategically chose this feature to exploit natural seasonal variations inherent in their chosen dataset, such as the prevalence of snow in winter or heat in summer. The framework, however, readily accommodates alternative time embeddings as required.</p>
<p>For their proof of concept, the authors selected Met Office weather reports as the primary dataset. This text corpus is characterised by its limited vocabulary and simple, repetitive language, making it an ideal candidate for initial validation. The UK’s national meteorological service issues these daily reports, and historical data remains accessible through its digital archive.</p>
</section>
<section id="dataset-curation-and-processing" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="dataset-curation-and-processing"><span class="header-section-number">15.5</span> Dataset Curation and Processing</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The team systematically acquired the dataset by scraping daily weather reports from Met Office PDFs spanning the years 2018 to 2024. This process yielded approximately 2,500 reports, each comprising between 150 and 200 words. For text processing, they employed <em>tf.keras.layers.TextVectorization</em>, standardising the input by converting text to lowercase and stripping punctuation.</p>
<p>Crucially, the tokenization process avoided sub-word segmentation and deliberately neglected case and interpunctuation, reflecting the inherently simple nature of the language. This straightforward approach resulted in a remarkably concise vocabulary of just 3,395 unique words across the entire seven-year corpus. An illustrative example, the Daily Weather Summary for Sunday 04 August 2019, details showery rain and mist, whilst its Daily Extremes table highlights a highest maximum temperature of 27.5°C recorded in Writtle, Essex.</p>
</section>
<section id="baseline-model-architecture" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="baseline-model-architecture"><span class="header-section-number">15.6</span> Baseline Model Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>To establish a baseline for language pattern learning, the authors constructed a modest-sized, decoder-only <em>Transformer</em> architecture, which they term the <em>Vanilla model</em>. This architecture processes input through an Embedding Layer (d_model=512), followed by Positional Encoding and a Dropout layer (rate=0.1). Subsequently, the input traverses a stack of four Multi-Head Attention Decoder Blocks. The final output from these layers feeds into a Dense Layer, sized to the vocabulary, which produces the model’s predictions.</p>
<p>This compact model contains 39 million parameters, a stark contrast to models such as <em>GPT-4</em>, which commands 1.8 trillion parameters. Despite its modest scale, the model trains with remarkable efficiency on an HPC cluster, completing each epoch in merely 11 seconds. During training, its accuracy steadily improved, with validation accuracy stabilising around 0.38, demonstrating its capacity to reproduce the language patterns observed in the weather reports.</p>
</section>
<section id="integrating-the-temporal-dimension" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="integrating-the-temporal-dimension"><span class="header-section-number">15.7</span> Integrating the Temporal Dimension</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>The established <em>Vanilla model</em> demonstrates a robust capacity to reproduce the language of weather reports. When provided with a seed sequence such as ‘During the night, a band…’, the model autoregressively generates coherent and contextually relevant text that closely mimics actual forecasts.</p>
<p>The transition to a <em>Time Transformer</em> involves a remarkably minimal architectural adjustment. Instead of embedding all information within a 512-dimensional latent semantic space, the authors reserve one dimension specifically for temporal data. This dedicated dimension explicitly informs each token about the precise date on which its sequence was uttered. The current implementation employs a non-trainable, min-max normalised ‘day of the year’ as this time embedding, a choice driven by the desire to leverage natural seasonal variations inherent in weather data.</p>
</section>
<section id="dataset-suitability-for-time-aware-models" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="dataset-suitability-for-time-aware-models"><span class="header-section-number">15.8</span> Dataset Suitability for Time-Aware Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>To test their proposed time-aware model rigorously, the authors required a dataset characterised by restricted, repetitive language and a small vocabulary, thereby simplifying the task of pattern learning. The UK Met Office weather reports proved an ideal choice, being readily accessible online from the national meteorological service. The team also identified the <em>TinyStories</em> dataset as a potential alternative for future investigations.</p>
<p>The data, originally presented as monthly PDFs, was systematically scraped for the period spanning 2018 to 2024, yielding approximately 2,500 reports. The intentionally simple tokenization process, which eschewed sub-word segmentation and neglected case, resulted in a compact vocabulary of just 3,400 words. This linguistic simplicity makes the dataset an excellent testbed for isolating and analysing the model’s ability to learn temporal dependencies.</p>
</section>
<section id="a-comparative-architecture" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="a-comparative-architecture"><span class="header-section-number">15.9</span> A Comparative Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>The <em>Time Transformer</em> represents a minimal yet impactful architectural adjustment to the standard <em>Transformer</em> decoder. In a conventional <em>Vanilla Transformer</em>, the input flows directly into an Embedding Layer (d_model=512), followed by Positional Encoding, Dropout, and a series of Decoder Layers before the final output.</p>
<p>Conversely, the <em>Time Transformer</em> introduces two distinct inputs: textual data and temporal data. The textual input undergoes embedding into a d_model of 511, whilst the temporal data is embedded into a dedicated d_model of 1. These two embedded streams are then concatenated, maintaining the overall embedding dimension, before proceeding through the identical sequence of Positional Encoding, Dropout, and Decoder Layers. The time dimension itself is implemented as a non-trainable, min-max normalised ‘day of the year’. This explicit integration directly addresses the challenge that real-world token likelihoods are inherently time-dependent, a dynamic often overlooked by conventional training processes.</p>
</section>
<section id="experiment-1-synonymic-succession" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="experiment-1-synonymic-succession"><span class="header-section-number">15.10</span> Experiment 1: Synonymic Succession</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>To assess the <em>Time Transformer’s</em> capacity for learning temporal drift efficiently, the authors conducted an experiment they term <em>synonymic succession</em>. This involved injecting a synthetic, time-dependent drift directly into the training data: the word ‘rain’ was progressively replaced by ‘liquid sunshine’ throughout the year. The objective was to ascertain whether the model could reproduce this engineered temporal dependence in its predictions.</p>
<p>The probability of this replacement followed an S-shaped curve, commencing near zero in January and ascending to approach 1.00 by the year’s end. Analysis of monthly occurrences in the generated sequences clearly demonstrated the model’s successful capture of this drift. Occurrences of ‘Rain’ predominated in the earlier months, whilst ‘Liquidsunshine’ became significantly more frequent and eventually dominant in the latter half of the year.</p>
</section>
<section id="experiment-2-collocation-fixation" class="level2" data-number="15.11">
<h2 data-number="15.11" class="anchored" data-anchor-id="experiment-2-collocation-fixation"><span class="header-section-number">15.11</span> Experiment 2: Collocation Fixation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>A second experiment, described as ‘changing a weather pattern’ or <em>fixation of a collocation</em>, further investigated the model’s ability to capture temporal shifts. This involved injecting a synthetic change in co-occurrence: the pattern ‘rain’ followed by any word other than ‘and’ was progressively altered to ‘rain and snow’. Linguistically, this process simulates the fixation of a collocation, akin to an established phrase such as ‘bread and butter’.</p>
<p>The results, visualised through monthly comparisons, clearly demonstrated the injected temporal shift. Occurrences of ‘Rain Only’ were notably higher in the first half of the year, whilst ‘Rain and Snow’ became significantly more frequent in the latter half. Furthermore, an analysis of attention weights revealed that the token ‘snow’ consistently exhibited the highest attention on ‘rain’, indicating that the model successfully learned this evolving co-occurrence pattern.</p>
</section>
<section id="conclusions-and-future-directions" class="level2" data-number="15.12">
<h2 data-number="15.12" class="anchored" data-anchor-id="conclusions-and-future-directions"><span class="header-section-number">15.12</span> Conclusions and Future Directions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>The conducted research establishes a clear proof of concept: <em>Transformer</em>-based models can be efficiently rendered time-aware by integrating a dedicated temporal dimension within their token embeddings. This innovation opens several compelling applications. A foundational <em>Time Transformer</em>, for instance, could provide an exceptional basis for downstream tasks involving historical data. Moreover, an instruction-tuned variant would empower users to ‘talk to a specific time’, potentially yielding superior results even in common usage.</p>
<p>Beyond temporal dynamics, the methodology readily extends to modelling dependencies on other contextual dimensions, such as country or genre. Future research endeavours include benchmarking this approach against explicit time-token methods and testing for potential increases in training efficiency. Nevertheless, challenges persist. Uncertainty surrounds the feasibility of fine-tuning due to the architectural modifications. Furthermore, the approach necessitates a departure from metadata-free self-supervised learning, plunging the team into the complexities of data curation.</p>
</section>
<section id="supplementary-resource" class="level2" data-number="15.13">
<h2 data-number="15.13" class="anchored" data-anchor-id="supplementary-resource"><span class="header-section-number">15.13</span> Supplementary Resource</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>A supplementary resource, specifically a <em>ChatGPT</em> conversation, is available for further exploration at the following URL: https://chatgpt.com/c/67b8237a-2a48-8012-9862-80af84830a17.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_016.html" class="pagination-link" aria-label="A Comparative Study of LDA and BERTopic Performance Across Text Levels">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Study of LDA and BERTopic Performance Across Text Levels</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_018.html" class="pagination-link" aria-label="Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>