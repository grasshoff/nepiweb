<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jochen Büttner">
<meta name="dcterms.date" content="2025-01-01">

<title>15&nbsp; Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_018.html" rel="next">
<link href="./chapter_ai-nepi_016.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-a126389619fad6dbfb296a5315d49fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_017.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Philosophy at Scale: Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modelling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">SDG-Research in Bibliometric DBs - LLMs for HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems for Philosophical Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Unlocking Science’s Hidden Dynamics: A Computational Approach to Archival Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Transforming Biographical Sources into Knowledge Graphs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#addressing-implicit-temporal-understanding" id="toc-addressing-implicit-temporal-understanding" class="nav-link" data-scroll-target="#addressing-implicit-temporal-understanding"><span class="header-section-number">15.1</span> Addressing Implicit Temporal Understanding</a></li>
  <li><a href="#formalising-time-dependent-probabilities-the-time-transformer" id="toc-formalising-time-dependent-probabilities-the-time-transformer" class="nav-link" data-scroll-target="#formalising-time-dependent-probabilities-the-time-transformer"><span class="header-section-number">15.2</span> Formalising Time-Dependent Probabilities: The Time Transformer</a></li>
  <li><a href="#empirical-validation-data-and-architecture" id="toc-empirical-validation-data-and-architecture" class="nav-link" data-scroll-target="#empirical-validation-data-and-architecture"><span class="header-section-number">15.3</span> Empirical Validation: Data and Architecture</a></li>
  <li><a href="#time-transformer-implementation-and-results" id="toc-time-transformer-implementation-and-results" class="nav-link" data-scroll-target="#time-transformer-implementation-and-results"><span class="header-section-number">15.4</span> Time Transformer: Implementation and Results</a></li>
  <li><a href="#proof-of-concept-applications-and-challenges" id="toc-proof-of-concept-applications-and-challenges" class="nav-link" data-scroll-target="#proof-of-concept-applications-and-challenges"><span class="header-section-number">15.5</span> Proof of Concept, Applications, and Challenges</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></h1>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jochen Büttner <a href="mailto:buettner@gea.mpg.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Max Planck Institute of Geoanthropology
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This report details a novel approach for imbuing large language models (<em>LLMs</em>) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, <em>LLMs</em> derive their understanding of time implicitly from statistical patterns within training texts <span class="citation" data-cites="Brown2020 Devlin2019">(<a href="#ref-Brown2020" role="doc-biblioref"><strong>Brown2020?</strong></a>; <a href="#ref-Devlin2019" role="doc-biblioref"><strong>Devlin2019?</strong></a>)</span>. However, this implicit method proves insufficient for tasks demanding precise temporal context, often leading to “recency bias” or an inability to reconcile temporally contradictory information.</p>
<p>To overcome this, researchers propose the “<em>Time Transformer</em>”, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities arising from temporally contradictory information within training data.</p>
<p>To validate this concept, engineers developed a modest <em>Transformer</em> model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the <em>Time Transformer</em>’s efficacy. Experiments involved injecting synthetic temporal drifts—both synonymic succession (e.g., replacing “rain” with “liquid sunshine”) and co-occurrence changes (e.g., <code>rain</code> becoming <code>rain and snow</code>)—into the training data. The <em>Time Transformer</em> consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.</p>
<p>Beyond this proof of concept, the <em>Time Transformer</em> holds significant implications for historical analysis, offering a robust foundation for downstream tasks on historical data and enabling instruction-tuned models to “talk to a specific time.” Whilst this architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking against explicit time-token approaches and investigating the utility of a modest, targeted encoder model. This work represents a crucial step towards developing more temporally intelligent and historically accurate language models.</p>
</section>
<section id="addressing-implicit-temporal-understanding" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="addressing-implicit-temporal-understanding"><span class="header-section-number">15.1</span> Addressing Implicit Temporal Understanding</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>Current large language models (<em>LLMs</em>) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora <span class="citation" data-cites="Brown2020">(<a href="#ref-Brown2020" role="doc-biblioref"><strong>Brown2020?</strong></a>)</span>. Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.</p>
<p>A critical challenge arises when training data contains temporally contradictory information. Consider, for instance, two sentences: “The primary architectures for processing text through NNs are <em>LSTMs</em>” (true in 2017) and “The primary architectures for processing text through NNs are <em>Transformers</em>” (true in 2025) <span class="citation" data-cites="Vaswani2017 Hochreiter1997">(<a href="#ref-Vaswani2017" role="doc-biblioref"><strong>Vaswani2017?</strong></a>; <a href="#ref-Hochreiter1997" role="doc-biblioref"><strong>Hochreiter1997?</strong></a>)</span>. Without explicit temporal context, an <em>LLM</em> treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a “recency bias,” favouring more recent information in next-token prediction. Current workarounds, such as prompt engineering—inserting explicit temporal cues like “In 2017”—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge <span class="citation" data-cites="Liu2023">(<a href="#ref-Liu2023" role="doc-biblioref"><strong>Liu2023?</strong></a>)</span>. A more robust solution necessitates an architecture that enables <em>LLMs</em> to explicitly learn and reproduce these changing patterns as a direct function of time.</p>
</section>
<section id="formalising-time-dependent-probabilities-the-time-transformer" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="formalising-time-dependent-probabilities-the-time-transformer"><span class="header-section-number">15.2</span> Formalising Time-Dependent Probabilities: The Time Transformer</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_05.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>Formalising the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, <code>x_n</code>, given a sequence of preceding tokens, <code>x_1, ..., x_{n-1}</code> <span class="citation" data-cites="Radford2018">(<a href="#ref-Radford2018" role="doc-biblioref"><strong>Radford2018?</strong></a>)</span>. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as <code>p(x_n | x_1, ..., x_{n-1}, t)</code>. Consequently, the probability for an entire sequence of tokens uttered at a specific time <code>t</code> is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing <em>LLMs</em> largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.</p>
<p>To overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution <code>p(x_n | x_1, ..., x_{n-1}, t)</code>. Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, termed the “<em>Time Transformer</em>”, introduces a simple yet profound modification: an additional dimension, <code>φ(t)</code>, is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, <code>E(x, t)</code>, which then serves as input to the <em>Transformer</em> architecture. Consequently, the <em>Transformer</em> processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution <code>p_θ(x_n | x_1, ..., x_{n-1}, t)</code>. The training objective remains the standard maximisation of log likelihood across all sequences <span class="citation" data-cites="Goodfellow2016">(<a href="#ref-Goodfellow2016" role="doc-biblioref"><strong>Goodfellow2016?</strong></a>)</span>. This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.</p>
</section>
<section id="empirical-validation-data-and-architecture" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="empirical-validation-data-and-architecture"><span class="header-section-number">15.3</span> Empirical Validation: Data and Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_16.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>To empirically validate the <em>Time Transformer</em> concept, researchers required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, proved an ideal choice <span class="citation" data-cites="UKMetOffice">(<a href="#ref-UKMetOffice" role="doc-biblioref"><strong>UKMetOffice?</strong></a>)</span>. Researchers scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The tokenisation process was intentionally simplistic, neglecting sub-word tokenisation, case, and interpunctuation, resulting in a compact vocabulary of only 3,395 unique words across the entire seven-year period. An alternative dataset, <em>TinyStories</em>, was also considered for its similar characteristics, offering short, synthetically generated narratives <span class="citation" data-cites="Xu2023">(<a href="#ref-Xu2023" role="doc-biblioref"><strong>Xu2023?</strong></a>)</span>.</p>
<p>A modest <em>Transformer</em> architecture, termed the “<em>Vanilla model</em>”, underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add &amp; Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output <span class="citation" data-cites="Vaswani2017">(<a href="#ref-Vaswani2017" role="doc-biblioref"><strong>Vaswani2017?</strong></a>)</span>. Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like <em>GPT-4</em>, which boasts 1.8 trillion parameters distributed across 120 layers <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref"><strong>OpenAI2023?</strong></a>)</span>. Training occurred on an HPC cluster in Munich, utilising two H100 GPUs. Remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The code for this implementation is publicly available on GitHub <span class="citation" data-cites="Büttner2025GitHub">(<a href="#ref-Büttner2025GitHub" role="doc-biblioref"><strong>Büttner2025GitHub?</strong></a>)</span>, though it was developed primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a perfect ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as “During the night, a band …”, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.</p>
</section>
<section id="time-transformer-implementation-and-results" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="time-transformer-implementation-and-results"><span class="header-section-number">15.4</span> Time Transformer: Implementation and Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_15.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>Implementing the <em>Time Transformer</em> required only a minimal architectural adjustment to the previously described <em>Vanilla model</em>. Engineers reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as <code>(day of year - 1) / (365 - 1)</code>. This specific encoding was chosen to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.</p>
<p>The first experiment aimed to demonstrate the model’s capacity for learning synthetic temporal drift through synonymic succession. Researchers injected a time-dependent replacement rule into the training data: <code>rain</code> was replaced by <code>liquid sunshine</code> according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of <code>rain</code> versus <code>liquid sunshine</code>. The <em>Time Transformer</em> flawlessly reproduced the injected sigmoid pattern, exhibiting <code>rain</code> predominantly early in the year and <code>liquid sunshine</code> towards the end, with the transition occurring precisely mid-year.</p>
<p>The second experiment explored the model’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the “fixation of a collocation.” Here, instances of <code>rain</code> not immediately followed by <code>and</code> were synthetically replaced with <code>rain and snow</code>. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of <code>rain and snow</code> against <code>rain only</code>. The model successfully acquired this pattern, generating <code>rain and snow</code> almost exclusively in the latter part of the year, whilst early-year occurrences of <code>rain</code> (sometimes accompanied by <code>snow</code>) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year <code>rain and snow</code> on the presence of a “cold system,” underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.</p>
</section>
<section id="proof-of-concept-applications-and-challenges" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="proof-of-concept-applications-and-challenges"><span class="header-section-number">15.5</span> Proof of Concept, Applications, and Challenges</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_21.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>This research unequivocally establishes a proof of concept: <em>Transformer</em>-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation <em>Time Transformer</em>, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to “talk to a specific time,” potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.</p>
<p>Several promising avenues for future research emerge from this work. Benchmarking the <em>Time Transformer</em> against explicit time-token approaches will quantify its performance advantages and identify optimal use cases. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.</p>
<p>Nevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained <em>LLMs</em> remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to <em>BERT</em> <span class="citation" data-cites="Devlin2019">(<a href="#ref-Devlin2019" role="doc-biblioref"><strong>Devlin2019?</strong></a>)</span>, built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward for historical language analysis.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_016.html" class="pagination-link" aria-label="Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_018.html" class="pagination-link" aria-label="Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>