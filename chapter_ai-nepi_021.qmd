---
title: "From Source to Structure: Extracting Knowledge Graphs with LLMs"
author:
- name: "Raphael Schlattmann, Aleksandra Kaye & Malte Vogl"
  affiliation: "Technische Universität Berlin"
  email: "raphael.schlattmann@tu-berlin.de"
date: '2025'
bibliography: bibliography.bib
---
---
title: "Transforming Biographical Sources into Knowledge Graphs"
author:
  - name: Malte
  - name: Raphael
  - name: Alex K.
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-tools: true
    highlight-style: atom-one
    theme: cosmo
---

## Overview {.unnumbered}

Malte, Raphael, and Alex K., participating via Zoom, explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. This approach facilitates sophisticated querying and analysis. Their work directly addresses the challenge of computationally accessing the rich information embedded within traditional formats, such as printed books and archival materials, which typically lack inherent digital structure.

The core objective involves leveraging Large Language Models (*LLMs*) not as standalone solutions, but as integral components within a multi-stage pipeline. This pipeline is meticulously designed for specific tasks, aiming to impose structure on unstructured data in a controllable manner. The process commences with diverse sources, including Polish biographical materials and German biographical handbooks, such as *Wer war wer in der DDR?*. It then proceeds to extract entities—persons, places, countries, and works—and their intricate relationships, representing them as nodes and edges within a knowledge graph. Tools like *Neo4j* enable the visualisation of these complex networks.

This structured representation significantly facilitates complex queries. Researchers can, for instance, investigate network formations amongst professionals during specific historical periods or meticulously trace the evolution of ideas. The methodology champions a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies precisely tailored to research questions, and disambiguated entities linked to external resources like *Wikidata*. Ultimately, the project aims to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through advanced technologies such as *GraphRAG*.

## Introduction: Unlocking Unstructured Biographical Knowledge

![Slide 01](images/ai-nepi_021_slide_01.jpg)

Researchers consistently confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its inherent lack of digital structure. Whilst earlier tools like *Get Grasso* aimed to digitise and process printed materials, the current investigation centres on biographical sources replete with detailed personal data, crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.

To address this fundamental limitation, investigators propose employing Large Language Models (*LLMs*). The core idea involves harnessing *LLMs* not as all-encompassing solutions, but as specialised tools within a controllable pipeline designed to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook *Wer war wer in der DDR?*, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like *Neo4j*.

Crucially, the project seeks the most *useful* *LLM* for specific tasks within this chain, rather than pursuing a universally perfect model. This pragmatic approach allows for complex queries, such as mapping an individual's birth and work locations or analysing how professional networks formed and evolved during particular historical periods.

## Conceptual Framework: From Text to Knowledge Graph

![Slide 04](images/ai-nepi_021_slide_04.jpg)

The transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or "chunks". From these chunks, an extraction pipeline works to identify key entities and their interrelations, which are then assembled into a knowledge graph. For instance, a biographical entry for "Bartsch Henryk" might yield entities like his name (PERSON), role ("ks. ewang." - evangelical priest, ROLE), birth date (DATE), and birthplace ("Władysławowo", LOCATION), alongside relationships such as "born in" or "travelled to" various locations like Italy ("Włochy") or Egypt ("Egipt"). These relationships manifest as structured triples, for example, "(Bartsch Henryk, is a, ks. ewang.)".

This entire process operates within a two-stage pipeline. The first stage, "Ontology agnostic Open Information Extraction (*OIE*)", focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, "Ontology driven Knowledge Graph (*KG*) building", commences with the formulation of Competency Questions (*CQs*) that guide ontology creation. This stage further involves creating *SHACL* (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to *Wiki IDs*), and finally validating the resultant graph using *RDF Star* and *SHACL*. Both stages integrate *LLM* interaction and maintain a crucial "Human-in-the-Loop" layer for oversight and refinement.

Five core principles underpin this methodology:

-   A research-driven and data-oriented approach ensures relevance.

-   Human-in-the-loop mechanisms guarantee quality and address ambiguities.

-   Transparency allows for process verification.

-   Task decomposition simplifies complexity.

-   Modularity facilitates iterative development and improvement of individual components.

## Pipeline Stage 1: Ontology-Agnostic Open Information Extraction

![Slide 09](images/ai-nepi_021_slide_09.jpg)

The initial stage of the pipeline, ontology-agnostic Open Information Extraction (*OIE*), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself ("Biografie"), and a unique chunk identifier.

Following data preparation, the *OIE* Extraction step performs an "Extraction Call". Using the person's name and the relevant biographical text chunk as input (for example, 'Havemann, Robert' and text like '... 1935 Prom. mit ... an der Univ. Berlin...'), this process generates raw Subject-Predicate-Object (*SPO*) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an *OIE* Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a "Validation Call" produces validated *SPO* triples, now augmented with a confidence score for each assertion.

To ensure the reliability of this extraction phase, the *OIE* Standard step compares the validated triples against a "Gold Standard"—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the *OIE* quality suffices to proceed to the next stage of the pipeline or if further refinement of the *OIE* steps proves necessary.

## Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction

![Slide 12](images/ai-nepi_021_slide_12.jpg)

Once high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (*KG*) construction, commences. This phase begins with the formulation of Competency Questions (*CQs*), which are manually refined based on a sample of the validated triples. These *CQs* articulate the specific research queries the final knowledge graph should address; for instance, "Which *GDR* scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?".

Guided by these *CQs* and the sample triples, an Ontology Creation step, via a "Generation Call", produces an ontology definition. This definition specifies classes (e.g., "Person" as an `owl:Class`, "doctorate" as a class of "academicEvent") and properties (e.g., `:eventYear`, `:eventInstitution`, `:eventTopic`). Concurrently, *SHACL* (Shapes Constraint Language) shapes are created to define constraints for validating the graph's structure and content.

The subsequent Ontology Mapping step, through a "Mapping Call", aligns the validated triples with this newly defined ontology and *SHACL* shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual *RDF*. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation *Wiki ID* step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as *Wikidata IDs* (e.g., mapping "Robert Havemann" to `wd:Q77116`), producing disambiguated triples and *RDF-star* statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes *RDF Star* and *SHACL* Validation to ensure its integrity and conformance to the defined ontology and constraints.

## Illustrative Applications: Zieliński Compilations and GDR Biographies

![Slide 15](images/ai-nepi_021_slide_15.jpg)

Researchers illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński's compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying the knowledge-graph approach to this corpus, investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.

The second case study focuses on the German biographical lexicon *Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien*. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists; the presentation displays sample entries for Gustav Hertz and Robert Havemann. An analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and *SED* (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the *Karl-Marx-Orden* and the *Nationalpreis der DDR*. Further comparative data highlights differences between recipients of the *Karl-Marx-Orden* and non-recipients regarding *SED* membership share, average birth year, and holding of specific high-ranking positions within structures like the *Politbüro* or *Ministerrat*.

## Conclusion and Future Trajectories

![Slide 20](images/ai-nepi_021_slide_20.jpg)

The project successfully demonstrates a method for progressing from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to rigorously assess performance.

Immediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the team intends to scale their methodology to analyse full datasets comprehensively.

Looking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. Investigators will also explore the potential of *GraphRAG* (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, plans are underway to construct multilayered networks, potentially using frameworks like *ModelSEN*, to facilitate deeper and more nuanced structural analyses of the biographical information.