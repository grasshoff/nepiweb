---
title: "From Source to Structure: Extracting Knowledge Graphs with LLMs"
author:
- name: "Raphael Schlattmann, Aleksandra Kaye & Malte Vogl"
  affiliation: "Technische Universität Berlin"
  email: "raphael.schlattmann@tu-berlin.de"
bibliography: bibliography.bib
---
## Overview {.unnumbered}

Malte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (*LLMs*) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.

The process commences with sources such as Polish biographical materials and German biographical handbooks, including *Wer war wer in der DDR?*. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like *Neo4j*. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like *Wikidata*. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like *GraphRAG*.

## Introduction: Accessing Unstructured Biographical Knowledge

Investigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like *Get Grasso* aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.

To address this limitation, the authors propose employing Large Language Models (*LLMs*). Their core idea involves harnessing *LLMs* not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook *Wer war wer in der DDR?*, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like *Neo4j*. Crucially, the project seeks the most *useful* *LLM* for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual's birth and work locations or analysing how professional networks formed and evolved during particular historical periods.

## Conceptual Framework: From Text to Knowledge Graph

The transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or "chunks". From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for "Bartsch Henryk" might yield entities like his name (PERSON), role ("ks. ewang." - evangelical priest, ROLE), birth date (DATE), and birthplace ("Władysławowo", LOCATION), alongside relationships such as "born in" or "travelled to" various locations like Italy (*Włochy*) or Egypt (*Egipt*). These relationships manifest as structured triples, for example, "(Bartsch Henryk, is a, ks. ewang.)".

This entire process unfolds within a two-stage pipeline. The first stage, "Ontology agnostic Open Information Extraction (OIE)", focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, "Ontology driven Knowledge Graph (KG) building", commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating *SHACL* (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to *Wiki IDs*), and finally validating the resultant graph using *RDF Star* and *SHACL*. Both stages integrate *LLM* interaction and maintain a crucial "Human-in-the-Loop" layer for oversight and refinement.

Five core principles underpin this methodology:

-   A research-driven and data-oriented approach ensures relevance.

-   Human-in-the-loop mechanisms guarantee quality and address ambiguities.

-   Transparency allows for process verification.

-   Task decomposition simplifies complexity.

-   Modularity facilitates iterative development and improvement of individual components.

## Pipeline Stage 1: Ontology-Agnostic Open Information Extraction

The initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself ("Biografie"), and a unique chunk identifier.

Following data preparation, the OIE Extraction step performs an "Extraction Call". Using the person's name and the relevant biographical text chunk as input (for example, 'Havemann, Robert' and text like '... 1935 Prom. mit ... an der Univ. Berlin...'), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a "Validation Call" produces validated SPO triples, now augmented with a confidence score for each assertion.

To ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a "Gold Standard"—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.

## Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction

Once high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, "Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?".

Guided by these CQs and the sample triples, an Ontology Creation step, via a "Generation Call", produces an ontology definition. This definition specifies classes (e.g., "Person" as an `owl:Class`, "doctorate" as a class of "academicEvent") and properties (e.g., `:eventYear`, `:eventInstitution`, `:eventTopic`). Concurrently, the team creates *SHACL* (Shapes Constraint Language) shapes to define constraints for validating the graph's structure and content.

The subsequent Ontology Mapping step, through a "Mapping Call", aligns the validated triples with this newly defined ontology and *SHACL* shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation *Wiki ID* step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as *Wikidata IDs* (e.g., mapping "Robert Havemann" to `wd:Q77116`), producing disambiguated triples and *RDF-star* statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes *RDF Star* and *SHACL* Validation to ensure its integrity and conformance to the defined ontology and constraints.

## Illustrative Applications: Zieliński Compilations and GDR Biographies

Malte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński's compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.

![Slide 20](images/ai-nepi_2024_slide_20.png)

The second case study focuses on the German biographical lexicon *Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien*. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.

![Slide 21](images/ai-nepi_2024_slide_21.png)

An analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the *Karl-Marx-Orden* and the *Nationalpreis der DDR*.

![Slide 22](images/ai-nepi_2024_slide_22.png)

Further comparative data highlights differences between recipients of the *Karl-Marx-Orden* and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the *Politbüro* or *Ministerrat*.

![Slide 23](images/ai-nepi_2024_slide_23.png)

## Conclusion and Future Trajectories

The project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.

Immediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.

Looking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of *GraphRAG* (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like *ModelSEN*, to facilitate deeper and more nuanced structural analyses of the biographical information.