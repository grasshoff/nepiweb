---
title: "Genre Classification for Historical Medical Periodicals"
author:
- name: "Vera Danilova"
  affiliation: "Uppsala University"
  email: "vera.danilova@idehist.uu.se"
date: '2025'
bibliography: bibliography.bib
---
## Overview {.unnumbered}

Researchers at Uppsala University have pioneered a procedural investigation into genre classification for historical medical periodicals. This endeavour forms a core component of the *ERC*-funded ActDisease project, which meticulously examines the history of patient organisations in 20th-century Europe. The European Research Council (*ERC*) provides funding for this initiative. The project's central aim involves scrutinising the profound influence of these organisations on disease concepts, illness experiences, and prevailing medical practices. Primarily, the project leverages a private, recently digitised collection of patient organisation magazines from Sweden, Germany, France, and Great Britain, encompassing an impressive 96,186 pages published between approximately 1890 and 1990.

Digitisation efforts, employing *ABBYY FineReader Server 14*, successfully processed most common layouts. Nevertheless, complex layouts, slanted text, and rare fonts posed persistent challenges, frequently leading to Optical Character Recognition (*OCR*) errors, particularly in German and French texts. Recognising the diverse and co-occurring text types within these periodicals, the team identified genre classification as a crucial methodological advancement. This approach directly addresses the limitations and potential biases of traditional topic models and term counts, which often fail to account for the varied communicative purposes embedded within single pages.

To address the scarcity of annotated data, the project explored both zero-shot and few-shot learning paradigms. Zero-shot learning involves applying a model to data from categories it has not seen during training, relying on its understanding of the categories' descriptions. Few-shot learning, conversely, trains a model with only a very small number of examples per category. Researchers defined a bespoke set of nine genre labels—Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and Question and Answer (*QA*)—under the direct supervision of a specialist historian. Annotation involved six project members, achieving a high inter-annotator agreement of 0.95 Krippendorff's alpha on paragraphs sampled from Swedish and German periodicals.

For zero-shot experiments, the team leveraged publicly available datasets, including the *Corpus of Online Registers of English* (*CORE*), *Functional Text Dimensions* (*FTD*), and *UD-MULTIGENRE* (*UDM*). They performed a rigorous cross-dataset genre mapping to align these external datasets with their custom labels. Multilingual encoder models, specifically *XLM-Roberta*, *mBERT*, and *historical mBERT*, underwent fine-tuning across 48 configurations. Findings indicated that models fine-tuned on *FTD* performed optimally with the custom mapping, whilst *historical mBERT* demonstrated particular efficacy in distinguishing between fiction and nonfiction prose in few-shot settings.

Furthermore, the project investigated few-shot prompting with *Llama 3.1 8b Instruct*, a large language model (LLM), revealing its capacity to handle certain genre labels effectively. However, a limited number of examples proved insufficient for comprehensive representation across all categories. Ultimately, the research concludes that genre classification significantly enhances the accessibility of historical periodical sources for text mining. Few-shot learning of multilingual encoders, particularly *historical mBERT* with prior Masked Language Model (*MLM*) fine-tuning, offers the most robust performance. Ongoing work includes developing a more fine-grained annotation scheme, generating synthetic data, and implementing active learning strategies.

## The ActDisease Project: Historical Inquiry into Patient Organisations

![Slide 01](images/ai-nepi_005_slide_01.jpg)

Researchers have embarked upon the ActDisease project, formally titled "Acting out Disease: How Patient Organizations Shaped Modern Medicine." This *ERC*-funded initiative meticulously investigates the historical trajectory of patient organisations across Europe during the 20th century. Its central purpose involves scrutinising how these organisations fundamentally influenced the evolution of disease concepts, the lived experience of illness, and prevailing medical practices.

The project’s scope encompasses ten distinct European patient organisations. It draws its primary source material from their periodicals—predominantly magazines—published in England, Germany, France, and Great Britain between approximately 1890 and 1990. Notably, the Hay Fever Association of Heligoland, established in 1897, exemplifies the type of historical entity under examination, with Heligoland, Germany, serving as its foundational site.

## The ActDisease Dataset: Periodical Collection and Scope

![Slide 06](images/ai-nepi_005_slide_06.jpg)

The ActDisease project has assembled a private, recently digitised collection of patient organisation magazines, constituting a substantial dataset of 96,186 pages. This extensive archive spans various countries, diseases, and publication periods.

Specifically, the dataset includes:
*   **German collection:** 10,926 pages on Allergy/Asthma from two magazines (1901-1985); 19,324 pages on Diabetes from one magazine (1931-1990); and 5,646 pages on Multiple Sclerosis from one magazine (1954-1990).
*   **Swedish materials:** 4,054 pages on Allergy/Asthma (1957-1990); 7,150 pages on Diabetes (1949-1990); and 16,790 pages on Lung Diseases (1938-1991), each from a single magazine.
*   **French contributions:** 6,206 pages on Diabetes (1947-1990); and 9,317 pages on Rheumatism/Paralysis from three magazines (1935-1990).
*   **UK segment:** 11,127 pages on Diabetes (1935-1990); and 5,646 pages on Rheumatism (1950-1990), each sourced from one magazine.

## Digitisation Processes and Persistent Challenges

![Slide 07](images/ai-nepi_005_slide_07.jpg)

The digitisation process for the ActDisease dataset primarily employed *ABBYY FineReader Server 14* for Optical Character Recognition (*OCR*). This tool generally performed well, accurately recognising most common layouts and fonts present in the historical periodicals.

Nevertheless, significant challenges persisted. Complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality frequently hindered optimal *OCR* performance. Consequently, researchers observed persistent *OCR* errors, particularly prevalent in German and French texts, alongside instances of disrupted reading order. Notably, creative text segments, including advertisements, humour pages, and poems, exhibited a higher frequency of *OCR* inaccuracies. To mitigate these issues, the team conducted specific experiments, focusing on post-*OCR* correction of German texts through the application of instruction-tuned generative models. This work has been documented in a publication by Danilova and Aangenendt, presented at the *RESOURCEFUL-2025* workshop.

## Rationale for Genre Classification in Historical Periodicals

![Slide 11](images/ai-nepi_005_slide_11.jpg)

Researchers observed a profound diversity in the textual content of the historical periodicals, yet these varied text types consistently appeared across all magazines. Crucially, distinct text types frequently co-occurred on a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This inherent textual complexity posed a significant challenge for conventional analytical methods.

Traditional yearly and decade-based topic models and term counts, the team realised, failed to account for this side-by-side textual variation. Consequently, these methods likely introduced a bias towards the most frequently occurring text type, potentially distorting analytical outcomes. To overcome this limitation, genre emerged as a highly pertinent concept for distinguishing between text types. Genres inherently align with the communicative purposes of authors, as defined in language technology by *Petrenz (2004)* and *Kessler (1997)*.

Implementing genre classification directly supports the project's core objective: exploring the dataset from multiple perspectives to formulate robust historical arguments. Specifically, this approach enables a nuanced study of communicative strategies as they evolved over time, allowing for comparisons across different countries, diseases, and publications, as highlighted by *Broersma (2010)*. Furthermore, it facilitates a more granular analysis of term distributions and topic models, enabling insights within specific genre groups.

## Illustrative Genre Examples within the ActDisease Dataset

![Slide 19](images/ai-nepi_005_slide_19.jpg)

The ActDisease dataset encompasses a rich array of genres, each serving distinct communicative functions. Researchers identified examples such as poetry, which often provided emotional engagement. Academic reports, exemplified by studies on the pancreas, conveyed scientific and medical information. Legal documents, including deeds of covenant, established formal agreements and regulations. Advertisements, such as those promoting chocolate for diabetics, aimed to market products or services.

Furthermore, the collection featured instructive or guidance messages, offering practical advice like recipes, doctor's recommendations, or dietary guidelines. Patient organisation reports documented internal affairs, detailing meetings and activities. Finally, narratives about patient lives provided first-hand accounts of illness experiences, offering a unique perspective on the human dimension of disease.

## Experimental Design for Genre Classification with Limited Data

![Slide 04](images/ai-nepi_005_slide_04.jpg)

Confronted with a scarcity of annotated data, researchers systematically explored two distinct methodological paradigms: zero-shot learning and few-shot learning. For zero-shot learning, the investigation posed two primary research questions: firstly, whether an efficient mapping of genre labels from existing public datasets to custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might fluctuate across different datasets and models.

Conversely, the few-shot learning inquiry focused on two further questions: how performance changes in relation to varying training set sizes across different models; and whether a prior fine-tuning process on the entire dataset could significantly boost classification performance. This comprehensive experimental design forms the basis of a forthcoming publication by Danilova and Söderfeldt, scheduled for presentation at the *LaTeCH-CLFL 2025* workshop.

## Defining Genre Labels for Historical Periodical Content

![Slide 32](images/ai-nepi_005_slide_32.jpg)

Researchers meticulously defined the genre labels under the direct supervision of the project's lead historian, an expert in patient organisations. This collaborative process aimed to create categories that would effectively segment the content within the historical periodicals, thereby facilitating deeper historical analysis. Crucially, the team endeavoured to formulate these labels with sufficient generality to enable the classifier's application to comparable datasets in the future.

Nine distinct genres emerged from this process, each with a precise definition:
*   **Academic:** Encompasses research-based reports or scientific explanations, designed to bridge the gap between the scientific medical community and the magazine's readership.
*   **Administrative:** Documents organisational activities, reporting on patient organisation events and internal affairs.
*   **Advertisement:** Specifically promotes commercial products or services.
*   **Guide:** Provides step-by-step instructions, ranging from health tips to recipes.
*   **Fiction:** Aims to entertain and emotionally engage through stories, poems, or humour.
*   **Legal:** Explains terms, conditions, or contracts.
*   **News:** Reports on recent events.
*   **Nonfiction Prose:** Narrates real events or describes cultural and historical topics, including memoirs and essays.
*   **QA (Question and Answer):** Designates sections structured as questions with expert responses.

## Annotation Protocol and Inter-Annotator Agreement

![Slide 37](images/ai-nepi_005_slide_37.jpg)

Researchers employed paragraphs as the fundamental annotation unit, extracting them from the *ABBYY OCR* output. The team subsequently merged these paragraphs based on consistent font patterns—including type, size, italicisation, and the absence of bolding—within each page. Researchers sampled content from two specific periodicals: the Swedish "Diabetes" and the German "Diabetiker Journal," focusing on their first and mid-year issues across all publication years.

Six dedicated project members undertook the annotation task, comprising four historians and two computational linguists. All annotators possessed either native fluency or high proficiency in both Swedish and German. For each paragraph, two independent annotations were meticulously collected. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, as measured by Krippendorff's alpha, signifying a remarkably high level of consistency amongst the annotators.

## Dataset Splitting and Experimental Configurations

![Slide 39](images/ai-nepi_005_slide_39.jpg)

For the experimental phase, researchers initially partitioned the annotated data into a primary training set of 1182 paragraphs and a held-out set comprising 552 paragraphs, which represented approximately 30% of the total annotated material. Researchers stratified both these sets by genre label to ensure representative distributions.

Within the few-shot experiments, the team systematically varied the training set size, employing six distinct configurations: 100, 200, 300, 400, 500, and the full 1182 paragraphs. Each of these smaller training sets was randomly sampled from the main training pool, whilst maintaining a balance across genre labels. The team subsequently divided the held-out set equally into validation and test portions, similarly balanced by label. Notably, researchers excluded the Legal and News genres from these few-shot experiments, as their limited data volume precluded sufficient training. Conversely, the zero-shot experiments leveraged the entirety of the held-out test set.

## Genre Distribution within ActDisease Training and Held-Out Sets

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Analysis of the genre distribution across both the training and held-out samples of the ActDisease dataset revealed a pronounced imbalance. Specifically, the Advertisement and Non-fictional Prose genres exhibited significant disparities in their representation across different languages. This imbalance necessitates careful consideration during model training and evaluation to prevent potential biases.

## Leveraging External Datasets for Zero-Shot Classification

![Slide 39](images/ai-nepi_005_slide_39.jpg)

For the zero-shot experiments, researchers incorporated external, modern datasets previously utilised in automatic web genre classification. The *Corpus of Online Registers of English* (*CORE*), developed by *Egbert et al. (2015)*, provides document-level annotations. It encompasses English content, with main categories also available in Swedish, Finnish, and French.

Similarly, *Sharoff's (2018) Functional Text Dimensions* (*FTD*) dataset, also annotated at the document level, offers balanced content in English and Russian. *Kuzman et al. (2023)* had previously leveraged this dataset for web genre classification. Additionally, the team employed *UD-MULTIGENRE* (*UDM*), a subset of *Universal Dependencies* (*de Marneffe et al., 2021*), which features genre annotations at the sentence level across 38 languages, as detailed by *Danilova and Stymne (2023)*.

## Cross-Dataset Genre Mapping and Alignment

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Researchers meticulously performed genre mapping across datasets, with two independent annotators undertaking the task. The team only included assignments achieving full agreement in the final mapping, ensuring robust alignment.

This process established correspondences between ActDisease genres and their equivalents in *CORE*, *UDM*, and *FTD*. For instance, "Academic" in ActDisease mapped to "research article" (*RA*) in *CORE*, "academic" in *UDM*, and "academic (*A14*)" in *FTD*. "Advertisement" aligned with "advertisement (*AD*)" in *CORE*, "description with intent to sell (*DS*)" in *UDM*, and "commercial (*A12*)" in *FTD*. Similarly, "Fiction" found its counterparts in "poem" (*PO*) and "short story" (*SS*) in *CORE*, "fiction" in *UDM*, and "fictive (*A4*)" and "poetic (*A19*)" in *FTD*. However, the team encountered a limitation: some ActDisease genres lacked suitable corresponding labels within the available external datasets.

## Training Data Generation and Multilingual Encoder Models

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Researchers generated training data through a meticulous pipeline encompassing mapping, preprocessing, chunking, and systematic sampling. They configured training sets in four distinct ways for each dataset: one configuration (*[G+]) focused exclusively on Germanic languages; another (*[B1]*) balanced data according to ActDisease labels; a third (*[G-]*) incorporated all language families; and the final configuration (*[B2]*) balanced data by both ActDisease and original labels. This yielded four *FTD*, four *CORE*, four *UDM*, and four merged training samples, all subjected to fine-tuning.

For classification, the team employed several multilingual encoder models, which are transformer-based neural networks capable of processing text in multiple languages. *XLM-Roberta*, developed by *Conneau et al. (2020)*, served as a state-of-the-art web genre classifier, as noted by *Kuzman et al. (2023)*. *mBERT* (*Devlin et al., 2019*) provided a baseline for comparison with *historical mBERT* (*Schweter et al., 2022*). Crucially, *historical mBERT*, pretrained on an extensive corpus of multilingual historical newspapers, proved particularly relevant given its inclusion of the target languages. These *BERT*-like models have consistently demonstrated efficacy in prior web register and genre classification studies, as evidenced by *Lepekhin and Sharoff (2022)*, *Kuzman and Ljubešić (2023)*, and *Laippala et al. (2023)*. Ultimately, the fine-tuning process generated 48 distinct models, and the team subsequently averaged performance metrics across all configurations.

## Zero-Shot Learning Evaluation and Performance Analysis

![Slide 11](images/ai-nepi_005_slide_11.jpg)

Evaluating the zero-shot predictions presented a unique challenge: the imperfect overlap of label sets prevented direct comparison of overall performance metrics. Consequently, researchers meticulously assessed the performance of each genre individually, complementing this analysis with a thorough examination of confusion matrices to mitigate potential biases. The *X-GENRE* web genre classifier, as detailed by *Kuzman et al. (2023)*, served as a robust baseline, with predictions focusing exclusively on the most similar labels directly mappable to the ActDisease genres.

The experimental setup often involved a cross-lingual context. *FTD* and *X-GENRE*, for instance, operated without German or Swedish data, whilst *UDM* and *CORE* datasets exhibited partially cross-lingual characteristics. Overall, models fine-tuned on *FTD* consistently demonstrated superior performance when integrated with the ActDisease mapping. Conversely, other datasets revealed distinct class-specific biases.

*UDM*, for example, exhibited a bias towards news, primarily because its news training data contained the highest proportion of Germanic instances, overwhelmingly German. Similarly, *CORE* displayed a bias towards the guide genre, as its training data for this category was uniquely multilingual.

Intriguingly, certain models excelled in specific genre predictions. *XLM-Roberta*, when fine-tuned on *UDM*, achieved an average of 32% more correct predictions in the *QA* genre compared to *mBERT* and *hmBERT*. Conversely, *hmBERT*, also on *UDM*, produced an average of 16% more accurate predictions in the Administrative genre than *XLM-Roberta* and *mBERT*. Furthermore, *CORE*-based models consistently proved proficient at predicting the legal genre. Confusion matrices visually underscored these observed behavioural patterns, whilst detailed average per-category *F1* scores provided a comprehensive quantitative assessment across all data configurations.

## Few-Shot Learning Performance and Model Advantages

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Few-shot learning experiments unequivocally demonstrated the advantage of further training models on the ActDisease dataset, particularly when incorporating Masked Language Model (*MLM*) fine-tuning. The *F1* score consistently improved as the number of training instances increased, though it remained below 0.8 even with the full training set of 1182 instances.

Amongst the models tested, *hmBERT-MLM* consistently outperformed its counterparts. A detailed examination of its performance revealed that, unlike other models, *hmBERT-MLM* retained its capacity to differentiate between fiction and nonfiction genres even when exposed to the full dataset. Conversely, other models, notably *XLM-Roberta*, exhibited a drastic decline in their ability to distinguish these two categories.

Analysis of *XLM-Roberta*'s confusion matrix, when fine-tuned with *MLM* on the full dataset, indicated a frequent overprediction of nonfiction prose for fiction. This phenomenon likely stems from the shared thematic content within the ActDisease data, where both fictional and autobiographical narratives often revolve around patient experiences, leading to similar themes and narrative structures. Consequently, researchers propose that an increased volume of data is essential to enhance performance in distinguishing these increasingly similar genres.

## Few-Shot Prompting with Llama-3.1 8b Instruct

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Given the current insufficiency of data for comprehensive instruction tuning, researchers opted to evaluate few-shot prompting using *Llama 3.1 8b Instruct*, a widely recognised multilingual generative model with open weights. The prompt structure provided clear genre definitions, complemented by two or three illustrative examples for each category.

The results indicated that the model handled certain genre labels with reasonable efficacy. For instance, it achieved an *F1*-score of 0.84 for Legal content and 0.72 for Academic texts. However, the limited number of examples proved insufficient for robust representation across all genres. Notably, nonfictional prose yielded a lower *F1*-score of 0.49, whilst advertisement and administrative content also demonstrated suboptimal performance, with *F1*-scores of 0.73 and 0.60 respectively. The overall accuracy stood at 0.62, with a macro average *F1*-score of 0.59 and a weighted average of 0.63.

## Key Findings and Strategic Recommendations

![Slide 39](images/ai-nepi_005_slide_39.jpg)

Popular magazines, unlike more specialised scientific journals or books, frequently encompass a multitude of genres. This characteristic significantly complicates text mining efforts. Researchers have concluded that genres inherently reflect deliberate choices in communicative strategies; consequently, accounting for these distinctions, whilst challenging, proves crucial for achieving accurate and detailed interpretations of text mining outcomes. Fundamentally, genre classification renders these rich historical sources accessible for advanced text mining.

For scenarios lacking dedicated training data, two viable strategies emerge. Firstly, one can successfully leverage existing modern datasets, provided the target categories maintain a general purpose. Alternatively, few-shot instruction of a proficient generative model offers another effective pathway.

However, when some training data is available, few-shot learning of multilingual encoders, particularly those with prior Masked Language Model (*MLM*) fine-tuning—such as *XLM-Roberta* or *historical multilingual BERT*—demonstrates superior performance. Indeed, this latter approach emerged as the optimal solution for the project. Notably, *historical multilingual BERT* exhibited particularly strong gains, achieving a 24% improvement, which significantly surpassed the 14.5% gain for *mBERT-MLM* and 16.9% for *XLM-RoBERTa*. These findings underscore the value of domain-specific pre-training for historical text analysis.

## Ongoing and Future Research Directions

![Slide 39](images/ai-nepi_005_slide_39.jpg)

The research team is actively pursuing several avenues to enhance the quality and scope of this work for both the project and the wider academic community. Currently, researchers are engaging with specific historical hypotheses, leveraging the insights gained from genre classification. Furthermore, they are developing a new, more fine-grained annotation scheme for genres, a project notably financed by *Swe-CLARIN*. Methodologically, the team is exploring advanced techniques, including synthetic data generation and active learning, to further refine their classification capabilities.

## Acknowledgements

![Slide 19](images/ai-nepi_005_slide_19.jpg)

The project gratefully acknowledges the invaluable contributions of its annotators and core team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. Funding for this research came generously from the European Research Council under grant *ERC-2021-STG 10104099*. The Centre for Digital Humanities and Social Sciences offered crucial institutional support, supplying essential *GPUs* and data storage facilities. Finally, the team extends its gratitude to the diligent reviewers, Dr Maria Skeppstedt and other anonymous contributors, whose feedback significantly enhanced the work. Further details are available on the project website.
