---
title: "Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project"
author:
- name: "Vera Danilova"
  affiliation: "Uppsala University"
  email: "vera.danilova@idehist.uu.se"
date: '2025-06-21'
bibliography: bibliography.bib
---
## Overview {.unnumbered}

The authors present a comprehensive study on genre classification within historical patient periodicals, conducted as part of the ERC-funded ActDisease project. This programme investigates how patient organisations in 20th-century Europe shaped concepts of disease and medical practices, employing their magazines as a primary data source. A substantial corpus, the ActDisease Dataset, underpins this work, comprising 96,186 digitised pages from patient magazines across Germany, Sweden, France, and the United Kingdom.

The core technical challenge the authors address is the classification of diverse and often ambiguous textual genres found within these historical documents. This task is complicated by issues in Optical Character Recognition (OCR) and the limitations of traditional methods like topic modelling. To overcome these obstacles, the research team developed a sophisticated, expert-driven genre-labelling scheme and conducted extensive experiments using both zero-shot and few-shot learning paradigms.

Their study evaluates a range of multilingual encoder models, including *XLM-Roberta*, *mBERT*, and a specialised historical *mBERT* (*hmBERT*), alongside generative models such as *Llama-3.1 8b*. Key findings demonstrate that few-shot learning, particularly with Masked Language Model (MLM) fine-tuning, significantly improves performance. The *hmBERT-MLM* model emerges as the most effective, highlighting the value of domain-specific pre-training for historical texts. The work concludes that the rich generic diversity of popular magazines makes them a more complex text mining target than scientific journals, underscoring the necessity of robust genre classification for fine-grained historical analysis.

## Presentation Outline

![Slide 02](images/ai-nepi_005_slide_02.png)

The authors structure their research programme into three principal sections. It begins with an introduction to the ActDisease project, followed by a detailed examination of the genre classification experiments. The presentation culminates in a summary of the key conclusions drawn from the research.

## The ActDisease Project

![Slide 03](images/ai-nepi_005_slide_03.png)

The authors initiated the 'ActDisease' project, an ERC-funded research programme designed to explore the influence of patient organisations on medicine and society. Central to the project is an analysis of how these groups, active throughout 20th-century Europe, shaped concepts of disease, the personal experience of illness, and prevailing medical practices. To achieve this, the investigators use patient-published periodicals as the primary source material for their historical analysis. An image of Heligoland, Germany, provides visual context for the historical settings under investigation.

## Dataset Composition

![Slide 04](images/ai-nepi_005_slide_04.png)

The project team compiled the ActDisease Dataset, a specialised corpus of digitised magazines published by patient organisations. This collection spans materials from Germany, Sweden, France, and the UK, amounting to a total of 96,186 pages. A summary table provides a detailed breakdown of the dataset, specifying the number of unique magazine titles, total page counts, and the range of publication years for different diseases within each country.

## Digitisation and OCR Challenges

![Slide 05](images/ai-nepi_005_slide_05.png)

The digitisation of historical documents presented a significant technical hurdle. The process of Optical Character Recognition (OCR), in particular, often yields suboptimal results when applied to older, varied source materials. This challenge necessitates further research into effective post-OCR correction methods, which are crucial for enhancing the accuracy and utility of the digitised text for subsequent analysis.

## Genre Classification Challenges

![Slide 06](images/ai-nepi_005_slide_06.png)

Analysing patient periodicals introduces a distinct challenge in genre classification. The source materials contain a wide diversity of text types, ranging from medical advice to personal stories and advertisements. Consequently, conventional methods such as topic modelling or basic term-counting prove inadequate for accurately differentiating these nuanced genres, which often share overlapping vocabularies.

## Motivation for Classification

![Slide 07](images/ai-nepi_005_slide_07.png)

The authors' focus on genre classification stems from its analytical utility. From a language technology standpoint, genre provides a framework for understanding the communicative purpose of a text. By classifying content into distinct genres, the team can conduct more rigorous historical investigations, separating, for instance, official announcements from personal testimonials. Ultimately, this capability enables a more fine-grained and context-aware analysis of the entire dataset.

## Illustrating Genre Diversity

![Slide 08](images/ai-nepi_005_slide_08.png)

A collage of documents illustrates the project's central challenge, showcasing the diverse textual genres related to a specific disease, likely diabetes. This visual representation effectively communicates the variety of formats and styles—from scientific articles to personal letters and advertisements—that the authors must categorise. It underscores the complexity of performing automated analysis on such heterogeneous source material.

## Textual Examples of Variation

![Slide 09](images/ai-nepi_005_slide_09.png)

To reinforce the concept of genre diversity, the authors provide several concrete textual examples from the 'ActDisease' domain, specifically concerning diabetes. These snippets highlight the distinct linguistic and structural features of different genres found within the patient magazines. By presenting these varied examples, the team clarifies the practical difficulties and the importance of developing a robust classification system.

## Defining Genre Labels

![Slide 11](images/ai-nepi_005_slide_11.png)

The authors established a formal set of genre labels to structure their classification task. Rather than being algorithmically derived, these labels were defined by subject-matter experts to ensure historical and contextual relevance. The primary function of this schema is to enable the systematic separation of content according to its type, which is essential for nuanced historical analysis. Moreover, the team designed the labels with a view towards general-purpose applicability, aiming for a system that could be adapted for other historical text analysis projects.

## Genre Classification Schema

![Slide 12](images/ai-nepi_005_slide_12.png)

A detailed classification schema formally defines the nine distinct text genres used in the project: *Academic*, *Administrative*, *Advertisement*, *Guide*, *Fiction*, *Legal*, *News*, *Nonfiction Prose*, and *QA (Question & Answer)*. For clarity and consistency in annotation, a comprehensive table outlines the specific characteristics of each genre and provides representative examples drawn from the source material.

## Annotation Methodology

![Slide 13](images/ai-nepi_005_slide_13.png)

The team developed a rigorous methodology for creating the ground-truth dataset, establishing the paragraph as the fundamental unit for genre annotation. Two student annotators, working with German patient magazines focused on diabetes, applied the predefined genre labels to the text. To ensure the reliability of this process, the authors calculated the inter-annotator agreement, achieving a Cohen's Kappa score of 0.77, which signifies a substantial level of consistency.

## Annotation in Practice

![Slide 14](images/ai-nepi_005_slide_14.png)

A practical example demonstrates the annotation process. Three sample paragraphs extracted from the German magazine *Der Diabetiker* are presented in a table. Each paragraph is paired with its assigned genre label, clearly illustrating how the classification schema is applied to actual source text. This example serves to clarify the task for both training and evaluation purposes.

## Dataset Splits for Experiments

![Slide 15](images/ai-nepi_005_slide_15.png)

For their machine learning experiments, the authors partitioned the annotated ActDisease data into specific training and held-out sets. They carefully designed this division to evaluate model performance in both few-shot and zero-shot learning scenarios. The training set consists exclusively of annotated German texts. In contrast, the held-out (test) set comprises texts in German, French, and Swedish, and crucially, it includes genres that are deliberately absent from the training data to test zero-shot generalisation.

## Genre and Language Distribution

![Slide 16](images/ai-nepi_005_slide_16.png)

An analysis of the dataset reveals the distribution of text instances across different languages and genres. A comparison between the training and held-out sets highlights two important characteristics. First, there are significant imbalances in the prevalence of certain genres, a common feature of real-world data. Second, the held-out set intentionally includes novel genres absent from the training set, a design critical for rigorously assessing the zero-shot classification capabilities of the models.

## External Datasets for Zero-Shot

![Slide 17](images/ai-nepi_005_slide_17.png)

To enhance their zero-shot learning experiments, the team incorporated several publicly available, multilingual datasets for genre classification. These external resources, which include *CORE*, *UDM*, and *FTD*, provide a mix of document-level and sentence-level annotations. Leveraging these datasets allows for more robust training and evaluation of the models' ability to generalise to unseen labels and data distributions.

## Cross-Dataset Label Mapping

![Slide 18](images/ai-nepi_005_slide_18.png)

A comparative table illustrates the mapping of genre labels across the four datasets used in the study: *ActDisease*, *CORE*, *UDM*, and *FTD*. This visualisation reveals considerable variation in the classification schemas, with different datasets using distinct and sometimes conflicting genre definitions. Such heterogeneity poses a significant challenge for zero-shot learning, as models trained on one schema must adapt to another, necessitating a thoughtful approach to label mapping.

## Training Data Pipeline

![Slide 19](images/ai-nepi_005_slide_19.png)

The authors designed a comprehensive and flexible pipeline for generating training data. This process integrates data from all four sources—*ActDisease*, *CORE*, *UDM*, and *FTD*—and subjects them to a series of preprocessing steps. Crucially, the pipeline incorporates configurable sampling strategies, allowing the team to create various training set compositions to test different hypotheses about model performance and data balancing systematically.

## Multilingual Encoder Models

![Slide 20](images/ai-nepi_005_slide_20.png)

The experiments leverage several powerful multilingual encoder models as the basis for classification. The selected models include the widely used *XLM-Roberta* (*xlmr*) and multilingual *BERT* (*mBERT*). In addition, the team evaluates a specialised historical *mBERT* (*hmbert*), which has been pre-trained on a large corpus of historical texts. This selection allows for a comparison between general-purpose multilingual models and a model adapted for the specific domain of historical language.

## Fine-Tuning Setup

![Slide 21](images/ai-nepi_005_slide_21.png)

The experimental design for fine-tuning involved a systematic and large-scale approach. The authors created 16 unique training set configurations by varying the data sources and sampling strategies. They then fine-tuned each of the three base language models (*XLM-R*, *mBERT*, and *hmBERT*) on every one of these 16 configurations. This comprehensive methodology resulted in a total of 48 distinct fine-tuned models, enabling a thorough analysis of how training data composition affects performance.

## Evaluating Zero-Shot Learning

![Slide 22](images/ai-nepi_005_slide_22.png)

The investigation now shifts its focus towards evaluating the models' performance in a zero-shot learning context. This phase assesses the ability of the fine-tuned models to classify genres that they have not encountered during their training phase.

## Zero-Shot Evaluation Methodology

![Slide 23](images/ai-nepi_005_slide_23.png)

Evaluating zero-shot predictions requires a specialised methodology to handle inherent complexities. The primary challenges include managing the partial overlap between genre label sets from different source datasets and accounting for cross-lingual scenarios where a model is tested on a language not present in its training data. The authors' evaluation protocol is designed specifically to navigate these issues, ensuring a robust and fair assessment of model generalisation.

## Zero-Shot Results Overview

![Slide 24](images/ai-nepi_005_slide_24.png)

An overview of the zero-shot experiment results reveals several key patterns. For the *FTD* dataset, employing a specific label mapping strategy yields a noticeable improvement in model performance. Across other datasets, however, results indicate the presence of class-specific and language-related biases. On the *UDM* dataset, the investigators observed intriguing performance variations between the different models for certain classification tasks. Notably, models fine-tuned using the *CORE* dataset demonstrate a particular aptitude for correctly identifying texts belonging to the *Legal* genre.

## Comparative Confusion Matrices

![Slide 25](images/ai-nepi_005_slide_25.png)

Four confusion matrices provide a visual comparison of the performance of different genre classification models. The matrices detail the results for models such as *hmbert_UDM*, *xlmr_CORE*, *xlmr_UDM*, and *xlmr_FTD*, each representing a unique combination of base architecture and training data. These visualisations allow for a direct comparison of error patterns and classification accuracy under various experimental conditions when evaluated on the held-out ActDisease data.

## Zero-Shot F1 Scores

![Slide 26](images/ai-nepi_005_slide_26.png)

A summary table presents the zero-shot performance using the per-category F1 score as the primary metric. These scores, which measure the balance between precision and recall for each genre, are averaged across the various training data configurations. This allows for a consolidated view of how well each language model performs on individual genre categories in a zero-shot setting.

## Impact of Data Configuration

![Slide 27](images/ai-nepi_005_slide_27.png)

The authors analysed the average F1 performance of the classification models across three distinct target tasks, corresponding to the *FTD*, *CORE*, and *UDM* datasets. This analysis specifically investigates how performance is affected by two key factors in the training data construction: the application of data balancing techniques and the inclusion or exclusion of certain language families. The results illuminate the sensitivity of model performance to the composition of the training corpus.

## Evaluating Few-Shot Learning

![Slide 28](images/ai-nepi_005_slide_28.png)

The report now transitions to an evaluation of the models under a few-shot learning paradigm. This section assesses how effectively the models can learn to classify genres when provided with only a small number of training examples from the target ActDisease dataset.

## Few-Shot Learning Performance

![Slide 29](images/ai-nepi_005_slide_29.png)

The evaluation of few-shot learning reveals clear performance trends. As expected, F1 scores for all models generally improve as the number of available training examples increases. A crucial finding is that an intermediate step of Masked Language Model (MLM) fine-tuning on the target domain's text confers a distinct advantage, consistently boosting classification accuracy. Amongst all configurations, the *hmBERT-MLM* model, which combines historical pre-training with domain-specific MLM fine-tuning, achieves the highest performance.

## Few-Shot F1 Score Details

![Slide 30](images/ai-nepi_005_slide_30.png)

A detailed table presents the per-category F1 scores for the few-shot learning experiments. The results are broken down for each language model and are shown at two distinct levels of data availability (for example, with 16 and 32 training examples per class). In addition to the granular, per-category scores, the table also includes overall performance metrics, allowing for a comprehensive comparison of the models' effectiveness in a data-scarce environment.

## Full-Dataset Performance

![Slide 31](images/ai-nepi_005_slide_31.png)

When trained on the full dataset, the *XLM-Roberta-MLM* model demonstrates strong classification capabilities, as illustrated by a confusion matrix. The matrix reveals specific patterns of misclassification, which in turn provide insights into the nature of the data. For instance, it highlights the thematic and stylistic similarities between the *Guide*, *Nonfiction Prose*, and *QA* genres as they appear within the context of diabetes patient magazines, explaining why the model sometimes confuses them.

## Evaluating Few-Shot Prompting

![Slide 32](images/ai-nepi_005_slide_32.png)

The final experimental section shifts to an evaluation of few-shot prompting. This approach assesses the ability of large generative language models to perform genre classification based on instructions and a small number of examples provided directly in the input prompt.

## Instruction-Based Classification

![Slide 33](images/ai-nepi_005_slide_33.png)

For the prompting experiment, the authors formulated a detailed set of instructions for a text genre classification task. The prompt explicitly defines the nine target genres, complete with illustrative examples for each, and specifies the required input and output format. The chosen language model for executing this instruction-based, few-shot task is *Llama-3.1 8b*.

## Llama-3.1 8b Performance

![Slide 34](images/ai-nepi_005_slide_34.png)

The performance of the *Llama-3.1 8b Instruct* model on the few-shot prompting task is presented. The results include F1-scores for each genre, quantifying the model's accuracy. A detailed confusion matrix is also provided, offering a granular view of the model's classification decisions and revealing which genres were most frequently confused with one another.

## Conclusion on Text Mining

![Slide 35](images/ai-nepi_005_slide_35.png)

A primary conclusion from this work is that applying text mining techniques to popular magazines is an inherently more complex task than analysing more uniform corpora like scientific journals or books. The authors attribute this increased difficulty directly to the rich and varied multitude of genres that coexist within a single magazine issue, demanding more sophisticated analytical approaches.

## Summary of Conclusions

![Slide 37](images/ai-nepi_005_slide_37.png)

The authors' research yields several key conclusions:

- The generic diversity of historical magazines poses a substantial challenge for text mining.

- Genre classification proves to be an indispensable tool, enabling the kind of fine-grained analysis necessary for deep historical inquiry.

- Modern, large-scale datasets can be successfully leveraged to improve the analysis of historical texts.

- Contemporary generative models exhibit promising quality on these classification tasks.

- Few-shot learning with multilingual encoders is a highly effective strategy, with performance being particularly strong when using models specifically adapted for historical language.