---
title: "Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance"
author:
- name: "Francis Lareau & Christophe Malaterre"
  affiliation: "Université du Québec à Montréal; Université de Sherbrooke; CIRST"
  email: "francislareau@hotmail.com"
date: '2025'
bibliography: bibliography.bib
---

## Overview {.unnumbered}

This chapter presents a comprehensive comparative study systematically evaluating the performance of *Latent Dirichlet Allocation* (*LDA*) and *BERTopic* across distinct textual granularities: titles, abstracts, and full texts. This investigation addresses a pressing question within topic modelling, a crucial analytical tool for navigating vast volumes of scientific literature, particularly within the history, philosophy, and sociology of science. Topic modelling, indeed, extracts thematic content from corpora, thereby enabling the identification of research trends, paradigm shifts, substructures, thematic interrelations, and the evolution of scientific vocabulary.

The study's core objective was to ascertain whether analysing titles or abstracts suffices for effective topic modelling, or if full-text analysis remains indispensable. This inquiry is particularly pertinent given the substantial resources required for obtaining, preprocessing, and analysing comprehensive corpora. To achieve this, the researchers meticulously constituted a corpus of scientific articles, precisely identifying and segmenting title, abstract, and full-text sections.

Subsequently, they applied both *LDA* and *BERTopic* approaches to each textual level. A dual analytical framework, encompassing both qualitative and quantitative methods, then facilitated the comparison of the resulting topic models. This rigorous methodology involved assessing model similarities, topic diversity, joint recall, and coherence, whilst leveraging a well-known astrobiology corpus for qualitative validation.

## Research Question and Study Design

![Slide 01](images/ai-nepi_016_slide_01.jpg)

This research addresses a pivotal question within the domain of topic modelling: does analysing titles or abstracts provide sufficient data, or does full-text analysis remain a prerequisite? Topic modelling, a technique for extracting thematic content from textual corpora, has emerged as an indispensable tool for scrutinising extensive scientific literature, particularly within the history, philosophy, and sociology of science. Scholars employ it for diverse tasks, including identifying research trends, discerning paradigm shifts, uncovering substructures, mapping thematic interrelations, and tracing the evolution of scientific vocabulary.

Crucially, existing studies apply topic modelling across various textual structures, encompassing titles, abstracts, and complete articles. This practice, however, raises a significant concern: obtaining, preprocessing, and analysing full-text corpora demand considerable resources. Consequently, the efficiency of utilising shorter textual forms becomes a pressing inquiry.

To investigate this, the researchers meticulously constituted a corpus of scientific articles. They then precisely identified and isolated the title, abstract, and full-text sections within each document. Subsequently, they applied two distinct topic modelling approaches—*Latent Dirichlet Allocation* (*LDA*) and *BERTopic*—to each of these textual levels. A comprehensive analytical framework, integrating both qualitative and quantitative methods, facilitated the systematic comparison of the resultant topic models. This rigorous design ensured a thorough evaluation of performance across different model types and textual granularities.

## Topic Modelling Methodologies

![Slide 05](images/ai-nepi_016_slide_05.jpg)

The study employed two principal topic modelling methodologies: *Latent Dirichlet Allocation* (*LDA*) and *BERTopic*. Both approaches fundamentally postulate that documents can be represented as numerical vectors. Within this framework, topics become identifiable through the detection of linguistic regularities, specifically repetitions, whilst machine learning algorithms facilitate the automatic discovery of these patterns.

*LDA*, a classical statistical technique, constructs simple vector representations by counting words within documents. In this established approach, topics manifest as latent variables, adhering to Dirichlet's law. Crucially, *LDA* readily accommodates extensive textual content, allowing for its application to titles, abstracts, or full texts.

Conversely, *BERTopic* represents a more recent, modular methodology. It leverages *Large Language Model* (*LLM*)-based vector representations, originally drawing upon *BERT*, which lends the approach its name. Here, topics emerge as clusters of documents. Historically, *BERTopic* struggled with processing lengthy texts; however, for this investigation, the researchers integrated a novel embedding technique. This advancement significantly enhanced *BERTopic*'s capacity, enabling it to process approximately 131,000 tokens, thereby facilitating its application to full-text analysis.

## Corpus and Qualitative Comparison

![Slide 07](images/ai-nepi_016_slide_07.jpg)

The study’s qualitative comparisons drew upon a meticulously analysed astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a comprehensive evaluation, the researchers selected a full-text *LDA* model comprising 25 distinct topics to serve as a foundational reference.

Scholars meticulously analysed these 25 topics, examining their most representative words and documents. This process facilitated the generation of a concise label for each topic, derived directly from its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric based on the topics' presence within individual documents. A community detection algorithm then identified four distinct thematic clusters, designated A, B, C, and D, and visually distinguished by red, green, yellow, and blue hues respectively.

A graphical representation visually conveyed these findings, illustrating the correlations amongst the 25 topics, complete with their assigned labels and cluster affiliations. In this visualisation, the thickness of the connecting lines denoted the strength of the correlation between topics, whilst the size of each circular node indicated the topic’s overall prevalence across the entire document collection. This established analytical framework provided a robust basis for the qualitative assessment of the six topic models under investigation.

## Quantitative Analysis Metrics

![Slide 08](images/ai-nepi_016_slide_08.jpg)

For quantitative analysis, the researchers employed four distinct metrics to rigorously compare the topic models. Firstly, the *Adjusted Rand Index* (*ARI*) served to evaluate the similarity between any two document clusterings, whilst correcting for chance agreement. This metric precisely assessed the degree to which documents tended to cluster together across different models.

Secondly, *Topic Diversity* quantified the proportion of distinct top words within a given topic model, thereby evaluating whether individual topics were indeed characterised by unique vocabularies. Thirdly, *Joint Recall* measured the average document-topic recall in relation to any topic's top words. This metric critically assessed how effectively the top words collectively represented the documents assigned to each topic. Finally, *Coherence CV*, calculated as the average cosine relative distance between top words within topics, determined whether the constituent words of a topic exhibited a meaningful semantic relationship.

## Results: Adjusted Rand Index

![Slide 09](images/ai-nepi_016_slide_09.jpg)

The *Adjusted Rand Index* (*ARI*) provided initial insights into the similarities amongst the six topic models. A score of zero on this metric signifies a random clustering, establishing a baseline for comparison. Analysis revealed that the *LDA* model applied to titles exhibited the most pronounced dissimilarity from all other models, consistently registering *ARI* values below 0.2 within the heatmap.

Conversely, the remaining models generally achieved a superior overall match, with *ARI* scores exceeding 0.2. Notably, *BERTopic* models demonstrated a stronger mutual fit, consistently yielding values above 0.35. Amongst these, the *BERTopic* abstract model emerged as particularly central, correlating effectively with every other model, save for the outlier *LDA* title model.

## Results: LDA Model Inter-Comparisons

![Slide 09](images/ai-nepi_016_slide_09.jpg)

A more granular analysis of the *LDA* models provided detailed insights into their inter-relationships. Comparing *LDA* Full-text with *LDA* Abstract (Table A) revealed a generally strong fit. A distinct reddish diagonal in the table indicated that each topic from one model largely corresponded to a topic in the other, sharing a high proportion of common documents.

Despite this overall alignment, some dynamic shifts occurred: three full-text *LDA* topics entirely disappeared, whilst another three split into multiple topics within the abstract model. Concurrently, three novel abstract topics emerged, and three abstract topics resulted from the merger of others. Furthermore, one small class within the abstract topics contained fewer than 50 documents.

In stark contrast, the comparison between *LDA* Full-text and *LDA* Title (Table B) demonstrated a poor fit, necessitating substantial reorganisation. This disparity manifested as numerous full-text topics vanishing and a proliferation of new topics appearing within the title model, underscoring the limited correspondence between these two textual granularities for *LDA*.

## Results: BERTopic Model Inter-Comparisons

![Slide 11](images/ai-nepi_016_slide_11.jpg)

Analysis of the *BERTopic* models, particularly in comparison with *LDA* Full-text, revealed varied levels of correspondence. Comparing *LDA* Full-text with *BERTopic* Full-text (Table C) indicated an average overall fit. Within this comparison, eight topics from the *LDA* model disappeared, whilst six topics split into the *BERTopic* model. Conversely, five new topics emerged within the *BERTopic* model, and one topic resulted from mergers. Furthermore, the document distribution showed four small classes alongside one notably large class.

The comparison between *LDA* Full-text and *BERTopic* Abstract (Table D) demonstrated a relatively good fit. Here, four topics disappeared, six topics split, two new topics appeared, and four topics resulted from mergers.

Finally, examining *LDA* Full-text against *BERTopic* Title (Table E) again indicated an average overall fit. In this instance, seven topics disappeared, whilst one topic split. Simultaneously, seven new topics emerged, and one topic resulted from a merger. The document distribution for this comparison revealed three small classes and one large class.

## Results: LDA Top-Word Correspondence

![Slide 13](images/ai-nepi_016_slide_13.jpg)

An examination of the top words within the *LDA* models revealed that topics generally maintained a relatively well-formed structure across all iterations. The researchers identified several robust topics exhibiting strong correspondence across every *LDA* model; "A-Radiation spore" serves as a prime example of such consistency.

Conversely, certain topics from the full-text model fragmented across the abstract and title models. For instance, "A-Life civilization" split into multiple sub-topics, a division that logically aligned with the broader theme of research in astrobiology. However, the fragmentation of "B-Chemistry" proved more challenging to interpret without deeper investigation.

Furthermore, the analysis uncovered instances where topics from the full-text model merged into new, consolidated topics within the abstract and title models. The fusion of "B-Amino-acid" and "B-Protein-gene-RNA" exemplified this phenomenon, forming a more generalised and coherent thematic unit.

## Results: BERTopic Top-Word Correspondence

![Slide 14](images/ai-nepi_016_slide_14.jpg)

Continuing the assessment of top words, the three *BERTopic* models consistently yielded relatively well-formed topics. Notably, "A-Radiation spore" again demonstrated remarkable robustness, maintaining its coherence across all *BERTopic* iterations. Similarly, "A-Life civilization" remained comparatively stable across the models, albeit with some observed splitting.

This fragmentation of "A-Life civilization" specifically led to the emergence of narrower topics, focusing precisely on extraterrestrial life. Furthermore, the splitting of "B-Chemistry" across the *BERTopic* models also resulted in more specialised, narrower thematic categories.

## Results: Coherence Performance

![Slide 15](images/ai-nepi_016_slide_15.jpg)

An evaluation of the models' coherence, a metric assessing the meaningfulness of topic top words, revealed distinct performance patterns across a range of 5 to 50 topics. Titles consistently yielded the poorest coherence scores, indicating a less meaningful grouping of their constituent words. Conversely, abstract models generally demonstrated superior coherence compared to their full-text counterparts.

Across the board, *BERTopic* models exhibited better coherence than *LDA*, particularly for abstract and title analyses. However, this performance gap narrowed as the number of topics increased. Ultimately, the *BERTopic* Abstract model emerged as the unequivocal leader in terms of coherence.

## Results: Diversity Performance

![Slide 16](images/ai-nepi_016_slide_16.jpg)

Assessing the diversity of top words representing the topics, a clear trend emerged: diversity generally diminished as the number of topics increased. Titles, surprisingly, offered the highest diversity amongst all models, suggesting a broader range of unique words characterising their topics.

Furthermore, *BERTopic* consistently outperformed *LDA* in terms of diversity. Ultimately, the *BERTopic* Title model secured the top position for diversity, with *BERTopic* Full-text closely trailing.

## Results: Joint Recall Performance

![Slide 17](images/ai-nepi_016_slide_17.jpg)

The joint recall metric, which evaluates the efficacy of top words in collectively representing documents classified within each topic, revealed distinct performance hierarchies. Titles consistently yielded the poorest recall scores, indicating a limited ability of their top words to capture the full scope of associated documents. Conversely, full-text models demonstrated superior recall compared to both their abstract and title counterparts.

Between the two primary approaches, *LDA* generally exhibited better joint recall than *BERTopic*. Ultimately, *LDA* Full-text and *BERTopic* Full-text emerged as joint leaders in this category, with *BERTopic* Abstract following very closely behind.

## Overall Model Performance Summary

![Slide 17](images/ai-nepi_016_slide_17.jpg)

The researchers compiled the comprehensive results into a summary table, visually representing each model's performance across various assessments using a graded circle system: black denoted the highest score, whilst white indicated the lowest. Crucially, this synthesis underscored the absence of an absolute "best" model, as varying research objectives inherently dictate differing needs and, consequently, distinct model choices.

Consider, for instance, an objective focused solely on discovering main topics, where precise document classification is not paramount. In such a scenario, issues like poor recall or significant class imbalance might prove negligible. Here, full-text *BERTopic* performed commendably, despite exhibiting some class imbalance. Similarly, whilst far from optimal, title *BERTopic* nonetheless generated several robust topics that consistently appeared across other models. Conversely, the researchers strongly advise against employing *LDA* Title, given its consistently poor performance across nearly all assessment criteria.

Ultimately, the study recommends conducting topic modelling on either abstract or full-text data, utilising both *LDA* and *BERTopic*, provided such an approach does not result in the misclassification of documents pertinent to the identified topics.

## Discussion and Future Directions

![Slide 17](images/ai-nepi_016_slide_17.jpg)

This research yielded several crucial findings, informing future approaches to topic modelling. Firstly, title models consistently demonstrated poor performance. This deficiency likely stems from the inherent lack of information within titles, which can lead to the false classification of documents. Nevertheless, the *BERTopic* title model surprisingly revealed several meaningful topics, suggesting a potential balance between well-defined topics and comprehensive document coverage remains achievable.

Secondly, full-text models, whilst offering comprehensive data, sometimes struggle to process vast quantities of information effectively. With *LDA*, topics can become more loosely defined and broader in scope, occasionally encompassing secondary themes such as methodology. Conversely, *BERTopic*, when applied to full text, can generate overly narrow topics, resulting in inadequate document coverage and issues with class size.

Thirdly, abstract models consistently performed well with summary information. Notably, the results obtained from *LDA* full text exhibited strong consistency with both abstract models, underscoring their utility. Fourthly, the study revealed a remarkable robustness of topics across all models. Researchers identified very similar topics across the board, a consistency that facilitates the application of meta-analytic methods to pinpoint the most robust thematic elements. Moreover, leveraging the relative distance across models could enable the identification of an optimal solution, as exemplified by the *BERTopic* abstract model in this study, which performed exceptionally well across numerous metrics.

Finally, the findings prompt consideration of new model paradigms. It appears feasible to exploit the inherent structural information—encompassing full text, abstracts, and titles—to extract more semantically meaningful sets of topics or top words, thereby advancing the precision and utility of topic modelling.
