<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-01-01">

<title>3&nbsp; Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-a126389619fad6dbfb296a5315d49fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Philosophy at Scale: Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">SDG-Research in Bibliometric DBs - LLMs for HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems for Philosophical Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#the-foundational-transformer-architecture" id="toc-the-foundational-transformer-architecture" class="nav-link" data-scroll-target="#the-foundational-transformer-architecture"><span class="header-section-number">3.1</span> The Foundational Transformer Architecture</a></li>
  <li><a href="#differentiating-bert-and-gpt-models" id="toc-differentiating-bert-and-gpt-models" class="nav-link" data-scroll-target="#differentiating-bert-and-gpt-models"><span class="header-section-number">3.2</span> Differentiating BERT and GPT Models</a></li>
  <li><a href="#scientific-llm-evolution-and-adaptation-strategies" id="toc-scientific-llm-evolution-and-adaptation-strategies" class="nav-link" data-scroll-target="#scientific-llm-evolution-and-adaptation-strategies"><span class="header-section-number">3.3</span> Scientific LLM Evolution and Adaptation Strategies</a></li>
  <li><a href="#retrieval-augmented-generation-and-key-distinctions" id="toc-retrieval-augmented-generation-and-key-distinctions" class="nav-link" data-scroll-target="#retrieval-augmented-generation-and-key-distinctions"><span class="header-section-number">3.4</span> Retrieval Augmented Generation and Key Distinctions</a></li>
  <li><a href="#applications-and-trends-in-hpss-research" id="toc-applications-and-trends-in-hpss-research" class="nav-link" data-scroll-target="#applications-and-trends-in-hpss-research"><span class="header-section-number">3.5</span> Applications and Trends in HPSS Research</a></li>
  <li><a href="#critical-reflections-and-future-directions-for-hpss" id="toc-critical-reflections-and-future-directions-for-hpss" class="nav-link" data-scroll-target="#critical-reflections-and-future-directions-for-hpss"><span class="header-section-number">3.6</span> Critical Reflections and Future Directions for HPSS</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span></h1>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            AI-NEPI Conference Participant
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This presentation systematically introduces the foundational architecture of large language models (LLMs), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (HPSS). Initially, the speaker, a co-organiser, provides a primer on the seminal <em>Transformer</em> architecture, explaining its encoder-decoder structure and its original purpose in language translation. Subsequently, the discussion differentiates between encoder-based models, such as <em>BERT</em>, which offer bidirectional full-context understanding, and decoder-based generative models, including <em>GPT</em>, capable of producing novel text.</p>
<p>The presentation then charts the proliferation of domain-specific LLMs across various scientific fields, outlining diverse adaptation strategies. These include pre-training, fine-tuning, and the sophisticated Retrieval Augmented Generation (RAG) pipeline. Crucially, the speaker categorises current LLM applications in HPSS, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, the presentation offers critical reflections on HPSS-specific challenges, such as historical language evolution and sparse data, whilst advocating for enhanced LLM literacy and a steadfast adherence to HPSS methodologies. This approach highlights new opportunities for bridging qualitative and quantitative research.</p>
</section>
<section id="the-foundational-transformer-architecture" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-foundational-transformer-architecture"><span class="header-section-number">3.1</span> The Foundational Transformer Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>This presentation offers a foundational primer on large language models (LLMs), detailing their adaptation for scientific domains and summarising their current applications within the history, philosophy, and sociology of science (HPSS). Furthermore, it shares critical reflections intended to stimulate discussion throughout the workshop. Addressing a heterogeneous audience with diverse technical backgrounds, the speaker aims to ensure accessibility whilst maintaining scholarly rigour.</p>
<p>At the core of all contemporary large language models lies the seminal <em>Transformer</em> architecture, pioneered by Vaswani and colleagues in 2017. The engineers originally designed this model for language translation, facilitating conversions such as German to English. The architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.</p>
<p>The encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence meaning. Conversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness.</p>
</section>
<section id="differentiating-bert-and-gpt-models" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="differentiating-bert-and-gpt-models"><span class="header-section-number">3.2</span> Differentiating BERT and GPT Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Immediately following the <em>Transformer</em>’s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (NLP) tasks with minimal additional training.</p>
<p>From the encoder side, the <em>BERT</em> family of models emerged, standing for Bidirectional Encoder Representations from <em>Transformers</em>. <em>BERT</em> operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. Conversely, the decoder side gave rise to the <em>GPT</em> models, or Generative Pre-trained <em>Transformers</em>, which now power applications like <em>ChatGPT</em>. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text, a function not inherently present in <em>BERT</em>-like models.</p>
<p>Consequently, a fundamental distinction arises between these two model types: generative models, exemplified by <em>GPT</em>, primarily produce language, whereas full-context models, such as <em>BERT</em>, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like <em>XLM</em> and <em>XLNet</em>.</p>
</section>
<section id="scientific-llm-evolution-and-adaptation-strategies" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="scientific-llm-evolution-and-adaptation-strategies"><span class="header-section-number">3.3</span> Scientific LLM Evolution and Adaptation Strategies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>A comprehensive overview reveals the rapid evolution of large language models, particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to <em>BERT</em>, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included <em>BioBERT</em>, <em>Specter</em>, and <em>SciBERT</em>. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.</p>
<p>Researchers employ several methods to adapt these models to specific scientific language. Pre-training constitutes the initial phase, where a model learns language by predicting the next token, as in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many. Consequently, continued pre-training offers a viable alternative; researchers utilise an already pre-trained model, subsequently training it on domain-specific language, such as adapting a <em>BERT</em> model for physics texts.</p>
<p>Beyond this, engineers can add extra layers atop pre-trained models, effectively training them for classification tasks like sentiment analysis or named entity recognition. Crucially, contrastive learning emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with <em>Sentence BERT</em> serving as a widely adopted example.</p>
</section>
<section id="retrieval-augmented-generation-and-key-distinctions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="retrieval-augmented-generation-and-key-distinctions"><span class="header-section-number">3.4</span> Retrieval Augmented Generation and Key Distinctions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>Retrieval Augmented Generation (RAG) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins <em>ChatGPT</em>’s internet search capabilities. The process commences with a user query, such as “What are LLMs?”. Subsequently, a <em>BERT</em>-type model encodes this query into a sentence embedding. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the RAG pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context.</p>
<p>Beyond RAG, advanced reasoning models and agents are emerging; these are not isolated LLMs but rather intricate systems that combine LLMs with a diverse array of other tools. Consequently, a clear understanding of key distinctions proves crucial for navigating the LLM landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone LLMs, complex pipelines like RAG, and sophisticated agents becomes paramount for effective application.</p>
</section>
<section id="applications-and-trends-in-hpss-research" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="applications-and-trends-in-hpss-research"><span class="header-section-number">3.5</span> Applications and Trends in HPSS Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>A current survey explores the burgeoning uses of large language models as tools within history, philosophy, and sociology of science (HPSS) research. This investigation has identified four primary categories for sorting these applications:</p>
<ul>
<li><p>LLMs assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations.</p></li>
<li><p>They contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields.</p></li>
<li><p>LLMs illuminate knowledge dynamics, particularly through the study of conceptual histories of words.</p></li>
<li><p>Finally, they support the analysis of knowledge practices, including citation context analysis—an older HPSS tradition now also employed for evaluatory purposes.</p></li>
</ul>
<p>A notable trend indicates an accelerating interest in LLMs, with findings predominantly appearing in information science journals like <em>Scientometrics</em> and <em>Jasis</em>. Increasingly, however, papers featuring LLM applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers. Furthermore, the degree of customisation in LLM deployment varies widely, spanning from straightforward off-the-shelf use of <em>ChatGPT</em> to the development of entirely new model architectures.</p>
<p>Despite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models, and persistent shortages of training data and benchmarks. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by <em>BERTopic</em>, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.</p>
</section>
<section id="critical-reflections-and-future-directions-for-hpss" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="critical-reflections-and-future-directions-for-hpss"><span class="header-section-number">3.6</span> Critical Reflections and Future Directions for HPSS</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Crucially, scholars must acknowledge the specific challenges inherent to HPSS when engaging with large language models. Foremost amongst these is the historical evolution of concepts and language; models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models or the judicious use of existing ones with a keen awareness of their limitations. Furthermore, HPSS adopts a reconstructive and critically reflective perspective, reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive HPSS “reading.” Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts.</p>
<p>Consequently, building robust LLM literacy becomes imperative. Researchers must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.</p>
<p>Ultimately, HPSS researchers must remain true to their established methodologies. This involves translating complex HPSS problems into specific NLP tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon HPSS’s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s, offers valuable theoretical grounding, particularly given the resonance of <em>Actor-Network Theory</em> (ANT) concepts with contemporary LLM developments.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Philosophy at Scale: Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Philosophy at Scale: Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>