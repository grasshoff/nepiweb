<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-06-21">

<title>3&nbsp; Large Language Models for the History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals, ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Computational Epistemology and Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#presentation-agenda-and-introduction" id="toc-presentation-agenda-and-introduction" class="nav-link" data-scroll-target="#presentation-agenda-and-introduction"><span class="header-section-number">3.1</span> Presentation Agenda and Introduction</a></li>
  <li><a href="#the-transformer-architecture-foundations-of-llms" id="toc-the-transformer-architecture-foundations-of-llms" class="nav-link" data-scroll-target="#the-transformer-architecture-foundations-of-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of LLMs</a></li>
  <li><a href="#evolution-to-pre-trained-language-models-encoder-focus" id="toc-evolution-to-pre-trained-language-models-encoder-focus" class="nav-link" data-scroll-target="#evolution-to-pre-trained-language-models-encoder-focus"><span class="header-section-number">3.3</span> Evolution to Pre-trained Language Models: Encoder Focus</a></li>
  <li><a href="#bidirectional-and-generative-models-bert-and-gpt" id="toc-bidirectional-and-generative-models-bert-and-gpt" class="nav-link" data-scroll-target="#bidirectional-and-generative-models-bert-and-gpt"><span class="header-section-number">3.4</span> Bidirectional and Generative Models: BERT and GPT</a></li>
  <li><a href="#landscape-of-scientific-large-language-models" id="toc-landscape-of-scientific-large-language-models" class="nav-link" data-scroll-target="#landscape-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Landscape of Scientific Large Language Models</a></li>
  <li><a href="#domain-and-task-adaptation-strategies-for-llms" id="toc-domain-and-task-adaptation-strategies-for-llms" class="nav-link" data-scroll-target="#domain-and-task-adaptation-strategies-for-llms"><span class="header-section-number">3.6</span> Domain and Task Adaptation Strategies for LLMs</a></li>
  <li><a href="#scientific-llms-timeline-and-survey-context" id="toc-scientific-llms-timeline-and-survey-context" class="nav-link" data-scroll-target="#scientific-llms-timeline-and-survey-context"><span class="header-section-number">3.7</span> Scientific LLMs Timeline and Survey Context</a></li>
  <li><a href="#key-distinctions-in-large-language-models" id="toc-key-distinctions-in-large-language-models" class="nav-link" data-scroll-target="#key-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Models</a></li>
  <li><a href="#applications-of-llms-in-hpss-research" id="toc-applications-of-llms-in-hpss-research" class="nav-link" data-scroll-target="#applications-of-llms-in-hpss-research"><span class="header-section-number">3.9</span> Applications of LLMs in HPSS Research</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Arno Simons, from Technische Universität Berlin and funded by the European Research Council, presents ‘Large Language Models for the History, Philosophy, and Sociology of Science’. This presentation offers a foundational primer on Large Language Models (<em>LLMs</em>) and their applications within the History, Philosophy, and Sociology of Science (<em>HPSS</em>) purview. Simons delineates current <em>HPSS</em> applications and provokes critical reflection for discussion. He details the pivotal <em>Transformer</em> architecture, distinguishing between <em>encoder</em>-based (<em>BERT</em>) and <em>decoder</em>-based (<em>GPT</em>) models, and explores various domain adaptation strategies, including pre-training, fine-tuning, and <em>Retrieval-Augmented Generation</em> (<em>RAG</em>). Furthermore, Simons outlines specific <em>HPSS</em> applications such as data parsing, entity extraction, conceptual history analysis, and discourse analysis. The presentation concludes by addressing <em>HPSS</em>-specific challenges, including historical language evolution and the imperative for <em>LLM</em> literacy, whilst highlighting new opportunities for bridging qualitative and quantitative research approaches.</p>
</section>
<section id="presentation-agenda-and-introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="presentation-agenda-and-introduction"><span class="header-section-number">3.1</span> Presentation Agenda and Introduction</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>The speaker systematically articulates three core areas: an essential primer on Large Language Models (<em>LLMs</em>) and their adaptation to scientific domains, a detailed exposition of current applications within the History, Philosophy, and Sociology of Science (<em>HPSS</em>), and a suite of critical reflections intended to stimulate workshop discussions. Recognising the audience’s varied disciplinary backgrounds, Arno Simons aims to provide a lucid yet exacting introduction to the subject matter.</p>
</section>
<section id="the-transformer-architecture-foundations-of-llms" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture-foundations-of-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The foundational <em>Transformer</em> architecture forms the bedrock of virtually all contemporary Large Language Models. Vaswani and colleagues originally conceived this architecture in 2017 for inter-language translation, as detailed in their seminal 2017 paper, ‘<em>Attention is All You Need</em>’. This model comprises two interlinked streams: an <em>Encoder</em> and a <em>Decoder</em>. The <em>Encoder</em> processes input words, transforming them into numerical representations that then undergo extensive processing before transfer to the <em>Decoder</em>. Subsequently, the <em>Decoder</em> generates output words, iteratively feeding each newly produced word back into the system to facilitate the prediction of the next word in the sequence. Within these numerical processing stages, various layers incrementally refine and contextualise word embeddings. Crucially, the <em>Encoder</em> processes the entire input sentence simultaneously, allowing each word to interact reciprocally with all other words, thereby constructing a comprehensive contextual understanding of the complete input. Conversely, the <em>Decoder</em> operates unidirectionally; it can only access preceding words to predict the next token, preventing it from ‘looking ahead’ into the future sequence. Immediately following its introduction, researchers began re-engineering these individual <em>Encoder</em> and <em>Decoder</em> streams to develop the pre-trained language models prevalent today.</p>
</section>
<section id="evolution-to-pre-trained-language-models-encoder-focus" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="evolution-to-pre-trained-language-models-encoder-focus"><span class="header-section-number">3.3</span> Evolution to Pre-trained Language Models: Encoder Focus</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>The field has profoundly transitioned from models primarily designed for translation to those capable of deep linguistic comprehension and generation. These pre-trained models, requiring only minimal supplementary training, now effectively execute a wide array of Natural Language Processing (<em>NLP</em>) tasks. For clarity, the discussion initially focuses on the <em>Encoder</em> component, which plays a central role in this evolution.</p>
</section>
<section id="bidirectional-and-generative-models-bert-and-gpt" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bidirectional-and-generative-models-bert-and-gpt"><span class="header-section-number">3.4</span> Bidirectional and Generative Models: BERT and GPT</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>The <em>Encoder</em> stream evolved into models such as <em>BERT</em>, an acronym for <em>Bidirectional Encoder Representations from Transformers</em>. This architecture enables each word within an input sequence to interact with all other words, thereby constructing a holistic, context-rich understanding of the entire input. Devlin and colleagues pioneered this architectural paradigm in 2018. Conversely, the <em>Decoder</em> stream gave rise to the <em>GPT</em> family of models, or <em>Generative Pre-trained Transformers</em>, which form the bedrock of contemporary systems like <em>ChatGPT</em>. These models, constrained to process information unidirectionally by only observing preceding tokens, possess the unique capacity to generate novel text, a function not inherently present in <em>BERT</em>-like models. Consequently, <em>BERT</em> models primarily facilitate coherent sentence understanding, whilst <em>GPT</em> models specialise in text generation. Beyond these foundational distinctions, the field has developed more complex architectures, including combined <em>encoder-decoder</em> models and sophisticated methods for adapting <em>decoders</em> to perform functions traditionally associated with <em>encoders</em>, such as <em>XLNet</em>, upon which <em>XLM</em> builds.</p>
</section>
<section id="landscape-of-scientific-large-language-models" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="landscape-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Landscape of Scientific Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>The landscape of Large Language Models has evolved considerably, demonstrating a pronounced focus on applications within specific scientific domains. An overview reveals a more extensive proliferation of <em>Encoder</em>-based, <em>BERT</em>-type models compared to <em>Decoder</em>-based architectures in scientific contexts. Early and influential models in this area included <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>. Presently, researchers have developed a varied array of domain-specific models tailored for fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science, amongst others, reflecting the pervasive applicability of <em>LLMs</em> across scientific disciplines.</p>
</section>
<section id="domain-and-task-adaptation-strategies-for-llms" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="domain-and-task-adaptation-strategies-for-llms"><span class="header-section-number">3.6</span> Domain and Task Adaptation Strategies for LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific scientific language demands several sophisticated strategies. Initially, models undergo pre-training, where they learn language by predicting the next token, as seen in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. However, full pre-training necessitates prohibitive computational resources and expansive datasets. Consequently, continued pre-training offers a more accessible alternative, involving the further training of an already pre-trained model on domain-specific language; for instance, researchers might fine-tune a <em>BERT</em> model on physics texts.</p>
<p>Beyond pre-training, researchers employ additional adaptation techniques. They can append extra layers to pre-trained models and train them for specific classification tasks, such as sentiment analysis or named entity recognition. Prompt-based methods also effectively guide model outputs. Crucially, contrastive learning stands as a pivotal method for generating sentence or document embeddings, enabling the placement of entire documents or sentences within the same embedding space as individual words. <em>Sentence BERT</em>, a widely adopted approach, exemplifies this technique, which Irina Gurevich may further elaborate upon.</p>
<p>Furthermore, <em>Retrieval-Augmented Generation</em> (<em>RAG</em>) represents a significant pipeline for domain adaptation that does not necessitate explicit model re-training. This system integrates multiple models, typically employing a <em>BERT</em>-type model for scoring similarity between a user query and a document database. The <em>BERT</em> model encodes the query into a sentence embedding, facilitating the retrieval of the most relevant passages from the documents. Subsequently, this pipeline integrates the retrieved sentences into the prompt of a generative model, which then produces an informed answer based on this newly provided context. This <em>RAG</em> mechanism forms the bedrock of much contemporary <em>LLM</em> usage, including internet searches performed by systems like <em>ChatGPT</em>. It is vital to recognise that modern reasoning models and agents are not monolithic <em>LLMs</em> but rather complex systems comprising multiple <em>LLMs</em> integrated with various other tools.</p>
</section>
<section id="scientific-llms-timeline-and-survey-context" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="scientific-llms-timeline-and-survey-context"><span class="header-section-number">3.7</span> Scientific LLMs Timeline and Survey Context</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>A timeline comprehensively documents the rapid evolution of scientific Large Language Models from 2018 to 2024, categorising models according to their underlying architecture: ‘Others’, ‘<em>Encoder-Decoder</em>’, ‘<em>Decoders</em>’, and ‘<em>Encoders</em>’. Ho and colleagues detail this landscape in their 2024 survey, ‘<em>A Survey of Pre-trained Language Models for Processing Scientific Text</em>’, highlighting the predominance of <em>Encoder</em>-based models and distinguishing between open-source and closed-source implementations. This timeline provides crucial context for an ongoing survey investigating the utilisation of <em>LLMs</em> as research tools within the History, Philosophy, and Sociology of Science.</p>
</section>
<section id="key-distinctions-in-large-language-models" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="key-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Understanding Large Language Models demands an appreciation of several key distinctions. Architecturally, models exhibit significant variation, encompassing <em>encoder</em>-based, <em>decoder</em>-based, and combined <em>encoder-decoder</em> designs. Furthermore, researchers employ diverse fine-tuning strategies to adapt pre-trained models for specific tasks. A crucial conceptual difference lies between word embeddings and sentence embeddings, as these represent distinct levels of linguistic abstraction. Moreover, <em>LLMs</em> operate at varying levels of complexity, ranging from standalone models to intricate pipelines or advanced agents. Ultimately, no single model serves all purposes; rather, the selection of an appropriate model hinges entirely upon the specific task at hand.</p>
</section>
<section id="applications-of-llms-in-hpss-research" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="applications-of-llms-in-hpss-research"><span class="header-section-number">3.9</span> Applications of LLMs in HPSS Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>A recent survey delineates the diverse applications of Large Language Models within History, Philosophy, and Sociology of Science research into four primary domains.</p>
<p>Firstly, in dealing with data and sources, <em>LLMs</em> facilitate the parsing and extraction of salient information such as publication types, acknowledgements, and citations. They also enable more interactive engagement with source materials through summarisation and <em>Retrieval-Augmented Generation</em> (<em>RAG</em>)-type conversational interfaces.</p>
<p>Secondly, regarding knowledge structures, these models prove invaluable for entity extraction, identifying specific elements like scientific instruments, celestial bodies, and chemicals. They also assist in mapping complex relationships, including the demarcation of disciplines, interdisciplinary fields, and science-policy discourses.</p>
<p>Thirdly, in analysing knowledge dynamics, <em>LLMs</em> contribute to the study of conceptual histories, charting the evolution of terminology such as ‘theory’ in Digital Humanities or ‘virtual’ and ‘Planck’ in physics. Furthermore, they aid in novelty detection, identifying seminal papers and nascent technologies.</p>
<p>Finally, concerning knowledge practices, <em>LLMs</em> support argument reconstruction by identifying premises, conclusions, and causal links. They also enhance citation context analysis, discerning the purpose and affective tone behind citations—a traditional <em>HPSS</em> method now often employed for evaluatory purposes. Moreover, these models assist in discourse analysis, identifying hedging statements, specialised jargon, and instances of boundary work.</p>
<p>Evidently, interest in <em>LLMs</em> within <em>HPSS</em> is accelerating. Whilst findings predominantly appear in information science journals like <em>Scientometrics</em> and <em>Jasis</em>, a notable trend indicates increasing publication in traditionally non-computational journals. This shift underscores the models’ semantic capabilities, which are attracting qualitative researchers and philosophers alike.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>