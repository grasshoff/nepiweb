<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-06-21">

<title>3&nbsp; Large Language Models: Architecture, Adaptation, and Applications in HPSS – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models: Architecture, Adaptation, and Applications in HPSS</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models: Architecture, Adaptation, and Applications in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Validation is All You Need</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#presentation-agenda-and-core-topics" id="toc-presentation-agenda-and-core-topics" class="nav-link" data-scroll-target="#presentation-agenda-and-core-topics"><span class="header-section-number">3.1</span> Presentation Agenda and Core Topics</a></li>
  <li><a href="#the-foundational-transformer-architecture" id="toc-the-foundational-transformer-architecture" class="nav-link" data-scroll-target="#the-foundational-transformer-architecture"><span class="header-section-number">3.2</span> The Foundational Transformer Architecture</a></li>
  <li><a href="#encoder-based-models-bert-and-bidirectional-context" id="toc-encoder-based-models-bert-and-bidirectional-context" class="nav-link" data-scroll-target="#encoder-based-models-bert-and-bidirectional-context"><span class="header-section-number">3.3</span> Encoder-Based Models: BERT and Bidirectional Context</a></li>
  <li><a href="#berts-bidirectional-processing-and-hpss-applications" id="toc-berts-bidirectional-processing-and-hpss-applications" class="nav-link" data-scroll-target="#berts-bidirectional-processing-and-hpss-applications"><span class="header-section-number">3.4</span> BERT’s Bidirectional Processing and HPSS Applications</a></li>
  <li><a href="#generative-models-gpt-and-unidirectional-generation" id="toc-generative-models-gpt-and-unidirectional-generation" class="nav-link" data-scroll-target="#generative-models-gpt-and-unidirectional-generation"><span class="header-section-number">3.5</span> Generative Models: GPT and Unidirectional Generation</a></li>
  <li><a href="#architectural-distinctions-bert-vs.-gpt" id="toc-architectural-distinctions-bert-vs.-gpt" class="nav-link" data-scroll-target="#architectural-distinctions-bert-vs.-gpt"><span class="header-section-number">3.6</span> Architectural Distinctions: BERT vs.&nbsp;GPT</a></li>
  <li><a href="#evolution-of-scientific-llms-and-domain-adaptation" id="toc-evolution-of-scientific-llms-and-domain-adaptation" class="nav-link" data-scroll-target="#evolution-of-scientific-llms-and-domain-adaptation"><span class="header-section-number">3.7</span> Evolution of Scientific LLMs and Domain Adaptation</a></li>
  <li><a href="#domain-and-task-adaptation-strategies" id="toc-domain-and-task-adaptation-strategies" class="nav-link" data-scroll-target="#domain-and-task-adaptation-strategies"><span class="header-section-number">3.8</span> Domain and Task Adaptation Strategies</a></li>
  <li><a href="#retrieval-augmented-generation-rag-and-llm-systems" id="toc-retrieval-augmented-generation-rag-and-llm-systems" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-and-llm-systems"><span class="header-section-number">3.9</span> Retrieval-Augmented Generation (RAG) and LLM Systems</a></li>
  <li><a href="#survey-findings-on-llm-use-in-hpss-research" id="toc-survey-findings-on-llm-use-in-hpss-research" class="nav-link" data-scroll-target="#survey-findings-on-llm-use-in-hpss-research"><span class="header-section-number">3.10</span> Survey Findings on LLM Use in HPSS Research</a></li>
  <li><a href="#customisation-spectrum-and-recurring-challenges" id="toc-customisation-spectrum-and-recurring-challenges" class="nav-link" data-scroll-target="#customisation-spectrum-and-recurring-challenges"><span class="header-section-number">3.11</span> Customisation Spectrum and Recurring Challenges</a></li>
  <li><a href="#core-distinctions-in-language-model-design" id="toc-core-distinctions-in-language-model-design" class="nav-link" data-scroll-target="#core-distinctions-in-language-model-design"><span class="header-section-number">3.12</span> Core Distinctions in Language Model Design</a></li>
  <li><a href="#hpss-specific-challenges-and-future-directions-for-llms" id="toc-hpss-specific-challenges-and-future-directions-for-llms" class="nav-link" data-scroll-target="#hpss-specific-challenges-and-future-directions-for-llms"><span class="header-section-number">3.13</span> HPSS-Specific Challenges and Future Directions for LLMs</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models: Architecture, Adaptation, and Applications in HPSS</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter systematically explores the foundational architectures, adaptation strategies, and practical applications of Large Language Models (<em>LLMs</em>), with a particular emphasis on their utility within the History, Philosophy, and Sociology of Science (HPSS) domain. It commences with a primer on the <em>Transformer</em> architecture, detailing its encoder-decoder components and their evolution into distinct model types, such as <em>BERT</em> for understanding and <em>GPT</em> for generation. Subsequently, the discussion shifts to advanced adaptation techniques, including continued pre-training, fine-tuning, prompt-based methods, and contrastive learning, whilst introducing the Retrieval-Augmented Generation (<em>RAG</em>) pipeline as a sophisticated system for domain-specific information retrieval and synthesis. The chapter further outlines a survey of <em>LLM</em> applications in HPSS, categorising their use in data processing, knowledge structure analysis, dynamics, and practices. It concludes by addressing HPSS-specific challenges, such as historical language evolution and the need for critical interpretation, whilst advocating for enhanced <em>LLM</em> literacy, shared datasets, and a methodological fidelity to HPSS principles. Ultimately, this exploration highlights new opportunities for bridging qualitative and quantitative research paradigms.</p>
</section>
<section id="presentation-agenda-and-core-topics" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="presentation-agenda-and-core-topics"><span class="header-section-number">3.1</span> Presentation Agenda and Core Topics</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>This presentation outlines a structured exploration of Large Language Models (<em>LLMs</em>) and their relevance to academic inquiry. Initially, it provides a concise primer on <em>LLMs</em>, specifically addressing their adaptation for various scientific domains. Subsequently, the discourse shifts to summarising current applications within the interdisciplinary fields of History, Philosophy, and Sociology of Science (HPSS). Finally, the presentation offers critical reflections, intended to stimulate further discussion throughout the workshop.</p>
</section>
<section id="the-foundational-transformer-architecture" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-foundational-transformer-architecture"><span class="header-section-number">3.2</span> The Foundational Transformer Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The <em>Transformer</em> architecture constitutes the fundamental framework for all contemporary Large Language Models (<em>LLMs</em>). Initially conceived in 2017 for language translation tasks, such as converting German to English, this model comprises two interconnected streams: an Encoder and a Decoder. The Encoder, typically positioned on the left, processes input words by transforming them into numerical representations. Crucially, it reads the entire input sentence simultaneously, enabling each word to interact with every other word, thereby constructing a comprehensive contextual understanding of the complete input. Conversely, the Decoder, situated on the right, generates output words from these encoded numerical representations. Each word produced is subsequently fed back into the system, forming a sequential generation process. Distinctively, words within the Decoder can only attend to their predecessors, not to future tokens, as their primary function involves predicting the subsequent word. Within both the Encoder and Decoder, various layers progressively refine and contextualise the word embeddings, as previously described. Following the seminal paper, researchers promptly re-engineered these individual streams, leading to the development of pre-trained language models.</p>
</section>
<section id="encoder-based-models-bert-and-bidirectional-context" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="encoder-based-models-bert-and-bidirectional-context"><span class="header-section-number">3.3</span> Encoder-Based Models: BERT and Bidirectional Context</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>The field has transitioned from models designed solely for translation to those capable of profound language understanding and generation. These advanced models, with minimal or supplementary training, now serve a multitude of Natural Language Processing (<em>NLP</em>) tasks. Specifically, the Encoder component of the <em>Transformer</em> architecture evolved into models such as <em>BERT</em>, an acronym for Bidirectional Encoder Representations from <em>Transformers</em>. <em>BERT</em> models facilitate a ‘bidirectional full-context’ understanding; this implies that each word within the input stream can interact with every other word, thereby constructing a holistic comprehension of the entire input sequence concurrently. The term ‘bidirectional’ precisely denotes the capacity of these words to consider contextual information from both preceding and succeeding tokens.</p>
</section>
<section id="berts-bidirectional-processing-and-hpss-applications" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="berts-bidirectional-processing-and-hpss-applications"><span class="header-section-number">3.4</span> BERT’s Bidirectional Processing and HPSS Applications</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p><em>BERT</em>, an acronym for Bidirectional Encoder Representations from <em>Transformers</em>, operates as an encoder-based model. It achieves a ‘bidirectional full-context’ understanding by processing linguistic input, converting words into numerical representations, and subsequently transforming these back into words. A defining characteristic of <em>BERT</em> lies in its capacity to comprehend context from both preceding and succeeding words within a sequence. The presentation explicitly raises the question of ‘<em>LLMs</em> for HPSS?’, thereby highlighting the prospective utility of Large Language Models, including <em>BERT</em>, within the domains of History, Philosophy, and Sociology of Science. This architectural innovation traces its foundational work to Devlin et al., published in 2018.</p>
</section>
<section id="generative-models-gpt-and-unidirectional-generation" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="generative-models-gpt-and-unidirectional-generation"><span class="header-section-number">3.5</span> Generative Models: GPT and Unidirectional Generation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Distinct from <em>BERT</em> models, Generative Pre-trained <em>Transformers</em> (<em>GPT</em>) operate as decoder-based architectures, inherently constrained to processing only preceding tokens within a sequence. Crucially, <em>GPT</em> models possess the capacity to generate novel text, a functionality not intrinsically present in <em>BERT</em> models. These generative capabilities underpin widely adopted applications, including <em>ChatGPT</em>. Beyond these two primary architectural paradigms, the field encompasses models that integrate both encoder and decoder components, whilst others employ sophisticated decoder configurations to emulate encoder-like functionalities, exemplified by <em>XLM</em>, which builds upon <em>XLNet</em>. Fundamentally, the core distinction resides between generative models, primarily focused on language production, and full-context models, which excel at coherent sentence understanding.</p>
</section>
<section id="architectural-distinctions-bert-vs.-gpt" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="architectural-distinctions-bert-vs.-gpt"><span class="header-section-number">3.6</span> Architectural Distinctions: BERT vs.&nbsp;GPT</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p><em>BERT</em> models, operating as encoder-only architectures, excel at bidirectional full-context understanding. Conversely, <em>GPT</em> models, which are decoder-only, specialise in unidirectional generative tasks. Both architectural paradigms originate from the seminal <em>Transformer</em> model. The presentation implicitly raises questions regarding their specific applicability and utility within the History, Philosophy, and Sociology of Science (HPSS) domain. The foundational research for <em>BERT</em> is credited to Devlin et al.&nbsp;(2018), whilst Radford et al.&nbsp;(2018) are recognised for their pioneering work on <em>GPT</em>.</p>
</section>
<section id="evolution-of-scientific-llms-and-domain-adaptation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="evolution-of-scientific-llms-and-domain-adaptation"><span class="header-section-number">3.7</span> Evolution of Scientific LLMs and Domain Adaptation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>The contemporary landscape of Large Language Models (<em>LLMs</em>) tailored for scientific domains demonstrates considerable diversity and continuous evolution. Notably, encoder-type models, akin to <em>BERT</em>, exhibit a higher prevalence compared to decoder models, underscoring the significant research and development activity focused on the encoder side for scientific text comprehension. Early influential models in this domain include <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>. Presently, specialised <em>LLMs</em> cater to a broad spectrum of scientific disciplines, encompassing biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. This comprehensive overview draws upon the survey conducted by Ho et al.&nbsp;(2024), which meticulously documents pre-trained language models designed for processing scientific texts.</p>
</section>
<section id="domain-and-task-adaptation-strategies" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="domain-and-task-adaptation-strategies"><span class="header-section-number">3.8</span> Domain and Task Adaptation Strategies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>Adapting Large Language Models (<em>LLMs</em>) to specific scientific language necessitates the employment of several distinct strategies. Initially, pre-training involves the model learning language by either predicting the subsequent token, as observed in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This foundational process, however, demands prohibitive computational resources and vast datasets, rendering it largely unfeasible for individual research efforts. Consequently, continued pre-training offers a more accessible alternative, wherein an existing pre-trained model undergoes further training on domain-specific language; for instance, a <em>BERT</em> model might be refined using physics-related texts, a methodology previously implemented by Micha and the presenter. Furthermore, fine-tuning entails augmenting pre-trained models with additional layers, which are then trained for specific classification tasks, such as sentiment analysis or named entity recognition. Prompt-based methods offer another avenue, framing tasks as natural language queries. Crucially, contrastive learning emerges as a pivotal technique for generating robust sentence or document embeddings, thereby enabling the placement of entire documents or sentences within the same semantic embedding space as individual words. <em>Sentence BERT</em> stands as a widely adopted example of this approach, with Irina Gurevich anticipated to elaborate on it. The slide visually reinforces these methods, depicting:</p>
<ul>
<li><p>Pre-training with vocabulary adaptation for HPSS</p></li>
<li><p>The addition of extra parameters for sentiment and Named Entity Recognition (<em>NER</em>)</p></li>
<li><p>Prompt-based masked language modelling and definition elicitation</p></li>
<li><p>Contrastive learning through similarity scoring via pooling</p></li>
</ul>
</section>
<section id="retrieval-augmented-generation-rag-and-llm-systems" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-and-llm-systems"><span class="header-section-number">3.9</span> Retrieval-Augmented Generation (RAG) and LLM Systems</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>Retrieval-Augmented Generation (<em>RAG</em>) represents a sophisticated pipeline that orchestrates multiple models in concert. This framework commonly underpins applications such as <em>ChatGPT</em>, which leverages internet searches to furnish comprehensive results. <em>RAG</em> facilitates the adaptation of models to specific domains without necessitating extensive re-training. The process commences with a <em>BERT</em>-type model, which assesses the similarity between a user’s query—for instance, ‘What are <em>LLMs</em>?’—and documents within a designated database, achieved by encoding the query into a sentence embedding. Subsequently, the system retrieves the most pertinent passages from these documents. These retrieved sentences are then seamlessly integrated into the prompt presented to a generative model. Consequently, the generative model formulates an answer, drawing upon this newly augmented context. Crucially, contemporary reasoning models or agents do not operate as singular Large Language Models; rather, they function as intricate systems, combining <em>LLMs</em> with a diverse array of other tools. Fundamental distinctions to bear in mind encompass:</p>
<ul>
<li><p>The varying architectures—encoder, decoder, and encoder-decoder</p></li>
<li><p>The diverse fine-tuning strategies employed</p></li>
<li><p>The critical difference between word and sentence embeddings</p></li>
<li><p>The distinct levels of abstraction, ranging from standalone <em>LLMs</em> to complex pipelines and intelligent agents</p></li>
</ul>
</section>
<section id="survey-findings-on-llm-use-in-hpss-research" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="survey-findings-on-llm-use-in-hpss-research"><span class="header-section-number">3.10</span> Survey Findings on LLM Use in HPSS Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>A comprehensive survey is presently being conducted to evaluate the utilisation of Large Language Models (<em>LLMs</em>) as instrumental tools within History, Philosophy, and Sociology of Science (HPSS) research. This investigation has delineated four principal categories, or ‘bins’, for classifying <em>LLM</em> applications in the HPSS domain:</p>
<ul>
<li><p><em>Dealing with data and sources</em>: This encompasses methods for parsing and extracting specific elements, such as publication types, acknowledgements, and citations, alongside interactive engagements with sources through summarisation and Retrieval-Augmented Generation (<em>RAG</em>)-type chatting.</p></li>
<li><p><em>Knowledge structures</em>: This addresses the identification of entities, including scientific instruments, celestial bodies, and chemical compounds, whilst also facilitating the mapping of intricate relationships between disciplines, interdisciplinary fields, and science policy discourses.</p></li>
<li><p><em>Knowledge dynamics</em>: This delves into the evolution of concepts, exemplified by tracing the conceptual histories of terms like ‘theory’ in Digital Humanities or ‘virtual’ and ‘Planck’ in physics, concurrently with the detection of novelty, such as breakthrough papers and emerging technologies.</p></li>
<li><p><em>Knowledge practices</em>: This examines analytical aspects of scholarly communication, including argument reconstruction to identify premises and causal relationships, citation context analysis to ascertain the purpose and sentiment of citations, and discourse analysis to uncover features like hedge sentences, disciplinary jargon, and boundary work.</p></li>
</ul>
<p>A discernible trend indicates an accelerating interest in <em>LLMs</em>, with research findings primarily appearing in information science journals such as <em>Scientometrics</em> and <em>Jasis</em>. Notably, an increasing number of papers are now being published in journals traditionally not amenable to computational methods, signalling a burgeoning interest from qualitative researchers and philosophers, largely attributable to the profound semantic capabilities of these models.</p>
</section>
<section id="customisation-spectrum-and-recurring-challenges" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="customisation-spectrum-and-recurring-challenges"><span class="header-section-number">3.11</span> Customisation Spectrum and Recurring Challenges</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>The customisation of Large Language Models (<em>LLMs</em>) spans a broad spectrum, ranging from the straightforward application of off-the-shelf tools like <em>ChatGPT</em> to the intricate development of entirely novel architectures. Nevertheless, several recurring concerns persistently challenge <em>LLM</em> implementation. These include the demanding requirements for computational resources, the inherent opaqueness of model operations, a pervasive scarcity of adequate training data, and the notable absence of standardised benchmarks. Furthermore, researchers consistently encounter fundamental trade-offs between distinct model types, such as <em>BERT</em>-like models optimised for understanding versus <em>GPT</em>-like models geared towards generation. Crucially, no singular model proves universally applicable; instead, the selection of an appropriate model remains contingent upon the specific research objective. Despite these challenges, a discernible trend towards enhanced accessibility is emerging, exemplified by tools like <em>BERTTopic</em> for topic modelling, which benefits from robust maintenance and user-friendly design.</p>
</section>
<section id="core-distinctions-in-language-model-design" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="core-distinctions-in-language-model-design"><span class="header-section-number">3.12</span> Core Distinctions in Language Model Design</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Understanding Large Language Models (<em>LLMs</em>) necessitates grasping several core distinctions in their design and application:</p>
<ul>
<li><p>Firstly, models are categorised by their fundamental architecture and pre-training paradigm, encompassing encoder-based designs, exemplified by <em>BERT</em> for understanding tasks; decoder-based architectures, such as <em>GPT</em> for generative purposes; and integrated encoder-decoder models, like <em>T5</em>, which excel in sequence-to-sequence operations.</p></li>
<li><p>Secondly, fine-tuning involves employing diverse strategies to adapt these pre-trained models to specific downstream tasks, ranging from comprehensive fine-tuning to more parameter-efficient methods.</p></li>
<li><p>Thirdly, a critical differentiation exists between word embeddings, which represent individual lexical units, and sentence embeddings, which capture the semantic essence of entire sentences or longer textual segments, thus operating at distinct granularities.</p></li>
<li><p>Finally, the level of abstraction defines how <em>LLMs</em> integrate into broader computational systems, from functioning as standalone models to serving as components within multi-step pipelines or acting as intelligent agents capable of complex reasoning, planning, and interaction with external tools.</p></li>
</ul>
</section>
<section id="hpss-specific-challenges-and-future-directions-for-llms" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="hpss-specific-challenges-and-future-directions-for-llms"><span class="header-section-number">3.13</span> HPSS-Specific Challenges and Future Directions for LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>Research within the History, Philosophy, and Sociology of Science (HPSS) presents distinct challenges for the application of Large Language Models (<em>LLMs</em>). Foremost amongst these is the historical evolution of concepts and language; <em>LLMs</em>, predominantly trained on contemporary linguistic data, risk introducing biases and misinterpretations when applied to historical texts. Furthermore, HPSS scholars adopt a reconstructive and critically reflective perspective, interpreting scientific texts beyond their literal meaning to uncover situated contexts, authorial intentions, and subtle discursive strategies, such as boundary work—a nuanced interpretative capacity for which <em>LLMs</em> are not inherently trained. Practical data characteristics, including sparsity, multilingualism, and the presence of old scripts, also pose considerable impediments. In response to these challenges, several recommendations emerge for HPSS engagement with <em>LLMs</em>:</p>
<ul>
<li><p>Researchers should cultivate <em>LLM</em> literacy, encompassing both a theoretical understanding and practical awareness of these tools’ implications, potentially involving the acquisition of coding skills, although Natural Language Processing (<em>NLP</em>)-based coding may mitigate this necessity.</p></li>
<li><p>Developing shared datasets and standardised benchmarks is crucial for rigorous evaluation and advancement.</p></li>
<li><p>Moreover, it remains imperative to maintain HPSS methodological fidelity, ensuring that the translation of HPSS problems into <em>NLP</em> tasks—such as classification, generation, or summarisation—does not inadvertently eclipse the core HPSS objectives.</p></li>
</ul>
<p>Despite these hurdles, <em>LLM</em> integration presents novel opportunities. Notably, it offers a compelling avenue for bridging qualitative and quantitative research approaches. Additionally, it prompts a valuable reflection on HPSS’s own historical trajectory, drawing parallels with earlier computational methodologies like co-word analysis, pioneered by Callon and Rip in the 1980s, which were deeply rooted in Actor-Network Theory.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy, and Sociology of Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>