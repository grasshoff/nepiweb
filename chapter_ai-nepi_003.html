<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-01-01">

<title>Large Language Models for the History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals, ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction-and-chapter-structure" id="toc-introduction-and-chapter-structure" class="nav-link" data-scroll-target="#introduction-and-chapter-structure"><span class="header-section-number">3.1</span> Introduction and Chapter Structure</a></li>
  <li><a href="#the-transformer-architecture-foundations-of-modern-llms" id="toc-the-transformer-architecture-foundations-of-modern-llms" class="nav-link" data-scroll-target="#the-transformer-architecture-foundations-of-modern-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of Modern LLMs</a></li>
  <li><a href="#encoder-based-models-the-bert-family" id="toc-encoder-based-models-the-bert-family" class="nav-link" data-scroll-target="#encoder-based-models-the-bert-family"><span class="header-section-number">3.3</span> Encoder-Based Models: The BERT Family</a></li>
  <li><a href="#decoder-based-models-the-gpt-family" id="toc-decoder-based-models-the-gpt-family" class="nav-link" data-scroll-target="#decoder-based-models-the-gpt-family"><span class="header-section-number">3.4</span> Decoder-Based Models: The GPT Family</a></li>
  <li><a href="#evolution-and-landscape-of-scientific-large-language-models" id="toc-evolution-and-landscape-of-scientific-large-language-models" class="nav-link" data-scroll-target="#evolution-and-landscape-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution and Landscape of Scientific Large Language Models</a></li>
  <li><a href="#domain-and-task-adaptation-strategies-for-llms" id="toc-domain-and-task-adaptation-strategies-for-llms" class="nav-link" data-scroll-target="#domain-and-task-adaptation-strategies-for-llms"><span class="header-section-number">3.6</span> Domain and Task Adaptation Strategies for LLMs</a></li>
  <li><a href="#retrieval-augmented-generation-rag-for-domain-adaptation" id="toc-retrieval-augmented-generation-rag-for-domain-adaptation" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval Augmented Generation (RAG) for Domain Adaptation</a></li>
  <li><a href="#fundamental-distinctions-in-large-language-models" id="toc-fundamental-distinctions-in-large-language-models" class="nav-link" data-scroll-target="#fundamental-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Fundamental Distinctions in Large Language Models</a></li>
  <li><a href="#applications-of-large-language-models-in-hpss-research" id="toc-applications-of-large-language-models-in-hpss-research" class="nav-link" data-scroll-target="#applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in HPSS Research</a></li>
  <li><a href="#trends-and-concerns-in-llm-adoption-for-hpss" id="toc-trends-and-concerns-in-llm-adoption-for-hpss" class="nav-link" data-scroll-target="#trends-and-concerns-in-llm-adoption-for-hpss"><span class="header-section-number">3.10</span> Trends and Concerns in LLM Adoption for HPSS</a></li>
  <li><a href="#critical-reflections-on-llms-in-hpss-research" id="toc-critical-reflections-on-llms-in-hpss-research" class="nav-link" data-scroll-target="#critical-reflections-on-llms-in-hpss-research"><span class="header-section-number">3.11</span> Critical Reflections on LLMs in HPSS Research</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Large Language Models for the History, Philosophy, and Sociology of Science</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter explores the application of Large Language Models (<em>LLMs</em>) within the History, Philosophy, and Sociology of Science (<em>HPSS</em>) domain. It commences with a foundational primer on <em>LLM</em> architectures, detailing the seminal <em>Transformer</em> model, its encoder-decoder structure, and the subsequent development of <em>BERT</em>-like (encoder-based) and <em>GPT</em>-like (decoder-based) models. The discussion then traces the evolution of scientific <em>LLMs</em>, categorising them by architectural type and highlighting prominent examples such as <em>BioBERT</em> and <em>GPT2</em>.</p>
<p>The chapter progresses to various domain and task adaptation strategies, including pre-training, continued pre-training, fine-tuning with extra parameters, and Retrieval Augmented Generation (<em>RAG</em>) pipelines. Crucially, it identifies key distinctions in <em>LLM</em> understanding, emphasising the differences between word and sentence embeddings and the varying levels of abstraction from individual <em>LLMs</em> to complex agentic systems.</p>
<p>Furthermore, the chapter systematically categorises current and potential <em>HPSS</em> applications into four areas: dealing with data and sources, analysing knowledge structures, understanding knowledge dynamics, and examining knowledge practices. The analysis reveals an accelerating interest in <em>LLMs</em>, even within traditionally non-computational journals, alongside a spectrum of customisation from off-the-shelf use to novel architectural development. Persistent concerns include computational resource demands, model opaqueness, and data scarcity.</p>
<p>Finally, the chapter offers critical reflections, acknowledging <em>HPSS</em>-specific challenges such as the historical evolution of concepts, the need for reconstructive reading, and issues with sparse or multi-lingual historical data. It advocates for building <em>LLM</em> literacy amongst <em>HPSS</em> scholars, fostering the development of shared datasets and benchmarks, and maintaining methodological integrity whilst embracing new opportunities for bridging qualitative and quantitative research.</p>
</section>
<section id="introduction-and-chapter-structure" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction-and-chapter-structure"><span class="header-section-number">3.1</span> Introduction and Chapter Structure</h2>
<p>This chapter aims to furnish readers with a concise primer on Large Language Models, particularly focusing on their adaptation to specialised scientific domains. Subsequently, it summarises current applications of these models within the History, Philosophy, and Sociology of Science. Finally, the chapter offers several critical reflections, intended to stimulate further discussion and research within the <em>HPSS</em> community.</p>
</section>
<section id="the-transformer-architecture-foundations-of-modern-llms" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture-foundations-of-modern-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundations of Modern LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The <em>Transformer</em> architecture constitutes the fundamental framework underpinning most contemporary Large Language Models. Vaswani and colleagues originally designed this pivotal model in 2017 to facilitate language translation, for instance, from German to English (Vaswani et al., 2017).</p>
<p>The architecture comprises two interconnected streams: an encoder on the left and a decoder on the right. The encoder processes the entire input sentence concurrently, allowing each word to interact with every other word. This mechanism thereby constructs a comprehensive contextual representation of the complete sentence meaning.</p>
<p>Conversely, the decoder generates output words sequentially; it can only access preceding words, preventing it from “looking into the future” whilst predicting the subsequent word. Within these streams, various layers perform complex numerical computations, progressively contextualising word embeddings. Vaswani <em>et al.</em> introduced this architecture in their seminal 2017 paper, “Attention is All You Need.”</p>
</section>
<section id="encoder-based-models-the-bert-family" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="encoder-based-models-the-bert-family"><span class="header-section-number">3.3</span> Encoder-Based Models: The BERT Family</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Immediately following the <em>Transformer</em>’s introduction, Devlin and colleagues began re-engineering its individual streams to develop pre-trained language models. This marked a significant shift from direct translation towards models capable of profound language understanding. The encoder stream, in particular, formed the basis for the <em>BERT</em> family of models, which remain highly prevalent (Devlin et al., 2018).</p>
<p><em>BERT</em>, or Bidirectional Encoder Representations from Transformers, operates by allowing each word within the input stream to interact with every other word. This mechanism facilitates a comprehensive, full-context understanding of the entire input at once. Subsequently, researchers can apply these models to various Natural Language Processing tasks, often requiring only minor or additional training. Devlin <em>et al.</em> introduced the <em>BERT</em> model in 2018.</p>
</section>
<section id="decoder-based-models-the-gpt-family" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="decoder-based-models-the-gpt-family"><span class="header-section-number">3.4</span> Decoder-Based Models: The GPT Family</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Conversely, the <em>Transformer</em>’s decoder stream gave rise to the <em>GPT</em> models, standing for Generative Pre-trained Transformers. These models, which power contemporary systems like <em>ChatGPT</em>, exhibit a distinct unidirectional structure, allowing words to consider only their predecessors. This architectural choice grants them the crucial ability to generate novel text, a function not intrinsically performed by <em>BERT</em> models (Radford et al., 2018).</p>
<p>Consequently, <em>GPT</em> models and <em>BERT</em> models serve different primary purposes: <em>GPT</em> excels at language generation, whilst <em>BERT</em> prioritises coherent sentence understanding. Beyond these two archetypes, other architectures have emerged, including models that combine encoder-decoders and sophisticated methods that enable decoders to function more akin to encoders, exemplified by models such as <em>XLM</em> and <em>XLNet</em>. Radford <em>et al.</em> introduced the <em>GPT</em> model in 2018.</p>
</section>
<section id="evolution-and-landscape-of-scientific-large-language-models" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evolution-and-landscape-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution and Landscape of Scientific Large Language Models</h2>
<p>The landscape of Large Language Models specifically adapted for scientific domains and tasks reveals considerable diversity. Ho and colleagues categorise these models primarily by their underlying architecture (Ho et al., 2024). Encoder-based models constitute the most populated category, encompassing prominent examples such as <em>BioBERT</em>, <em>SciBERT</em>, <em>ChemBERT</em>, and <em>CyberBERT</em>. Decoder models, whilst less numerous, include <em>GPT2</em> and <em>OPT2</em>. Encoder-Decoder architectures feature models like <em>T5</em> and <em>BART</em>, whilst other notable models include <em>FLAIR</em> and <em>ClinicalBERT</em>. Significantly, the majority of these scientific <em>LLMs</em> operate as open-source projects. Ho <em>et al.</em>’s 2024 survey, “A Survey of Pre-trained Language Models for Processing Scientific Text,” provides a comprehensive overview of this evolving field.</p>
</section>
<section id="domain-and-task-adaptation-strategies-for-llms" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="domain-and-task-adaptation-strategies-for-llms"><span class="header-section-number">3.6</span> Domain and Task Adaptation Strategies for LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific domains and tasks involves several distinct training strategies. Pre-training represents the initial learning phase, where models acquire language understanding by predicting the next token, as seen in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This foundational process, however, demands immense computational resources and vast datasets.</p>
<p>A more accessible approach involves continued pre-training. Here, researchers take an already pre-trained model and further train it on domain-specific language; for instance, a <em>BERT</em> model might undergo additional training on physics texts. Alternatively, fine-tuning with extra parameters involves appending additional layers atop pre-trained models. Researchers then train these augmented models for specific Natural Language Processing tasks, such as classifying sentiment or recognising named entities.</p>
<p>Whilst briefly mentioned, prompt-based approaches also offer a means of adaptation. Crucially, contrastive learning stands as a pivotal method for generating sentence or document embeddings. This technique enables the placement of longer textual units into the same embedding space as individual words, a capability exemplified by <em>Sentence BERT</em>. Ultimately, understanding the fundamental distinction between word embeddings and sentence or document embeddings proves vital, as they represent entirely different levels of abstraction.</p>
</section>
<section id="retrieval-augmented-generation-rag-for-domain-adaptation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval Augmented Generation (RAG) for Domain Adaptation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>Retrieval Augmented Generation, or <em>RAG</em>, offers a sophisticated pipeline system for adapting models to specific domains without necessitating extensive model re-training. This architecture orchestrates multiple models to operate in concert. The retrieval component typically employs <em>BERT</em>-type models, which encode user queries, such as “What are <em>LLMs</em>?”, into precise sentence embeddings. These query embeddings then undergo comparison with document embeddings, derived from sources like expert interviews, language models, or other relevant documents stored within a database. The system subsequently retrieves the most similar or relevant passages.</p>
<p>The generation component then seamlessly integrates these retrieved sentences or documents into the prompt of a generative model. This generative model, now equipped with enriched context, proceeds to produce an informed answer. Beyond <em>RAG</em>, it is crucial to recognise that contemporary reasoning models and agents represent complex systems, intricately combining Large Language Models with a diverse array of other tools, thereby extending their capabilities far beyond the functionalities of a single <em>LLM</em>.</p>
</section>
<section id="fundamental-distinctions-in-large-language-models" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="fundamental-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Fundamental Distinctions in Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Understanding Large Language Models necessitates grasping several fundamental distinctions:</p>
<ul>
<li>Models vary significantly in their underlying architecture and pre-training methodology, broadly categorised as Encoder-based, Decoder-based, or Encoder-Decoder-based.</li>
<li>Researchers employ diverse fine-tuning strategies to adapt these models for specific tasks.</li>
<li>A crucial differentiation exists between word embeddings and sentence embeddings, as they represent distinct levels of textual representation.</li>
<li>The level of abstraction varies considerably, ranging from individual Large Language Models to intricate pipelines and highly sophisticated agents, each offering unique capabilities and complexities.</li>
</ul>
</section>
<section id="applications-of-large-language-models-in-hpss-research" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in HPSS Research</h2>
<p>A recent survey on Large Language Model usage in History, Philosophy, and Sociology of Science research identifies four primary categories of application (Simons, 2025).</p>
<p>Firstly, scholars employ <em>LLMs</em> for effectively dealing with data and sources. This involves parsing and extracting specific information, such as publication types, acknowledgements, and citations. Furthermore, researchers interact with sources through summarisation and <em>RAG</em>-type chatting, enhancing data accessibility.</p>
<p>Secondly, <em>LLMs</em> facilitate the analysis of knowledge structures. This includes the precise extraction of entities like scientific instruments, celestial bodies, and chemicals. Moreover, these models enable the mapping of complex relationships, such as those between disciplines, interdisciplinary fields, and science-policy discourses.</p>
<p>Thirdly, the models contribute to understanding knowledge dynamics. Researchers utilise them for conceptual histories, tracing the evolution of terms like “theory” within Digital Humanities, or “virtual” and “Planck” in physics. They also aid in novelty detection, pinpointing breakthrough papers and emerging technologies.</p>
<p>Finally, <em>LLMs</em> offer new avenues for studying knowledge practices. This encompasses argument reconstruction, where models extract premises, conclusions, and causal links. They also revitalise citation context analysis, a traditional <em>HPSS</em> method, by determining the purpose and sentiment of citations. Additionally, <em>LLMs</em> support discourse analysis, identifying nuanced linguistic features such as hedge sentences, jargon, and instances of boundary work.</p>
</section>
<section id="trends-and-concerns-in-llm-adoption-for-hpss" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="trends-and-concerns-in-llm-adoption-for-hpss"><span class="header-section-number">3.10</span> Trends and Concerns in LLM Adoption for HPSS</h2>
<p>The adoption of Large Language Models within History, Philosophy, and Sociology of Science research reveals several discernible trends and persistent concerns. We observe an accelerating interest in <em>LLMs</em>, extending even to traditionally non-computational journals, a phenomenon largely attributable to the models’ powerful semantic processing capabilities. Furthermore, scholars exhibit varying degrees of customisation in their <em>LLM</em> engagement, ranging from the straightforward, off-the-shelf application of <em>ChatGPT</em> to the development of entirely new architectures, alongside custom pre-training and fine-tuning.</p>
<p>Despite this burgeoning interest, several concerns persist. These include the overwhelming computational resources demanded by <em>LLMs</em>, the inherent opaqueness of their internal operations, a pervasive lack of sufficient training data, and the absence of standardised benchmarks for evaluation. Moreover, researchers frequently grapple with the trade-offs between different model types, such as <em>BERT</em>-like versus <em>GPT</em>-like architectures. Nevertheless, a positive trend towards increased accessibility is evident, with tools like <em>BERTopic</em> exemplifying the growing ease of use for complex tasks such as topic modelling.</p>
</section>
<section id="critical-reflections-on-llms-in-hpss-research" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="critical-reflections-on-llms-in-hpss-research"><span class="header-section-number">3.11</span> Critical Reflections on LLMs in HPSS Research</h2>
<p>Critical engagement with Large Language Models in <em>HPSS</em> research necessitates acknowledging several domain-specific challenges. Firstly, the historical evolution of concepts and language presents a significant hurdle; <em>LLMs</em>, typically trained on modern linguistic corpora, may introduce biases or misinterpret historical nuances. Furthermore, <em>HPSS</em> scholarship often adopts a reconstructive perspective, demanding the ability to “read between the lines,” comprehend situated contexts, and discern subtle discursive strategies, such as boundary work—capabilities <em>LLMs</em> are not inherently trained to perform. Data limitations also persist, including sparse datasets, the prevalence of multiple languages and old scripts, and a general lack of digitalisation for historical sources.</p>
<p>Addressing these challenges requires building robust <em>LLM</em> literacy within the <em>HPSS</em> community. This involves familiarising oneself with <em>LLMs</em>, Natural Language Processing, and Deep Learning, encompassing both the practical tools and their underlying theoretical frameworks. Scholars must also cultivate the discernment to select the most appropriate architecture and training regimen for their specific research problems. Crucially, the <em>HPSS</em> community should collaboratively develop shared datasets and benchmarks tailored to its unique needs.</p>
<p>Whilst embracing these technological advancements, researchers must remain steadfastly true to <em>HPSS</em> methodologies. This entails translating complex <em>HPSS</em> problems into tractable <em>NLP</em> tasks without compromising the core methodological focus or allowing the technical tasks to inadvertently hijack the research purpose. Nevertheless, these models present novel opportunities for bridging qualitative and quantitative research approaches. Moreover, the <em>HPSS</em> community should reflect upon its own pre-history in developing <em>LLM</em>-like tools, such as the co-word analysis pioneered by Callon and Rip in the 1980s (Callon &amp; Rip, 1986), which emerged from an Actor-Network Theory theoretical mindset.</p>
<p>In conclusion, Large Language Models offer transformative potential for <em>HPSS</em> research, enabling novel approaches to data analysis, knowledge structure mapping, and the study of knowledge dynamics and practices. By fostering <em>LLM</em> literacy, developing tailored datasets, and maintaining methodological rigour, the <em>HPSS</em> community can effectively harness these powerful tools to advance its scholarship and bridge traditional disciplinary divides.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>