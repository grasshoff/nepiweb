<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-06-21">

<title>3&nbsp; Large Language Models in History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-c2d8198b7f72dec16de60f0cb3fab69f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a0afd4a9b901cc50d8ed64d4ec5e2aec.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">NEPI Workshop Content Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Investigating Transdisciplinary Applications with OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Validation is All You Need</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study with the Stanford Encyclopedia of Philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Comparative Study of LDA and BERTopic Performance Across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling context for the analysis of language variation and change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#agenda-and-scope" id="toc-agenda-and-scope" class="nav-link" data-scroll-target="#agenda-and-scope"><span class="header-section-number">3.1</span> Agenda and Scope</a></li>
  <li><a href="#the-transformer-architecture" id="toc-the-transformer-architecture" class="nav-link" data-scroll-target="#the-transformer-architecture"><span class="header-section-number">3.2</span> The <em>Transformer</em> Architecture</a></li>
  <li><a href="#encoder-only-models" id="toc-encoder-only-models" class="nav-link" data-scroll-target="#encoder-only-models"><span class="header-section-number">3.3</span> Encoder-Only Models</a></li>
  <li><a href="#bert-bidirectional-understanding" id="toc-bert-bidirectional-understanding" class="nav-link" data-scroll-target="#bert-bidirectional-understanding"><span class="header-section-number">3.4</span> <em>BERT</em>: Bidirectional Understanding</a></li>
  <li><a href="#gpt-unidirectional-generation" id="toc-gpt-unidirectional-generation" class="nav-link" data-scroll-target="#gpt-unidirectional-generation"><span class="header-section-number">3.5</span> <em>GPT</em>: Unidirectional Generation</a></li>
  <li><a href="#the-evolution-of-scientific-llms" id="toc-the-evolution-of-scientific-llms" class="nav-link" data-scroll-target="#the-evolution-of-scientific-llms"><span class="header-section-number">3.6</span> The Evolution of Scientific LLMs</a></li>
  <li><a href="#domain-and-task-adaptation" id="toc-domain-and-task-adaptation" class="nav-link" data-scroll-target="#domain-and-task-adaptation"><span class="header-section-number">3.7</span> Domain and Task Adaptation</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag"><span class="header-section-number">3.8</span> Retrieval-Augmented Generation (<em>RAG</em>)</a></li>
  <li><a href="#a-timeline-of-scientific-llms" id="toc-a-timeline-of-scientific-llms" class="nav-link" data-scroll-target="#a-timeline-of-scientific-llms"><span class="header-section-number">3.9</span> A Timeline of Scientific LLMs</a></li>
  <li><a href="#adaptation-approaches-in-detail" id="toc-adaptation-approaches-in-detail" class="nav-link" data-scroll-target="#adaptation-approaches-in-detail"><span class="header-section-number">3.10</span> Adaptation Approaches in Detail</a></li>
  <li><a href="#llm-applications-in-hpss" id="toc-llm-applications-in-hpss" class="nav-link" data-scroll-target="#llm-applications-in-hpss"><span class="header-section-number">3.11</span> LLM Applications in HPSS</a></li>
  <li><a href="#key-distinctions-and-challenges" id="toc-key-distinctions-and-challenges" class="nav-link" data-scroll-target="#key-distinctions-and-challenges"><span class="header-section-number">3.12</span> Key Distinctions and Challenges</a></li>
  <li><a href="#hpss-challenges-and-future-directions" id="toc-hpss-challenges-and-future-directions" class="nav-link" data-scroll-target="#hpss-challenges-and-future-directions"><span class="header-section-number">3.13</span> HPSS Challenges and Future Directions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter provides a comprehensive primer on Large Language Models (LLMs), charting their foundational architectures, adaptation for scientific domains, and specific applications within the History, Philosophy, and Sociology of Science (HPSS). It begins by introducing the <em>Transformer</em>, a core neural network architecture that underpins modern LLMs. The discussion then differentiates between encoder-based models like <em>BERT</em>, which excel at bidirectional contextual understanding, and decoder-based models such as <em>GPT</em>, renowned for their generative capabilities.</p>
<p>The chapter traces the rapid evolution of scientific LLMs, highlighting a proliferation of domain-specific models since 2018. It meticulously outlines key adaptation methods, including pre-training, continued pre-training, and prompt-based techniques. A significant focus is placed on Retrieval-Augmented Generation (<em>RAG</em>), a multi-model pipeline that enhances LLM responses by integrating retrieved information.</p>
<p>Furthermore, the analysis categorises LLM applications in HPSS into four distinct areas: managing data and sources, analysing knowledge structures, understanding knowledge dynamics, and examining knowledge practices. Finally, the chapter addresses HPSS-specific challenges, such as the historical evolution of language and the need for critical, reconstructive reading. It advocates for greater LLM literacy and shared datasets, whilst championing the integration of computational methods without compromising core HPSS methodologies.</p>
</section>
<section id="agenda-and-scope" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="agenda-and-scope"><span class="header-section-number">3.1</span> Agenda and Scope</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>This chapter outlines an agenda to deliver a foundational primer on Large Language Models and their adaptation to scientific domains. It further intends to summarise current applications within the History, Philosophy, and Sociology of Science (HPSS) and to share critical reflections for wider discussion. Recognising a heterogeneous audience, the text prioritises accessibility whilst delivering necessary technical insights.</p>
</section>
<section id="the-transformer-architecture" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture"><span class="header-section-number">3.2</span> The <em>Transformer</em> Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>The foundational <em>Transformer</em> architecture, which underpins all contemporary Large Language Models, was introduced by Vaswani and colleagues in their 2017 paper, <em>Attention is all you need</em>. Initially conceived for language translation, this model operates through two interconnected streams: an encoder and a decoder. The encoder processes an entire input sentence simultaneously, allowing each word to interact bidirectionally with every other, thereby constructing a full contextual representation.</p>
<p>Conversely, the decoder generates the output sentence sequentially, with each new word able to access only its predecessors. This unidirectional flow is essential for coherent text generation. Each word produced by the decoder is then fed back into the input stream, creating a loop that continues until the target sentence is complete. Following the <em>Transformer’s</em> introduction, its individual encoder and decoder streams were re-engineered to develop distinct families of pre-trained language models.</p>
</section>
<section id="encoder-only-models" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="encoder-only-models"><span class="header-section-number">3.3</span> Encoder-Only Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>Focusing on the encoder component of the <em>Transformer</em> architecture reveals its function in transforming linguistic input into numerical representations. The process begins as words undergo input embedding, converting discrete tokens into continuous vectors. Positional encoding is then appended to provide vital information about the tokens’ sequential order, a crucial addition given the <em>Transformer’s</em> non-sequential processing nature.</p>
<p>This combined input then feeds into a stack of identical encoder layers. Each layer comprises two primary sub-layers: a multi-head attention mechanism, which enables the model to attend to information from diverse representational subspaces concurrently, and a simple feed-forward network. Both sub-layers are followed by an ‘add and norm’ step, incorporating residual connections and layer normalisation essential for training deep networks. This architecture aims to develop models capable of profound language understanding, which can then be applied to various Natural Language Processing tasks.</p>
</section>
<section id="bert-bidirectional-understanding" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bert-bidirectional-understanding"><span class="header-section-number">3.4</span> <em>BERT</em>: Bidirectional Understanding</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>The encoder side of the <em>Transformer</em> architecture gave rise to the <em>BERT</em> family of models, which remain highly prevalent. <em>BERT</em>, an acronym for Bidirectional Encoder Representations from Transformers, operates on the principle that each word within an input stream can interact with every other word. This bidirectional interaction enables the model to construct a comprehensive, full-context understanding of the entire input sequence simultaneously.</p>
<p>The term ‘bidirectional’ signifies that words can consider context from both preceding and succeeding tokens, whilst ‘encoder-based’ denotes its derivation from the original <em>Transformer</em> encoder. Devlin and colleagues pioneered this model in 2018. Its architecture holds significant promise for applications as LLMs within HPSS, particularly when engaging with specialised vocabularies.</p>
</section>
<section id="gpt-unidirectional-generation" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="gpt-unidirectional-generation"><span class="header-section-number">3.5</span> <em>GPT</em>: Unidirectional Generation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Conversely, the decoder side of the <em>Transformer</em> architecture forms the basis for Generative Pre-trained Transformer (<em>GPT</em>) models, which power contemporary systems such as <em>ChatGPT</em>. These models are inherently unidirectional, capable of examining only preceding tokens. This structural characteristic, however, enables them to generate novel text, a capability largely absent in <em>BERT</em>-like models.</p>
<p>Consequently, <em>GPT</em> and <em>BERT</em> models serve fundamentally different purposes: <em>GPT</em> excels at language generation, whilst <em>BERT</em> specialises in coherent sentence understanding. Beyond these two primary distinctions, the field also encompasses models combining encoder-decoder functionalities, alongside sophisticated methods for employing decoders in an encoder-like fashion, exemplified by architectures such as <em>XLM</em> and <em>XLNet</em>. Radford and colleagues introduced the foundational <em>GPT</em> model in 2018.</p>
</section>
<section id="the-evolution-of-scientific-llms" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="the-evolution-of-scientific-llms"><span class="header-section-number">3.6</span> The Evolution of Scientific LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>A comprehensive overview reveals the substantial evolution of Large Language Models, particularly those tailored for scientific domains. The landscape exhibits a greater proliferation of encoder-based (<em>BERT</em>-type) models compared to decoders, indicating considerable developmental activity on the encoder side. Early, popular models in this scientific context included <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>.</p>
<p>The field has since expanded to encompass a diverse array of domain-specific models, catering to areas such as biomedicine, chemistry, material science, climate science, and social science. For researchers in HPSS, this trend suggests a growing opportunity either to leverage existing models or to develop bespoke architectures pertinent to their specific research needs.</p>
</section>
<section id="domain-and-task-adaptation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="domain-and-task-adaptation"><span class="header-section-number">3.7</span> Domain and Task Adaptation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific scientific language necessitates several distinct approaches. Initial pre-training involves the model learning language by either predicting the next token, as seen in <em>GPT</em> models, or by predicting randomly masked words, characteristic of <em>BERT</em> models. This foundational stage, however, demands prohibitive computational resources.</p>
<p>A more accessible strategy involves continued pre-training, where an existing model is further trained on domain-specific language. Alternatively, researchers can add extra layers atop pre-trained models, fine-tuning them to function as classifiers for specific tasks. Contrastive learning also emerges as a crucial technique for generating sentence or document embeddings from existing word embeddings, thereby facilitating similarity comparisons. <em>Sentence BERT</em>, a widely adopted model, exemplifies this approach.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">3.8</span> Retrieval-Augmented Generation (<em>RAG</em>)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>Retrieval-Augmented Generation (<em>RAG</em>) represents a sophisticated pipeline rather than a singular model, orchestrating the action of at least two distinct models. The process commences when a user query is encoded into a sentence embedding by a <em>BERT</em>-type model. This model then searches a document database, retrieving the most semantically similar passages.</p>
<p>These retrieved passages are then integrated into the prompt of a generative model, which formulates a comprehensive answer based on this newly augmented context. The <em>RAG</em> framework is now commonplace in contemporary LLM applications, exemplified by <em>ChatGPT’s</em> ability to search the internet. More broadly, advanced reasoning agents are not merely single LLMs but intricate systems that combine LLMs with a diverse array of other computational tools.</p>
</section>
<section id="a-timeline-of-scientific-llms" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="a-timeline-of-scientific-llms"><span class="header-section-number">3.9</span> A Timeline of Scientific LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>A comprehensive timeline documented by Ho and colleagues in 2024, spanning from 2018 to 2024, illustrates the rapid proliferation of pre-trained LLMs engineered for scientific texts. The timeline commences in 2018 with the seminal <em>BERT</em> model. Subsequent years witnessed a significant expansion: 2019 introduced <em>FLAIR</em>; 2020 saw <em>BioFLAIR</em>, <em>G-BERT</em>, <em>BioELMo</em>, and <em>RoBERTa</em>. By 2021, <em>T5</em>, <em>GPT-2</em>, <em>RadBERT</em>, <em>SciBERT</em>, and <em>ClinicalBERT</em> had emerged.</p>
<p>The year 2022 marked a substantial increase across all categories, with models such as <em>SciFive</em>, <em>BioBART</em>, <em>MedGPT</em>, <em>SciGPT2</em>, and a multitude of encoders including <em>CovidBERT</em>, <em>SPECTER</em>, <em>NukeBERT</em>, and <em>BioBERT</em>. Development accelerated in 2023, introducing <em>BioReader</em>, <em>DRAGON</em>, <em>BioMedGPT</em>, <em>Clinical-T5</em>, <em>Galactica</em>, <em>BioGPT</em>, and an extensive array of encoders like <em>SciEdBERT</em>, <em>MatSciBERT</em>, <em>ChemBERT</em>, <em>PubMedBERT</em>, <em>MathBERT</em>, <em>ClimateBERT</em>, and <em>ProteinBERT</em>. The timeline extends into 2024 with <em>GIT-Mol</em>, <em>Patton</em>, and <em>MOTOR</em>, underscoring the dynamic and expanding landscape of scientific LLMs.</p>
</section>
<section id="adaptation-approaches-in-detail" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="adaptation-approaches-in-detail"><span class="header-section-number">3.10</span> Adaptation Approaches in Detail</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific domains and tasks primarily involves four distinct approaches. These methods allow for the customisation of general-purpose models to meet the nuanced demands of specialised fields like HPSS.</p>
<p>The main approaches are:</p>
<ul>
<li><p><em>Pre-training</em>, which focuses on refining the model’s vocabulary for domain-specific queries, often through continuous training on relevant textual corpora.</p></li>
<li><p><em>Extra Parameters</em>, which facilitates task-specific adaptation by adding dedicated layers to the model for functions like sentiment analysis or named entity recognition.</p></li>
<li><p><em>Prompt-Based</em> methods, which guide the LLM’s behaviour through carefully constructed prompts for tasks like masked language modelling or term definition.</p></li>
<li><p><em>Contrastive</em> learning, which generates robust representations by comparing inputs, proving invaluable for information retrieval and semantic search.</p></li>
</ul>
</section>
<section id="llm-applications-in-hpss" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="llm-applications-in-hpss"><span class="header-section-number">3.11</span> LLM Applications in HPSS</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>A recent survey identifies four principal categories for the application of Large Language Models within HPSS research.</p>
<ul>
<li><p><em>Dealing with data and sources</em>: This includes parsing and extracting information like publication types or citations, alongside interactive engagement with sources through summarisation or <em>RAG</em>-type conversational interfaces.</p></li>
<li><p><em>Knowledge structures</em>: This focuses on identifying and mapping elements within knowledge domains, such as scientific instruments or chemicals, and delineating relationships between disciplines.</p></li>
<li><p><em>Knowledge dynamics</em>: This addresses the evolution of knowledge over time, facilitating the study of conceptual histories and the identification of novelty, such as breakthrough papers.</p></li>
<li><p><em>Knowledge practices</em>: This examines the social dimensions of knowledge production, involving argument reconstruction, citation context analysis, and discourse analysis.</p></li>
</ul>
<p>Notably, there is an accelerating interest in LLMs within HPSS, with findings increasingly appearing in journals traditionally less amenable to computational methods. This trend is attributed to the semantic power of these models, which appeals to qualitative researchers and philosophers.</p>
</section>
<section id="key-distinctions-and-challenges" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="key-distinctions-and-challenges"><span class="header-section-number">3.12</span> Key Distinctions and Challenges</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>The application of LLMs exhibits a spectrum of customisation, from off-the-shelf usage of tools like <em>ChatGPT</em> to the development of novel architectures. Despite this versatility, several recurring concerns persist, including the demand for significant computational resources, the opaqueness of complex models, and a lack of sufficient training data. Researchers consistently encounter trade-offs between model types, underscoring the principle that no single model serves all purposes; selecting the adequate model for a specific objective remains paramount.</p>
<p>Encouragingly, a discernible trend towards increased accessibility is evident, exemplified by tools like <em>BERTTopic</em> for topic modelling. Fundamentally, understanding key distinctions proves vital: differentiating between encoder, decoder, and encoder-decoder architectures; recognising the various fine-tuning strategies; and appreciating the profound difference between word and sentence embeddings.</p>
</section>
<section id="hpss-challenges-and-future-directions" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="hpss-challenges-and-future-directions"><span class="header-section-number">3.13</span> HPSS Challenges and Future Directions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>Applying LLMs within HPSS presents unique challenges. The historical evolution of concepts and language poses a significant hurdle, as models trained on modern data may introduce anachronistic biases. Furthermore, HPSS adopts a reconstructive and critically reflective perspective, necessitating an ability to read between the lines and discern subtle discursive strategies for which current LLMs are not inherently equipped.</p>
<p>To address these issues, several recommendations emerge. Cultivating LLM literacy is paramount to prevent the uncritical adoption of off-the-shelf tools. The HPSS community should also actively develop shared datasets and benchmarks tailored to its specific needs. Whilst translating HPSS problems into Natural Language Processing tasks, researchers must remain steadfast in their methodological focus.</p>
<p>Nevertheless, these models present novel opportunities for bridging qualitative and quantitative research. Reflecting upon HPSS’s own pre-history, particularly methods like co-word analysis pioneered by scholars such as Callon and Rip in the 1980s, offers valuable insights into theoretically informed tool development for contemporary computational endeavours.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="NEPI Workshop Content Overview">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">NEPI Workshop Content Overview</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Investigating Transdisciplinary Applications with OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Investigating Transdisciplinary Applications with OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>