<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-06-21">

<title>Large Language Models for the History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals, ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction-to-large-language-models-and-hpss-applications" id="toc-introduction-to-large-language-models-and-hpss-applications" class="nav-link" data-scroll-target="#introduction-to-large-language-models-and-hpss-applications"><span class="header-section-number">3.1</span> Introduction to Large Language Models and HPSS Applications</a></li>
  <li><a href="#the-transformer-architecture-foundation-of-modern-llms" id="toc-the-transformer-architecture-foundation-of-modern-llms" class="nav-link" data-scroll-target="#the-transformer-architecture-foundation-of-modern-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundation of Modern LLMs</a></li>
  <li><a href="#encoder-based-models-bert-for-bidirectional-contextual-understanding" id="toc-encoder-based-models-bert-for-bidirectional-contextual-understanding" class="nav-link" data-scroll-target="#encoder-based-models-bert-for-bidirectional-contextual-understanding"><span class="header-section-number">3.3</span> Encoder-Based Models: BERT for Bidirectional Contextual Understanding</a></li>
  <li><a href="#decoder-based-models-gpt-for-unidirectional-generative-capabilities" id="toc-decoder-based-models-gpt-for-unidirectional-generative-capabilities" class="nav-link" data-scroll-target="#decoder-based-models-gpt-for-unidirectional-generative-capabilities"><span class="header-section-number">3.4</span> Decoder-Based Models: GPT for Unidirectional Generative Capabilities</a></li>
  <li><a href="#evolution-and-specialisation-of-scientific-llms" id="toc-evolution-and-specialisation-of-scientific-llms" class="nav-link" data-scroll-target="#evolution-and-specialisation-of-scientific-llms"><span class="header-section-number">3.5</span> Evolution and Specialisation of Scientific LLMs</a></li>
  <li><a href="#strategies-for-domain-and-task-adaptation-of-llms" id="toc-strategies-for-domain-and-task-adaptation-of-llms" class="nav-link" data-scroll-target="#strategies-for-domain-and-task-adaptation-of-llms"><span class="header-section-number">3.6</span> Strategies for Domain and Task Adaptation of LLMs</a></li>
  <li><a href="#retrieval-augmented-generation-rag-pipelines" id="toc-retrieval-augmented-generation-rag-pipelines" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-pipelines"><span class="header-section-number">3.7</span> Retrieval-Augmented Generation (RAG) Pipelines</a></li>
  <li><a href="#key-distinctions-in-large-language-model-paradigms" id="toc-key-distinctions-in-large-language-model-paradigms" class="nav-link" data-scroll-target="#key-distinctions-in-large-language-model-paradigms"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Model Paradigms</a></li>
  <li><a href="#applications-of-llms-in-history-philosophy-and-sociology-of-science" id="toc-applications-of-llms-in-history-philosophy-and-sociology-of-science" class="nav-link" data-scroll-target="#applications-of-llms-in-history-philosophy-and-sociology-of-science"><span class="header-section-number">3.9</span> Applications of LLMs in History, Philosophy, and Sociology of Science</a></li>
  <li><a href="#observations-and-concerns-in-hpss-llm-adoption" id="toc-observations-and-concerns-in-hpss-llm-adoption" class="nav-link" data-scroll-target="#observations-and-concerns-in-hpss-llm-adoption"><span class="header-section-number">3.10</span> Observations and Concerns in HPSS LLM Adoption</a></li>
  <li><a href="#reflections-on-hpss-specific-challenges-and-llm-integration" id="toc-reflections-on-hpss-specific-challenges-and-llm-integration" class="nav-link" data-scroll-target="#reflections-on-hpss-specific-challenges-and-llm-integration"><span class="header-section-number">3.11</span> Reflections on HPSS-Specific Challenges and LLM Integration</a></li>
  <li><a href="#additional-visual-materials" id="toc-additional-visual-materials" class="nav-link" data-scroll-target="#additional-visual-materials"><span class="header-section-number">3.12</span> Additional Visual Materials</a>
  <ul class="collapse">
  <li><a href="#slide-02" id="toc-slide-02" class="nav-link" data-scroll-target="#slide-02"><span class="header-section-number">3.12.1</span> Slide 02</a></li>
  <li><a href="#slide-08" id="toc-slide-08" class="nav-link" data-scroll-target="#slide-08"><span class="header-section-number">3.12.2</span> Slide 08</a></li>
  <li><a href="#slide-14" id="toc-slide-14" class="nav-link" data-scroll-target="#slide-14"><span class="header-section-number">3.12.3</span> Slide 14</a></li>
  <li><a href="#slide-15" id="toc-slide-15" class="nav-link" data-scroll-target="#slide-15"><span class="header-section-number">3.12.4</span> Slide 15</a></li>
  <li><a href="#slide-16" id="toc-slide-16" class="nav-link" data-scroll-target="#slide-16"><span class="header-section-number">3.12.5</span> Slide 16</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Large Language Models for the History, Philosophy, and Sociology of Science</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2025-06-21</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter comprehensively explores the application of Large Language Models (LLMs) within the History, Philosophy, and Sociology of Science (HPSS) domain. It commences by providing a foundational primer on LLM architectures, specifically detailing the <em>Transformer</em> model and its derivatives, <em>BERT</em> and <em>GPT</em>. The discussion then transitions to summarising current applications of LLMs in HPSS research, categorising them into data handling, knowledge structuring, knowledge dynamics, and knowledge practices.</p>
<p>Crucially, the authors identify key distinctions in LLM types and adaptation strategies, including pre-training, fine-tuning, and Retrieval-Augmented Generation (RAG) pipelines. The chapter highlights the accelerating interest in LLMs across academic journals, whilst acknowledging persistent concerns regarding computational resources, model opaqueness, data scarcity, and the absence of standardised benchmarks.</p>
<p>The authors conclude by offering critical reflections on HPSS-specific challenges, such as the historical evolution of concepts and sparse, multilingual data. They advocate for enhanced LLM literacy amongst researchers and a steadfast adherence to HPSS methodologies to prevent technological tools from dictating research objectives. Ultimately, this chapter underscores the potential of LLMs to bridge qualitative and quantitative research approaches within the HPSS field, fostering new avenues for interdisciplinary inquiry.</p>
</section>
<section id="introduction-to-large-language-models-and-hpss-applications" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction-to-large-language-models-and-hpss-applications"><span class="header-section-number">3.1</span> Introduction to Large Language Models and HPSS Applications</h2>
<p>This chapter establishes a foundational understanding of Large Language Models (LLMs) and their specific applications within the History, Philosophy, and Sociology of Science (HPSS). Primarily, it aims to furnish a concise primer on LLMs, detailing their adaptation to diverse scientific domains. Subsequently, the discussion summarises current LLM applications pertinent to HPSS research. Finally, the authors offer critical reflections, intended to stimulate further discussion throughout the workshop. Given the varied technical backgrounds of the audience, the initial segment focuses on accessible explanations of complex LLM concepts.</p>
</section>
<section id="the-transformer-architecture-foundation-of-modern-llms" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture-foundation-of-modern-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundation of Modern LLMs</h2>
<p>The foundational architecture underpinning contemporary Large Language Models is the <em>Transformer</em>. Vaswani and colleagues originally introduced this model in their seminal 2017 paper, “<em>Attention is all you need</em>” <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref"><strong>vaswani2017attention?</strong></a>)</span>. Initially conceived for language translation tasks, such as converting German to English, this architecture comprises two interconnected streams: an encoder and a decoder.</p>
<p>The encoder, positioned on the left, processes the entire input sentence simultaneously. Crucially, it enables each word within the input to interact with every other word, thereby constructing a comprehensive contextual representation of the complete sentence. Words initially feed into this component and subsequently encode into numerical representations.</p>
<p>Conversely, the decoder, located on the right, generates the output sentence. This component operates sequentially, predicting each subsequent word based solely on its predecessors; it cannot access future words. The numerical representations from the encoder transmit to the decoder, which then transforms them back into words. Each generated word subsequently feeds back into the decoder’s input, forming a loop that continues until the complete sentence is produced. Within both the encoder and decoder, various layers refine contextualised word embeddings, progressively enhancing their contextual understanding.</p>
</section>
<section id="encoder-based-models-bert-for-bidirectional-contextual-understanding" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="encoder-based-models-bert-for-bidirectional-contextual-understanding"><span class="header-section-number">3.3</span> Encoder-Based Models: BERT for Bidirectional Contextual Understanding</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Following the introduction of the <em>Transformer</em> model, Devlin and colleagues promptly began re-engineering its individual streams to develop pre-trained language models <span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref"><strong>devlin2018bert?</strong></a>)</span>. These models, rather than focusing solely on translation, excel at understanding and generating language, making them highly adaptable for various Natural Language Processing (NLP) tasks with minimal additional training.</p>
<p>A prominent example of such a model is <em>BERT</em>, or Bidirectional Encoder Representations from Transformers. <em>BERT</em> specifically leverages the encoder side of the <em>Transformer</em> architecture. Its defining characteristic is its capacity for bidirectional, full-context understanding; every word within the input stream can interact with all other words, thereby constructing a comprehensive contextual representation of the entire input at once. During its pre-training phase, <em>BERT</em> learns by predicting randomly masked words within a text. Consequently, <em>BERT</em>-like models are primarily designed for coherent sentence understanding, distinguishing them from models focused on generating novel text. Despite newer developments, the <em>BERT</em> family of models maintains a significant presence and utility in the field.</p>
</section>
<section id="decoder-based-models-gpt-for-unidirectional-generative-capabilities" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="decoder-based-models-gpt-for-unidirectional-generative-capabilities"><span class="header-section-number">3.4</span> Decoder-Based Models: GPT for Unidirectional Generative Capabilities</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>Conversely, the <em>GPT</em> models, standing for Generative Pre-trained Transformers, derive from the decoder side of the <em>Transformer</em> architecture, as detailed by Radford and colleagues in 2018 <span class="citation" data-cites="radford2018improving">(<a href="#ref-radford2018improving" role="doc-biblioref"><strong>radford2018improving?</strong></a>)</span>. These models possess a unidirectional generative capability, meaning they can only consider preceding words when generating new text. This structural difference enables <em>GPT</em> models to produce novel language, a feature that powers widely used applications such as <em>ChatGPT</em>.</p>
<p>Notably, <em>GPT</em> models fundamentally differ from <em>BERT</em> models: whilst <em>GPT</em> excels at generating new words, <em>BERT</em> primarily focuses on understanding the full context of an input. Beyond these two primary types, the field also features hybrid architectures that combine encoders and decoders, alongside decoder-only models engineered to emulate encoder-like functionalities, such as <em>XLNet</em> and <em>XLM</em>. Understanding this core distinction between generative models, which produce language, and full-context models, which coherently understand sentences, remains paramount for effective application.</p>
</section>
<section id="evolution-and-specialisation-of-scientific-llms" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evolution-and-specialisation-of-scientific-llms"><span class="header-section-number">3.5</span> Evolution and Specialisation of Scientific LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The landscape of Large Language Models has undergone rapid and diverse evolution between 2018 and 2024, particularly within scientific domains. Researchers categorise these models primarily by their underlying architecture: encoders, such as the <em>BERT</em>-type models; decoders, exemplified by <em>GPT</em>-type models; and hybrid encoder-decoder configurations, alongside other distinct architectures.</p>
<p>Early popular examples of domain-specific models include <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>. Subsequently, the field has witnessed the development of specialised LLMs tailored for a wide array of scientific disciplines, encompassing biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. For researchers in the History, Philosophy, and Sociology of Science, this proliferation signifies a substantial opportunity either to leverage existing domain-specific models or to develop bespoke solutions. Analysis of the current landscape indicates a greater prevalence of encoder models in scientific applications compared to decoder models.</p>
</section>
<section id="strategies-for-domain-and-task-adaptation-of-llms" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="strategies-for-domain-and-task-adaptation-of-llms"><span class="header-section-number">3.6</span> Strategies for Domain and Task Adaptation of LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific scientific languages and tasks involves several distinct strategies. The most fundamental approach is pre-training, wherein a model initially learns language by predicting either the next token, as seen in <em>GPT</em> models, or random masked words, characteristic of <em>BERT</em> models. However, this process demands prohibitive computational resources and vast datasets, rendering it generally unfeasible for individual researchers.</p>
<p>A more accessible alternative is continued pre-training. This involves taking an already pre-trained model and further training it on domain-specific language; for instance, the authors’ own research has refined a <em>BERT</em> model using physics texts. Beyond this, researchers can adapt models by adding extra parameters or layers atop pre-trained architectures, which then train for specific downstream tasks such as sentiment analysis or named entity recognition.</p>
<p>Whilst prompt-based adaptation offers another avenue for guiding model behaviour without extensive retraining, contrastive learning represents a crucial method for generating sentence or document embeddings. This technique, often leveraging <em>BERT</em> models, produces embeddings where semantically similar inputs position closely in the embedding space, whilst dissimilar inputs keep distant. <em>Sentence BERT</em>, for example, is a widely adopted approach that facilitates the creation of comprehensive sentence and document embeddings from initial word embeddings.</p>
</section>
<section id="retrieval-augmented-generation-rag-pipelines" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-pipelines"><span class="header-section-number">3.7</span> Retrieval-Augmented Generation (RAG) Pipelines</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>Retrieval-Augmented Generation (RAG) constitutes a sophisticated pipeline system rather than a singular Large Language Model. This architecture typically integrates at least two or more models that operate in concert. Specifically, <em>BERT</em>-type models are employed to assess the semantic similarity between a user’s query and a corpus of documents. Concurrently, generative models are tasked with formulating responses, drawing upon the context retrieved by the <em>BERT</em>-type components.</p>
<p>The RAG workflow unfolds systematically: a user’s query, such as “What are LLMs?”, first encodes into a sentence embedding by a <em>BERT</em>-type model. This model then queries a database of relevant documents, identifying and retrieving passages that exhibit the highest semantic similarity. Subsequently, these retrieved passages seamlessly integrate into the prompt provided to a generative model. Armed with this augmented context, the generative model then synthesises a comprehensive answer. This sophisticated pipeline is now a ubiquitous feature in contemporary LLM applications, exemplified by <em>ChatGPT</em>’s capacity to search the internet and present relevant results. More broadly, advanced reasoning models and agents represent complex systems that combine LLMs with a diverse array of other computational tools.</p>
</section>
<section id="key-distinctions-in-large-language-model-paradigms" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="key-distinctions-in-large-language-model-paradigms"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Model Paradigms</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>To navigate the complex landscape of Large Language Models effectively, several key distinctions warrant careful consideration.</p>
<ul>
<li>Firstly, models fundamentally differ in their underlying architecture, broadly categorised as encoder-based, decoder-based, or hybrid encoder-decoder configurations.</li>
<li>Secondly, a diverse array of fine-tuning strategies exists, each designed to adapt pre-trained models for specific tasks.</li>
<li>Thirdly, a critical conceptual difference lies between word embeddings and sentence embeddings, as these represent distinct levels of semantic representation.</li>
<li>Finally, it is imperative to recognise the varying levels of abstraction at which LLMs operate: they can function as individual models, as integral components within larger pipelines such as RAG systems, or as sophisticated agents that combine LLMs with a multitude of other computational tools.</li>
</ul>
</section>
<section id="applications-of-llms-in-history-philosophy-and-sociology-of-science" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="applications-of-llms-in-history-philosophy-and-sociology-of-science"><span class="header-section-number">3.9</span> Applications of LLMs in History, Philosophy, and Sociology of Science</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>A recent survey of Large Language Model users within History, Philosophy, and Sociology of Science (HPSS) research has delineated four primary categories of application.</p>
<ul>
<li><strong>Dealing with data and sources:</strong> LLMs prove invaluable for facilitating the parsing and extraction of diverse information, including publication types, acknowledgements, and citations. Moreover, they enable more dynamic interaction with sources through summarisation and RAG-type conversational interfaces.</li>
<li><strong>Analysis of knowledge structures:</strong> LLMs contribute significantly to this area. This encompasses the extraction of specific entities, such as scientific instruments, celestial bodies, or chemicals, alongside the mapping of complex relationships, including disciplinary boundaries, interdisciplinary fields, and science-policy discourses.</li>
<li><strong>Investigation of knowledge dynamics:</strong> The models offer powerful tools for this purpose. This involves tracing conceptual histories—for instance, the evolution of terms like “theory” in Digital Humanities or “virtual” and “Planck” in physics. Furthermore, LLMs can assist in detecting novelty, identifying breakthrough papers, and pinpointing emerging technologies.</li>
<li><strong>Enhancement of knowledge practices:</strong> LLMs support argument reconstruction by identifying premises, conclusions, and causal links within texts. Citation context analysis, an established HPSS tradition often now confined to evaluatory purposes, gains renewed utility for broader HPSS tasks. Additionally, LLMs facilitate nuanced discourse analysis, enabling the detection of subtle linguistic features such as hedge sentences, specialised jargon, and instances of boundary work.</li>
</ul>
</section>
<section id="observations-and-concerns-in-hpss-llm-adoption" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="observations-and-concerns-in-hpss-llm-adoption"><span class="header-section-number">3.10</span> Observations and Concerns in HPSS LLM Adoption</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Current trends indicate an accelerating interest in Large Language Models within the academic community, extending even to journals not traditionally associated with computational methods, such as <em>Scientometrics</em> and <em>JASIST</em>. This growing adoption reflects the profound semantic capabilities of these models, which increasingly appeal to qualitative researchers and philosophers.</p>
<p>The degree of customisation in LLM application varies considerably, ranging from the straightforward, off-the-shelf use of tools like <em>ChatGPT</em> to the development of entirely new architectures, alongside custom pre-training and fine-tuning. Despite this burgeoning interest, several recurring concerns persist. Researchers frequently highlight the overwhelming computational resources required, the inherent opaqueness of many models, and a pervasive lack of both sufficient training data and standardised benchmarks.</p>
<p>Furthermore, the necessity of navigating trade-offs between different model types, such as <em>BERT</em>-like and <em>GPT</em>-like architectures, underscores that no single model serves all purposes. Rather, the selection of an adequate model remains contingent upon the specific research objective. Nevertheless, a discernible trend towards greater accessibility is emerging, exemplified by tools like <em>BERTopic</em> for topic modelling, which are gaining widespread adoption due to their user-friendliness and robust maintenance.</p>
</section>
<section id="reflections-on-hpss-specific-challenges-and-llm-integration" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="reflections-on-hpss-specific-challenges-and-llm-integration"><span class="header-section-number">3.11</span> Reflections on HPSS-Specific Challenges and LLM Integration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Integrating Large Language Models into History, Philosophy, and Sociology of Science (HPSS) necessitates a careful consideration of several HPSS-specific challenges.</p>
<ul>
<li>Firstly, the historical evolution of concepts and language presents a significant hurdle. As LLMs predominantly train on modern language, they may exhibit biases when applied to historical texts. This necessitates either the development of bespoke models or the judicious application of existing ones, whilst acknowledging their inherent limitations.</li>
<li>Secondly, HPSS adopts a reconstructive and critically reflective perspective, involving reading “between the lines” to understand authorial context and subtle discursive strategies, such as boundary work. Current LLMs do not inherently train for such nuanced interpretation, demanding innovative approaches to align model capabilities with HPSS methodologies.</li>
<li>Thirdly, practical data issues persist, including sparse datasets, the presence of multiple languages, old scripts, and a general lack of digitalisation.</li>
</ul>
<p>Addressing these challenges requires cultivating robust LLM literacy amongst researchers. This involves familiarising oneself with the tools and theoretical underpinnings of LLMs, Natural Language Processing (NLP), and Deep Learning (DL). Researchers must learn to identify the most appropriate architecture and training regimen for their specific problems, whilst collectively developing shared datasets and benchmarks tailored to HPSS needs. Although advancements in natural language processing may gradually reduce the need for extensive coding, a foundational understanding remains crucial to avoid merely generating aesthetically pleasing but analytically shallow visualisations.</p>
<p>Crucially, HPSS researchers must remain steadfast in their methodological principles. This entails translating complex HPSS problems into NLP tasks without compromising the original research focus or purpose. Nevertheless, LLMs present unprecedented opportunities for bridging qualitative and quantitative approaches, fostering interdisciplinary collaboration. Furthermore, it is pertinent to reflect upon HPSS’s own historical contributions to the pre-history of these models, exemplified by the co-word analysis developed by scholars like Colon and Ari Rip in the 1980s, which emerged from an Actor-Network Theory perspective and demonstrates a long-standing engagement with computational tools for conceptual analysis.</p>
</section>
<section id="additional-visual-materials" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="additional-visual-materials"><span class="header-section-number">3.12</span> Additional Visual Materials</h2>
<p>The following slides provide supplementary visual information that complements the main chapter content:</p>
<section id="slide-02" class="level3" data-number="3.12.1">
<h3 data-number="3.12.1" class="anchored" data-anchor-id="slide-02"><span class="header-section-number">3.12.1</span> Slide 02</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>The slide presents the agenda for the presentation, framed as “Today’s Menu”. The main sections to be covered are listed as: “Primer on LLMs” (Large Language Models), “Applications in HPSS” (History, Philosophy, and Sociology of Science), and “Reflections”. This slide serves as a roadmap for the audience.</p>
</section>
<section id="slide-08" class="level3" data-number="3.12.2">
<h3 data-number="3.12.2" class="anchored" data-anchor-id="slide-08"><span class="header-section-number">3.12.2</span> Slide 08</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>This slide, titled “Transformer”, presents the complete <em>Transformer</em> model architecture, now augmented with specific examples of Large Language Models (LLMs). On the “Encoder” side, “<em>BERT</em>” is shown, characterised as “bidirectional full-context” and linked to “LLMs for HPSS ?” and “Vocab”. On the “Decoder” side, “<em>GPT</em>” is introduced, characterised as “unidirectional generative” and also linked to “LLMs for HPSS ?” and “Vocab”. The full internal components of the <em>Transformer</em> (Input/Output Embedding, Positional Encoding, Multi-Head Attention, Feed Forward, Add &amp; Norm, Softmax, Linear) are visible. The slide includes citations for “Vaswani et al.&nbsp;2017: <em>Attention is all you need</em>”, “Devlin et al.&nbsp;2018. <em>BERT: Pre-training of…</em>”, and “Radford et al.&nbsp;2018. <em>Improving Language…</em>”.</p>
</section>
<section id="slide-14" class="level3" data-number="3.12.3">
<h3 data-number="3.12.3" class="anchored" data-anchor-id="slide-14"><span class="header-section-number">3.12.3</span> Slide 14</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>The slide, titled “Applications in HPSS”, provides a comprehensive list of potential uses for Large Language Models within the History, Philosophy, and Sociology of Science. These applications are categorised into four main areas: “Dealing with data and sources”, which includes “Parsing and extracting (publication types, acknowledgements, citations)” and “Interacting with sources (summarisation, RAG-type chatting)”; “Knowledge structures”, covering “Entity extraction (scientific instruments, celestial bodies, chemicals)” and “Mappings (disciplines, interdisciplinary fields, science-policy discourses)”; “Knowledge dynamics”, addressing “Conceptual histories (“theory” in DH, “virtual” and “Planck” in physics)” and “Novelty (breakthrough papers, emerging technologies)”; and “Knowledge practices”, encompassing “Argument reconstruction (premises &amp; conclusions, causality)”, “Citation context analysis (purpose, sentiment)”, and “Discourse analysis (hedge sentences, jargon, boundary work)”.</p>
</section>
<section id="slide-15" class="level3" data-number="3.12.4">
<h3 data-number="3.12.4" class="anchored" data-anchor-id="slide-15"><span class="header-section-number">3.12.4</span> Slide 15</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>The slide, titled “Applications in HPSS”, continues the discussion by highlighting current trends and challenges. It notes “Accelerating interest in LLMs”, even in “non-computational journals”. It describes “Varying degrees of customisation”, ranging “From architectural tweakings and custom pretraining over custom fine-tuning to off-the-shelf use of <em>ChatGPT</em>”. The slide also lists “Repeating concerns” such as “Overwhelming computational resources, opaqueness, lack of training data, lack of benchmarks, trade-offs between model types (<em>BERT</em>-like vs.&nbsp;<em>GPT</em>-like)”. Finally, it points to a “Trend toward accessibility”, exemplified by the question “E.g. <em>BERTopic</em> as the new pyLDAvis?”.</p>
</section>
<section id="slide-16" class="level3" data-number="3.12.5">
<h3 data-number="3.12.5" class="anchored" data-anchor-id="slide-16"><span class="header-section-number">3.12.5</span> Slide 16</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>The slide, titled “Reflections”, presents key considerations and future directions for LLMs in HPSS. It outlines “Acknowledging HPSS-specific challenges”, including “Historical evolution of concepts and language”, adopting a “Reconstructive perspective, reading between the lines, reflecting social implications”, and dealing with “Sparse data, multiple languages, old scripts, lack of digitalisation”. It emphasises “Building LLM literacy” by encouraging familiarity with “LLMs, NLP, and DL, both tools and theory”, learning “what’s the right architecture and training for our problems”, and developing “our own shared datasets and benchmarks”. Lastly, it stresses “Staying true to HPSS methodologies”, which involves translating “HPSS problems into NLP tasks without losing our focus”, exploring “New opportunities for bridging qualitative and quantitative approaches”, and reflecting on “HPSS’ own role in LLM pre-history (e.g.&nbsp;co-word analysis)”.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>