<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">

<title>3&nbsp; Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">LLMs and Footnotes: Challenges in Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Possible applications of RAG systems in philosophy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#the-transformer-architecture-foundations-of-large-language-models" id="toc-the-transformer-architecture-foundations-of-large-language-models" class="nav-link" data-scroll-target="#the-transformer-architecture-foundations-of-large-language-models"><span class="header-section-number">3.1</span> The Transformer Architecture: Foundations of Large Language Models</a></li>
  <li><a href="#encoder-based-models-bert-and-bidirectional-context-understanding" id="toc-encoder-based-models-bert-and-bidirectional-context-understanding" class="nav-link" data-scroll-target="#encoder-based-models-bert-and-bidirectional-context-understanding"><span class="header-section-number">3.2</span> Encoder-Based Models: BERT and Bidirectional Context Understanding</a></li>
  <li><a href="#decoder-based-models-gpt-and-generative-capabilities" id="toc-decoder-based-models-gpt-and-generative-capabilities" class="nav-link" data-scroll-target="#decoder-based-models-gpt-and-generative-capabilities"><span class="header-section-number">3.3</span> Decoder-Based Models: GPT and Generative Capabilities</a></li>
  <li><a href="#evolution-and-specialisation-of-scientific-large-language-models" id="toc-evolution-and-specialisation-of-scientific-large-language-models" class="nav-link" data-scroll-target="#evolution-and-specialisation-of-scientific-large-language-models"><span class="header-section-number">3.4</span> Evolution and Specialisation of Scientific Large Language Models</a></li>
  <li><a href="#strategies-for-domain-and-task-adaptation-of-large-language-models" id="toc-strategies-for-domain-and-task-adaptation-of-large-language-models" class="nav-link" data-scroll-target="#strategies-for-domain-and-task-adaptation-of-large-language-models"><span class="header-section-number">3.5</span> Strategies for Domain and Task Adaptation of Large Language Models</a></li>
  <li><a href="#retrieval-augmented-generation-rag-for-domain-adaptation" id="toc-retrieval-augmented-generation-rag-for-domain-adaptation" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.6</span> Retrieval-Augmented Generation (RAG) for Domain Adaptation</a></li>
  <li><a href="#fundamental-distinctions-in-large-language-model-paradigms" id="toc-fundamental-distinctions-in-large-language-model-paradigms" class="nav-link" data-scroll-target="#fundamental-distinctions-in-large-language-model-paradigms"><span class="header-section-number">3.7</span> Fundamental Distinctions in Large Language Model Paradigms</a></li>
  <li><a href="#categorisation-of-large-language-model-applications-in-hpss-research" id="toc-categorisation-of-large-language-model-applications-in-hpss-research" class="nav-link" data-scroll-target="#categorisation-of-large-language-model-applications-in-hpss-research"><span class="header-section-number">3.8</span> Categorisation of Large Language Model Applications in HPSS Research</a></li>
  <li><a href="#trends-and-challenges-in-hpss-applications-of-large-language-models" id="toc-trends-and-challenges-in-hpss-applications-of-large-language-models" class="nav-link" data-scroll-target="#trends-and-challenges-in-hpss-applications-of-large-language-models"><span class="header-section-number">3.9</span> Trends and Challenges in HPSS Applications of Large Language Models</a></li>
  <li><a href="#critical-reflections-on-integrating-large-language-models-into-hpss-research" id="toc-critical-reflections-on-integrating-large-language-models-into-hpss-research" class="nav-link" data-scroll-target="#critical-reflections-on-integrating-large-language-models-into-hpss-research"><span class="header-section-number">3.10</span> Critical Reflections on Integrating Large Language Models into HPSS Research</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            AI-NEPI Conference Participant
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter offers a comprehensive introduction to large language models (<em>LLMs</em>) and their applications within the History, Philosophy, and Sociology of Science (<em>HPSS</em>) domain. It commences with a foundational primer on <em>LLMs</em> and their adaptation to scientific contexts, followed by a summary of their contemporary applications in <em>HPSS</em>. The author also shares critical reflections intended to stimulate workshop discussions.</p>
<p>The primer details the core <em>Transformer</em> architecture, explaining its encoder-decoder structure and its evolution into distinct model types, such as <em>BERT</em> (bidirectional, encoder-based for understanding) and <em>GPT</em> (unidirectional, decoder-based for generation). It then explores various strategies for adapting these models to specific scientific domains and tasks, including pre-training, fine-tuning, and the sophisticated <em>Retrieval-Augmented Generation</em> (<em>RAG</em>) pipeline, which integrates multiple models for enhanced contextual responses.</p>
<p>A systematic categorisation of <em>LLM</em> applications in <em>HPSS</em> research is presented, covering data handling, knowledge structure analysis, knowledge dynamics, and knowledge practices. The discussion highlights accelerating interest in <em>LLMs</em> across diverse academic journals whilst acknowledging recurring concerns, such as computational resource demands, model opaqueness, and data scarcity. Crucially, the chapter concludes with critical reflections on <em>HPSS</em>-specific challenges, including the historical evolution of language and the need for reconstructive analysis, advocating for enhanced <em>LLM</em> literacy amongst scholars. It also emphasises the importance of aligning <em>NLP</em> tasks with core <em>HPSS</em> methodologies and exploring new opportunities for bridging qualitative and quantitative research.</p>
</section>
<section id="the-transformer-architecture-foundations-of-large-language-models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-transformer-architecture-foundations-of-large-language-models"><span class="header-section-number">3.1</span> The Transformer Architecture: Foundations of Large Language Models</h2>
<p>The <em>Transformer</em> architecture constitutes the fundamental framework underpinning all contemporary <em>Large Language Models</em>. Engineers designed this model in 2017 for language translation tasks, such as converting German text into English. The architecture comprises two interconnected streams: an encoder and a decoder.</p>
<p>The encoder, positioned on the left, processes input words—for instance, a German sentence—by converting them into numerical representations. Crucially, it reads the entire input sentence simultaneously, enabling each word to interact with every other word. This comprehensive interaction allows the model to construct a full contextual representation of the sentence’s complete meaning.</p>
<p>Conversely, the decoder, located on the right, receives these processed numerical representations from the encoder. It then generates output words, such as an English sentence, feeding each newly produced word back into its input stream. This iterative process continues until the complete English sentence emerges. A key distinction of the decoder lies in its unidirectional nature: words can only access their predecessors, preventing them from “looking into the future” whilst predicting the subsequent word. Within both the encoder and decoder, multiple layers progressively refine contextualised word embeddings. Vaswani and colleagues introduced this foundational design in their seminal 2017 paper, ‘Attention is all you need.’</p>
</section>
<section id="encoder-based-models-bert-and-bidirectional-context-understanding" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="encoder-based-models-bert-and-bidirectional-context-understanding"><span class="header-section-number">3.2</span> Encoder-Based Models: BERT and Bidirectional Context Understanding</h2>
<p>Immediately following the <em>Transformer</em>’s introduction, researchers began re-engineering its individual streams to develop pre-trained language models. These models excel at understanding language and readily adapt to various <em>Natural Language Processing</em> tasks with minimal additional training.</p>
<p>The encoder side of the <em>Transformer</em> gave rise to models such as <em>BERT</em>, which remains highly prevalent. <em>BERT</em>, an acronym for <em>Bidirectional Encoder Representations from Transformers</em>, operates by allowing each word within the input stream to interact with every other word. This bidirectional capability enables the model to construct a comprehensive contextual understanding of the entire input simultaneously. Consequently, <em>BERT</em>-like models primarily focus on coherently understanding sentences rather than generating new text. Devlin and colleagues introduced this architecture in their 2018 paper.</p>
</section>
<section id="decoder-based-models-gpt-and-generative-capabilities" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="decoder-based-models-gpt-and-generative-capabilities"><span class="header-section-number">3.3</span> Decoder-Based Models: GPT and Generative Capabilities</h2>
<p>Conversely, the decoder side of the <em>Transformer</em> architecture led to the development of <em>GPT</em> models, or <em>Generative Pre-trained Transformers</em>. These models now power widely used applications such as <em>ChatGPT</em>. Their distinct structure allows words to access only their predecessors, establishing a unidirectional context. This design, however, confers a powerful generative capability, enabling them to produce new text and language—a function not inherently present in <em>BERT</em> models.</p>
<p>Beyond these two primary types, various model architectures exist, including those that combine encoder and decoder components. Furthermore, researchers have devised sophisticated methods to enable decoder models to operate more akin to encoders, exemplified by models like <em>XLNet</em> and <em>XLM</em>. Fundamentally, understanding these distinctions is crucial: generative models, such as those in the <em>GPT</em> family, excel at producing language, whilst full-context models, like <em>BERT</em>, demonstrate superior capabilities in coherently understanding sentences. Radford and colleagues introduced the <em>GPT</em> architecture in 2018.</p>
</section>
<section id="evolution-and-specialisation-of-scientific-large-language-models" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="evolution-and-specialisation-of-scientific-large-language-models"><span class="header-section-number">3.4</span> Evolution and Specialisation of Scientific Large Language Models</h2>
<p>A comprehensive overview charts the evolution of large language models from 2018 to 2024, specifically highlighting their development for scientific domains and tasks. This landscape categorises models into Encoder-Decoder, Decoder, and Encoder types, encompassing both open-source and closed-source variants. Notably, encoder models, akin to <em>BERT</em>, appear significantly more prevalent within scientific applications.</p>
<p>Early pioneering models, such as <em>BioBERT</em>, <em>Specter</em>, and <em>SciBERT</em>, gained considerable popularity. Today, researchers have developed a wide array of domain-specific models tailored for fields including biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. This proliferation underscores the substantial potential for scholars in the History, Philosophy, and Sociology of Science to either leverage existing models or craft their own specialised tools. Ho and colleagues provided this 2024 survey, detailing pre-trained language models for scientific text processing.</p>
</section>
<section id="strategies-for-domain-and-task-adaptation-of-large-language-models" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="strategies-for-domain-and-task-adaptation-of-large-language-models"><span class="header-section-number">3.5</span> Strategies for Domain and Task Adaptation of Large Language Models</h2>
<p>Adapting large language models to specific scientific languages and tasks involves several key strategies. Pre-training represents the initial phase where a model acquires language by predicting either the next token, as seen in <em>GPT</em> models, or random masked words, characteristic of <em>BERT</em> models. This process, however, demands prohibitive computational resources and vast datasets, rendering it largely impractical for individual researchers.</p>
<p>A more accessible approach involves continued pre-training, where researchers take an already pre-trained model, such as a <em>BERT</em> variant, and further train it on domain-specific language; for instance, applying it to physics texts. Beyond this, fine-tuning through the addition of extra parameters allows researchers to append new layers atop pre-trained models. These layers then undergo training for specific <em>Natural Language Processing</em> tasks, including sentiment classification or named entity recognition.</p>
<p>Whilst prompt-based methods also exist, contrastive learning emerges as a pivotal technique. This method generates sentence or document embeddings from existing word embeddings, effectively mapping documents or sentences into the same embedding space as individual words. <em>Sentence BERT</em> stands out as a widely adopted and highly effective method within this domain.</p>
</section>
<section id="retrieval-augmented-generation-rag-for-domain-adaptation" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.6</span> Retrieval-Augmented Generation (RAG) for Domain Adaptation</h2>
<p><em>Retrieval-Augmented Generation</em>, or <em>RAG</em>, offers a sophisticated pipeline for adapting models to specific scientific domains without necessitating direct model training. This system integrates multiple models, typically at least two, working in concert. Users frequently encounter <em>RAG</em> in contemporary generative <em>AI</em> tools, such as <em>ChatGPT</em>, where it underpins functionalities like internet search.</p>
<p>The <em>RAG</em> workflow commences when a user submits a query, for example, “What are <em>LLMs</em>?”. A <em>BERT</em>-type model encodes this query into a sentence embedding. Subsequently, this model searches a database of relevant documents to identify and retrieve the most similar passages. These retrieved passages are then seamlessly integrated into the prompt provided to a generative model. Drawing upon this newly supplied context, the generative model formulates and produces its answer. Crucially, this demonstrates that advanced reasoning models and agents are not monolithic <em>LLMs</em> but intricate systems that combine <em>LLMs</em> with a diverse array of other computational tools.</p>
</section>
<section id="fundamental-distinctions-in-large-language-model-paradigms" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="fundamental-distinctions-in-large-language-model-paradigms"><span class="header-section-number">3.7</span> Fundamental Distinctions in Large Language Model Paradigms</h2>
<p>Understanding the landscape of large language models necessitates grasping several fundamental distinctions.</p>
<p>Firstly, models categorise by their core architectural types: encoder-based, such as <em>BERT</em>; decoder-based, exemplified by <em>GPT</em>; and hybrid encoder-decoder configurations.</p>
<p>Secondly, researchers employ diverse fine-tuning strategies to adapt these models for specific tasks.</p>
<p>A crucial differentiation lies in embeddings: word embeddings represent individual lexical units, whilst sentence embeddings capture the meaning of entire sentences. These two types operate at fundamentally different levels of abstraction. Beyond individual <em>LLMs</em>, the field distinguishes between pipelines, such as <em>Retrieval-Augmented Generation</em> (<em>RAG</em>), which combine multiple models, and agents, which represent complex systems integrating <em>LLMs</em> with a variety of external tools.</p>
</section>
<section id="categorisation-of-large-language-model-applications-in-hpss-research" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="categorisation-of-large-language-model-applications-in-hpss-research"><span class="header-section-number">3.8</span> Categorisation of Large Language Model Applications in HPSS Research</h2>
<p>An ongoing survey documents the applications of large language models as tools within History, Philosophy, and Sociology of Science (<em>HPSS</em>) research. This survey has identified four primary categories for classifying these applications.</p>
<p>The first category, “Dealing with data and sources,” focuses on how scholars interact with and locate their data. This includes tasks such as parsing and extracting specific information, exemplified by identifying publication types, acknowledgements, or citations.</p>
<p>The second category, “Knowledge structures,” concerns the analysis of knowledge organisation. Here, applications involve entity extraction—for instance, identifying scientific instruments, celestial bodies, or chemicals—alongside mapping science policy discourses and delineating interdisciplinary fields.</p>
<p>“Knowledge dynamics” constitutes the third category, addressing the evolution and change of knowledge over time. This encompasses conceptual histories of words—including the term “theory” in Digital Humanities or “virtual” and “Planck” in physics—and the reconstruction of arguments by identifying premises, conclusions, and causal relationships.</p>
<p>Finally, “Knowledge practices” forms the fourth category, focusing on the production and utilisation of knowledge. A notable example is citation context analysis, an established <em>HPSS</em> tradition that, whilst often employed for evaluatory purposes today, offers significant utility for other <em>HPSS</em> tasks.</p>
</section>
<section id="trends-and-challenges-in-hpss-applications-of-large-language-models" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="trends-and-challenges-in-hpss-applications-of-large-language-models"><span class="header-section-number">3.9</span> Trends and Challenges in HPSS Applications of Large Language Models</h2>
<p>Scholars observe an accelerating interest in large language models within the History, Philosophy, and Sociology of Science, extending beyond traditional computational journals, such as <em>Scientometrics</em> and <em>JASIS</em>, into publications not typically associated with computational methods. This expansion stems from the remarkable semantic power of <em>LLMs</em>, which increasingly attracts qualitative researchers and philosophers.</p>
<p>Applications demonstrate a wide spectrum of customisation, ranging from the straightforward, off-the-shelf use of tools such as <em>ChatGPT</em> to the intricate development of novel architectures and bespoke pre-training or fine-tuning. Despite this burgeoning interest, recurring concerns persist. These include the overwhelming computational resources demanded by <em>LLMs</em>, their inherent opaqueness, the scarcity of suitable training data, and the absence of standardised benchmarks. Furthermore, scholars consistently grapple with trade-offs between different model types, such as <em>BERT</em>-like versus <em>GPT</em>-like architectures, underscoring that no single model serves all purposes; rather, the appropriate model depends entirely on the specific research objective. Nevertheless, a discernible trend towards greater accessibility emerges, exemplified by tools like <em>BERTTopic</em> for topic modelling, which developers maintain meticulously to ensure ease of use.</p>
</section>
<section id="critical-reflections-on-integrating-large-language-models-into-hpss-research" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="critical-reflections-on-integrating-large-language-models-into-hpss-research"><span class="header-section-number">3.10</span> Critical Reflections on Integrating Large Language Models into HPSS Research</h2>
<p>Integrating large language models into History, Philosophy, and Sociology of Science research necessitates careful critical reflection, particularly concerning <em>HPSS</em>-specific challenges. A primary concern involves the historical evolution of concepts and language. Given that <em>LLMs</em> typically undergo training on modern language, scholars must devise strategies for adapting them to historical contexts whilst remaining acutely aware of potential biases. Furthermore, <em>HPSS</em> scholarship adopts a reconstructive and critically reflective perspective, demanding that scholars read between the lines, comprehend authorial context, and discern subtle discursive strategies, such as boundary work. <em>LLMs</em>, however, are not inherently trained to detect such nuances. Practical data challenges, including sparse datasets, the presence of multiple languages, and old scripts, further complicate their application.</p>
<p>Consequently, building <em>LLM</em> literacy becomes paramount. Scholars must thoroughly understand these tools, encompassing both their theoretical foundations and practical implications. This may entail acquiring coding skills, although natural language processing coding is progressively becoming more accessible. Crucially, scholars must avoid the superficial use of off-the-shelf tools without a profound comprehension of their outputs.</p>
<p>Finally, maintaining fidelity to <em>HPSS</em> methodologies is essential. Scholars must translate <em>HPSS</em> problems into <em>Natural Language Processing</em> tasks whilst steadfastly preserving their core <em>HPSS</em> focus. This prevents <em>NLP</em> tasks, such as classification, generation, or summarisation, from inadvertently “hijacking” the original research purpose. Nevertheless, <em>LLMs</em> present novel opportunities for bridging qualitative and quantitative research approaches. Moreover, scholars should reflect upon the pre-history of these models within <em>HPSS</em>, recognising connections to earlier developments like co-word analysis, pioneered by Callon and Rip in the 1980s, which itself emerged from <em>Actor-Network Theory</em>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>