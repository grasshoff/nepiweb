<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-06-21">

<title>3&nbsp; Large Language Models for the History, Philosophy, and Sociology of Science – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals, ActDisease Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG in HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit in Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#presentation-agenda-and-objectives" id="toc-presentation-agenda-and-objectives" class="nav-link" data-scroll-target="#presentation-agenda-and-objectives"><span class="header-section-number">3.1</span> Presentation Agenda and Objectives</a></li>
  <li><a href="#the-transformer-model-architecture" id="toc-the-transformer-model-architecture" class="nav-link" data-scroll-target="#the-transformer-model-architecture"><span class="header-section-number">3.2</span> The <em>Transformer</em> Model Architecture</a></li>
  <li><a href="#pre-trained-language-models-bert" id="toc-pre-trained-language-models-bert" class="nav-link" data-scroll-target="#pre-trained-language-models-bert"><span class="header-section-number">3.3</span> Pre-trained Language Models: <em>BERT</em></a></li>
  <li><a href="#pre-trained-language-models-gpt-and-model-distinctions" id="toc-pre-trained-language-models-gpt-and-model-distinctions" class="nav-link" data-scroll-target="#pre-trained-language-models-gpt-and-model-distinctions"><span class="header-section-number">3.4</span> Pre-trained Language Models: <em>GPT</em> and Model Distinctions</a></li>
  <li><a href="#evolution-of-scientific-large-language-models" id="toc-evolution-of-scientific-large-language-models" class="nav-link" data-scroll-target="#evolution-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution of Scientific Large Language Models</a></li>
  <li><a href="#domain-and-task-adaptation-of-large-language-models" id="toc-domain-and-task-adaptation-of-large-language-models" class="nav-link" data-scroll-target="#domain-and-task-adaptation-of-large-language-models"><span class="header-section-number">3.6</span> Domain and Task Adaptation of Large Language Models</a></li>
  <li><a href="#retrieval-augmented-generation-rag-for-domain-adaptation" id="toc-retrieval-augmented-generation-rag-for-domain-adaptation" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval Augmented Generation (<em>RAG</em>) for Domain Adaptation</a></li>
  <li><a href="#key-distinctions-in-large-language-models" id="toc-key-distinctions-in-large-language-models" class="nav-link" data-scroll-target="#key-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Models</a></li>
  <li><a href="#applications-of-large-language-models-in-hpss-research" id="toc-applications-of-large-language-models-in-hpss-research" class="nav-link" data-scroll-target="#applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in <em>HPSS</em> Research</a></li>
  <li><a href="#trends-and-concerns-in-llm-adoption-for-hpss" id="toc-trends-and-concerns-in-llm-adoption-for-hpss" class="nav-link" data-scroll-target="#trends-and-concerns-in-llm-adoption-for-hpss"><span class="header-section-number">3.10</span> Trends and Concerns in <em>LLM</em> Adoption for <em>HPSS</em></a></li>
  <li><a href="#critical-reflections-and-future-directions-for-llms-in-hpss" id="toc-critical-reflections-and-future-directions-for-llms-in-hpss" class="nav-link" data-scroll-target="#critical-reflections-and-future-directions-for-llms-in-hpss"><span class="header-section-number">3.11</span> Critical Reflections and Future Directions for <em>LLMs</em> in <em>HPSS</em></a></li>
  <li><a href="#additional-visual-materials" id="toc-additional-visual-materials" class="nav-link" data-scroll-target="#additional-visual-materials"><span class="header-section-number">3.12</span> Additional Visual Materials</a>
  <ul class="collapse">
  <li><a href="#slide-02" id="toc-slide-02" class="nav-link" data-scroll-target="#slide-02"><span class="header-section-number">3.12.1</span> Slide 02</a></li>
  <li><a href="#slide-08" id="toc-slide-08" class="nav-link" data-scroll-target="#slide-08"><span class="header-section-number">3.12.2</span> Slide 08</a></li>
  <li><a href="#slide-13" id="toc-slide-13" class="nav-link" data-scroll-target="#slide-13"><span class="header-section-number">3.12.3</span> Slide 13</a></li>
  <li><a href="#slide-14" id="toc-slide-14" class="nav-link" data-scroll-target="#slide-14"><span class="header-section-number">3.12.4</span> Slide 14</a></li>
  <li><a href="#slide-15" id="toc-slide-15" class="nav-link" data-scroll-target="#slide-15"><span class="header-section-number">3.12.5</span> Slide 15</a></li>
  <li><a href="#slide-16" id="toc-slide-16" class="nav-link" data-scroll-target="#slide-16"><span class="header-section-number">3.12.6</span> Slide 16</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>The authors meticulously detail the architecture and application of Large Language Models (<em>LLMs</em>) within the History, Philosophy, and Sociology of Science (<em>HPSS</em>). They commence by providing a foundational primer on <em>LLMs</em>, particularly focusing on the <em>Transformer</em> architecture and its derivatives, <em>BERT</em> and <em>GPT</em>. The discussion then transitions to specific applications within <em>HPSS</em>, categorising them into data handling, knowledge structuring, dynamics, and practices. Crucially, the authors identify significant challenges inherent in adapting <em>LLMs</em> for <em>HPSS</em> research, such as the historical evolution of language and the discipline’s critical, reconstructive perspective. They advocate for enhanced <em>LLM</em> literacy amongst researchers, the development of shared datasets, and a steadfast adherence to <em>HPSS</em> methodologies whilst embracing new opportunities for bridging qualitative and quantitative approaches. The speaker highlights Retrieval Augmented Generation (<em>RAG</em>) as a pivotal pipeline for domain adaptation without extensive model retraining, and underscores that contemporary “agents” represent complex systems of multiple <em>LLMs</em> integrated with diverse tools, rather than standalone models.</p>
</section>
<section id="presentation-agenda-and-objectives" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="presentation-agenda-and-objectives"><span class="header-section-number">3.1</span> Presentation Agenda and Objectives</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.png" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>The authors establish a clear agenda, outlining three core components: a primer on Large Language Models, an exploration of their applications within the History, Philosophy, and Sociology of Science, and a series of critical reflections intended to stimulate workshop discussions. Recognising the diverse technical backgrounds amongst the audience, the speaker prioritises delivering a concise yet comprehensive introduction to <em>LLMs</em>, particularly focusing on their adaptation for scientific domains. Beyond this foundational overview, the session aims to summarise existing applications within <em>HPSS</em> and to present key considerations for future research and engagement with these powerful tools.</p>
</section>
<section id="the-transformer-model-architecture" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-model-architecture"><span class="header-section-number">3.2</span> The <em>Transformer</em> Model Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.png" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>At the heart of contemporary Large Language Models resides the renowned <em>Transformer</em> architecture, originally conceptualised by Vaswani and colleagues in their seminal 2017 paper, “<em>Attention is All You Need</em>”. The engineers initially designed this model for language translation tasks, such as converting German text into English. This architecture fundamentally comprises two interconnected streams: an Encoder on the left and a Decoder on the right.</p>
<p>The Encoder processes input words, transforming them into numerical representations. Crucially, it reads the entire input sentence at once, enabling each word to interact bidirectionally with every other word. This comprehensive interaction allows the Encoder to construct a full, contextualised representation of the sentence’s complete meaning. Conversely, the Decoder receives these encoded numerical representations and generates output words sequentially. The Decoder then feeds each produced word back into its own input, creating a loop. A key distinction lies in the Decoder’s unidirectional nature: it can only access preceding words to predict the next word, preventing it from “looking ahead” into the future. Within both the Encoder and Decoder, various layers perform complex numerical computations, progressively refining and contextualising the word embeddings.</p>
</section>
<section id="pre-trained-language-models-bert" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="pre-trained-language-models-bert"><span class="header-section-number">3.3</span> Pre-trained Language Models: <em>BERT</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Following the introduction of the <em>Transformer</em> model, researchers swiftly re-engineered its individual streams to develop pre-trained language models, marking a significant shift in the field. This innovation moved beyond direct translation, focusing instead on crafting models capable of profound language comprehension and generation. These pre-trained models subsequently serve as robust foundations, requiring only minor or additional training to accomplish diverse Natural Language Processing tasks.</p>
<p>One prominent example is <em>BERT</em>, an encoder-side model that remains widely prevalent. <em>BERT</em> operates on the principle that every word within an input stream can interact with all other words. This bidirectional, full-contextual understanding allows the model to construct a comprehensive representation of the entire input at once.</p>
</section>
<section id="pre-trained-language-models-gpt-and-model-distinctions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="pre-trained-language-models-gpt-and-model-distinctions"><span class="header-section-number">3.4</span> Pre-trained Language Models: <em>GPT</em> and Model Distinctions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>Conversely, the <em>GPT</em> models, or Generative Pre-trained Transformers, represent the decoder-side evolution of the <em>Transformer</em> architecture. These models power prominent generative AI systems such as <em>ChatGPT</em>. Their unidirectional structure, which permits them to consider only preceding words, enables them to generate entirely new text, a capability not inherent in <em>BERT</em> models. This fundamental difference delineates their primary functions: <em>GPT</em> models excel at producing novel language, whereas <em>BERT</em>-like models demonstrate superior coherence in understanding existing sentences. Beyond these two primary types, researchers have also developed models that combine encoder and decoder elements, and have even devised sophisticated methods to enable decoders to operate with encoder-like functionalities, as exemplified by models such as <em>XLM</em> and <em>XLNet</em>.</p>
</section>
<section id="evolution-of-scientific-large-language-models" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evolution-of-scientific-large-language-models"><span class="header-section-number">3.5</span> Evolution of Scientific Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Ho and colleagues’ 2024 work, “<em>A Survey of Pre-trained Language Models for Processing Scientific Text</em>”, provides a comprehensive overview charting the evolution of Large Language Models from 2018 to 2024, specifically highlighting their development for scientific domains and tasks. Their intricate diagram categorises models into Encoders (typified by <em>BERT</em>), Decoders (typified by <em>GPT</em>), Encoder-Decoders, and other less common architectures. Notably, the Encoder category exhibits a far greater population of models compared to the Decoder category within scientific applications. This detailed survey identifies early influential models such as <em>BioBERT</em>, <em>Specter</em>, and <em>Cyber</em>. The observed proliferation of encoder models suggests their particular relevance and potential utility for researchers in the History, Philosophy, and Sociology of Science, who might either leverage these existing models or embark upon developing bespoke solutions.</p>
</section>
<section id="domain-and-task-adaptation-of-large-language-models" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="domain-and-task-adaptation-of-large-language-models"><span class="header-section-number">3.6</span> Domain and Task Adaptation of Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>Adapting Large Language Models to specific scientific language necessitates several distinct training approaches. Initial pretraining, where models learn language by predicting subsequent tokens, as in <em>GPT</em>, or by predicting masked words, as in <em>BERT</em>, demands an exorbitant amount of computational power and data, rendering it largely impractical for individual researchers. A more accessible strategy involves continued pretraining, where an already pre-trained model, such as <em>BERT</em>, undergoes further training on domain-specific language; for instance, the speaker and Micha applied this method to physics texts.</p>
<p>Beyond this, researchers can implement fine-tuning by adding extra layers to pre-trained models and training them to function as classifiers for tasks like sentiment analysis or named entity recognition. Whilst prompt-based adaptation receives a brief mention, contrastive learning emerges as a pivotal method for generating sentence or document embeddings from existing word embeddings. This technique proves crucial for positioning documents and sentences within the same embedding space as individual words, thereby facilitating comparative analysis. <em>Sentence BERT</em>, a widely utilised model, exemplifies this approach, and its intricacies may feature in forthcoming discussions by Irina Gurevich.</p>
</section>
<section id="retrieval-augmented-generation-rag-for-domain-adaptation" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="retrieval-augmented-generation-rag-for-domain-adaptation"><span class="header-section-number">3.7</span> Retrieval Augmented Generation (<em>RAG</em>) for Domain Adaptation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>Retrieval Augmented Generation, or <em>RAG</em>, represents a sophisticated pipeline approach to domain adaptation, fundamentally differing from the direct training of a singular model. This system orchestrates the concerted action of at least two or more models, often integrating additional tools to achieve its objectives.</p>
<p>The process commences with retrieval: a <em>BERT</em>-type model encodes a user’s query, such as “What are <em>LLMs</em>?”, into a precise sentence embedding. Subsequently, this embedding facilitates a search within a dedicated document database, identifying and extracting the most semantically similar passages. These retrieved sentences or passages then become integral to the generation phase. Here, the system seamlessly incorporates them into the prompt of a generative model, typically a <em>GPT</em>-type architecture. This model then synthesises an answer, critically informed by the newly provided context. Modern generative AI systems, including current iterations of <em>ChatGPT</em>, extensively employ <em>RAG</em> for tasks such as internet searching and presenting relevant results. Crucially, this paradigm underscores a broader trend: contemporary reasoning models and “agents” are not monolithic Large Language Models but rather intricate systems that combine <em>LLMs</em> with a diverse array of supplementary tools.</p>
</section>
<section id="key-distinctions-in-large-language-models" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="key-distinctions-in-large-language-models"><span class="header-section-number">3.8</span> Key Distinctions in Large Language Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>To understand Large Language Models, one must grasp several key distinctions. Firstly, their fundamental architecture and pretraining methodologies vary significantly, encompassing Encoder-based models like <em>BERT</em>, Decoder-based models such as <em>GPT</em>, and hybrid Encoder-Decoder configurations. Secondly, a diverse array of fine-tuning strategies exists, each designed to adapt these models for specific tasks. Thirdly, a critical conceptual difference lies between word embeddings and sentence embeddings; whilst both capture semantic meaning, they operate at distinct levels of abstraction. Finally, the level of abstraction itself provides a crucial categorisation, moving from individual Large Language Models to complex pipelines, exemplified by Retrieval Augmented Generation, and culminating in sophisticated agents that integrate multiple <em>LLMs</em> with a suite of other tools.</p>
</section>
<section id="applications-of-large-language-models-in-hpss-research" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="applications-of-large-language-models-in-hpss-research"><span class="header-section-number">3.9</span> Applications of Large Language Models in <em>HPSS</em> Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>An ongoing survey systematically categorises the diverse applications of Large Language Models within History, Philosophy, and Sociology of Science research into four principal areas:</p>
<ul>
<li><p><em>Dealing with Data and Sources</em>: This area encompasses functionalities such as parsing and extracting specific information, including publication types, acknowledgements, and citations. It also involves more interactive engagements with sources, facilitating summarisation and <em>RAG</em>-type conversational interfaces.</p></li>
<li><p><em>Knowledge Structures</em>: This category focuses on extracting entities like scientific instruments, celestial bodies, and chemicals. Furthermore, it enables the mapping of complex relationships, including disciplinary boundaries, interdisciplinary fields, and science-policy discourses.</p></li>
<li><p><em>Knowledge Dynamics</em>: This third area sees <em>LLMs</em> assisting in tracing conceptual histories, exemplified by tracking terms such as “theory” within Digital Humanities or “virtual” and “Planck” in physics. This category also extends to identifying novelty, pinpointing breakthrough papers and emerging technologies.</p></li>
<li><p><em>Knowledge Practices</em>: Finally, this area delves into the analysis of how knowledge is constructed and communicated. This includes argument reconstruction, discerning premises, conclusions, and causal links. It also encompasses citation context analysis, a long-standing <em>HPSS</em> tradition now frequently employed for evaluatory purposes, which determines the purpose and sentiment behind citations. Moreover, <em>LLMs</em> facilitate discourse analysis, identifying nuanced linguistic features such as hedge sentences, specialised jargon, and instances of boundary work.</p></li>
</ul>
</section>
<section id="trends-and-concerns-in-llm-adoption-for-hpss" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="trends-and-concerns-in-llm-adoption-for-hpss"><span class="header-section-number">3.10</span> Trends and Concerns in <em>LLM</em> Adoption for <em>HPSS</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>The adoption of Large Language Models within academic research demonstrates an accelerating interest, extending beyond traditionally computational journals like <em>Scientometrics</em> and <em>JASIST</em> into publications not typically associated with computational methods. This expansion largely stems from the models’ potent semantic capabilities, which appeal to qualitative researchers and philosophers alike.</p>
<p>Researchers exhibit varying degrees of customisation in their <em>LLM</em> usage, ranging from straightforward, off-the-shelf applications of <em>ChatGPT</em> to the development of entirely new architectures and bespoke pretraining regimes. Despite this burgeoning interest, several recurring concerns persist. These include the prohibitive computational resources often required, the inherent opaqueness of model operations, a pervasive lack of suitable training data, and the absence of standardised benchmarks for evaluation. Furthermore, researchers consistently encounter trade-offs between different model types, recognising that no single model serves all purposes; rather, the selection of an adequate model remains contingent upon the specific research objective. Nevertheless, a discernible trend towards greater accessibility is emerging, exemplified by tools such as <em>BERTopic</em>, which offers a user-friendly and well-maintained solution for topic modelling.</p>
</section>
<section id="critical-reflections-and-future-directions-for-llms-in-hpss" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="critical-reflections-and-future-directions-for-llms-in-hpss"><span class="header-section-number">3.11</span> Critical Reflections and Future Directions for <em>LLMs</em> in <em>HPSS</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Critical reflections on the integration of Large Language Models into <em>HPSS</em> research highlight several discipline-specific challenges. The historical evolution of concepts and language poses a significant hurdle, as <em>LLMs</em> are predominantly trained on contemporary language. This necessitates either the development of bespoke models or the careful application of existing ones, whilst actively mitigating inherent biases when analysing historical texts. Furthermore, <em>HPSS</em> researchers typically adopt a reconstructive and critically reflective perspective, reading between the lines to understand authorial context and subtle discursive strategies, such as boundary work. <em>LLMs</em> are not inherently trained to detect such nuances, thus demanding innovative methods to enable this deeper form of “reading.” Practical data challenges, including sparse datasets, the presence of multiple languages, and archaic scripts, further complicate their application.</p>
<p>To navigate these complexities, several recommendations emerge for <em>HPSS</em> researchers:</p>
<ul>
<li><p>Cultivating <em>LLM</em> literacy is paramount, requiring a thorough understanding of both the theoretical underpinnings and practical implications of these tools.</p></li>
<li><p>Acquiring coding skills remains essential, even as <em>NLP</em> coding interfaces become more intuitive; this prevents the uncritical adoption of off-the-shelf tools that might yield visually appealing but ultimately uninterpretable outputs.</p></li>
<li><p>The development of shared datasets and standardised benchmarks is crucial for fostering collaborative progress and ensuring rigorous evaluation.</p></li>
<li><p>Researchers must remain steadfast in their <em>HPSS</em> methodologies, carefully translating disciplinary problems into <em>NLP</em> tasks without allowing the technical objectives of classification, generation, or summarisation to “hijack” the core research purpose.</p></li>
</ul>
<p>Despite these challenges, <em>LLMs</em> present compelling new opportunities. They offer a unique pathway for bridging qualitative and quantitative research approaches, fostering greater methodological integration within <em>HPSS</em>. Moreover, these developments prompt a valuable reflection on the discipline’s own history, drawing connections to the pre-history of related methods, such as the co-word analysis pioneered by Colon and Ari Rip in the 1980s, which itself emerged from an Actor-Network Theory framework. A compelling example of <em>LLMs</em>’ transformative potential lies in their capacity to track the evolution of word meanings over time through contextualised word embeddings. This enables researchers to distinguish various senses of a term, such as “Planck” (referring to Max Planck, the Planck Institutes, or the Planck satellite mission), and to observe their changing dominance within historical physics archives—a feat previously unattainable without <em>LLMs</em>. Ultimately, these models hold the promise of addressing core <em>STS</em> and <em>HPSS</em> problems, including the nuanced understanding of paradigm shifts, from Newton’s physics to Einstein’s relativity.</p>
</section>
<section id="additional-visual-materials" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="additional-visual-materials"><span class="header-section-number">3.12</span> Additional Visual Materials</h2>
<p>The following slides provide supplementary visual information that complements the main presentation content:</p>
<section id="slide-02" class="level3" data-number="3.12.1">
<h3 data-number="3.12.1" class="anchored" data-anchor-id="slide-02"><span class="header-section-number">3.12.1</span> Slide 02</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>This slide presents the agenda for the presentation, titled ‘<em>Today’s Menu</em>’. The main sections outlined are: ‘Primer on <em>LLMs</em>’, ‘Applications in <em>HPSS</em>’ (History, Philosophy, and Sociology of Science), and ‘Reflections’. The visual is a chalkboard, reinforcing the ‘menu’ theme.</p>
</section>
<section id="slide-08" class="level3" data-number="3.12.2">
<h3 data-number="3.12.2" class="anchored" data-anchor-id="slide-08"><span class="header-section-number">3.12.2</span> Slide 08</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>This slide offers a comprehensive comparison of <em>BERT</em> and <em>GPT</em> models within the context of the <em>Transformer</em> architecture. On the left, the ‘<em>BERT</em>’ model is shown as ‘bidirectional full-context’, associated with the ‘Encoder’ part of the <em>Transformer</em>. On the right, a new ‘<em>GPT</em>’ block is introduced, described as ‘unidirectional generative’ and associated with the ‘Decoder’. Both <em>BERT</em> and <em>GPT</em> blocks include ‘<em>LLMs</em> for <em>HPSS</em> ?’ and ‘Vocab’ labels. The central <em>Transformer</em> model architecture remains fully visible. Three citations are present: ‘Devlin et al.&nbsp;2018. <em>BERT: Pre-training of…</em>’, ‘Vaswani et al.&nbsp;2017: <em>Attention is all you need</em>’, and ‘Radford et al.&nbsp;2018. <em>Improving Language…</em>’. This slide appears to be the complete version of the model comparison.</p>
</section>
<section id="slide-13" class="level3" data-number="3.12.3">
<h3 data-number="3.12.3" class="anchored" data-anchor-id="slide-13"><span class="header-section-number">3.12.3</span> Slide 13</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>This slide outlines key distinctions related to Large Language Models. It presents four main categories:</p>
<ul>
<li><p><em>Architecture/Pretraining</em>, distinguishing between ‘Encoder-based vs.&nbsp;Decoder-based vs.&nbsp;Encoder-Decoder-based’ models.</p></li>
<li><p><em>Fine-tuning</em>, noting ‘Various strategies’.</p></li>
<li><p><em>Embeddings</em>, differentiating between ‘Word vs.&nbsp;Sentence’ embeddings.</p></li>
<li><p><em>Level of abstraction</em>, comparing ‘<em>LLMs</em> vs.&nbsp;Pipelines vs.&nbsp;Agents’.</p></li>
</ul>
<p>This slide provides a structured overview of important conceptual differences in the field of <em>LLMs</em>.</p>
</section>
<section id="slide-14" class="level3" data-number="3.12.4">
<h3 data-number="3.12.4" class="anchored" data-anchor-id="slide-14"><span class="header-section-number">3.12.4</span> Slide 14</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>This slide details applications in <em>HPSS</em> (History, Philosophy, and Sociology of Science). It categorises applications into four areas:</p>
<ul>
<li><p><em>Dealing with data and sources</em>, including ‘Parsing and extracting (publication types, acknowledgements, citations)’ and ‘Interacting with sources (summarization, <em>RAG</em>-type chatting)’.</p></li>
<li><p><em>Knowledge structures</em>, covering ‘Entity extraction (scientific instruments, celestial bodies, chemicals)’ and ‘Mappings (disciplines, interdisciplinary fields, science-policy discourses)’.</p></li>
<li><p><em>Knowledge dynamics</em>, with ‘Conceptual histories (’theory’ in DH, ‘virtual’ and ‘Planck’ in physics)’ and ‘Novelty (breakthrough papers, emerging technologies)’.</p></li>
<li><p><em>Knowledge practices</em>, which includes ‘Argument reconstruction (premises &amp; conclusions, causality)’, ‘Citation context analysis (purpose, sentiment)’, and ‘Discourse analysis (hedge sentences, jargon, boundary work)’.</p></li>
</ul>
<p>This slide provides concrete examples of how <em>LLMs</em> can be applied in <em>HPSS</em> research.</p>
</section>
<section id="slide-15" class="level3" data-number="3.12.5">
<h3 data-number="3.12.5" class="anchored" data-anchor-id="slide-15"><span class="header-section-number">3.12.5</span> Slide 15</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>This slide continues the discussion on applications in <em>HPSS</em>, focusing on broader trends and challenges. It notes accelerating interest in <em>LLMs</em>, even in non-computational journals. It highlights varying degrees of customisation, ranging ‘From architectural tweakings and custom pretraining over custom fine-tuning to off-the-shelf use of <em>ChatGPT</em>’. The slide also addresses repeating concerns such as ‘Overwhelming computational resources, opaqueness, lack of training data, lack of benchmarks, trade-offs between model types (<em>BERT</em>-like vs.&nbsp;<em>GPT</em>-like)’. Finally, it points to a trend toward accessibility, exemplified by ‘<em>BERTopic</em> as the new <em>pyLDAvis</em>?’. This slide provides a critical perspective on the adoption and challenges of <em>LLMs</em> in <em>HPSS</em>.</p>
</section>
<section id="slide-16" class="level3" data-number="3.12.6">
<h3 data-number="3.12.6" class="anchored" data-anchor-id="slide-16"><span class="header-section-number">3.12.6</span> Slide 16</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>This slide presents reflections on the use of <em>LLMs</em> in <em>HPSS</em>. It identifies three key areas:</p>
<ul>
<li><p><em>Acknowledging HPSS-specific challenges</em>, including ‘Historical evolution of concepts and language’, ‘Reconstructive perspective, reading between the lines, reflecting social implications’, and ‘Sparse data, multiple languages, old scripts, lack of digitalization’.</p></li>
<li><p><em>Building LLM literacy</em>, which involves familiarising with ‘<em>LLMs</em>, <em>NLP</em>, and <em>DL</em>, both tools and theory’, learning ‘what’s the right architecture and training for our problems’, and developing ‘our own shared datasets and benchmarks’.</p></li>
<li><p><em>Staying true to HPSS methodologies</em>, by translating ‘<em>HPSS</em> problems into <em>NLP</em> tasks without losing our focus’, exploring ‘New opportunities for bridging qualitative and quantitative approaches’, and reflecting on ‘<em>HPSS</em>’ own role in <em>LLM</em> pre-history (e.g.&nbsp;co-word analysis)’.</p></li>
</ul>
<p>This slide offers a forward-looking and critical summary of the integration of <em>LLMs</em> into <em>HPSS</em>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="Introducing OpenAlex Mapper">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>