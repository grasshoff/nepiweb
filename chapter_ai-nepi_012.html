<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paul Näger">
<meta name="dcterms.date" content="2025-01-01">

<title>12&nbsp; Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_015.html" rel="next">
<link href="./chapter_ai-nepi_011.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-09b140d2d032adf2aedb8b099be3ee13.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-350fb9e808f7eb2950c9598fb3f8c4a0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_012.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_001_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_003_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_004_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Workflow and Utility of OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_005_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_006_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_007_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-nepi_008_chapter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science Dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of <em>LDA</em> and <em>BERTopic</em> Performance across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Beyond Traditional Views of Science Funding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation" id="toc-addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation" class="nav-link" data-scroll-target="#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation"><span class="header-section-number">12.1</span> Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation</a></li>
  <li><a href="#applications-of-rag-systems-in-philosophical-scholarship" id="toc-applications-of-rag-systems-in-philosophical-scholarship" class="nav-link" data-scroll-target="#applications-of-rag-systems-in-philosophical-scholarship"><span class="header-section-number">12.2</span> Applications of RAG Systems in Philosophical Scholarship</a></li>
  <li><a href="#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project" id="toc-developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project" class="nav-link" data-scroll-target="#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project"><span class="header-section-number">12.3</span> Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project</a></li>
  <li><a href="#sep-rag-system-interface-and-functionality-details" id="toc-sep-rag-system-interface-and-functionality-details" class="nav-link" data-scroll-target="#sep-rag-system-interface-and-functionality-details"><span class="header-section-number">12.4</span> SEP RAG System: Interface and Functionality Details</a></li>
  <li><a href="#optimising-document-chunking-for-philosophical-corpora" id="toc-optimising-document-chunking-for-philosophical-corpora" class="nav-link" data-scroll-target="#optimising-document-chunking-for-philosophical-corpora"><span class="header-section-number">12.5</span> Optimising Document Chunking for Philosophical Corpora</a></li>
  <li><a href="#implementing-reranking-to-enhance-retrieval-relevance" id="toc-implementing-reranking-to-enhance-retrieval-relevance" class="nav-link" data-scroll-target="#implementing-reranking-to-enhance-retrieval-relevance"><span class="header-section-number">12.6</span> Implementing Reranking to Enhance Retrieval Relevance</a></li>
  <li><a href="#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy" id="toc-overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy" class="nav-link" data-scroll-target="#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy"><span class="header-section-number">12.7</span> Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Paul Näger <a href="mailto:paul.naeger@cis.lmu.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            LMU München
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Philosophical inquiry, a discipline demanding exceptional linguistic and semantic precision, increasingly explores the application of Retrieval Augmented Generation (RAG) systems. Standard Large Language Models (LLMs), however, pose significant limitations for rigorous philosophical research. These include restricted access to complete textual sources, a propensity for hallucination, an inability to learn texts verbatim, and constrained context windows.</p>
<p>RAG architecture directly addresses these issues. By integrating a curated data corpus, a sophisticated retrieval mechanism, and prompt augmentation with retrieved text, RAG enables direct text access, manages extensive corpora, and crucially, facilitates robust source attribution. Its potential applications in philosophy are broad, encompassing both didactic and research uses. In didactics, RAG systems could allow students to interactively explore complex texts, such as Locke’s <em>Essay concerning Human Understanding</em>. For research, applications range from efficient fact retrieval from handbooks and the exploration of previously unexamined corpora, to the identification of specific passages for close reading, and ultimately, the potential to answer nuanced research questions.</p>
<p>To investigate these promising applications, researchers constructed an example RAG system, utilising the <em>Stanford Encyclopedia of Philosophy</em> (SEP) as its primary data source. They meticulously scraped the SEP’s content, converting it into markdown format. Initial development, however, revealed surprisingly poor performance from basic RAG configurations, prompting a qualitative study into optimal system architectures. This comprehensive study necessitated extensive refinement of various components. Researchers meticulously tweaked model choices, including generative LLMs like <em>gpt-4o-mini</em> and embedding models, alongside numerous hyperparameters such as <em>top-k</em>, token limits, temperature, chunk size, and overlap. They also explored algorithmic enhancements, notably reranking mechanisms. Evaluating the unstructured text answers, which frequently articulated complex philosophical propositions, proved particularly challenging, underscoring the critical need for robust evaluation standards.</p>
<p>Crucial findings emerged regarding chunking strategies: these profoundly impact system performance. For the highly systematised SEP, treating entire main sections—averaging 3,000 words—as individual retrievable documents yielded demonstrably superior results. This outcome proved counter-intuitive, as these sections considerably exceeded the embedding model’s typical input length of approximately 500 words. Furthermore, reranking retrieved documents using a generative LLM to assess relevance substantially enhanced performance. This process, which scores documents based on informativeness and the length of relevant passages, significantly improved the quality of generated answers, albeit at an increased computational cost. Whilst RAG systems adeptly integrate verbatim corpora, mitigate hallucinations, and provide citations, they demand meticulous, corpus-specific tuning. A significant challenge, however, became apparent: RAG systems may underperform on broad overview questions. Their inherent focus on locally retrieved information can inadvertently obscure the larger conceptual landscape. This observation points towards a compelling need for more flexible, perhaps agentic, RAG systems, capable of discerning and adapting to diverse question types.</p>
</section>
<section id="addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation"><span class="header-section-number">12.1</span> Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>Philosophical inquiry frequently grapples with intricate questions. Consider, for instance, elucidating Aristotle’s theory of matter within the <em>Physics</em>, or tracing the evolution of Einstein’s concept of locality from his early works on relativity to his 1948 paper addressing quantum mechanics and ‘Wirklichkeit’. Whilst standard Large Language Models (LLMs) like <em>ChatGPT</em> can generate superficially plausible and differentiated responses to such queries, they exhibit significant limitations when applied to rigorous philosophical research.</p>
<p>A primary constraint involves access to textual sources. LLMs typically lack dynamic access to the full text of scholarly works, even if those texts formed part of their training data. Consequently, requests for specific quotations from chapters or papers may lead to hallucinations or an admission of inability. Even when online search capabilities are activated, copyright restrictions can prevent the reproduction of material.</p>
<p>Furthermore, the fundamental training mechanisms of LLMs are engineered to avoid mere parroting of texts. Instead, they learn generalisable statistical patterns of language production, with explicit mechanisms preventing verbatim memorisation. This contrasts sharply with the needs of philosophical research, which hinges on meticulous engagement with original source materials and their precise, fine-grained formulations. The limited context window of current LLMs—for instance, <em>ChatGPT-4o</em>’s 128,000 tokens—also poses a significant hurdle when dealing with extensive philosophical corpora.</p>
<p>To surmount these challenges, researchers propose Retrieval Augmented Generation (RAG) systems. A RAG system’s architecture typically involves a curated data source, such as the complete corpus of Aristotle’s or Einstein’s writings. From this source, documents are retrieved using methods like semantic search, hybrid approaches, or traditional keyword search. These retrieved documents, or relevant chunks thereof, then augment the user’s original prompt before processing by the LLM. This setup directly tackles the problem of text access, provides a mechanism for managing large context sizes by focusing on relevant segments, and crucially, facilitates attribution by enabling the system to cite the sources for its generated claims, much like the numbered citations seen in tools such as <em>Perplexity AI</em>.</p>
</section>
<section id="applications-of-rag-systems-in-philosophical-scholarship" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="applications-of-rag-systems-in-philosophical-scholarship"><span class="header-section-number">12.2</span> Applications of RAG Systems in Philosophical Scholarship</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_09.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>RAG systems offer a promising avenue for interacting with philosophical corpora, distinguished by their capacity to integrate detailed domain knowledge and rely on verbatim textual evidence. This capability opens several valuable applications within philosophical scholarship, broadly categorised into didactic and research uses.</p>
<p>In the realm of didactics, RAG systems can transform how students engage with challenging philosophical texts. For instance, students approaching Locke’s <em>Essay concerning Human Understanding</em> can benefit immensely from the ability to pose repeated questions. This iterative process proves highly instructive, allowing them to start with broad inquiries, such as “What is Locke’s general idea?”, and progressively delve into more specific aspects, like “What is his idea in epistemology?” or “What is his theory of matter?”. Such interactions foster a deeper, more nuanced understanding of the material.</p>
<p>Beyond educational settings, RAG systems hold significant potential for research. They can streamline the process of looking up facts in handbooks, a task that traditionally involved manually searching physical volumes for information, perhaps for a footnote. RAG offers a more efficient method, potentially with greater reliability than relying on the unverified outputs of standard LLMs. Furthermore, researchers can employ RAG systems to explore unexamined corpora; once digitised, collections of unpublished manuscripts or less-studied texts can be “chatted with” to gain an overview and deeper insights into their content. Another key research application is the identification of specific passages relevant to a particular research question, thereby facilitating focused close reading. Ultimately, the aspiration is that RAG systems might, at some point, become capable of providing detailed answers to at least certain components of complex philosophical research questions.</p>
</section>
<section id="developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project"><span class="header-section-number">12.3</span> Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_09.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>Researchers embarked on a project to construct an example RAG system, selecting the <em>Stanford Encyclopedia of Philosophy</em> (SEP)—a widely respected online handbook—as the primary data source. The initial step involved scraping the SEP’s content and converting it into markdown format. Originally, the ambition was to develop a directly useful tool for the philosophical community. However, early attempts to implement a standard textbook RAG system, comprising distinct retrieval and generation components, produced answers of surprisingly poor quality; indeed, these initial outputs were often inferior to those obtainable from a standalone LLM like <em>ChatGPT</em>.</p>
<p>This experience prompted a significant shift in focus. The project evolved into a qualitative study aimed at determining the optimal configuration for RAG systems tailored to the specific demands of philosophical texts. Achieving improved performance necessitated a meticulous process of refining numerous aspects of the system. This involved careful selection of both the generative LLM and the embedding model responsible for understanding text semantics. Extensive hyperparameter tuning became essential, covering parameters such as <em>top-k</em> (the number of documents retrieved), maximum input and output token lengths, the temperature or <em>top-p</em> settings influencing the creativity of the generated text, and the strategies for chunk size and overlap in document segmentation. Beyond parameter adjustments, researchers explored more complex algorithmic solutions, such as implementing reranking mechanisms to mitigate problems like semantic mismatch between the query and retrieved documents.</p>
<p>The methodology for enhancing the system was predominantly one of trial and error, guided by theoretical insights into how RAG components interact. A significant hurdle encountered throughout this process was the evaluation of the system’s output. Philosophical RAG systems generate answers in free, unstructured text, often articulating complex propositions rather than simple atomic facts (unlike, for example, a historical query seeking Wittgenstein’s last place of living, which expects a city name). Consequently, robust evaluation standards are paramount to assess whether these generated propositions accurately convey the intended philosophical concepts and facts, a non-trivial task.</p>
</section>
<section id="sep-rag-system-interface-and-functionality-details" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="sep-rag-system-interface-and-functionality-details"><span class="header-section-number">12.4</span> SEP RAG System: Interface and Functionality Details</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_09.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>The developed <em>Stanford Encyclopedia of Philosophy RAG</em> (<em>SEP RAG</em>) system features a user-facing frontend and a backend constructed with Python, amounting to a few thousand lines of code. The frontend interface provides users with considerable control over the generation process. Within its input section, users can configure several key parameters: they can select the generative model (with <em>gpt-4o-mini</em> shown as an example), view the chosen model’s maximum prompt token limit (e.g., 128,000 tokens), and set a specific prompt token limit for the RAG system’s input (e.g., 15,000 tokens). Additionally, users can define a persona for the LLM—for instance, “You are an expert philosopher. You answer meticulously and precisely”—and specify the number of texts to retrieve for context (e.g., 15). A dedicated field allows for the input of a philosophical question, such as “What is priority monism?”, followed by a “Generate answer” button to initiate the process.</p>
<p>Upon generation, the system presents its output in a structured manner. Notably, it offers a comparative view, displaying the answer from a standalone LLM (serving as a benchmark) alongside the answer produced by the <em>SEP RAG</em> system. This side-by-side presentation facilitates a more effective assessment of the RAG system’s contribution. Furthermore, the output includes a detailed list of the texts retrieved from the SEP. This list specifies the names of the articles and the particular section headings that the system identified as relevant. Crucially, it also indicates which of these retrieved texts were ultimately included in the augmented prompt passed to the LLM and which, if any, were truncated due to the imposed prompt length limitations.</p>
</section>
<section id="optimising-document-chunking-for-philosophical-corpora" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="optimising-document-chunking-for-philosophical-corpora"><span class="header-section-number">12.5</span> Optimising Document Chunking for Philosophical Corpora</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_15.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>Researchers dedicated particular attention to optimising the hyperparameter of chunk size, which dictates how documents are segmented for retrieval and processing. They explored several distinct options for this segmentation. One approach involved a fixed number of words or tokens, for example, 500, a method often favoured in computer science for its straightforward implementation. Another considered using natural paragraph breaks as delimiters. A third strategy focused on segmenting the source material by its inherent sections, potentially at various levels of the document hierarchy.</p>
<p>Through experimentation with the <em>Stanford Encyclopedia of Philosophy</em>, a clear finding emerged: the most effective results were achieved when entire main sections of SEP articles were treated as the individual “documents” for retrieval. This outcome was somewhat surprising because the average length of these main sections—around 3,000 words—considerably surpassed the input limit of the embedding model, which could process only a little over 500 words at a time.</p>
<p>The proposed explanation for this counterintuitive success rests on the specific nature of the SEP. It is a highly systematised and meticulously structured encyclopedic work. Within such a well-organised corpus, the initial portion of a main section (the first 500 words or so that the embedding model can ingest) likely contains enough concentrated information to represent the semantic core of the entire section adequately. However, it is important to note a caveat: this successful strategy of using large, section-level chunks may not be universally applicable. Its efficacy is probably tied to the SEP’s unique characteristics and might not translate effectively to more heterogeneous textual collections or corpora that lack such a high degree of internal systematisation and clear structural demarcation.</p>
</section>
<section id="implementing-reranking-to-enhance-retrieval-relevance" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="implementing-reranking-to-enhance-retrieval-relevance"><span class="header-section-number">12.6</span> Implementing Reranking to Enhance Retrieval Relevance</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_18.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>Researchers identified that an initial retrieval process, even one based on semantic similarity, can sometimes include documents that are not genuinely relevant to the user’s specific question—these are known as false positives. To address this limitation, they incorporated an additional step: reranking. The primary aim of reranking is to re-evaluate and reorder the initially retrieved set of documents, arranging them according to their true relevance to the posed query.</p>
<p>The implemented solution involves leveraging a generative Large Language Model (gLLM) to perform this relevance assessment. This choice stems from the understanding that gLLMs exhibit more sophisticated semantic differentiation capabilities than embedding models can offer on their own. Consequently, a gLLM can provide a more nuanced and accurate judgement of how well each retrieved text pertains to the question. During the reranking process, the gLLM scores each candidate document based on specific categories, notably its informativeness concerning the query and the length of the relevant passage contained within it. These individual scores are then aggregated into a “Total Score,” which quantifies the overall relevance of each document.</p>
<p>The introduction of this reranking stage proved highly effective. Evaluations demonstrated that it leads to very good results, significantly enhancing the quality and relevance of the documents ultimately used to generate the answer. However, this improvement comes at a cost: the reranking step, by invoking a powerful gLLM for each retrieved document, substantially multiplies the computational resources required, which in turn can increase the monetary expense of operating the RAG system.</p>
</section>
<section id="overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy"><span class="header-section-number">12.7</span> Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_18.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>Retrieval Augmented Generation systems present several distinct advantages for philosophical scholarship. They can seamlessly integrate verbatim corpora, ensuring that answers are grounded in authentic textual evidence, and can effectively incorporate domain-specific and specialised knowledge. These capabilities lead to the generation of more detailed answers and, crucially, a dramatic reduction in the incidence of hallucinations. Furthermore, the ability of RAG systems to cite the relevant documents underpinning their responses directly supports scientific rigour and verifiability, making them, in principle, well-suited for assisting in scholarly tasks.</p>
<p>Nevertheless, several points of caution warrant consideration. RAG systems are not “plug-and-play” solutions; they inherently demand extensive and continuous tweaking to achieve optimal performance. The ideal settings for hyperparameters and model choices are not universal but are instead highly contingent upon the specific characteristics of the corpus in use and the nature of the questions typically posed to the system. Rigorous evaluation of RAG outputs is paramount. This requires establishing a representative set of test questions along with clearly defined expected or ideal answers. Such evaluation processes become particularly challenging when working with unexplored or novel corpora, and the active involvement of domain experts—in this case, philosophers—is indispensable for any meaningful assessment of quality and accuracy.</p>
<p>Researchers also identified specific challenges and limitations. A significant issue arises if the retrieval mechanism fails to locate any relevant documents; in such instances, the quality of the generated answer tends to decrease substantially, often necessitating adjustments to the user’s prompt. An intriguing, somewhat counterintuitive finding was that RAG systems can sometimes produce worse results for broad, widely discussed overview questions, such as “What are the central arguments against scientific realism?”. The hypothesised reason for this phenomenon is that the RAG system’s operational prompt directs it to focus intently on the local information contained within the retrieved texts. This localised focus, whilst beneficial for specific queries, can inadvertently distract from or fail to adequately synthesise the broader, more encompassing perspective required to answer overview questions comprehensively.</p>
<p>Looking ahead, these observations underscore the need for more flexible RAG systems. Future developments may involve systems capable of discerning between different types of questions and adapting their strategies accordingly, potentially moving in the direction of more sophisticated “agentic” RAG architectures.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_011.html" class="pagination-link" aria-label="Science Dynamics and AI">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science Dynamics and AI</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_015.html" class="pagination-link" aria-label="Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>