<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paul Näger">
<meta name="dcterms.date" content="2025-06-21">

<title>12&nbsp; RAG systems solve central problems of LLMs – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_015.html" rel="next">
<link href="./chapter_ai-nepi_011.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_012.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Validation is All You Need</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#addressing-core-llm-challenges-with-rag-systems" id="toc-addressing-core-llm-challenges-with-rag-systems" class="nav-link" data-scroll-target="#addressing-core-llm-challenges-with-rag-systems"><span class="header-section-number">12.1</span> Addressing Core LLM Challenges with RAG Systems</a></li>
  <li><a href="#rag-system-architecture-and-workflow" id="toc-rag-system-architecture-and-workflow" class="nav-link" data-scroll-target="#rag-system-architecture-and-workflow"><span class="header-section-number">12.2</span> RAG System Architecture and Workflow</a></li>
  <li><a href="#overcoming-llm-limitations-access-and-verbatim-learning" id="toc-overcoming-llm-limitations-access-and-verbatim-learning" class="nav-link" data-scroll-target="#overcoming-llm-limitations-access-and-verbatim-learning"><span class="header-section-number">12.3</span> Overcoming LLM Limitations: Access and Verbatim Learning</a></li>
  <li><a href="#addressing-context-window-limitations-and-attribution" id="toc-addressing-context-window-limitations-and-attribution" class="nav-link" data-scroll-target="#addressing-context-window-limitations-and-attribution"><span class="header-section-number">12.4</span> Addressing Context Window Limitations and Attribution</a></li>
  <li><a href="#rag-system-workflow-query-retrieval-augmentation-generation" id="toc-rag-system-workflow-query-retrieval-augmentation-generation" class="nav-link" data-scroll-target="#rag-system-workflow-query-retrieval-augmentation-generation"><span class="header-section-number">12.5</span> RAG System Workflow: Query, Retrieval, Augmentation, Generation</a></li>
  <li><a href="#philosophical-applications-of-rag-systems-didactics-and-research" id="toc-philosophical-applications-of-rag-systems-didactics-and-research" class="nav-link" data-scroll-target="#philosophical-applications-of-rag-systems-didactics-and-research"><span class="header-section-number">12.6</span> Philosophical Applications of RAG Systems: Didactics and Research</a></li>
  <li><a href="#rag-for-philosophical-corpora-enhanced-domain-knowledge" id="toc-rag-for-philosophical-corpora-enhanced-domain-knowledge" class="nav-link" data-scroll-target="#rag-for-philosophical-corpora-enhanced-domain-knowledge"><span class="header-section-number">12.7</span> RAG for Philosophical Corpora: Enhanced Domain Knowledge</a></li>
  <li><a href="#pedagogical-utility-deepening-textual-engagement" id="toc-pedagogical-utility-deepening-textual-engagement" class="nav-link" data-scroll-target="#pedagogical-utility-deepening-textual-engagement"><span class="header-section-number">12.8</span> Pedagogical Utility: Deepening Textual Engagement</a></li>
  <li><a href="#research-applications-fact-finding-and-corpus-exploration" id="toc-research-applications-fact-finding-and-corpus-exploration" class="nav-link" data-scroll-target="#research-applications-fact-finding-and-corpus-exploration"><span class="header-section-number">12.9</span> Research Applications: Fact-Finding and Corpus Exploration</a></li>
  <li><a href="#advanced-research-applications-close-reading-and-question-answering" id="toc-advanced-research-applications-close-reading-and-question-answering" class="nav-link" data-scroll-target="#advanced-research-applications-close-reading-and-question-answering"><span class="header-section-number">12.10</span> Advanced Research Applications: Close Reading and Question Answering</a></li>
  <li><a href="#example-rag-implementation-stanford-encyclopedia-of-philosophy" id="toc-example-rag-implementation-stanford-encyclopedia-of-philosophy" class="nav-link" data-scroll-target="#example-rag-implementation-stanford-encyclopedia-of-philosophy"><span class="header-section-number">12.11</span> Example RAG Implementation: Stanford Encyclopedia of Philosophy</a></li>
  <li><a href="#initial-aim-community-tool" id="toc-initial-aim-community-tool" class="nav-link" data-scroll-target="#initial-aim-community-tool"><span class="header-section-number">12.12</span> Initial Aim: Community Tool</a></li>
  <li><a href="#evolving-aims-from-tool-to-qualitative-study" id="toc-evolving-aims-from-tool-to-qualitative-study" class="nav-link" data-scroll-target="#evolving-aims-from-tool-to-qualitative-study"><span class="header-section-number">12.13</span> Evolving Aims: From Tool to Qualitative Study</a></li>
  <li><a href="#qualitative-study-focus-model-choices-and-hyperparameter-tuning" id="toc-qualitative-study-focus-model-choices-and-hyperparameter-tuning" class="nav-link" data-scroll-target="#qualitative-study-focus-model-choices-and-hyperparameter-tuning"><span class="header-section-number">12.14</span> Qualitative Study Focus: Model Choices and Hyperparameter Tuning</a></li>
  <li><a href="#methodological-challenges-retrieval-semantic-mismatch-and-reranking" id="toc-methodological-challenges-retrieval-semantic-mismatch-and-reranking" class="nav-link" data-scroll-target="#methodological-challenges-retrieval-semantic-mismatch-and-reranking"><span class="header-section-number">12.15</span> Methodological Challenges: Retrieval Semantic Mismatch and Reranking</a></li>
  <li><a href="#frontend-overview-configuration-and-comparative-answers" id="toc-frontend-overview-configuration-and-comparative-answers" class="nav-link" data-scroll-target="#frontend-overview-configuration-and-comparative-answers"><span class="header-section-number">12.16</span> Frontend Overview: Configuration and Comparative Answers</a></li>
  <li><a href="#backend-code-and-output-details" id="toc-backend-code-and-output-details" class="nav-link" data-scroll-target="#backend-code-and-output-details"><span class="header-section-number">12.17</span> Backend Code and Output Details</a></li>
  <li><a href="#optimising-chunk-size-for-philosophical-corpora" id="toc-optimising-chunk-size-for-philosophical-corpora" class="nav-link" data-scroll-target="#optimising-chunk-size-for-philosophical-corpora"><span class="header-section-number">12.18</span> Optimising Chunk Size for Philosophical Corpora</a></li>
  <li><a href="#reranking-addressing-retrieval-semantic-mismatch" id="toc-reranking-addressing-retrieval-semantic-mismatch" class="nav-link" data-scroll-target="#reranking-addressing-retrieval-semantic-mismatch"><span class="header-section-number">12.19</span> Reranking: Addressing Retrieval Semantic Mismatch</a></li>
  <li><a href="#advantages-of-rag-systems-in-scientific-tasks" id="toc-advantages-of-rag-systems-in-scientific-tasks" class="nav-link" data-scroll-target="#advantages-of-rag-systems-in-scientific-tasks"><span class="header-section-number">12.20</span> Advantages of RAG Systems in Scientific Tasks</a></li>
  <li><a href="#cautions-and-challenges-in-rag-implementation" id="toc-cautions-and-challenges-in-rag-implementation" class="nav-link" data-scroll-target="#cautions-and-challenges-in-rag-implementation"><span class="header-section-number">12.21</span> Cautions and Challenges in RAG Implementation</a></li>
  <li><a href="#additional-visual-materials" id="toc-additional-visual-materials" class="nav-link" data-scroll-target="#additional-visual-materials"><span class="header-section-number">12.22</span> Additional Visual Materials</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Paul Näger <a href="mailto:paul.naeger@cis.lmu.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            LMU München
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This report systematically documents the application of Retrieval-Augmented Generation (<em>RAG</em>) systems within philosophy, particularly addressing the discipline’s stringent requirements for linguistic and semantic accuracy. Paul Näger presents a compelling perspective on <em>RAG</em>, highlighting its capacity to resolve core limitations inherent in Large Language Models (<em>LLMs</em>), such as restricted access to full texts, finite context windows, and challenges in attribution. Whilst <em>LLMs</em> excel at generating generalisable statistical rules for text production, they are not designed for verbatim text learning. This poses a critical necessity for philosophical inquiry, which demands deep engagement with original sources and their fine-grained formulations. Consequently, <em>RAG</em> systems emerge as a vital solution.</p>
<p>Näger’s presentation explores diverse applications, ranging from pedagogical tools to advanced research functionalities. For instance, <em>RAG</em> enables students to interact conversationally with philosophical corpora, such as Locke’s <em>Oeuvre</em>, fostering deeper textual understanding. For researchers, <em>RAG</em> facilitates efficient fact-finding in handbooks, exploration of previously unexamined corpora, identification of passages for close reading, and the precise answering of specific research questions.</p>
<p>A practical <em>RAG</em> implementation, utilising the <em>Stanford Encyclopedia of Philosophy</em> (<em>SEP</em>) as its data source, demonstrates these capabilities. Initially conceived as a community tool, the project evolved into a qualitative study investigating optimal <em>RAG</em> system configurations for philosophical contexts. This research meticulously examines model choices, including generative <em>LLMs</em> (e.g., <em>gpt-4o-mini</em>) and embedding models, alongside the intricate tuning of hyperparameters such as the number of retrieved documents (<em>top-k</em>), input/output token limits, generation temperature, and chunk size and overlap.</p>
<p>The methodology employs a theoretically grounded trial-and-error approach, emphasising the criticality of robust evaluation standards for assessing complex, unstructured philosophical propositions. A key finding reveals that chunking content into main sections, despite their average length (approximately 3000 words) exceeding the embedding model’s cutoff (512 words), yields superior results. This efficacy largely stems from the highly systematised nature of the <em>SEP</em>. Furthermore, the system incorporates reranking as an additional step to enhance retrieval accuracy. It leverages a generative <em>LLM</em> for more advanced semantic differentiation, albeit at increased computational cost.</p>
<p>Ultimately, <em>RAG</em> systems offer significant advantages by integrating verbatim corpora and specialised domain knowledge, thereby reducing hallucinations and enabling precise citation. Their effective deployment, however, necessitates careful tweaking, rigorous evaluation with representative question sets, and the indispensable involvement of domain experts, particularly when exploring unfamiliar corpora. Challenges persist, notably the degradation of answer quality when relevant documents are scarce. Paradoxically, <em>RAGs</em> tend to perform less effectively on broad overview questions, suggesting a need for more flexible, potentially agentic <em>RAG</em> architectures in future developments.</p>
</section>
<section id="addressing-core-llm-challenges-with-rag-systems" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="addressing-core-llm-challenges-with-rag-systems"><span class="header-section-number">12.1</span> Addressing Core LLM Challenges with RAG Systems</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Philosophical inquiry frequently poses complex questions. One might seek to elucidate Aristotle’s theory of matter within his <em>Physics</em> or trace the evolution of Einstein’s concept of locality across his works, from early relativity papers to his 1948 publication on ‘<em>Quantenmechanik und Wirklichkeit</em>’. Whilst Large Language Models (<em>LLMs</em>) like <em>ChatGPT</em> can furnish reasonably differentiated responses to these queries, they exhibit several fundamental limitations. Retrieval-Augmented Generation (<em>RAG</em>) systems specifically address these challenges, offering a robust framework for enhancing <em>LLM</em> performance in knowledge-intensive domains.</p>
</section>
<section id="rag-system-architecture-and-workflow" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="rag-system-architecture-and-workflow"><span class="header-section-number">12.2</span> RAG System Architecture and Workflow</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>A <em>RAG</em> system fundamentally integrates external data sources to augment the capabilities of Large Language Models. This architecture necessitates a dedicated data source, which, for philosophical research, might comprise a specific corpus such as the complete works of Aristotle or Einstein. Researchers retrieve relevant documents from this corpus, typically employing semantic search, though hybrid or classic search methods also remain viable options. Subsequently, these retrieved text chunks dynamically augment the prompts directed to the <em>LLM</em>. Crucially, this augmentation process directly resolves a significant limitation of standalone <em>LLMs</em>: their lack of direct access to full, original texts. Whilst <em>LLMs</em> may have encountered these texts during their training, they cannot reliably quote specific passages or avoid factual inaccuracies, often leading to ‘hallucinations’.</p>
</section>
<section id="overcoming-llm-limitations-access-and-verbatim-learning" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="overcoming-llm-limitations-access-and-verbatim-learning"><span class="header-section-number">12.3</span> Overcoming LLM Limitations: Access and Verbatim Learning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>Large Language Models, despite their sophisticated conversational abilities, inherently lack direct access to the complete texts they discuss. Consequently, when prompted to quote specific sections from a paper, an <em>LLM</em> may either admit its inability or, more problematically, generate fabricated content. This limitation stems from their training methodology; <em>LLMs</em> are not engineered to memorise texts verbatim. Instead, their design explicitly prevents rote learning, compelling them to acquire generalisable statistical rules for text production. Philosophical research, however, with its profound emphasis on linguistic and semantic accuracy, critically depends upon direct engagement with original textual sources and their precise, fine-grained formulations. <em>RAG</em> systems, by providing explicit access to these corpora, directly address this fundamental disparity.</p>
</section>
<section id="addressing-context-window-limitations-and-attribution" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="addressing-context-window-limitations-and-attribution"><span class="header-section-number">12.4</span> Addressing Context Window Limitations and Attribution</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>Beyond facilitating direct textual access, <em>RAG</em> systems adeptly navigate two further critical challenges posed by Large Language Models. Firstly, they mitigate the issue of a limited context window. Although models like <em>ChatGPT 4.0</em> boast a substantial context of 128,000 tokens, extensive corpora can rapidly exhaust this capacity. <em>RAG</em> systems circumvent this by intelligently retrieving and supplying only the most pertinent text chunks, thereby ensuring that the <em>LLM</em> operates within its operational limits whilst still receiving highly relevant information. Secondly, <em>RAG</em> systems inherently resolve the attribution problem. They furnish explicit citations for the information provided, mirroring the functionality observed in platforms like <em>Perplexity</em>, where numbered references link claims directly to their source documents. This capability is paramount for academic rigour, ensuring the verifiability and trustworthiness of generated content.</p>
</section>
<section id="rag-system-workflow-query-retrieval-augmentation-generation" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="rag-system-workflow-query-retrieval-augmentation-generation"><span class="header-section-number">12.5</span> RAG System Workflow: Query, Retrieval, Augmentation, Generation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>The operational workflow of a <em>RAG</em> system follows a systematic, multi-stage process. Initially, a user submits a query to a dedicated application. This application then initiates a retrieval query, directing it towards various data sources, which may include vector databases or APIs. Upon receiving this query, the data sources return relevant text chunks to the application. Crucially, the application then combines the original user query with these newly retrieved chunks, forming an ‘augmented’ query. This enriched input is subsequently transmitted to a Large Language Model (<em>LLM</em>) for processing. The <em>LLM</em>, leveraging both the query and the contextual chunks, generates a comprehensive answer, which it relays back to the application. Finally, the application delivers this refined answer to the user, ensuring that responses are both informative and grounded in specific, verifiable sources.</p>
</section>
<section id="philosophical-applications-of-rag-systems-didactics-and-research" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="philosophical-applications-of-rag-systems-didactics-and-research"><span class="header-section-number">12.6</span> Philosophical Applications of RAG Systems: Didactics and Research</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p><em>RAG</em> systems offer transformative potential across various facets of philosophical engagement. Fundamentally, they enable conversational interaction with extensive philosophical corpora, such as Locke’s complete works, mirroring the intuitive style of <em>ChatGPT</em> whilst providing significantly more detailed domain knowledge and a verifiable verbatim text basis. This capability proves invaluable for didactics; repeated questioning becomes a highly instructive method for students to progressively deepen their understanding of complex texts, moving from general concepts to intricate details. For instance, students can explore Locke’s epistemology or his theory of matter.</p>
<p>Moreover, <em>RAG</em> systems hold considerable promise for research. They facilitate efficient fact-finding within handbooks, streamlining the process of locating specific information for orientation, remarks, or footnotes. Researchers can also employ these systems to explore previously unexamined corpora, provided the texts are first digitised, gaining a comprehensive overview of their contents. Furthermore, <em>RAG</em> aids in identifying precise passages for close reading that directly pertain to specific research questions. Ultimately, these systems aspire to furnish detailed answers to at least components of complex research questions, painting a compelling vision for future philosophical inquiry.</p>
</section>
<section id="rag-for-philosophical-corpora-enhanced-domain-knowledge" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="rag-for-philosophical-corpora-enhanced-domain-knowledge"><span class="header-section-number">12.7</span> RAG for Philosophical Corpora: Enhanced Domain Knowledge</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>The overarching concept driving the application of <em>RAG</em> systems in philosophy centres on enabling conversational interaction with extensive philosophical corpora, such as the complete works of John Locke. This approach aims to replicate the intuitive user experience of platforms like <em>ChatGPT</em>. Crucially, however, it significantly enhances the interaction by providing a far more detailed domain-specific knowledge base and, critically, a verifiable verbatim textual foundation, ensuring scholarly rigour and precision.</p>
</section>
<section id="pedagogical-utility-deepening-textual-engagement" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="pedagogical-utility-deepening-textual-engagement"><span class="header-section-number">12.8</span> Pedagogical Utility: Deepening Textual Engagement</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>For pedagogical purposes, <em>RAG</em> systems offer a remarkably effective instructional approach. Students can engage with challenging philosophical texts, such as Locke’s <em>Essay Concerning Human Understanding</em>, by initiating a conversational dialogue. This allows them to begin with broad inquiries, like Locke’s general philosophical tenets, and then progressively delve into more specific areas, such as his ideas on epistemology or his theory of matter. Through this iterative questioning, <em>RAG</em> systems provide a dynamic and instructive pathway for students to achieve a profound understanding of complex textual content.</p>
</section>
<section id="research-applications-fact-finding-and-corpus-exploration" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="research-applications-fact-finding-and-corpus-exploration"><span class="header-section-number">12.9</span> Research Applications: Fact-Finding and Corpus Exploration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>In the realm of research, <em>RAG</em> systems prove indispensable for tasks such as fact-finding within handbooks, providing essential orientation, facilitating remarks, and generating accurate footnotes. Historically, scholars manually consulted physical books; now, whilst <em>LLMs</em> can offer information, its reliability remains questionable, often leading to hallucination. Consequently, robust <em>RAG</em> systems become critical for ensuring the veracity of retrieved facts. Furthermore, these systems enable the exploration of previously unexamined corpora. Once digitised, such texts can be interrogated conversationally, yielding deeper overviews of their content and unlocking new avenues for scholarly investigation.</p>
</section>
<section id="advanced-research-applications-close-reading-and-question-answering" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="advanced-research-applications-close-reading-and-question-answering"><span class="header-section-number">12.10</span> Advanced Research Applications: Close Reading and Question Answering</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Beyond basic fact-finding, <em>RAG</em> systems significantly advance philosophical research by enabling the precise identification of passages for close reading that directly pertain to a specific research question. Ultimately, these systems hold the potential to furnish detailed answers, at least to components of complex research questions. This capability paints a compelling vision for the future of philosophical inquiry, promising to streamline and deepen scholarly engagement with intricate textual and conceptual challenges.</p>
</section>
<section id="example-rag-implementation-stanford-encyclopedia-of-philosophy" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="example-rag-implementation-stanford-encyclopedia-of-philosophy"><span class="header-section-number">12.11</span> Example RAG Implementation: Stanford Encyclopedia of Philosophy</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Paul Näger and his team developed an illustrative <em>RAG</em> system, leveraging the <em>Stanford Encyclopedia of Philosophy</em> (<em>SEP</em>) as its foundational data source. They systematically scraped the content of this well-regarded online handbook and converted it into markdown format, preparing it for integration into the <em>RAG</em> architecture.</p>
</section>
<section id="initial-aim-community-tool" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="initial-aim-community-tool"><span class="header-section-number">12.12</span> Initial Aim: Community Tool</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>Initially, Näger’s project aimed to develop a practical and beneficial tool specifically for the academic community, providing a valuable resource for philosophical inquiry.</p>
</section>
<section id="evolving-aims-from-tool-to-qualitative-study" class="level2" data-number="12.13">
<h2 data-number="12.13" class="anchored" data-anchor-id="evolving-aims-from-tool-to-qualitative-study"><span class="header-section-number">12.13</span> Evolving Aims: From Tool to Qualitative Study</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>Paul Näger and his team implemented a <em>RAG</em> system utilising the <em>Stanford Encyclopedia of Philosophy</em> as its data source. However, initial attempts to configure the system using conventional textbook approaches for retrieval and generation produced unsatisfactory results; indeed, the answers proved inferior to those generated by <em>ChatGPT</em> alone. This unexpected outcome prompted a significant re-evaluation of the project’s objectives, shifting its primary aim from merely developing a functional tool to undertaking a comprehensive qualitative study on the optimal setup of <em>RAG</em> systems specifically tailored for philosophical applications.</p>
</section>
<section id="qualitative-study-focus-model-choices-and-hyperparameter-tuning" class="level2" data-number="12.14">
<h2 data-number="12.14" class="anchored" data-anchor-id="qualitative-study-focus-model-choices-and-hyperparameter-tuning"><span class="header-section-number">12.14</span> Qualitative Study Focus: Model Choices and Hyperparameter Tuning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>Näger’s qualitative study meticulously investigates two critical areas for optimising <em>RAG</em> system performance in philosophy. Firstly, it scrutinises model choices, specifically evaluating the efficacy of various generative Large Language Models and their corresponding embedding models. Secondly, the study delves into the intricate process of hyperparameter tuning. This involves systematically adjusting parameters such as the number of documents retrieved (<em>top-k</em>), the maximum input and output token lengths, the temperature or <em>top-p</em> settings for text generation, and the optimal chunk size and overlap for document segmentation. Each of these parameters profoundly influences the quality and relevance of the generated responses.</p>
</section>
<section id="methodological-challenges-retrieval-semantic-mismatch-and-reranking" class="level2" data-number="12.15">
<h2 data-number="12.15" class="anchored" data-anchor-id="methodological-challenges-retrieval-semantic-mismatch-and-reranking"><span class="header-section-number">12.15</span> Methodological Challenges: Retrieval Semantic Mismatch and Reranking</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>Beyond model selection and hyperparameter tuning, Näger and his team confronted additional methodological challenges, notably the issue of retrieval semantic mismatch. To address this, they implemented reranking, an advanced technique designed to refine the relevance of retrieved documents. Their overarching methodology employs a theoretically grounded trial-and-error approach, systematically assessing how various adjustments improve answer quality. Crucially, sound evaluation remains paramount, particularly given the complex nature of philosophical propositions, which rarely reduce to simple, atomic facts. Evaluating the factual accuracy of these nuanced statements presents a significant challenge, demanding rigorous and context-sensitive assessment criteria.</p>
</section>
<section id="frontend-overview-configuration-and-comparative-answers" class="level2" data-number="12.16">
<h2 data-number="12.16" class="anchored" data-anchor-id="frontend-overview-configuration-and-comparative-answers"><span class="header-section-number">12.16</span> Frontend Overview: Configuration and Comparative Answers</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>The system’s frontend, as demonstrated by Näger, provides a comprehensive interface for configuration and comparative analysis. Users can specify the Generative Model, currently set to <em>gpt-4o-mini</em>, and define prompt token limits, with a model capacity of 128,000 tokens and a user-defined limit of 15,000. Furthermore, the interface allows setting the number of texts to retrieve, typically 15. A ‘Persona’ text area meticulously instructs the model to act as an ‘expert philosopher’, ensuring ‘meticulous and precise’ answers. Users input their philosophical questions, such as ‘What is priority monism?’, into a dedicated field. The system then presents a comparative output, displaying both an ‘Answer with <em>LLM</em> alone’—serving as a benchmark—and an ‘Answer with <em>RAG</em>’, facilitating direct qualitative assessment. Complementing these answers, a ‘Retrieved Texts Overview’ table details the source ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and whether each text was ‘included’ in the prompt, offering full transparency into the retrieval process.</p>
</section>
<section id="backend-code-and-output-details" class="level2" data-number="12.17">
<h2 data-number="12.17" class="anchored" data-anchor-id="backend-code-and-output-details"><span class="header-section-number">12.17</span> Backend Code and Output Details</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_20.png" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>Näger’s team crafted the system’s backend in Python, orchestrating the intricate processes of text handling and retrieval. Functions such as <code>print_section_overview</code> and <code>print_paragraph_overview</code> provide detailed insights into how text sections and paragraphs are processed, whilst <code>print_overview_intro</code> displays the total document count within the dataset. Conditional calls throughout the code exemplify a modular design, facilitating flexible text processing and retrieval logic. The system’s output meticulously lists the retrieved texts, detailing article names, specific section headings, and crucially, indicating which texts were fully included in the prompt and which were truncated due to token limitations, thereby ensuring transparency in the information provided to the <em>LLM</em>.</p>
</section>
<section id="optimising-chunk-size-for-philosophical-corpora" class="level2" data-number="12.18">
<h2 data-number="12.18" class="anchored" data-anchor-id="optimising-chunk-size-for-philosophical-corpora"><span class="header-section-number">12.18</span> Optimising Chunk Size for Philosophical Corpora</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>Optimising chunk size represents a critical hyperparameter in <em>RAG</em> system development. Näger and his team explored three primary chunking strategies: employing a fixed number of words (e.g., 500), segmenting by paragraphs, or dividing content into main sections, whether at a lower or higher hierarchical level. Surprisingly, chunking into main sections, inclusive of their headings, yielded the most favourable results. This outcome proved counter-intuitive, given that the average section length of approximately 3000 words substantially exceeded the embedding model’s typical cutoff of 512 words. This efficacy, however, likely stems from the highly systematised structure of the <em>Stanford Encyclopedia of Philosophy</em>, where the initial 500 words of a section often encapsulate its core ideas. Such a strategy might prove less effective for more heterogeneous or less rigorously structured texts. Consequently, future work plans to investigate embedding models with extended context windows, such as <em>Cohere Embed 3</em>, to better accommodate these longer semantic units.</p>
</section>
<section id="reranking-addressing-retrieval-semantic-mismatch" class="level2" data-number="12.19">
<h2 data-number="12.19" class="anchored" data-anchor-id="reranking-addressing-retrieval-semantic-mismatch"><span class="header-section-number">12.19</span> Reranking: Addressing Retrieval Semantic Mismatch</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_22.png" class="img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
<p>Reranking constitutes a crucial additional step within the retrieval pipeline, specifically designed to mitigate the problem of false positives, where initially retrieved texts lack true relevance to the query. The primary aim of reranking involves reordering documents based on their actual pertinence. To achieve this, Näger’s team employs a generative Large Language Model (<em>gLLM</em>) to evaluate the relevance of the texts. This approach leverages the <em>gLLM</em>’s superior semantic differentiation capabilities, which significantly surpass those of simpler embedding models. The evaluation process incorporates specific scoring categories, including the informativeness of the text and the length of its relevant passages, culminating in a comprehensive total score. Whilst reranking demonstrably yields highly effective results, it concurrently incurs a substantial increase in computational costs.</p>
</section>
<section id="advantages-of-rag-systems-in-scientific-tasks" class="level2" data-number="12.20">
<h2 data-number="12.20" class="anchored" data-anchor-id="advantages-of-rag-systems-in-scientific-tasks"><span class="header-section-number">12.20</span> Advantages of RAG Systems in Scientific Tasks</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p><em>RAG</em> systems offer compelling advantages for scientific tasks. They seamlessly integrate verbatim corpora alongside specialised domain knowledge, thereby furnishing more detailed answers and significantly reducing the incidence of hallucinations. Furthermore, these systems inherently facilitate the precise citation of relevant documents, a critical feature for academic integrity. Consequently, the <em>RAG</em> architecture proves exceptionally well-suited for assisting across a wide spectrum of scientific endeavours, enhancing both the accuracy and trustworthiness of <em>AI</em>-generated content.</p>
</section>
<section id="cautions-and-challenges-in-rag-implementation" class="level2" data-number="12.21">
<h2 data-number="12.21" class="anchored" data-anchor-id="cautions-and-challenges-in-rag-implementation"><span class="header-section-number">12.21</span> Cautions and Challenges in RAG Implementation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_24.png" class="img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
<p>Whilst <em>RAG</em> systems offer substantial benefits, their effective deployment necessitates careful consideration of several cautions and challenges. Firstly, <em>RAG</em> systems inherently demand extensive tweaking; optimal settings are highly contingent upon the specific corpus and the nature of the questions posed. Secondly, rigorous evaluation remains paramount, requiring a representative set of questions and meticulously prepared expected answers. When exploring previously unexamined corpora, the indispensable involvement of domain experts, such as philosophers, becomes critical for accurate assessment. A significant challenge arises when no relevant documents are retrieved, leading to a marked decrease in answer quality, which then necessitates prompt adjustment. Paradoxically, <em>RAG</em> systems often yield inferior results for widely discussed overview questions, such as ‘What are the central arguments against scientific realism?’ This phenomenon occurs because <em>RAGs</em>, by design, concentrate on local information, which can inadvertently distract from the broader perspective required for comprehensive overview responses. Consequently, future developments must focus on crafting more flexible systems, particularly agentic <em>RAG</em> architectures, capable of discerning between question types and adapting their approach accordingly.</p>
</section>
<section id="additional-visual-materials" class="level2" data-number="12.22">
<h2 data-number="12.22" class="anchored" data-anchor-id="additional-visual-materials"><span class="header-section-number">12.22</span> Additional Visual Materials</h2>
<p>The following slides provide supplementary visual information relevant to the presentation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>This slide, titled “SEP <em>RAG</em> Overview”, presents a detailed view of the system’s frontend. A “Configuration” section, which can be initialised, includes an “Options” subsection. Here, the “Generative Model” is set to <em>gpt-4o-mini</em>, with a “Prompt Token Limit (Model)” of 128,000 and a user-defined “Prompt Token Limit (here)” of 15,000. The system is configured to retrieve 15 texts. A “Persona” text area instructs the model to act as an “expert philosopher”, ensuring “meticulous and precise” answers. The “Philosophical Question” field contains the query: “What is priority monism?”. Below the “Generate answer” button, the slide presents two comparative answers. The “Answer with <em>LLM</em> alone” defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with pluralism and mereological nihilism, and citing examples like the universe and philosophers such as Spinoza. The “Answer with <em>RAG</em>”, titled “Response to ‘What is priority monism?’”, offers a similar but more nuanced definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative. It notes the contrast with existence monism, discusses mathematical expressions, ontological priority, and relevance to quantum mechanics (entangled systems). Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. Finally, a “Retrieved Texts Overview” table lists five documents, detailing their ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and ‘included’ status. The table indicates that the <em>RAG</em> system retrieved relevant sections from ‘monism’ and ‘disability-care-rationing’ files, with the first two ‘monism’ entries fully included, and a third ‘monism’ entry partially included. The ‘distances’ column suggests lower values indicate higher relevance. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>This slide, also titled “SEP <em>RAG</em> Overview”, visually divides into “Frontend” on the left and “Backend: Python Code” on the right. The frontend section displays a collapsed “Configuration” panel and an expanded “Options” panel, allowing selection of <em>gpt-4o-mini</em> as the “Generative Model” and setting token limits (128,000 model, 15,000 user-defined). The “Persona” is set to “expert philosopher”, and 15 texts are to be retrieved. The “Philosophical Question” is “What is priority monism?”. The interface then presents comparative answers from an “<em>LLM</em> alone” and “<em>RAG</em>”, with the <em>RAG</em> answer providing a more detailed, source-attributed philosophical definition. A “Retrieved Texts Overview” table details source files, section headings, distances, token lengths, and inclusion status. The “Backend: Python Code” section displays Python function definitions, including <code>print_section_overview</code>, <code>print_paragraph_overview</code>, and <code>print_overview_intro</code>, illustrating the modular and configurable nature of the backend’s text processing and retrieval logic. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_27.png" class="img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
<p>Titled “SEP <em>RAG</em> Details 1”, this slide focuses on the “Frontend: Input section”. The interface features a “Configuration” section with an “Initialize” button. The “Options” section provides detailed settings: “Generative Model” is <em>gpt-4o-mini</em>, “Prompt Token Limit (Model)” is 128,000, and “Prompt Token Limit (here)” is 15,000. The “Persona” instruction is “You are an expert philosopher. You answer meticulously and precisely.” The system is set to retrieve 15 texts. The “Philosophical Question” input box contains “What is priority monism?”. A “Generate answer” button is present at the bottom. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “7”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>This slide, “SEP <em>RAG</em> Details 2”, presents the “Frontend: Output-section answers”. It features a side-by-side comparison of answers to ‘What is priority monism?’. The ‘Answer with <em>LLM</em> alone’ defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with mereological nihilism and citing examples like the universe and philosophers such as Spinoza. The ‘Answer with <em>RAG</em>’, titled “Response to ‘What is priority monism?’”, provides a more detailed, augmented definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative and dependent on the fundamental whole (Text 0). It contrasts this with existence monism, discusses mathematical expressions, and highlights its relevance to emergent properties in quantum mechanics. Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. The <em>RAG</em> answer text is truncated, indicated by a scrollbar. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>Continuing the “SEP <em>RAG</em> Details 2” theme, this slide focuses on a “Comparative setup for qualitative evaluation” of output answers. It displays the “Answer with <em>LLM</em> alone” on the left, serving as a ‘benchmark’, which defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its derived parts, contrasting it with pluralism or mereological nihilism. On the right, the “Answer with <em>RAG</em>”, titled “Response to ‘What is priority monism?’”, provides a definition augmented by retrieved information, citing ‘Text 0’ multiple times. This <em>RAG</em> answer highlights the existence of exactly one basic concrete object (the universe), contrasts it with existence monism, discusses its mathematical expression, and notes its relevance to quantum mechanics and emergent properties. It also attributes the view to Plato and Spinoza and contrasts it with priority pluralism and nihilism. The <em>RAG</em> answer text is partially visible, with a scroll bar indicating more content. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_30.png" class="img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
<p>This slide, “SEP <em>RAG</em> Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The left section, “Answer with <em>LLM</em> alone”, explains priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its parts, which derive their existence from the whole. It contrasts this with pluralism and mereological nihilism, using the universe as an example and mentioning Spinoza. The right section, “Answer with <em>RAG</em>”, presents the “Response to ‘What is priority monism?’”. This augmented answer defines priority monism as the existence of one basic concrete object (the universe), with parts being derivative. It highlights phrases marked with ‘(Text 0)’ for source attribution, contrasts it with ‘existence monism’, mentions its mathematical expression, and its crucial role in understanding emergent properties in quantum mechanics. It also associates the view with ‘Plato and Spinoza’ and contrasts it with ‘priority pluralism’ and ‘nihilism’. The text on the right is truncated. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_31.png" class="img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
<p>The slide, “SEP <em>RAG</em> Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The “Answer with <em>LLM</em> alone” on the left defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its constituent parts, citing Spinoza and contrasting it with pluralism. The “Answer with <em>RAG</em>” on the right, titled “Response to ‘What is priority monism?’”, provides a definition with specific phrases highlighted and marked with ‘(Text 0)’, indicating retrieved information. This <em>RAG</em>-generated answer highlights ‘existence monism’ as a contrast, states ‘there exists exactly one basic entity’ (the cosmos), and notes its ‘crucial for understanding emergent properties found in quantum mechanics’. It associates ‘Plato’ and Spinoza with this view and contrasts it with ‘priority pluralism’ and ‘nihilism’. The <em>RAG</em> answer ends abruptly, suggesting an interactive or truncated output. The footer indicates “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_32.png" class="img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
<p>Titled “SEP <em>RAG</em> Details 3”, this slide presents the “Frontend: Output section retrieved texts” under the heading “Retrieved Texts Overview:”. It features a table with six columns: ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/_token’, ‘total_token’, and ‘included’. The table lists 15 retrieved text segments. For instance, the first entry is from ‘monism’, section ‘## 3. Priority Monism’, with a distance of 0.448, length of 12515 tokens, total tokens 12515, and marked ‘Yes’ for inclusion. The ‘distances’ column likely represents a similarity score, with lower values indicating higher relevance. The ‘included’ column shows that only the top two most relevant sections were fully included, and the third was partially included, suggesting a cutoff based on relevance or a maximum token limit for the context provided to the language model. The ‘total_token’ column tracks the cumulative token count of all retrieved sections. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>This slide, “SEP <em>RAG</em> Details 3”, continues to detail the “Frontend: Output section retrieved texts” under “Retrieved Texts Overview:”. It displays a comprehensive table of 15 retrieved text segments, indexed from 0 to 14. The table’s columns are ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/<em>token’, ‘total_token’, and ‘included’. The ‘file_names’ include ‘monism’, ‘disability-care-rationing’, ‘neutral-monism’, and others. The ‘sec_heading’ specifies the section (e.g., ‘## 3. Priority Monism’, ‘## Abstract’). ‘Distances’ range from 0.448 to 1.241, indicating relevance. ’length</em>/_token’ shows segment lengths, and ‘total_token’ tracks cumulative token count. The ‘included’ column indicates ‘Yes’, ‘No’, or ‘Partially’. Several rows are highlighted: the first two ‘monism’ entries are yellow-highlighted and marked ‘Yes’ for inclusion. The third ‘monism’ entry is yellow-highlighted and ‘Partially’ included. A ‘disability-care-rationing’ entry is highlighted in reddish-pink and marked ‘No’. This highlighting likely draws attention to specific retrieval outcomes and inclusion criteria. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_34.png" class="img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
<p>This slide introduces “Chunk size” as a “hyperparameter”, visually indicated by a green arrow pointing from the term to the label. The main body of the slide is blank, suggesting it serves as an introductory concept or a placeholder for further discussion on this key configurable parameter in <em>RAG</em> methodology. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_35.png" class="img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
<p>The slide, titled “Chunk size” and labelled as a “hyperparameter”, presents “Options:” for determining chunk size. These options are listed as:</p>
<ul>
<li><p>fixed number of words (e.g.&nbsp;500)</p></li>
<li><p>paragraphs</p></li>
<li><p>sections</p></li>
</ul>
<p>This outlines different strategies for segmenting text or data. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_36.png" class="img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
<p>This slide, titled “Chunk size” and identified as a “hyperparameter”, reiterates the “Options:” for chunking:</p>
<ul>
<li><p>fixed number of words (e.g.&nbsp;500)</p></li>
<li><p>paragraphs</p></li>
<li><p>sections</p></li>
</ul>
<p>It then states the “Best result: chunking into main sections (including headings).” A rationale is provided: “philosophical facts are rarely short and isolated, they need some space for presentation”. This leads to the conclusion: “best to stick to longer semantic units”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_37.png" class="img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
<p>The slide, titled “Chunk size” and labelled as a “hyperparameter”, lists “Options:” for chunking:</p>
<ul>
<li><p>fixed number of words (e.g.&nbsp;500)</p></li>
<li><p>paragraphs</p></li>
<li><p>sections</p></li>
</ul>
<p>It states the “Best result: chunking into main sections (including headings).” The rationale provided is that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) section highlights a key observation: “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this discrepancy, the slide notes “Nevertheless best results.” Another rationale explains that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_38.png" class="img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
<p>This slide, titled “Chunk size” and identified as a “hyperparameter”, details the “Options:” for chunking:</p>
<ul>
<li><p>fixed number of words (e.g.&nbsp;500)</p></li>
<li><p>paragraphs</p></li>
<li><p>sections</p></li>
</ul>
<p>The “Best result:” was “chunking into main sections (including headings)”. The rationale is that “philosophical facts are rarely short and isolated, they need some space for presentation”, thus “best to stick to longer semantic units”. A “NB:” (Nota Bene) section highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this, it notes “Nevertheless best results.” Another rationale states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. A “planned:” section outlines future work: “try emb. models with longer context window like <em>Cohere Embed 3</em>”, suggesting experimentation with larger input sizes. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_39.png" class="img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
<p>The slide, titled “Chunk size” and explicitly labelled as a “hyperparameter”, begins by listing “Options” for chunking:</p>
<ul>
<li><p>fixed number of words (e.g.&nbsp;500)</p></li>
<li><p>paragraphs</p></li>
<li><p>sections</p></li>
</ul>
<p>The “Best result” is stated as “chunking into main sections (including headings)”. An observation notes that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) point highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words). Nevertheless best results.” A “planned:” section indicates future work to “try emb. models with longer context window like <em>Cohere Embed 3</em>”. Another observation states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The slide concludes with a crucial lesson: “effective chunking highly depends on the specifics of the corpus and the kind of questions”. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_40.png" class="img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
<p>This slide introduces “Reranking”, prominently displayed and clarified as an “additional step to retrieval”. This visual element emphasises the sequential nature of reranking within an information processing pipeline. The main body of the slide is blank, indicating that further details or diagrams related to reranking would likely be presented incrementally or on subsequent slides. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_012_slide_41.png" class="img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
<p>This presentation slide focuses on “Reranking” as an “additional step to retrieval”. The main content area identifies the core problem that reranking aims to solve: “not all retrieved texts are relevant to the question (false positives)”. This highlights a common challenge in information retrieval where an initial search might yield documents that are not truly pertinent to the user’s query, necessitating a refinement step. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_011.html" class="pagination-link" aria-label="AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_015.html" class="pagination-link" aria-label="Quantum gravity and plural pursuit in science">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>