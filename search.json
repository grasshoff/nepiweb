[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview\nThe workshop, “Large Language Models for the History, Philosophy and Sociology of Science,” convened to explore the application of advanced AI methods within historical, philosophical, and sociological inquiries into science. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collaboratively organised this distinguished event. Its genesis lay in two distinct, yet complementary, initiatives. Firstly, the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932), provided a foundational interest in training large language models on physics texts and analysing conceptual issues within the discipline. Secondly, Gerd Graßhoff, a long-standing advocate for AI integration in the history and philosophy of science, particularly for understanding scientific discovery processes, proposed a workshop on novel AI-assisted methodologies. These converging interests consequently led to the joint organisation of the current event.\nThe NEPI project specifically investigates the internal communication dynamics of the ATLAS collaboration at CERN, aiming to elucidate how such a prominent, large-scale research collaboration collectively generates new knowledge. This research employs both network analysis to map communication structures and semantic tools, including large language models, to trace the flow of ideas within these networks. The workshop attracted significant interest, receiving over 50 paper submissions, from which the organisers selected 16 for presentation. It quickly reached full capacity for in-person attendance and garnered a substantial online audience, totalling approximately 220 registered participants. The programme features keynotes from leading researchers: Pierluigi Cassotti and Nina Tahmasebi, who focus on large-scale text analysis for cultural and societal change, and Iryna Gurevych, who addresses the elevation of Natural Language Processing to the cross-document level. Logistical arrangements include structured question sessions, an Etherpad for comments, a dedicated discussion session, and various networking opportunities. Recording protocols ensure the capture of presentations for future dissemination on the NEPI YouTube channel, subject to presenter consent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "href": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Genesis and Scope",
    "text": "2.1 Workshop Genesis and Scope\n\n\n\nSlide 02\n\n\nThe workshop, titled “Large Language Models for the History, Philosophy and Sociology of Science,” emerged from two distinct yet complementary initiatives. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collectively organised this event. One primary impetus stemmed from the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932). Within this project, Arno Simons pioneered the training of one of the earliest large language models specifically on physics texts, whilst Michael Zichert employed similar models to analyse conceptual issues prevalent in physics.\nConcurrently, Gerd Graßhoff, a long-standing collaborator and proponent of AI integration within the history and philosophy of science, particularly for analysing scientific discovery processes, conceived a workshop focused on novel AI-assisted methodologies. Consequently, these converging interests led to a joint endeavour, culminating in the present workshop. The NEPI project specifically investigates the internal communication of the ATLAS collaboration at CERN, the particle physics laboratory. The researchers aim to understand how one of the largest and most prominent research collaborations collectively generates new knowledge. This investigation employs network analysis to elucidate communication structures and semantic tools, including large language models, to trace the flow of ideas within these intricate networks.\nThe call for papers for this workshop garnered significant interest, attracting over 50 submissions, from which the organisers selected 16 for presentation. On-site attendance quickly reached full capacity, whilst a substantial online audience also registered, bringing the total participation to approximately 220 individuals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "href": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Recording Protocols and Consent",
    "text": "2.2 Recording Protocols and Consent\n\n\n\nSlide 03\n\n\nThe workshop sessions are currently undergoing recording. Attendees provided their consent for this during the registration process. A single camera captures the proceedings, specifically directed towards the presenter. Audio recording relies on four microphones, supplemented by an iPhone serving as a backup audio recorder. Subject to the presenters’ explicit consent, the videos of the talks, encompassing the subsequent discussion, will be uploaded to the NEPI YouTube Channel following the workshop. Crucially, the discussion segments will feature only the audio and video of the presenter, ensuring the privacy of the audience. Individuals requiring further information or wishing to withdraw their consent should approach the organisers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "href": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Workshop Logistics and Interaction Guidelines",
    "text": "2.3 Workshop Logistics and Interaction Guidelines\n\n\n\nSlide 04\n\n\nGiven the large group size and the limited time allocated for presentations and subsequent questions, participants are requested to keep their questions and comments concise and pertinent. Following each presentation, the organisers will collect up to four questions or comments, enabling the presenter to respond to them collectively, thereby optimising time. An Etherpad provides dedicated sections for each talk, alongside a general section, allowing participants to place their comments appropriately. Furthermore, a dedicated discussion session on the second day will facilitate the pooling and collective discussion of frequently arising questions and comments.\nBeyond these formal sessions, the workshop offers ample opportunities for informal networking amongst researchers and fellows. These include generous lunch and coffee breaks, a modest reception, and a workshop dinner. Notably, seating for the dinner is highly limited, reserved exclusively for participants who received confirmation of their attendance. Coffee breaks and refreshments are available on-site. Lunch and the reception will take place in Room H 2051, located down the hall, one floor below the main workshop area.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change",
    "text": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change\n\n\n\nSlide 05\n\n\nThe first keynote address, scheduled shortly, features Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi directs the “Change is Key” research programme, whilst Pierluigi Cassotti contributes as a researcher within the project. Their work has gained considerable recognition for its focus on semantic change detection. This research encompasses both technical aspects, such as the development of benchmarks, and broader methodological considerations, including the application of data science methods to questions within the humanities. This dual focus renders their expertise particularly relevant to the workshop’s objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level",
    "text": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 06\n\n\nIryna Gurevych will deliver the second keynote address tomorrow late afternoon. She leads the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University Darmstadt. Her research primarily concentrates on information extraction, semantic text processing, and machine learning. Crucially, her work also extends to the application of Natural Language Processing (NLP) techniques within the social sciences and humanities, aligning perfectly with the interdisciplinary focus of this workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "",
    "text": "Overview\nThis presentation offers a comprehensive introduction to large language models (LLMs) and their applications within the History, Philosophy, and Sociology of Science (HPSS) domain. It commences with a foundational primer on LLMs and their adaptation to scientific contexts, followed by a summary of their current applications in HPSS. The speaker also shares critical reflections, intended to stimulate workshop discussions.\nThe primer details the core Transformer architecture, explaining its encoder-decoder structure and its evolution into distinct model types such as BERT (bidirectional, encoder-based for understanding) and GPT (unidirectional, decoder-based for generation). It then explores various strategies for adapting these models to specific scientific domains and tasks, including pre-training, fine-tuning, and the sophisticated Retrieval-Augmented Generation (RAG) pipeline, which integrates multiple models for enhanced contextual responses.\nA systematic categorisation of LLM applications in HPSS research is presented, covering data handling, knowledge structure analysis, knowledge dynamics, and knowledge practices. The discussion highlights accelerating interest in LLMs across diverse academic journals, whilst acknowledging recurring concerns such as computational resource demands, model opaqueness, and data scarcity. Crucially, the presentation concludes with critical reflections on HPSS-specific challenges, including the historical evolution of language and the need for reconstructive analysis, advocating for enhanced LLM literacy amongst researchers. It also emphasises the importance of aligning Natural Language Processing (NLP) tasks with core HPSS methodologies and exploring new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture-foundations-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture-foundations-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.1 The Transformer Architecture: Foundations of Large Language Models",
    "text": "3.1 The Transformer Architecture: Foundations of Large Language Models\nThe Transformer architecture constitutes the fundamental framework underpinning all contemporary Large Language Models. Vaswani and colleagues originally designed this model in 2017 for language translation tasks, such as converting German text into English. Their architecture comprises two interconnected streams: an encoder and a decoder.\nThe encoder, positioned on the left, processes input words—for instance, a German sentence—by converting them into numerical representations. Crucially, it reads the entire input sentence at once, enabling each word to interact with every other word. This comprehensive interaction allows the model to construct a full contextual representation of the sentence’s complete meaning.\nConversely, the decoder, located on the right, receives these processed numerical representations from the encoder. It then generates output words, such as an English sentence, feeding each newly produced word back into its input stream. This iterative process continues until the complete English sentence emerges. A key distinction of the decoder is its unidirectional nature: words can only access their predecessors, preventing them from “looking into the future” whilst predicting the subsequent word. Within both the encoder and decoder, multiple layers progressively refine contextualised word embeddings. Vaswani and colleagues introduced this foundational design in their seminal 2017 paper, titled “Attention is all you need.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#encoder-based-models-bert-and-bidirectional-context-understanding",
    "href": "chapter_ai-nepi_003.html#encoder-based-models-bert-and-bidirectional-context-understanding",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.2 Encoder-Based Models: BERT and Bidirectional Context Understanding",
    "text": "3.2 Encoder-Based Models: BERT and Bidirectional Context Understanding\nImmediately following the introduction of the Transformer, researchers began re-engineering its individual streams to develop pre-trained language models. These models excel at understanding language and readily adapt to various Natural Language Processing tasks with minimal additional training.\nThe encoder side of the Transformer gave rise to models such as BERT, which remains highly prevalent. BERT, an acronym for Bidirectional Encoder Representations from Transformers, operates by allowing each word within the input stream to interact with every other word. This bidirectional capability enables the model to construct a comprehensive contextual understanding of the entire input simultaneously. Consequently, BERT-like models primarily focus on coherently understanding sentences rather than generating new text. Devlin and colleagues introduced this architecture in their 2018 paper.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#decoder-based-models-gpt-and-generative-capabilities",
    "href": "chapter_ai-nepi_003.html#decoder-based-models-gpt-and-generative-capabilities",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.3 Decoder-Based Models: GPT and Generative Capabilities",
    "text": "3.3 Decoder-Based Models: GPT and Generative Capabilities\nConversely, the decoder side of the Transformer architecture led to the development of GPT models, or Generative Pre-trained Transformers. These models now power widely used applications such as ChatGPT. Their distinct structure allows words to access only their predecessors, establishing a unidirectional context. This design, however, confers a powerful generative capability, enabling them to produce new text and language, a function not inherently present in BERT models.\nBeyond these two primary types, various model architectures exist, including those that combine encoder and decoder components. Furthermore, researchers have devised sophisticated methods to enable decoder models to operate more akin to encoders, exemplified by models like XLNet and XLM. Fundamentally, understanding these distinctions is crucial: generative models, such as those in the GPT family, excel at producing language, whilst full-context models, like BERT, demonstrate superior capabilities in coherently understanding sentences. Radford and colleagues introduced the GPT architecture in 2018.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-large-language-models",
    "href": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.4 Evolution and Specialisation of Scientific Large Language Models",
    "text": "3.4 Evolution and Specialisation of Scientific Large Language Models\nA comprehensive overview charts the evolution of large language models from 2018 to 2024, specifically highlighting their development for scientific domains and tasks. This landscape categorises models into Encoder-Decoder, Decoder, and Encoder types, encompassing both open-source and closed-source variants. Notably, encoder models, akin to BERT, appear significantly more prevalent within scientific applications.\nEarly pioneering models, such as BioBERT, Specter, and SciBERT, gained considerable popularity. Today, researchers have developed a wide array of domain-specific models tailored for fields including biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. This proliferation underscores the substantial potential for scholars in the History, Philosophy, and Sociology of Science to either leverage existing models or craft their own specialised tools. Ho and colleagues provided this survey in 2024, detailing pre-trained language models for scientific text processing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.5 Strategies for Domain and Task Adaptation of Large Language Models",
    "text": "3.5 Strategies for Domain and Task Adaptation of Large Language Models\nAdapting large language models to specific scientific languages and tasks involves several key strategies. Pre-training represents the initial phase where a model acquires language by predicting either the next token, as seen in GPT models, or random masked words, characteristic of BERT models. This process, however, demands prohibitive computational resources and vast datasets, rendering it largely impractical for individual researchers.\nA more accessible approach involves continued pre-training, where researchers take an already pre-trained model, such as a BERT variant, and further train it on domain-specific language; for instance, applying it to physics texts. Beyond this, fine-tuning through the addition of extra parameters allows researchers to append new layers atop pre-trained models. These layers then undergo training for specific Natural Language Processing tasks, including sentiment classification or named entity recognition.\nWhilst prompt-based methods also exist, contrastive learning emerges as a pivotal technique. This method generates sentence or document embeddings from existing word embeddings, effectively mapping documents or sentences into the same embedding space as individual words. Sentence BERT stands out as a widely adopted and highly effective method within this domain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-for-domain-adaptation",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-for-domain-adaptation",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.6 Retrieval-Augmented Generation (RAG) for Domain Adaptation",
    "text": "3.6 Retrieval-Augmented Generation (RAG) for Domain Adaptation\nRetrieval-Augmented Generation, or RAG, offers a sophisticated pipeline for adapting models to specific scientific domains without necessitating direct model training. This system integrates multiple models, typically at least two, working in concert. Users frequently encounter RAG in contemporary generative AI tools, such as ChatGPT, where it underpins functionalities like internet search.\nThe RAG workflow commences when a user submits a query, for example, “What are LLMs?”. A BERT-type model encodes this query into a sentence embedding. Subsequently, this model searches a database of relevant documents to identify and retrieve the most similar passages. These retrieved passages are then seamlessly integrated into the prompt provided to a generative model. Drawing upon this newly supplied context, the generative model formulates and produces its answer. Crucially, this demonstrates that advanced reasoning models and agents are not monolithic LLMs but intricate systems that combine LLMs with a diverse array of other computational tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#fundamental-distinctions-in-large-language-model-paradigms",
    "href": "chapter_ai-nepi_003.html#fundamental-distinctions-in-large-language-model-paradigms",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.7 Fundamental Distinctions in Large Language Model Paradigms",
    "text": "3.7 Fundamental Distinctions in Large Language Model Paradigms\nUnderstanding the landscape of large language models necessitates grasping several fundamental distinctions. Firstly, models categorise by their core architectural types: encoder-based, such as BERT; decoder-based, exemplified by GPT; and hybrid encoder-decoder configurations. Secondly, researchers employ diverse fine-tuning strategies to adapt these models for specific tasks.\nA crucial differentiation lies in embeddings: word embeddings represent individual lexical units, whilst sentence embeddings capture the meaning of entire sentences. These two types operate at fundamentally different levels of abstraction. Beyond individual LLMs, the field distinguishes between pipelines, like Retrieval-Augmented Generation (RAG), which combine multiple models, and agents, which represent complex systems integrating LLMs with a variety of external tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#categorisation-of-large-language-model-applications-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#categorisation-of-large-language-model-applications-in-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.8 Categorisation of Large Language Model Applications in HPSS Research",
    "text": "3.8 Categorisation of Large Language Model Applications in HPSS Research\nCurrent research involves an ongoing survey documenting the applications of large language models as tools within History, Philosophy, and Sociology of Science (HPSS) research. This survey has identified four primary categories for classifying these applications.\nThe first category, “Dealing with data and sources,” focuses on how researchers interact with and locate their data. This includes tasks such as parsing and extracting specific information, exemplified by identifying publication types, acknowledgements, or citations. The second category, “Knowledge structures,” concerns the analysis of how knowledge is organised. Here, applications involve entity extraction, for instance, identifying scientific instruments, celestial bodies, or chemicals, alongside mapping science policy discourses and delineating interdisciplinary fields.\n“Knowledge dynamics” constitutes the third category, addressing the evolution and change of knowledge over time. This encompasses conceptual histories of words, including the term “theory” in Digital Humanities or “virtual” and “Planck” in physics, and the reconstruction of arguments by identifying premises, conclusions, and causal relationships. Finally, “Knowledge practices” forms the fourth category, focusing on how knowledge is produced and utilised. A notable example is citation context analysis, an established HPSS tradition that, whilst often employed for evaluatory purposes today, offers significant utility for other HPSS tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#trends-and-challenges-in-hpss-applications-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#trends-and-challenges-in-hpss-applications-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.9 Trends and Challenges in HPSS Applications of Large Language Models",
    "text": "3.9 Trends and Challenges in HPSS Applications of Large Language Models\nResearchers observe an accelerating interest in large language models within the History, Philosophy, and Sociology of Science, extending beyond traditional computational journals like Scientometrics and Jasis into publications not typically associated with computational methods. This expansion stems from the remarkable semantic power of LLMs, which increasingly attracts qualitative researchers and philosophers.\nApplications demonstrate a wide spectrum of customisation, ranging from the straightforward, off-the-shelf use of tools such as ChatGPT to the intricate development of novel architectures and bespoke pre-training or fine-tuning. Despite this burgeoning interest, recurring concerns persist. These include the overwhelming computational resources demanded by LLMs, their inherent opaqueness, the scarcity of suitable training data, and the absence of standardised benchmarks. Furthermore, researchers consistently grapple with trade-offs between different model types, such as BERT-like versus GPT-like architectures, underscoring that no single model serves all purposes; rather, the appropriate model depends entirely on the specific research objective. Nevertheless, a discernible trend towards greater accessibility emerges, exemplified by tools like BERTTopic for topic modelling, which developers maintain meticulously to ensure ease of use.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#critical-reflections-on-integrating-large-language-models-into-hpss-research",
    "href": "chapter_ai-nepi_003.html#critical-reflections-on-integrating-large-language-models-into-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.10 Critical Reflections on Integrating Large Language Models into HPSS Research",
    "text": "3.10 Critical Reflections on Integrating Large Language Models into HPSS Research\nIntegrating large language models into History, Philosophy, and Sociology of Science research necessitates careful critical reflection, particularly concerning HPSS-specific challenges. A primary concern involves the historical evolution of concepts and language. Given that LLMs typically undergo training on modern language, researchers must devise strategies for adapting them to historical contexts whilst remaining acutely aware of potential biases. Furthermore, HPSS scholarship adopts a reconstructive and critically reflective perspective, demanding that scholars read between the lines, comprehend authorial context, and discern subtle discursive strategies, such as boundary work. LLMs, however, are not inherently trained to detect such nuances. Practical data challenges, including sparse datasets, the presence of multiple languages, and old scripts, further complicate their application.\nConsequently, building LLM literacy becomes paramount. Researchers must thoroughly understand these tools, encompassing both their theoretical foundations and practical implications. This may entail acquiring coding skills, although Natural Language Processing coding is progressively becoming more accessible. Crucially, scholars must avoid the superficial use of off-the-shelf tools without a profound comprehension of their outputs.\nFinally, maintaining fidelity to HPSS methodologies is essential. Researchers must translate HPSS problems into Natural Language Processing tasks whilst steadfastly preserving their core HPSS focus. This prevents NLP tasks, such as classification, generation, or summarisation, from inadvertently “hijacking” the original research purpose. Nevertheless, LLMs present novel opportunities for bridging qualitative and quantitative research approaches. Moreover, scholars should reflect upon the pre-history of these models within HPSS, recognising connections to earlier developments like co-word analysis, pioneered by Callon and Rip in the 1980s, which itself emerged from Actor-Network Theory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nOpenAlex Mapper, an innovative tool, addresses critical generalisation and validation challenges prevalent in History, Philosophy, and Sociology of Science (HPSS) research. This presentation introduces the tool, elucidates its technical underpinnings, demonstrates its interactive capabilities, and explores its diverse applications within transdisciplinary contexts.\nThe methodology centres on fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, the team sampled 300,000 English abstracts from the OpenAlex database, a comprehensive and openly accessible repository of scholarly material. Engineers embedded these abstracts, reducing their dimensionality to two dimensions using Uniform Manifold Approximation and Projection (UMAP), thereby creating a foundational 2D base map. OpenAlex Mapper then allows users to submit arbitrary queries, downloading and embedding the corresponding records before projecting them onto this pre-trained UMAP model.\nThe interactive map facilitates in-depth investigation of specific terms, authors, temporal distributions, and citation networks. Crucially, the tool provides a robust quantitative framework, grounding qualitative, heuristic investigations. This enables researchers to trace the diffusion of models, map the distribution of concepts, and analyse method usage patterns across vast interdisciplinary samples. For instance, one can track the Hopfield model’s adoption, visualise model templates such as Ising and Sherrington-Kirkpatrick, and contrast the spatial distribution of concepts like “phase transition” and “emergence,” alongside methods such as Random Forest and Logistic Regression.\nDespite its considerable utility, the system acknowledges several qualifications. It relies on the OpenAlex database, which, whilst robust, is not without imperfections, particularly concerning disciplinary representation. The current language model processes English-only sources, and the embedding step necessitates the presence of abstracts or well-formed titles. Furthermore, the UMAP algorithm, a stochastic process, introduces inherent trade-offs in dimensionality reduction; the 768 dimensions of the Specter model cannot be perfectly represented in two, inevitably leading to potential misalignments. A working paper, “Philosophy at Scale: Introducing OpenAlex Mapper,” offers more detailed technical insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper Architecture and Workflow",
    "text": "4.1 OpenAlex Mapper Architecture and Workflow\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila crafted OpenAlex Mapper, a novel tool funded by an ERC grant focused on “possible life.” This innovation aims to introduce the tool, elucidate its high-level technical operations, demonstrate its practical application, and ultimately discuss its utility for research within the History, Philosophy, and Sociology of Science (HPSS).\nThe core workflow of OpenAlex Mapper comprises several distinct stages. Initially, the team fine-tuned the Specter 2 embedding model, specifically enhancing its recognition of disciplinary boundaries. This process involved training the model on a dataset of articles originating from highly similar disciplinary backgrounds, with UMAP dimensionality reduction providing a visualisation of this training. Notably, these adjustments constituted minor modifications to the language model, rather than a comprehensive retraining effort.\nSubsequently, for base-map preparation, the authors leveraged the OpenAlex database, a vast and inclusive repository of scholarly material that surpasses the scale of Web of Science or Scopus. OpenAlex offers fully open data, facilitating easy batch querying and free accessibility, which distinguishes it from many proprietary alternatives. From this extensive database, the team sampled 300,000 random articles, imposing minimal restrictions beyond requiring reasonably well-formed English abstracts. The previously fine-tuned Specter 2 model then embedded these abstracts. Engineers further reduced these embeddings to two dimensions through Uniform Manifold Approximation and Projection (UMAP), yielding both a 2D base map and a trained UMAP model.\nFor individual user queries, OpenAlex Mapper allows submission of arbitrary searches to the OpenAlex database. The tool downloads the relevant records—for instance, the first 1,000 for demonstration purposes—and embeds their abstracts using the identical fine-tuned language model. These new embeddings are then projected through the pre-trained UMAP model, ensuring that the queried articles acquire positions on the two-dimensional map consistent with their hypothetical presence during the original layout process. The resulting interactive map is accessible online and available for download via data mappers, offering features such as temporal distributions and citation graph overlays. Users can access the slides and interactive tool via maxnoichl.eu/talk, whilst a version with a higher latency GPU setup is also available for processing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration of OpenAlex Mapper",
    "text": "4.2 Interactive Demonstration of OpenAlex Mapper\nThe OpenAlex Mapper tool, accessible via https://m7n-openalex-mapper.hf.space, offers a straightforward user experience. Users initiate their investigation by searching the OpenAlex database directly through its comprehensive search interface, for example, by entering a query such as “scale-free network models.”\nIn the backend, the system efficiently downloads the initial 1,000 records pertinent to the search query—a limit imposed to optimise processing time. Subsequently, it embeds all abstracts from these downloaded records. If the user enables the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as a projection of the search results onto a pre-existing grey base map, visually representing the disciplinary landscape.\nCrucially, the map is fully interactive, empowering users to delve into specific data points. For instance, one can investigate the presence of a term like “coriander” within unexpected fields such as epidemiology or public health, gaining nuanced insights into interdisciplinary connections. The demonstration showcased queries for both “coriander”—a standard OpenAlex example—and “scale-free network models.” Furthermore, the developers have made an alternative setup available, featuring a higher latency GPU, which accommodates larger and more complex queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper primarily addresses the persistent challenges of generalisation and validation that arise from the reliance on small samples and case studies within History, Philosophy, and Sociology of Science (HPSS). Whilst traditional HPSS methods—such as the close reading of scholarly papers, direct interaction with scientists, and studies conducted by researchers with scientific training—offer invaluable detailed, close-up views of scientific processes, they often struggle to scale. Generalising these granular insights to the vast, global, and rapidly evolving landscape of contemporary science presents a significant hurdle.\nOpenAlex Mapper contributes by providing robust quantitative methods that effectively ground qualitative, heuristic investigations. A key feature of the tool is its capacity to trace all analytical results directly back to their original textual sources, ensuring transparency and scholarly rigour.\nThe tool supports several specific applications, offering compelling examples of its utility. Researchers can trace the diffusion of particular models, such as the Hopfield model, to ascertain where it genuinely “stuck” or achieved widespread adoption and sustained reference across diverse scientific domains. Furthermore, the system facilitates the investigation of “model templates”—conceptual frameworks defining models of similar structure that emerge in disparate scientific fields, potentially structuring science in ways orthogonal to established disciplines. Examples like the Ising, Hopfield, and Sherrington-Kirkpatrick models often appear at specific, non-continuous locations on the base map, providing crucial insights for ongoing debates concerning model transfer in science.\nBeyond models, OpenAlex Mapper enables the mapping of concept distribution. For instance, it can visually contrast the spread of “phase transition” (depicted in blue) with “emergence” (in orange), broadening such analyses into interdisciplinary contexts and circumventing common problems associated with acquiring specific datasets. Finally, the tool proves invaluable for analysing method usage. It reveals distinguishable patterns of specific methods within interdisciplinary contexts; for example, neuroscientists frequently employ Random Forest algorithms, whilst researchers in psychiatry or mental health often utilise Logistic Regressions. This observation prompts profound philosophical questions regarding the underlying reasons for these patterns and their implications for debates on machine learning in science versus classical statistics, and the concept of “theory-free science.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Considerations",
    "text": "4.4 Limitations and Future Considerations\nWhilst OpenAlex Mapper offers significant analytical capabilities, its application is subject to several important qualifications. The system’s efficacy inherently depends on the OpenAlex database, which, despite its overall reasonable data quality compared to other available sources, is not without imperfections. Notably, certain disciplines, such as law, may exhibit underrepresentation within the database, potentially skewing comprehensive analyses.\nThe current language model processes English-only sources, which somewhat limits the tool’s global scope. Nevertheless, this constraint poses less of a problem for investigations focused on the more recent history of science. In principle, the integration of multilingual models could remedy this limitation, although high-quality, science-trained multilingual models remain scarce. Furthermore, the embedding step of the methodology necessitates that sources include either abstracts or well-formed titles, thereby restricting the range of processable data.\nCrucially, the method relies heavily on the Uniform Manifold Approximation and Projection (UMAP) algorithm, which presents its own set of imperfections. As a stochastic algorithm, UMAP generates one specific output amongst many possible configurations. Moreover, the algorithm must make inherent trade-offs during dimensionality reduction; the 768 dimensions of the Specter language model cannot be perfectly compressed into two, inevitably leading to some degree of “pushing and pulling and misaligning” of data points.\nFor those seeking further information, the presentation slides are available online at maxnoichl.eu/talk. Additionally, a working paper, titled “Philosophy at Scale: Introducing OpenAlex Mapper,” provides more exhaustive technical details regarding the tool’s development and operation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "",
    "text": "Overview\nThis report systematically documents the methodology and findings of ActDisease, an ERC-funded research initiative investigating the historical evolution of patient organisations in 20th-century Europe. The research team primarily utilises a substantial, recently digitised collection of patient organisation periodicals, encompassing 96,186 pages from Sweden, Germany, France, and the UK. Acknowledging the inherent challenges of digitisation, particularly Optical Character Recognition (OCR) errors in complex layouts and creative texts, the authors have explored post-OCR correction techniques using instruction-tuned generative models.\nA central objective involves classifying the diverse textual genres within these historical magazines. This step proves crucial for conducting fine-grained historical analysis that transcends the limitations of aggregate topic models. Under expert historical supervision, the project team defined nine distinct genres—Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and QA—ensuring both analytical utility and general applicability. An annotation project involving six project members achieved a high inter-annotator agreement (0.95 Krippendorff’s alpha) on paragraphs sampled from Swedish and German periodicals.\nGiven the scarcity of annotated data, the authors rigorously explored both zero-shot and few-shot learning paradigms. Zero-shot experiments leveraged publicly available, modern datasets, including the Corpus of Online Registers of English (CORE), Functional Text Dimensions (FTD), and UD-MULTIGENRE (UDM), through a meticulous genre mapping process. These experiments employed multilingual encoders such as XLM-Roberta, mBERT, and historical mBERT, revealing varying performance across genres and models, alongside class-specific biases.\nFew-shot learning, conducted on the ActDisease dataset, demonstrated clear performance advantages with increased training instances, particularly for historical mBERT when coupled with prior Masked Language Model (MLM) fine-tuning. Additionally, the team investigated few-shot prompting with the Llama 3.1 8b Instruct model, observing its capacity to handle certain genre labels effectively, whilst highlighting the need for more comprehensive examples for others.\nThe findings underscore the inherent complexity of text mining popular magazines due to their rich genre diversity. They simultaneously affirm genre classification as an indispensable tool for rendering these historical sources accessible for nuanced textual analysis. Future work encompasses refining annotation schemes, generating synthetic data, and implementing active learning strategies to further enhance classification quality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-source-materials",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-source-materials",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.1 The ActDisease Project: Objectives and Source Materials",
    "text": "5.1 The ActDisease Project: Objectives and Source Materials\n\n\n\nSlide 01\n\n\nThe ActDisease project, an initiative funded by the European Research Council, meticulously investigates the historical trajectory of patient organisations across 20th-century Europe. The project team specifically aims to understand how these organisations fundamentally shaped disease concepts, influenced illness experiences, and contributed to the evolution of medical practices. The study concentrates on ten distinct European patient organisations, drawing examples from Sweden, Germany, France, and Great Britain, covering the period from approximately 1890 to 1990.\nCrucially, the primary source material comprises periodicals, predominantly magazines, published by these patient organisations. The authors note the historical significance of locations such as Heligoland, Germany, which served as the founding place for the Hay Fever Association of Heligoland in 1897.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "href": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.2 ActDisease Dataset Composition",
    "text": "5.2 ActDisease Dataset Composition\n\n\n\nSlide 01\n\n\nThe ActDisease project has assembled a comprehensive private dataset, comprising a recently digitised collection of patient organisation magazines. This extensive corpus totals 96,186 pages, providing a rich foundation for historical inquiry. The dataset encompasses materials from various European countries, each focusing on specific diseases and spanning distinct chronological periods.\nSpecifically, the German collection includes two magazines on Allergy/Asthma, covering 10,926 pages from 1901 to 1985; one magazine on Diabetes, with 19,324 pages from 1931 to 1990; and one on Multiple Sclerosis, contributing 5,646 pages from 1954 to 1990. Swedish contributions feature one Allergy/Asthma magazine (4,054 pages, 1957-1990), one Diabetes magazine (7,150 pages, 1949-1990), and one Lung Diseases magazine (16,790 pages, 1938-1991).\nFrom France, the dataset incorporates one Diabetes magazine (6,206 pages, 1947-1990) and three magazines on Rheumatism/Paralysis (9,317 pages, 1935-1990). Finally, the UK component includes one Diabetes magazine (11,127 pages, 1935-1990) and one Rheumatism magazine (5,646 pages, 1950-1990). This detailed breakdown highlights the breadth and depth of the primary source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "href": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.3 Digitisation Challenges and Post-OCR Correction",
    "text": "5.3 Digitisation Challenges and Post-OCR Correction\n\n\n\nSlide 01\n\n\nThe digitisation process for the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition. This tool generally performed well, accurately recognising most common layouts and fonts encountered in the periodicals. Nevertheless, significant challenges persisted, particularly with complex page layouts, slanted text, rare font types, and inconsistencies arising from varying scan or photo quality.\nThese issues frequently resulted in OCR errors, notably in German and French texts, and often led to a disrupted reading order within documents. To address these limitations, Danilova and Aangenendt conducted specific experiments focusing on post-OCR correction of German texts, leveraging instruction-tuned generative models. Their work is detailed in a forthcoming publication. A notable observation during this phase was the high frequency of OCR errors in creative textual elements, such as advertisements, humour pages, and poems, which often feature non-standard formatting or stylistic choices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-imperative-for-genre-classification-addressing-textual-diversity-and-analytical-limitations",
    "href": "chapter_ai-nepi_005.html#the-imperative-for-genre-classification-addressing-textual-diversity-and-analytical-limitations",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.4 The Imperative for Genre Classification: Addressing Textual Diversity and Analytical Limitations",
    "text": "5.4 The Imperative for Genre Classification: Addressing Textual Diversity and Analytical Limitations\n\n\n\nSlide 02\n\n\nA significant challenge in analysing the ActDisease materials stems from their inherent textual diversity. The project team observed a wide array of text types within the periodicals, a diversity that remained consistent across all magazines. Crucially, these disparate text types frequently co-occurred on the same page; for instance, an administrative report might appear alongside an advertisement or a humour section.\nThis intricate intermingling of genres poses a significant limitation for conventional analytical methods. Existing approaches, such as yearly and decade-based topic models and term counts, inherently fail to account for such fine-grained textual variations. Consequently, these aggregate analyses are prone to bias, disproportionately emphasising the characteristics of the most frequently occurring text type within a given analytical unit. This limitation underscores the necessity for a more granular approach to textual analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-as-a-foundational-concept-for-historical-analysis",
    "href": "chapter_ai-nepi_005.html#genre-as-a-foundational-concept-for-historical-analysis",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.5 Genre as a Foundational Concept for Historical Analysis",
    "text": "5.5 Genre as a Foundational Concept for Historical Analysis\n\n\n\nSlide 03\n\n\nRecognising the limitations of aggregate analysis, genre emerged as a particularly useful concept for distinguishing between various text types. Within Language Technology, scholars define genre as a class of documents united by a shared communicative purpose, a definition articulated by Petrenz (2004) and Kessler (1997). This conceptual framework directly supports the project’s core objective: to explore the historical data from multiple perspectives, thereby facilitating robust historical arguments.\nSpecifically, genre classification enables a nuanced study of communicative strategies as they evolve over time, as highlighted by Broersma (2010). This approach allows for comparative analysis across different countries, diseases, and publications. Furthermore, it facilitates a more granular examination of term distributions and the application of topic models, ensuring that these analyses are conducted within specific genre groups, thereby enhancing their precision and historical relevance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.6 Illustrative Genre Examples within the ActDisease Dataset",
    "text": "5.6 Illustrative Genre Examples within the ActDisease Dataset\n\n\n\nSlide 03\n\n\nTo illustrate the practical application of genre classification within the ActDisease dataset, the authors provide several concrete examples. These instances demonstrate the diverse textual forms encountered in the historical periodicals and clarify how distinct genres manifest within the corpus.\n\nAcademic Reports: One illustrative example of genre within the dataset is an academic report. This specific instance details studies conducted on the pancreas, showcasing the presence of scientific and research-oriented content within the periodicals.\nLegal Documents: Another distinct genre identified is legal documentation. An example provided is a deed of covenant, demonstrating the inclusion of formal legal texts within the historical periodicals.\nAdvertisements: The dataset also features advertisements as a prominent genre. One particular example showcases an advertisement for chocolate, specifically formulated and marketed for individuals with diabetes, reflecting the commercial aspects present in these historical publications.\nInstructive/Guidance Texts: Instructive or guidance texts form another key genre. Examples include practical advice, such as recipes, or medical guidance provided by doctors, including dietary recommendations. These texts aim to inform and direct the reader on specific actions or health-related matters.\nPatient Organisation Reports: Patient organisation reports constitute a significant genre within the corpus. These documents typically detail the proceedings of meetings and outline the various activities undertaken by the organisations, providing insight into their operational aspects.\nPatient Experiences: Finally, the dataset includes narratives focused on patient experiences. These texts recount personal stories and aspects of patient lives, offering qualitative insights into the lived realities of individuals with specific health conditions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-classification-experiments-zero-shot-and-few-shot-learning",
    "href": "chapter_ai-nepi_005.html#genre-classification-experiments-zero-shot-and-few-shot-learning",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.7 Genre Classification Experiments: Zero-Shot and Few-Shot Learning",
    "text": "5.7 Genre Classification Experiments: Zero-Shot and Few-Shot Learning\n\n\n\nSlide 04\n\n\nGiven the significant constraint of very limited annotated data, Danilova and Söderfeldt systematically explored both zero-shot and few-shot learning paradigms for genre classification. The zero-shot investigations addressed two primary research questions: firstly, whether an efficient mapping of genre labels from publicly available datasets to the project’s custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might vary across different external datasets and models.\nFor few-shot learning, the inquiry focused on understanding how performance changes with varying training data sizes across different models. A further crucial question examined whether prior fine-tuning on the entire dataset could substantially enhance classification performance. These experiments form the basis of a forthcoming publication by Danilova and Söderfeldt, presented at LaTeCH-CLFL 2025.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-custom-genre-labels-for-historical-analysis",
    "href": "chapter_ai-nepi_005.html#defining-custom-genre-labels-for-historical-analysis",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.8 Defining Custom Genre Labels for Historical Analysis",
    "text": "5.8 Defining Custom Genre Labels for Historical Analysis\n\n\n\nSlide 05\n\n\nThe project meticulously defined its custom genre labels under the direct supervision of the main historian, an expert in patient organisations. This collaborative approach ensured that the labels would prove highly useful for segmenting content within the ActDisease materials, thereby facilitating more granular historical analysis. Furthermore, the design principle aimed for maximum generality, enabling the classifier’s potential application to similar historical datasets beyond the immediate scope of the project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defined-genres-and-their-communicative-purposes",
    "href": "chapter_ai-nepi_005.html#defined-genres-and-their-communicative-purposes",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.9 Defined Genres and Their Communicative Purposes",
    "text": "5.9 Defined Genres and Their Communicative Purposes\n\n\n\nSlide 05\n\n\nThe project established a comprehensive set of genres, each with a clearly defined communicative purpose:\n\nAcademic texts encompass research-based reports or explanations of scientific ideas, such as articles or formal reports, primarily aiming to convey information from the scientific and medical community to the magazine’s readership.\nAdministrative documents, including meeting minutes, reports, or announcements, serve to inform readers about the patient organisation’s events and activities.\nAdvertisements explicitly promote products or services for commercial ends.\nGuide texts offer step-by-step instructions, ranging from health tips and legal advice to recipes.\nFiction, conversely, seeks to entertain and emotionally engage through stories, poems, humour, or myths.\nLegal documents elucidate terms and conditions, encompassing contracts, rules, and amendments.\nNews reports recent events and developments.\nNonfiction Prose narrates real events or describes cultural and historical topics, exemplified by memoirs, essays, or documentaries.\nQA (Question and Answer) sections present structured questions alongside expert responses, directly sourced from the periodicals themselves.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-methodology-and-inter-annotator-agreement",
    "href": "chapter_ai-nepi_005.html#annotation-methodology-and-inter-annotator-agreement",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.10 Annotation Methodology and Inter-Annotator Agreement",
    "text": "5.10 Annotation Methodology and Inter-Annotator Agreement\n\n\n\nSlide 06\n\n\nFor the annotation process, the project team defined the paragraph as the primary unit. These paragraphs originated from the ABBYY OCR output and were subsequently merged based on consistent font patterns—including type, size, italics, and lack of bolding—within each page. The annotation effort focused on a carefully selected sample from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” specifically their first and mid-year issues from each publication year.\nSix project members undertook the annotation task, comprising four historians and two computational linguists, all possessing native or proficient fluency in either Swedish or German. Each paragraph received two independent annotations. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, measured by Krippendorff’s alpha, signifying a remarkably high level of consistency amongst the annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-process-illustration",
    "href": "chapter_ai-nepi_005.html#annotation-process-illustration",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.11 Annotation Process Illustration",
    "text": "5.11 Annotation Process Illustration\n\n\n\nSlide 06\n\n\nTo illustrate the annotation process, the authors presented a sample from the periodical “Der Diabetiker.” This example took the form of a screenshot from a .numbers file, the digital tool employed by the annotators. Whilst the original sentences appeared in German, the illustration provided Google Translate renditions for clarity.\nThe file structure featured columns for metadata such as Year, Volume, Issue Number, Title, and the Paragraph text itself. Following these, a series of binary flag columns corresponded to each defined genre—academic, administrative, advertisement, fiction, guide, nonfiction prose, legal, QA, and news. Annotators were tasked with providing definitive, hard assignments for the genre of each paragraph, a process that, whilst challenging, they successfully completed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.12 Dataset Splitting and Experimental Configurations",
    "text": "5.12 Dataset Splitting and Experimental Configurations\n\n\n\nSlide 07\n\n\nFor the experimental phase, the authors meticulously split the annotated data into distinct sets. The main division comprised a training set of 1182 paragraphs and a held-out set of 552 paragraphs, constituting approximately 30% of the total annotated data, with stratification applied by label.\nFor few-shot experiments, the training data was further subdivided into six different sizes: 100, 200, 300, 400, 500, and the full 1182 paragraphs. The authors randomly sampled these subsets from the main training set, ensuring balance across labels. The held-out set was then equally partitioned into validation and test sets, also maintaining label balance. Notably, the legal and news genres were excluded from these experiments due to insufficient training data. Conversely, zero-shot experiments utilised the entirety of the test set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-and-language-distribution-in-actdisease-datasets",
    "href": "chapter_ai-nepi_005.html#genre-and-language-distribution-in-actdisease-datasets",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.13 Genre and Language Distribution in ActDisease Datasets",
    "text": "5.13 Genre and Language Distribution in ActDisease Datasets\n\n\n\nSlide 07\n\n\nA visual representation of the ActDisease dataset’s composition reveals the distributions across languages and genres within both the training and held-out samples. This analysis highlighted a pronounced imbalance in the representation of advertisement and non-fictional prose genres when examined across different languages, indicating potential challenges for models in generalising across these categories.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-genre-classification",
    "href": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-genre-classification",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.14 External Datasets for Zero-Shot Genre Classification",
    "text": "5.14 External Datasets for Zero-Shot Genre Classification\n\n\n\nSlide 07\n\n\nFor the zero-shot experiments, the authors incorporated several external datasets, drawing primarily from modern collections previously utilised in automatic web genre classification. The Corpus of Online Registers of English (CORE), compiled by Egbert et al. (2015), provided English data, alongside main categories in Swedish, Finnish, and French, with annotations at the document level.\nSimilarly, the Functional Text Dimensions (FTD) dataset of web genres, developed by Sharoff (2018), offered balanced English and Russian content, also annotated at the document level, and had seen prior application in web genre classification by Kuzman et al. (2023). Additionally, a subset of Universal Dependencies, known as UD-MULTIGENRE (UDM) (de Marneffe et al., 2021), provided genre annotations at the sentence level across 38 languages, with recovered annotations by Danilova and Stymne (2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.15 Cross-Dataset Genre Mapping",
    "text": "5.15 Cross-Dataset Genre Mapping\n\n\n\nSlide 08\n\n\nA critical step in the zero-shot methodology involved mapping the project’s custom genre labels to those present in the external datasets. Two independent annotators performed this genre mapping, with only assignments achieving full agreement selected for the final mapping. The resulting table illustrates how ActDisease genres—including Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction, and QA—correspond to labels within the CORE, UDM, and FTD datasets. Notably, the authors observed that certain ActDisease genres lacked suitable, directly mappable equivalents within the available external datasets, posing a challenge for comprehensive cross-dataset alignment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-creation-pipeline-and-model-fine-tuning",
    "href": "chapter_ai-nepi_005.html#training-data-creation-pipeline-and-model-fine-tuning",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.16 Training Data Creation Pipeline and Model Fine-tuning",
    "text": "5.16 Training Data Creation Pipeline and Model Fine-tuning\n\n\n\nSlide 08\n\n\nThe training data creation pipeline commenced with the established genre mapping, followed by crucial preprocessing, chunking, and sampling stages. The authors generated training sets in four distinct configurations: one focusing exclusively on Germanic languages ([G+]), another balancing data according to the ActDisease labels ([B1]), a third incorporating all language families ([G-]), and a final configuration balancing data by both ActDisease and original labels ([B2]).\nThis systematic approach yielded four training samples each from the FTD, CORE, UDM, and a merged dataset. Subsequently, these samples underwent fine-tuning with a selection of multilingual encoder models, specifically XLM-Roberta, mBERT, and hmBERT. This comprehensive process ultimately produced a total of 48 fine-tuned models for evaluation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models-for-genre-classification",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.17 Multilingual Encoder Models for Genre Classification",
    "text": "5.17 Multilingual Encoder Models for Genre Classification\n\n\n\nSlide 09\n\n\nFor the genre classification experiments, the authors employed a selection of robust multilingual encoder models. These included XLM-Roberta, developed by Conneau et al. (2020); mBERT, introduced by Devlin et al. (2019); and historical mBERT, a model from Schweter et al. (2022). The choice of these models was deliberate, resting on their proven efficacy in prior research.\nBERT-like models, in general, have seen extensive application in web register and genre classification, as evidenced by works from Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). XLM-RoBERTa, in particular, stands out as a state-of-the-art web genre classifier, according to Kuzman et al. (2023). Historical mBERT held particular interest due to its pretraining on a substantial corpus of multilingual historical newspapers, which notably encompassed the languages relevant to the ActDisease project. Conversely, mBERT served as a crucial comparative baseline against historical mBERT, as direct comparisons with XLM-Roberta were not feasible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#model-training-and-evaluation-metrics",
    "href": "chapter_ai-nepi_005.html#model-training-and-evaluation-metrics",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.18 Model Training and Evaluation Metrics",
    "text": "5.18 Model Training and Evaluation Metrics\n\n\n\nSlide 09\n\n\nFollowing the comprehensive fine-tuning process across all specified configurations, the project successfully generated 48 distinct models. For evaluation purposes, the authors computed and presented the performance metrics as averages across these diverse configurations, providing a robust overview of the models’ capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-methodology",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-methodology",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.19 Zero-Shot Learning Evaluation Methodology",
    "text": "5.19 Zero-Shot Learning Evaluation Methodology\n\n\n\nSlide 09\n\n\nEvaluating the zero-shot predictions presented a unique challenge: the imperfect overlap between the project’s custom genre labels and those in the external datasets precluded a direct comparison of overall performance metrics. To circumvent this, the authors meticulously assessed the performance of each genre individually, whilst also analysing confusion matrices to identify and mitigate potential biases.\nThe X-GENRE web genre classifier, a state-of-the-art model developed by Kuzman et al. (2023), served as a crucial baseline. Predictions were strictly limited to the most similar labels that could be directly mapped to the project’s custom categories. Furthermore, the evaluation acknowledged the cross-lingual nature of certain scenarios: the FTD and X-GENRE applications were entirely cross-lingual, lacking German or Swedish training data, whilst the UDM and CORE datasets presented partially cross-lingual contexts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-classification-results-and-model-biases",
    "href": "chapter_ai-nepi_005.html#zero-shot-classification-results-and-model-biases",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.20 Zero-Shot Classification Results and Model Biases",
    "text": "5.20 Zero-Shot Classification Results and Model Biases\n\n\n\nSlide 10\n\n\nOverall, the zero-shot classification experiments revealed that models fine-tuned on the Functional Text Dimensions (FTD) dataset exhibited superior performance when applied with the project’s custom genre mapping. Whilst most configurations avoided significant bias, other datasets demonstrated distinct class-specific tendencies. For instance, models trained on UD-MULTIGENRE (UDM) showed a bias towards news, primarily because the news training data contained the highest proportion of Germanic instances, predominantly German. Conversely, CORE-based models displayed a bias towards the guide genre, as only its guide training data was multilingual.\nIntriguingly, specific models demonstrated notable strengths in particular genres. XLM-Roberta, when fine-tuned on UDM, achieved an average of 32% more correct predictions in the QA category compared to mBERT and hmBERT. Similarly, hmBERT, also on UDM, outperformed XLM-Roberta and mBERT in the Administrative genre by an average of 16% more correct predictions. Furthermore, models trained on CORE consistently performed well in predicting legal texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#visualising-zero-shot-performance-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#visualising-zero-shot-performance-confusion-matrices",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.21 Visualising Zero-Shot Performance: Confusion Matrices",
    "text": "5.21 Visualising Zero-Shot Performance: Confusion Matrices\n\n\n\nSlide 10\n\n\nTo provide a granular understanding of the zero-shot classification performance, the authors presented confusion matrices for several key configurations. These visualisations critically highlight specific classification behaviours, such as instances of particularly strong performance in certain categories or recurring patterns of misclassification, which were explicitly delineated with red frames for emphasis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#detailed-zero-shot-f1-scores-by-category",
    "href": "chapter_ai-nepi_005.html#detailed-zero-shot-f1-scores-by-category",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.22 Detailed Zero-Shot F1 Scores by Category",
    "text": "5.22 Detailed Zero-Shot F1 Scores by Category\n\n\n\nSlide 11\n\n\nThe detailed average F1 scores for each category provide a comprehensive overview of the zero-shot classification performance. Notably, specific values were highlighted, indicating instances where the observed performance did not stem from systematic biases towards those particular categories, thereby suggesting more robust and generalisable classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#overall-average-performance-across-configurations",
    "href": "chapter_ai-nepi_005.html#overall-average-performance-across-configurations",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.23 Overall Average Performance Across Configurations",
    "text": "5.23 Overall Average Performance Across Configurations\n\n\n\nSlide 11\n\n\nA detailed discussion of the overall average performance across configurations was omitted from the presentation, allowing for a more focused examination of specific genre and model behaviours.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-mlm-fine-tuning",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-mlm-fine-tuning",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.24 Few-Shot Learning Performance and MLM Fine-tuning",
    "text": "5.24 Few-Shot Learning Performance and MLM Fine-tuning\n\n\n\nSlide 11\n\n\nThe few-shot learning experiments unequivocally demonstrated that further training on the ActDisease dataset, especially when augmented with Masked Language Model (MLM) fine-tuning, confers a distinct advantage. The authors observed a consistent upward trend in the F1 score as the number of training instances increased, although performance remained below 0.8 even with the largest training set of 1182 instances.\nCrucially, hmBERT-MLM emerged as the top performer amongst the models. Prior MLM fine-tuning significantly boosted the performance of this historical model, enabling it to outperform all other models, albeit by a narrow margin. This finding underscores the value of domain-specific pre-training for enhanced classification accuracy in low-resource settings.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#detailed-few-shot-performance-per-category-f1-and-overall-metrics",
    "href": "chapter_ai-nepi_005.html#detailed-few-shot-performance-per-category-f1-and-overall-metrics",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.25 Detailed Few-Shot Performance: Per-Category F1 and Overall Metrics",
    "text": "5.25 Detailed Few-Shot Performance: Per-Category F1 and Overall Metrics\n\n\n\nSlide 12\n\n\nA granular examination of the detailed scores revealed the underlying reason for hmBERT’s superior performance in the few-shot learning paradigm. This model uniquely maintained its ability to distinguish between fiction and nonfiction genres, even when utilising the full dataset size. Conversely, other models, notably XLM-Roberta, exhibited a drastic decline in performance when attempting to differentiate these two categories, highlighting a specific area of weakness in their classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#challenges-in-distinguishing-fiction-and-nonfiction-prose",
    "href": "chapter_ai-nepi_005.html#challenges-in-distinguishing-fiction-and-nonfiction-prose",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.26 Challenges in Distinguishing Fiction and Nonfiction Prose",
    "text": "5.26 Challenges in Distinguishing Fiction and Nonfiction Prose\n\n\n\nSlide 12\n\n\nAn analysis of the XLM-Roberta-MLM confusion matrix, particularly with the full-sized training dataset, revealed a persistent challenge: nonfiction prose was frequently overpredicted as fiction. This phenomenon suggests that, with larger datasets, fiction and nonfictional prose may become increasingly similar in their textual characteristics. This convergence is likely exacerbated by the domain-specific nature of the corpus, as all genres are confined to patient organisation magazines primarily focused on diabetes. Consequently, both fictional and (auto)biographical texts frequently revolve around the shared experiences of diabetes patients, leading to an overlap in themes and narrative structures. The authors conclude that more data is likely required to enhance the model’s ability to accurately differentiate between these two closely related genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.27 Few-Shot Prompting Evaluation",
    "text": "5.27 Few-Shot Prompting Evaluation\n\n\n\nSlide 13\n\n\nThe project also undertook an evaluation of few-shot prompting techniques, exploring their efficacy in genre classification.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.28 Few-Shot Prompting with Llama 3.1 8b Instruct",
    "text": "5.28 Few-Shot Prompting with Llama 3.1 8b Instruct\n\n\n\nSlide 13\n\n\nGiven the current lack of sufficient data for comprehensive instruction tuning, the authors explored few-shot prompting with Llama 3.1 8b Instruct, a widely recognised multilingual generative model with open weights. The prompt structure incorporated clear genre definitions, supplemented by two to three carefully selected examples for each category.\nThe results, measured by F1-score, indicated varied performance across genres: QA achieved 0.62, academic 0.72, administrative 0.60, advertisement 0.73, fiction 0.64, guide 0.61, legal 0.84, news 0.08, and nonfictional prose 0.49. Overall metrics included an accuracy of 0.62, a macro average of 0.59, and a weighted average of 0.63. Whilst the model handled certain labels competently, the limited number of examples proved insufficient to adequately represent the nuances of some genres, particularly nonfictional prose, advertisement, and administrative texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusions",
    "href": "chapter_ai-nepi_005.html#conclusions",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.29 Conclusions",
    "text": "5.29 Conclusions\n\n5.29.1 Genre Diversity and Text Mining Challenges\n\n\n\nSlide 13\n\n\nPopular magazines, unlike more homogenous scientific journals or books, frequently contain a multitude of genres, which inherently complicates text mining efforts. Nevertheless, these magazines represent a highly promising source for historical research, particularly within the history of science. The rich diversity of genres within these publications directly reflects the deliberate choices of communicative strategies employed by their authors and editors. Whilst accounting for these varied genres presents a significant challenge, it remains critically important for ensuring the accurate and detailed interpretation of text mining results. Ultimately, genre classification emerges as a vital methodology, capable of rendering these complex historical sources accessible for advanced textual analysis.\n\n\n5.29.2 Leveraging Modern Datasets for Zero-Shot Classification\n\n\n\nSlide 14\n\n\nDespite the challenges posed by their rich genre diversity, genre classification offers a powerful means to render popular magazines accessible for advanced textual analysis. Crucially, even in the absence of specific training data, the authors successfully leveraged existing modern datasets for classification, particularly when working with broadly defined, general-purpose genre categories.\n\n\n5.29.3 Effectiveness of Open Generative Models\n\n\n\nSlide 14\n\n\nWhen confronted with a lack of specific training data, researchers can effectively utilise existing modern datasets. Furthermore, open generative models have demonstrated their capacity to achieve a decent level of classification quality, offering a viable alternative in resource-constrained scenarios.\n\n\n5.29.4 Superiority of Few-Shot Learning with Multilingual Encoders\n\n\n\nSlide 14\n\n\nWhilst open generative models can achieve a respectable level of quality, few-shot learning applied to multilingual encoders, particularly when combined with prior Masked Language Model (MLM) fine-tuning, consistently demonstrates superior performance.\n\n\n5.29.5 Significant Gains with Historical Multilingual BERT\n\n\n\nSlide 15\n\n\nNotably, historical mBERT exhibited particularly strong gains, achieving a 24% improvement, significantly outperforming mBERT-MLM (14.5%) and XLM-RoBERTa (16.9%).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.30 Ongoing and Future Research Directions",
    "text": "5.30 Ongoing and Future Research Directions\n\n\n\nSlide 15\n\n\nThe project continues to advance, currently engaging with specific historical hypotheses to deepen its analytical insights. The project team is actively developing a new annotation scheme designed to capture more fine-grained genre distinctions, an initiative supported by funding from Swe-CLARIN. Furthermore, the team is exploring advanced methodologies such as synthetic data generation and active learning to enhance the quality and efficiency of their classification efforts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#acknowledgements",
    "href": "chapter_ai-nepi_005.html#acknowledgements",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.31 Acknowledgements",
    "text": "5.31 Acknowledgements\n\n\n\nSlide 15\n\n\nThe project gratefully acknowledges the invaluable contributions of its annotators, comprising the dedicated project team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. Funding for this research was generously provided by the European Research Council under grant ERC-2021-STG 10104099. The Centre for Digital Humanities and Social Sciences offered crucial support, including access to GPUs and data storage facilities. Finally, the authors extend their gratitude to Dr Maria Skeppstedt and the anonymous reviewers for their insightful feedback.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#concluding-remarks",
    "href": "chapter_ai-nepi_005.html#concluding-remarks",
    "title": "Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report",
    "section": "5.32 Concluding Remarks",
    "text": "5.32 Concluding Remarks\nFor further details and ongoing updates, readers are invited to consult the project’s official website.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification in Historical Patient Organisation Periodicals: A Methodological Report</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant initiative, meticulously charts the profound influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition on the development of natural philosophy. Professor Cornelis J. Schilt and his team aim to uncover broad networks of texts, passages, themes, topics, and authors, many of which historians have largely left unexamined. The project employs a substantial, diverse multilingual dataset of approximately 430,000 printed texts, spanning from 1540 to 1728, sourced from the Early English Books Online (EEBO), Gallica, and the Bavarian State Library.\nAddressing significant challenges posed by variable OCR quality, early modern typography, and multilingual semantics, the VERITRACE team harnesses state-of-the-art digital techniques. These include keyword search, text matching, topic modelling, and sentiment analysis. The project strategically deploys Large Language Models (LLMs) in two primary capacities: GPT-based LLMs serve as “judges” for enriching and cleaning bibliographic metadata, whilst BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages, thereby facilitating sophisticated text matching.\nA complex 15-stage data processing pipeline transforms raw XML, HOCR, and HTML files into a structured Elasticsearch database, which underpins the VERITRACE web application. This alpha version of the application offers several functionalities: an “Explore” section for corpus statistics, a “Metadata Explorer” for detailed record examination (including granular language identification and OCR quality assessment), and a “Search” interface supporting complex keyword and field queries. Crucially, the “Match” section enables the identification of textual reuse through both lexical and semantic comparisons, supporting single-document, multi-document, and full-corpus analyses. Although the current BERT-based embedding model (LaBSE) shows promise for semantic matching across languages, its performance with early modern, out-of-domain data, coupled with OCR issues, mandates further investigation into alternative models or fine-tuning strategies. The project anticipates future challenges related to semantic drift over centuries, scaling computational power for the vast corpus, and optimising performance for user-facing queries.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-project-and-team",
    "href": "chapter_ai-nepi_006.html#the-veritrace-project-and-team",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 The VERITRACE Project and Team",
    "text": "6.1 The VERITRACE Project and Team\n\n\n\nSlide 01\n\n\nThe VERITRACE project, formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,” operates as an ERC-STG Project (101076836). A dedicated team of five individuals, primarily based at the Vrije Universiteit Brussel (VUB) in Brussels, drives this initiative. Professor Dr. Cornelis J. Schilt serves as the Principal Investigator, leading a diverse group that includes a class assistant and two historians. The speaker, himself a historian of science and medicine with an 18th-century specialism, fulfils the pivotal role of digital humanities specialist for the project. Further information is available on the project’s website, HTTPS://VERITRACE.EU.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-objectives-and-historical-context",
    "href": "chapter_ai-nepi_006.html#project-objectives-and-historical-context",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 Project Objectives and Historical Context",
    "text": "6.2 Project Objectives and Historical Context\n\n\n\nSlide 01\n\n\nThis five-year ERC Starting Grant project, active from 2023 to 2028, is firmly rooted at the Vrije Universiteit Brussel. Its central objective involves meticulously charting the profound influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia, upon the nascent field of early modern natural philosophy and science. This tradition manifests in significant historical works such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most famously for scholars of chemistry, the Corpus Hermeticum.\nHistorical evidence already confirms the tradition’s impact; Isaac Newton, for instance, engaged with the Sibylline Oracles, whilst Johannes Kepler demonstrated familiarity with the Corpus Hermeticum. Beyond these well-documented instances, the VERITRACE team aims to delve deeper. The researchers have assembled a close-reading corpus of 140 works specifically representing this tradition. Their broader ambition extends to uncovering a much wider, often overlooked, network of texts, passages, themes, topics, and authors who engaged with this tradition—a collection one scholar aptly terms the ‘great unread’, given the frequent neglect of these numerous works by lesser-known authors in historical scholarship.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-to-history-and-philosophy-of-science-hpss",
    "href": "chapter_ai-nepi_006.html#computational-approaches-to-history-and-philosophy-of-science-hpss",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Approaches to History and Philosophy of Science (HPSS)",
    "text": "6.3 Computational Approaches to History and Philosophy of Science (HPSS)\n\n\n\nSlide 03\n\n\nThe VERITRACE project fundamentally seeks to advance computational approaches within the history and philosophy of science. A primary goal involves applying large-scale multilingual exploration to the central research question. The researchers specifically aim to identify textual re-use, distinguishing between direct lexical instances, such as uncited quotations, and more indirect semantic re-use, encompassing paraphrases or conceptually similar content that contemporary readers would have recognised. This functionality effectively serves as an “early modern plagiarism detector.” Beyond this, the initiative strives to uncover potentially ignored networks of texts, passages, themes, topics, and authors. Ultimately, this systematic exploration promises to reveal novel patterns within the intellectual history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-construction-and-digital-analysis-techniques",
    "href": "chapter_ai-nepi_006.html#corpus-construction-and-digital-analysis-techniques",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Corpus Construction and Digital Analysis Techniques",
    "text": "6.4 Corpus Construction and Digital Analysis Techniques\n\n\n\nSlide 04\n\n\nThe VERITRACE project constructs a substantial, diverse, and multilingual dataset, focusing exclusively on printed works rather than handwritten materials for manageability. This corpus draws from three distinct multilingual sources, encompassing texts in at least six different languages. The selected publication period spans approximately 200 years, commencing in 1540 and concluding in 1728, shortly after Isaac Newton’s passing. Key data sources include the Early English Books Online (EEBO), materials downloaded from the French National Library via Gallica, and, as the largest contributor, the Bavarian State Library. Cumulatively, this effort has amassed a corpus of approximately 430,000 books. The researchers plan to analyse this extensive collection using state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, amongst others.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-llm-integration",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-llm-integration",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Core Challenges and LLM Integration",
    "text": "6.5 Core Challenges and LLM Integration\n\n\n\nSlide 04\n\n\nThe VERITRACE project confronts several core challenges inherent in processing historical texts at scale. A primary hurdle involves the variable quality of Optical Character Recognition (OCR) text, which libraries provide in raw formats such as XML, HOCR, and HTML, crucially without accompanying ground truth page images. This directly impacts all subsequent data processing. Furthermore, early modern typography and semantics present significant complexities across the project’s six or more languages. The sheer volume of data—hundreds of thousands of texts published across Europe over two centuries—also poses a considerable challenge.\nTo address these issues, the project integrates Large Language Models (LLMs) in a two-sided approach. On the decoder side, GPT-based LLMs function as “judges” to enrich and clean the project’s metadata. On the encoder side, BERT-based LLMs generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, a critical step for enabling the desired text matching functionalities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#case-study-llms-as-judges-for-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#case-study-llms-as-judges-for-metadata-enrichment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 Case Study: LLMs as Judges for Metadata Enrichment",
    "text": "6.6 Case Study: LLMs as Judges for Metadata Enrichment\n\n\n\nSlide 06\n\n\nA specific case study explores the application of LLMs as judges for metadata enrichment within the VERITRACE project. The fundamental motivation stems from the Universal Short Title Catalogue (USTC), recognised as a high-quality source of bibliographic metadata. The VERITRACE team aims to map their records onto USTC records, thereby generating enriched metadata that requires less manual cleaning. Whilst some mapping can be automated, for instance through external identifiers, the majority of records necessitate manual comparison due to the initial uncleaned state of the VERITRACE data. This manual process, involving the comparison of bibliographic metadata pairs to determine if they represent the same underlying printed text, proves exceptionally tedious; each team member was assigned 10,000 such pairs for review. Consequently, the project proposes employing LLMs to automate this laborious task.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llm-bench-for-bibliographic-record-evaluation",
    "href": "chapter_ai-nepi_006.html#llm-bench-for-bibliographic-record-evaluation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 LLM Bench for Bibliographic Record Evaluation",
    "text": "6.7 LLM Bench for Bibliographic Record Evaluation\n\n\n\nSlide 06\n\n\nTo evaluate bibliographic record matches, the researchers have devised a system employing a chain of LLMs, conceptualised as a “bench” or panel of judges. This chain comprises a Primary LLM, a Secondary LLM, a Tiebreaker LLM, and an Expert LLM, which handles edge cases and facilitates a circular review process. The LLMs’ task involves assessing pairs of bibliographic records—one originating from a low-quality metadata source and the other from a high-quality source—to ascertain whether they represent the identical underlying text. Crucially, the models must provide not only a judgment (match or no match) but also detailed reasoning and confidence levels for each decision. The project validates these LLM decisions against a ground truth dataset, with the VERITRACE team conducting a final review.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prompt-guidelines-and-output-structure-for-llm-judges",
    "href": "chapter_ai-nepi_006.html#prompt-guidelines-and-output-structure-for-llm-judges",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Prompt Guidelines and Output Structure for LLM Judges",
    "text": "6.8 Prompt Guidelines and Output Structure for LLM Judges\n\n\n\nSlide 06\n\n\nThe LLM judges receive pairs of bibliographic records alongside extensive prompt guidelines. These guidelines delineate field priorities, specific match criteria, and indicators for non-matches, ensuring a structured evaluation process. The expected output from the LLMs adheres to a defined structure, including the ground truth, the decisions made by the Primary, Secondary, and Tiebreaker models, the key factors influencing their judgments, and a detailed reasoning for each match or non-match decision. This comprehensive output facilitates analysis and validation of the LLMs’ performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-in-llm-based-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#challenges-in-llm-based-metadata-enrichment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Challenges in LLM-Based Metadata Enrichment",
    "text": "6.9 Challenges in LLM-Based Metadata Enrichment\n\n\n\nSlide 07\n\n\nThe implementation of LLMs for metadata enrichment remains a work in progress, not yet achieving full functionality. A significant challenge involves the frequent occurrence of hallucinations in the output, where LLMs generate records not present in the original input. This issue is particularly observed with open-source models like Llama. Furthermore, whilst requesting more structured output aims to reduce unhelpful responses, it often results in more generic and less insightful reasoning, especially regarding the justification for decisions. Finding the optimal balance between structured output and the utility of the responses proves difficult, akin to an art rather than a precise science. Despite these hurdles, the project recognises the substantial potential for time-saving if this approach can be successfully implemented, and actively seeks external advice on these ongoing challenges.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 VERITRACE Web Application: Alpha Version",
    "text": "6.10 VERITRACE Web Application: Alpha Version\n\n\n\nSlide 07\n\n\nThe VERITRACE web application currently exists as an alpha version, an extremely new development not yet publicly accessible. This iteration resides on the speaker’s local computer, with functionalities demonstrated through screenshots. It serves as a foundational promise of the project’s future capabilities. Engineers are presently testing a BERT-based LLM, specifically LaBSE, to generate vector embeddings for every passage within the extensive textual corpus. However, preliminary assessments suggest that LaBSE will likely prove insufficient for the project’s comprehensive requirements, despite demonstrating efficacy in certain specific cases.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-processing-pipeline-architecture",
    "href": "chapter_ai-nepi_006.html#data-processing-pipeline-architecture",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Data Processing Pipeline Architecture",
    "text": "6.11 Data Processing Pipeline Architecture\n\n\n\nSlide 08\n\n\nThe project employs a complex 15-stage data processing pipeline to transform raw textual data, received in XML, HOCR, and HTML formats from libraries, into a structured Elasticsearch database that serves as the web application’s backend. This intricate process demands meticulous optimisation at each stage. Key tasks within the pipeline include extracting text into clean text files, generating precise mappings of all character positions, segmenting the text into meaningful units such as sentences and passages, and rigorously assessing the OCR quality. The generation of vector embeddings, crucial for semantic analysis, occurs towards the latter stages of this comprehensive pipeline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-explore-section",
    "href": "chapter_ai-nepi_006.html#web-application-explore-section",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Web Application: Explore Section",
    "text": "6.12 Web Application: Explore Section\n\n\n\nSlide 08\n\n\nThe VERITRACE web application organises its functionalities across approximately five main sections. The “Explore” section serves as a central hub for users to gain insights into the corpus through various statistics and metadata visualisations. This statistical data is directly sourced from a Mongo database. Currently, the system holds 427,305 metadata records describing the books within the corpus. Users can examine detailed visualisations, including pie charts for language distribution, and bar charts illustrating documents by data source, documents by decade, and publication places.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#elasticsearch-metadata-explorer-and-ocr-quality-assessment",
    "href": "chapter_ai-nepi_006.html#elasticsearch-metadata-explorer-and-ocr-quality-assessment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.13 Elasticsearch Metadata Explorer and OCR Quality Assessment",
    "text": "6.13 Elasticsearch Metadata Explorer and OCR Quality Assessment\n\n\n\nSlide 09\n\n\nThe Elasticsearch Metadata Explorer provides users with the capability to browse and inspect the rich metadata generated for each text within the corpus. A crucial feature involves granular language identification, performed on every text down to segments of approximately 50 characters. This addresses the common issue of multilingual texts where primary metadata might only indicate a single language; for instance, the system can identify a text as 15% Greek and 85% Latin, correctly classifying it as substantively multilingual. Furthermore, the system undertakes OCR quality assessment. This challenging task is performed solely on the raw text, as ground truth page images are unavailable. The assessment is meticulously applied on a page-by-page basis, rather than assigning a single quality score to an entire book, aiming to provide a detailed quality assessment for every individual page.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-search-functionality",
    "href": "chapter_ai-nepi_006.html#advanced-search-functionality",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.14 Advanced Search Functionality",
    "text": "6.14 Advanced Search Functionality\n\n\n\nSlide 10\n\n\nScholars are anticipated to primarily utilise the “Search” section, which offers robust keyword search capabilities. Users can perform basic keyword searches, such as for “Hermes,” which, even in the current prototype indexing only 132 files, yields 22 documents with 332 total matches, resulting in a 15 GB index. The system supports more sophisticated queries leveraging Elasticsearch functionalities, including field queries (e.g., specifying “author:kepler ‘hermes’” to find one document from 1621), and complex Boolean operations (AND, OR, nested queries). Furthermore, users can execute proximity queries, for instance, locating texts where “Hermes” and “Plato” appear within ten words of each other. The full corpus, comprising over 400,000 texts, will eventually lead to an index size measured in terabytes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#planned-analytical-tools",
    "href": "chapter_ai-nepi_006.html#planned-analytical-tools",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.15 Planned Analytical Tools",
    "text": "6.15 Planned Analytical Tools\n\n\n\nSlide 11\n\n\nThe “Analyse” section of the website, whilst not yet implemented, will host a suite of advanced analytical tools. Planned features include topic modelling, latent semantic analysis (LSA), and various forms of diachronic analysis. These functionalities, drawing upon ongoing developments in the field, will be integrated into the platform to support deeper scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-reading-functionality",
    "href": "chapter_ai-nepi_006.html#text-reading-functionality",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.16 Text Reading Functionality",
    "text": "6.16 Text Reading Functionality\n\n\n\nSlide 11\n\n\nThe “Read” section of the web application provides scholars with the capability to access and read the texts in their original format. Integrating a Mirador viewer, the platform displays digital facsimiles (PDFs) for every text within the corpus. This functionality ensures that users can engage with the historical documents as they would on a library website, whilst also providing access to the associated metadata.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-match-textual-reuse-identification",
    "href": "chapter_ai-nepi_006.html#veritrace-match-textual-reuse-identification",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.17 VERITRACE Match: Textual Reuse Identification",
    "text": "6.17 VERITRACE Match: Textual Reuse Identification\n\n\n\nSlide 12\n\n\nThe “Match” section of the VERITRACE web application is specifically designed to identify textual reuse across different documents. This functionality supports various comparison modes: users can compare a single document against another, conduct multi-document comparisons (for instance, pitting Isaac Newton’s Latin Optics against all of Johannes Kepler’s works within the database), or even perform a full corpus match, comparing one text against the entire collection. The latter, however, presents a significant computational challenge, potentially leading to considerable wait times for users. To address the nuanced nature of text matching, the interface exposes numerous parameters, such as minimum similarity scores, allowing users to fine-tune the comparison settings and obtain tailored results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-types-and-cross-language-sanity-check",
    "href": "chapter_ai-nepi_006.html#text-matching-types-and-cross-language-sanity-check",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.18 Text Matching Types and Cross-Language Sanity Check",
    "text": "6.18 Text Matching Types and Cross-Language Sanity Check\n\n\n\nSlide 13\n\n\nThe text matching functionality offers distinct approaches: lexical matching, which relies on keyword and vocabulary similarity; semantic matching, employing vector embeddings to identify conceptually similar passages, crucially functioning across different languages without requiring shared vocabulary; and hybrid matching, which combines both approaches with adjustable weights. Users can also select from various matching modes: a standard mode, a comprehensive mode that utilises more computational power for exhaustive results, and a faster mode.\nA critical sanity check involves comparing Isaac Newton’s Latin Optics (1719 edition) with its English counterpart (Opticks, 1718 edition). When the system performs a lexical match in standard mode, it correctly yields no significant results, as anticipated for texts in different languages. However, switching to comprehensive mode reveals three matches, indicating the presence of some English text, likely from the preface, within the Latin edition. This outcome validates the system’s ability to differentiate between lexical and conceptual similarities whilst highlighting the nuances of multilingual historical texts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-self-matching-and-result-visualisation",
    "href": "chapter_ai-nepi_006.html#lexical-self-matching-and-result-visualisation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.19 Lexical Self-Matching and Result Visualisation",
    "text": "6.19 Lexical Self-Matching and Result Visualisation\n\n\n\nSlide 15\n\n\nA second sanity check involves performing a lexical match of a text against itself, yielding expectedly high scores: a Normalized Match Score of 100%, a Coverage Score of 99.7%, and a Quality Score of 100.0%. The system reports that both the query and comparison documents comprise 1,140 passages. This operation, taking 23 seconds, involved 1,299,600 comparisons to identify an estimated 1,137 matches, with 20 of 100 total matches displayed.\nThe results are presented with a clear match summary, providing helpful information about the query’s outcome. A bar chart illustrates the similarity score distribution, showing the majority of matches falling within the 90-100% range. For individual matches, the interface automatically highlights matching terms, presenting the source or query passage on the left, the comparison passage on the right, alongside their respective similarity scores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-matching-across-translations",
    "href": "chapter_ai-nepi_006.html#semantic-matching-across-translations",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.20 Semantic Matching Across Translations",
    "text": "6.20 Semantic Matching Across Translations\n\n\n\nSlide 15\n\n\nA third sanity check investigates semantic matching between a text and its translation, specifically comparing Isaac Newton’s Latin Optics with its English translation, Opticks. The expectation posits that whilst lexically dissimilar, these texts should exhibit strong semantic correspondence. The observed outcomes confirm this, with matches appearing reasonable despite underlying OCR issues. The interface presents the Latin source passage on the left and the English comparison passage on the right, accompanied by high similarity scores (e.g., 91.77%, 91.12%), demonstrating the model’s ability to capture conceptual equivalence across languages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-and-limitations-of-current-semantic-matching",
    "href": "chapter_ai-nepi_006.html#challenges-and-limitations-of-current-semantic-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.21 Challenges and Limitations of Current Semantic Matching",
    "text": "6.21 Challenges and Limitations of Current Semantic Matching\n\n\n\nSlide 16\n\n\nDespite promising initial results, the current semantic matching implementation faces several challenges. The overall match score calculation requires refinement, and a notably low coverage score, for example 36.9%, significantly impacts this metric. This lower coverage might be partially explained by the Latin edition being considerably longer than its English counterpart, alongside other substantive textual differences. Nevertheless, the primary concern centres on the adequacy of the current embedding model, LaBSE, which the team considers “not good enough” for the task. This inadequacy likely stems from an “out-of-domain model collapse,” as the model, trained on modern languages, struggles with the distinct semantics of early modern texts, compounded by poor OCR quality, complex typography, and mixed multilingual content. Whilst the quality score remains high, indicating the precision of the matches identified, the model’s overall performance suggests a need for further development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-research-directions",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-research-directions",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.22 Future Challenges and Research Directions",
    "text": "6.22 Future Challenges and Research Directions\n\n\n\nSlide 16\n\n\nThe project anticipates several significant challenges and outlines key research directions. Regarding embedding models, whilst LaBSE serves as a starting point, the researchers are exploring alternatives such as XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting trade-offs between accuracy, storage requirements, and inference time. A crucial consideration involves whether to fine-tune a base model on the unique historical corpus, given its distinct characteristics.\nA fundamental challenge involves addressing semantic drift, the change in meaning over centuries. The project must determine how to handle texts published across a broad temporal span (e.g., 1540 vs. 1700) and in different languages, whilst ensuring their representation within a consistent vector space. Poor OCR quality profoundly impacts all downstream processes, including the crucial segmentation of texts into sentences and passages. Re-OCR of the entire corpus is not feasible; therefore, the team may focus on re-OCR for only the lowest quality texts or invest in locating existing high-quality versions. Finally, scaling and performance present substantial hurdles. The current prototype, indexing merely 132 texts, already incurs query times of 15 seconds. Scaling this to the full corpus of 430,000 texts necessitates significant optimisation to ensure user-acceptable response times. The project actively welcomes expert advice on these complex issues.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis presentation elucidates the dual application of Artificial Intelligence: firstly, in enhancing the interpretability of complex models through Explainable AI (XAI); and secondly, in generating novel scientific insights within the humanities. The authors initially delineate XAI 1.0, focusing on feature attributions via heatmaps for classification models, whilst underscoring the imperative to verify predictions, identify biases, and ensure regulatory compliance. The discussion then transitions to XAI 2.0, which addresses the complexities of Generative AI and Large Language Models (LLMs) by exploring structured interpretability, feature interactions, and mechanistic views.\nThe work demonstrates that models can exhibit surprising errors, such as misclassifying objects based on correlated features or failing at multi-step planning tasks. To overcome the limitations of first-order explanations, the research introduces second-order (pairwise relationships) and higher-order (graph structures, feature walks) attributions. These reveal more intricate model behaviours and expose simplistic underlying strategies in embedding models. Specific examples illustrate how XAI uncovers biases in sentiment prediction and reveals LLMs’ tendency to prioritise recent information in long-context summarisation.\nWithin the humanities, the presentation showcases AI’s utility through several compelling case studies. The research team employed heatmap-based approaches to extract visual definitions from corpora of mathematical instruments, identifying fine-grained scales as crucial features. A significant project involved corpus-level analysis of early modern astronomical tables, such as the Sphaera and Sacrobosco Corpora. Here, a bespoke statistical model generating bigram representations proved remarkably effective where conventional Foundation Models failed, primarily due to heterogeneous, out-of-domain historical data. This innovation led to the concept of the “XAI-Historian,” enabling data-driven hypothesis generation at scale. Crucially, cluster entropy analysis, applied to publishing locations, revealed distinct patterns of innovation and control, identifying Frankfurt as a prominent reprinting hub and Wittenberg as a centre where political influence actively shaped the print programme. The presentation concludes by acknowledging the challenges of low-resource data and out-of-domain transfer for LLMs in humanities research, whilst affirming the transformative potential of Machine Learning (ML) and XAI for scaling scholarly inquiry and fostering new research directions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-and-ai-based-scientific-insights-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#explainable-ai-and-ai-based-scientific-insights-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI and AI-based Scientific Insights in the Humanities",
    "text": "7.1 Explainable AI and AI-based Scientific Insights in the Humanities\n\n\n\nSlide 01\n\n\nThis presentation delineates two principal areas of inquiry. Initially, it explores Explainable Artificial Intelligence (XAI), focusing on developing methodologies to comprehend the intricate operations of complex Large Language Models (LLMs). Subsequently, the discussion shifts to the application of AI for generating scientific insights, particularly within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Explainable AI (XAI) 1.0 Feature Attributions",
    "text": "7.2 Explainable AI (XAI) 1.0 Feature Attributions\n\n\n\nSlide 01\n\n\nThis section provides a concise introduction to Explainable Artificial Intelligence (XAI), outlining the core concepts that the machine learning community defines as ‘explanation’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai",
    "href": "chapter_ai-nepi_007.html#explainable-ai",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Explainable AI",
    "text": "7.3 Explainable AI\n\n\n\nSlide 01\n\n\nHistorically, machine learning predominantly focused on visual data, with a more recent surge of interest in language emerging over the past decade. To comprehend the internal workings of ‘black box’ machine learning models, Samek and colleagues (2017) typically examined classification tasks. For instance, an input image, such as a rooster, would yield a prediction like “Rooster”; however, users generally possessed no insight into the underlying basis for this classification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#post-hoc-explainability",
    "href": "chapter_ai-nepi_007.html#post-hoc-explainability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Post-Hoc Explainability",
    "text": "7.4 Post-Hoc Explainability\n\n\n\nSlide 02\n\n\nThe field of explainable AI has dedicated approximately a decade to tracing the origins of model predictions. Typically, this endeavour yields outputs such as heatmaps, which delineate the specific input features—for example, pixels—responsible for a given prediction, such as the recognition of a rooster.\nBeyond merely elucidating model behaviour, explainability serves several crucial purposes. It enables the verification of predictions, ensuring that models operate reasonably, and facilitates the identification and correction of errors by illuminating how mistakes occur. Furthermore, explainability offers profound insights into the underlying problem domain, as models frequently uncover surprising solutions. Increasingly, it also ensures compliance with evolving regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-advent-of-generative-ai",
    "href": "chapter_ai-nepi_007.html#the-advent-of-generative-ai",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 The Advent of Generative AI",
    "text": "7.5 The Advent of Generative AI\n\n\n\nSlide 03\n\n\nThe scenario of classification models, prevalent until approximately five years ago, has now given way to the era of Generative AI. Contemporary models exhibit remarkable versatility, performing diverse functions such as classification, identifying similar images, generating novel images, and answering a broad spectrum of questions. This expanded capability, however, significantly complicates the task of grounding predictions or answers from Large Language Models (LLMs) to their specific inputs.\nConsequently, the field necessitates moving beyond conventional heatmap representations, exploring feature interactions and adopting more mechanistic perspectives to achieve deeper understanding. Crucially, today’s foundation models function as both multi-task and ‘world models’, offering profound insights into societal dynamics and the evolution of textual features over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#model-limitations-and-unexpected-errors",
    "href": "chapter_ai-nepi_007.html#model-limitations-and-unexpected-errors",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Model Limitations and Unexpected Errors",
    "text": "7.6 Model Limitations and Unexpected Errors\n\n\n\nSlide 04\n\n\nModels frequently exhibit surprising errors, as evidenced by two prominent examples. In object detection, Lapuschkin and colleagues (2019) demonstrated how a standard classifier incorrectly identifies a boat by focusing on the surrounding water—a correlated and texturally simpler feature—rather than the boat itself. Furthermore, Large Language Models (LLMs) can falter in multi-step planning tasks. For instance, when presented with the Tower of Hanoi puzzle, Mondal, Webb, and their team (2024) observed that an LLM might immediately attempt to move the largest, inaccessible disc, thereby failing to comprehend the inherent physical constraints of the problem.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 XAI 2.0 Structured Interpretability",
    "text": "7.7 XAI 2.0 Structured Interpretability\n\n\n\nSlide 05\n\n\nWhilst more recent reasoning models may exhibit improved performance, the aforementioned Tower of Hanoi example originated from a standard Llama 3.x model. This section now shifts focus to structured interpretability, exploring methodologies that extend beyond conventional heatmap visualisations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-explanations-and-classifier-behaviour",
    "href": "chapter_ai-nepi_007.html#first-order-explanations-and-classifier-behaviour",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 First-Order Explanations and Classifier Behaviour",
    "text": "7.8 First-Order Explanations and Classifier Behaviour\n\n\n\nSlide 05\n\n\nFirst-order explanations, often visualised as heatmaps, prove particularly useful for elucidating classifier behaviour. For instance, the authors employed a table classifier on historical documents, aiming to distinguish specific subgroups of historical tables. After training the classifier, they verified its predictions using heatmaps, confirming that the model accurately focused on numerical content. This numerical focus served as an effective proxy for identifying numerical tables within the corpus.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-order-features-pairwise-relationships",
    "href": "chapter_ai-nepi_007.html#second-order-features-pairwise-relationships",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.9 Second-Order Features: Pairwise Relationships",
    "text": "7.9 Second-Order Features: Pairwise Relationships\n\n\n\nSlide 05\n\n\nThe team also investigated second-order features, specifically focusing on pairwise relationships such as similarity. Their method involved computing a dot product or similarity score between the embeddings of two entities, for example, two images. To explain these similarity predictions, they found that representing the interaction between features proved highly effective. This approach revealed interactions between specific digits, indicating identical tables and thereby verifying the model’s intended functionality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-and-graph-structures",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-and-graph-structures",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.10 Higher-Order Interactions and Graph Structures",
    "text": "7.10 Higher-Order Interactions and Graph Structures\n\n\n\nSlide 06\n\n\nIn more recent investigations, the authors have explored graph structures, discovering that higher-order interactions offer more meaningful insights. This approach applies to various networks, such as citation networks or networks of books and entities, typically trained on classification tasks. Here, relevant features emerge as feature subgraphs or feature walks, representing sets of features that become significant collectively. This methodology aims to yield more complex insights into models, ultimately progressing towards a circuit-level understanding of their operations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.11 First-Order Attributions in LLMs",
    "text": "7.11 First-Order Attributions in LLMs\n\n\n\nSlide 07\n\n\nThis section presents illustrative examples drawn from the domains of language and the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#biased-sentiment-predictions-in-transformer-llms",
    "href": "chapter_ai-nepi_007.html#biased-sentiment-predictions-in-transformer-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.12 Biased Sentiment Predictions in Transformer LLMs",
    "text": "7.12 Biased Sentiment Predictions in Transformer LLMs\n\n\n\nSlide 07\n\n\nAli and colleagues (2022) investigated biased sentiment predictions within Transformer Large Language Models (LLMs) by analysing the feature importance of names in movie reviews. Employing a standard sentiment prediction scenario, common within the language community, they ranked sentences and computed heatmaps using a novel method specifically designed for transformers. Their findings revealed a notable bias: positive sentiment predictions correlated with male Western names such as Lee, Barry, Raphael, or the Cohen Brothers, whilst negative scores were more likely associated with foreign-sounding names like Saddam, Castro, or Chan. This study underscores the utility of Explainable AI in detecting such fine-grained biases within models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-for-long-range-dependencies-in-llms",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-for-long-range-dependencies-in-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.13 First-Order Attributions for Long-Range Dependencies in LLMs",
    "text": "7.13 First-Order Attributions for Long-Range Dependencies in LLMs\n\n\n\nSlide 08\n\n\nJafari and colleagues (2024) explored first-order attributions for long-range dependencies in Large Language Models (LLMs), specifically examining text summarisation for extensive inputs, up to an 8,000-token context window. In a typical LLM scenario, users provide long texts, such as Wikipedia articles, and request a summary, which the model then generates as free text. Their investigation sought to determine the extent to which token dependencies spanned the input and whether models effectively utilised long-range information.\nFindings indicated that models predominantly focus on the later sections of the context, prioritising information presented closer to the prompt. Although models can draw upon information from the very beginning of the context, this occurs significantly less frequently, as evidenced by a log scale of counts. Consequently, summaries generated by LLMs may not offer a balanced representation of the entire input text, a crucial consideration for users.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "href": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.14 Second & Higher-Order Interactions in Text",
    "text": "7.14 Second & Higher-Order Interactions in Text\n\n\n\nSlide 09\n\n\nThe authors investigated second and higher-order interactions within textual data, employing a standard embedding scenario involving sentence pairs, such as “a cat I really like, it is a great cat.” Utilising models like the Bird model or a sentence Bird model, they observed that whilst a similarity score was produced, the underlying reasons for its specific value remained opaque.\nThe solution emerged through second-order explanations, which yielded interaction scores between individual tokens. These scores revealed that models primarily relied on simplistic strategies, such as noun matching (including synonyms and identical noun tokens), and to a lesser extent, noun-verb interactions, alongside separator and other token interactions. This reliance on basic strategies stems from the models’ inherent need to compress vast amounts of information. Understanding these mechanisms proves crucial when embedding data and subsequently computing rankings between them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions",
    "href": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.15 Graph Neural Networks for Structured Predictions",
    "text": "7.15 Graph Neural Networks for Structured Predictions\n\n\n\nSlide 11\n\n\nGraph Neural Networks (GNNs) offer a powerful mechanism for structured predictions, providing attributions in terms of ‘walks’ that represent interactions between features. Notably, GNNs, which inherently encode structural information, can be conceptualised as Large Language Models (LLMs) because the attention network within LLMs dictates which tokens can engage in message passing. This conceptual framework facilitates the analysis of language structures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#interaction-of-nodes-and-complex-language-structure",
    "href": "chapter_ai-nepi_007.html#interaction-of-nodes-and-complex-language-structure",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.16 Interaction of Nodes and Complex Language Structure",
    "text": "7.16 Interaction of Nodes and Complex Language Structure\n\n\n\nSlide 11\n\n\nSchnake and colleagues (2022) demonstrated how the interaction of nodes in graph structures enables the learning of complex language structures, particularly in sentiment analysis. Recognising that the hierarchical nature of natural language aligns well with graph structures, they trained a Graph Neural Network (or an LLM) on a movie review sentiment task and extracted ‘walks’ to understand its decision-making.\nFor instance, in the sentence “First I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen,” a first-order explanation would fail to capture the complexity, potentially assigning a high score to “like” despite its negation. Conversely, a higher-order explanation accurately assigns a negative score to the initial negative clause and correctly identifies the positive sentiment and hierarchical structure in the latter part of the sentence. This work highlights the superior interpretability offered by higher-order methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.17 AI-based Scientific Insights in the Humanities",
    "text": "7.17 AI-based Scientific Insights in the Humanities\n\n\n\nSlide 12\n\n\nThe discussion now transitions to the second principal area of inquiry: the generation of AI-based scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-corpora",
    "href": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-corpora",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.18 Extracting Visual Definitions from Corpora",
    "text": "7.18 Extracting Visual Definitions from Corpora\n\n\n\nSlide 12\n\n\nEl-Hajj, Eberle, and their colleagues (2023) initially explored heatmap-based methods for extracting visual definitions from corpora, focusing on a collection of mathematical instruments. Their objective involved classifying these instruments into categories such as ‘machine’ or ‘mathematical instrument’. Collaborating closely with historians, including Matteo Valeriani and Jochen Büttner, the team meticulously verified the visual definitions, underscoring the critical role of domain experts in ensuring the meaningfulness of such classifications. A key finding revealed that fine-grained scales present on the mathematical instruments were highly relevant for the model’s decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables",
    "href": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.19 Corpus-Level Analysis of Early Modern Astronomical Tables",
    "text": "7.19 Corpus-Level Analysis of Early Modern Astronomical Tables\n\n\n\nSlide 12\n\n\nIn their most extensive collaborative project, the authors undertook a corpus-level analysis of early modern astronomical tables, specifically focusing on numerical data. They utilised the Sphaera Corpus, an early modern text collection spanning 1472 to 1650, and the Sacrobosco Table Corpus (1472-1650). Historians expressed keen interest in developing an automated method for matching tables with similar semantics, a task previously unfeasible at scale. This significant endeavour is detailed in works by Valeriani and colleagues (2019) for the Sphaera Corpus and Eberle and colleagues (2024) for the Sacrobosco Table Corpus.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-xai-historian-enabling-historical-insights-at-scale",
    "href": "chapter_ai-nepi_007.html#the-xai-historian-enabling-historical-insights-at-scale",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.20 The XAI-Historian: Enabling Historical Insights at Scale",
    "text": "7.20 The XAI-Historian: Enabling Historical Insights at Scale\n\n\n\nSlide 13\n\n\nCollaborating with historians, Eberle and colleagues (2024) developed a sophisticated workflow to facilitate historical insights at scale, coining the term “XAI-Historian” to describe a historian leveraging AI and Explainable AI. This approach aims to uncover novel case studies and enable data-driven hypothesis generation.\nThe project focused on historical tables, which serve as crucial carriers of scientific knowledge processes, such as mathematisation, within the Sacrobosco Corpus—a collection of 76,000 pages of university textbooks from 1472 to 1650. A significant machine learning challenge arose from the data’s extreme heterogeneity, limited annotations, and the failure of conventional Optical Character Recognition (OCR) and Foundation Models (FMs).\nThe devised workflow encompassed three key stages: initially, data collection from book images; subsequently, atomisation and recomposition, involving input tables, bigram maps, and histograms; and finally, corpus-level analysis, which included embedding historical tables and assessing data similarity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#verifying-models-and-features-with-xai-and-historians",
    "href": "chapter_ai-nepi_007.html#verifying-models-and-features-with-xai-and-historians",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.21 Verifying Models and Features with XAI and Historians",
    "text": "7.21 Verifying Models and Features with XAI and Historians\n\n\n\nSlide 13\n\n\nEberle and colleagues (2022, 2024) crafted a statistical model specifically designed to generate bigram representations of historical tables, addressing the challenge posed by foundation models’ inability to process such out-of-domain data effectively. This bespoke model underwent rigorous verification: by detecting identical bigrams—for example, ‘38’ on two distinct inputs—the team confirmed its reliable operation, thereby establishing trust in its decisions.\nThe methodology involved representing tables as a bag of bigrams, such as ‘01’ or ‘21’, and, given the limited annotations, employed a combination of a learned feature extractor and a hard-coded structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#cluster-entropy-analysis-for-innovation-diffusion",
    "href": "chapter_ai-nepi_007.html#cluster-entropy-analysis-for-innovation-diffusion",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.22 Cluster Entropy Analysis for Innovation Diffusion",
    "text": "7.22 Cluster Entropy Analysis for Innovation Diffusion\n\n\n\nSlide 14\n\n\nIn a compelling case study, the authors applied cluster entropy analysis to investigate the diffusion of innovation across early modern Europe. Their focus centred on the publishing output of specific cities, each producing distinct “programmes” of textual types. Some cities exhibited diverse print programmes, whilst others concentrated on reprinting existing works; critically, this phenomenon had previously defied analysis at scale. The methodology involved calculating the difference between the observed cluster entropy H(p) and the maximum attainable entropy for each print location, drawing upon data from the Sphaera publication EPISD-626.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#insights-from-cluster-entropy-frankfurt-and-wittenberg",
    "href": "chapter_ai-nepi_007.html#insights-from-cluster-entropy-frankfurt-and-wittenberg",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.23 Insights from Cluster Entropy: Frankfurt and Wittenberg",
    "text": "7.23 Insights from Cluster Entropy: Frankfurt and Wittenberg\n\n\n\nSlide 15\n\n\nThe team devised a clustering approach, leveraging the model’s representations to compute a distance-based clustering and subsequently assess the diversity of print programmes produced by individual cities. They employed entropy as a metric: low entropy indicated a consistent reproduction of identical content, whilst higher entropy signified a more diverse print programme.\nThis analysis identified two particularly compelling cases with the lowest entropy scores: Frankfurt am Main and Wittenberg. Frankfurt am Main was already recognised as a major centre for reprinting editions. More notably, Wittenberg presented a historical anomaly where the political control exerted by Protestant reformers, particularly Melanchthon, actively restricted the print programme, dictating the curriculum to be published. This finding, detailed by Eberle and colleagues (2024), not only revealed a previously unquantifiable historical pattern but also corroborated existing historical intuition and scholarly support.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.24 Conclusion: AI-based Methods for the Humanities",
    "text": "7.24 Conclusion: AI-based Methods for the Humanities\n\n\n\nSlide 15\n\n\nConcluding the discussion, the presenter highlighted several key points regarding AI-based methods in the humanities. Whilst humanities and Digital Humanities (DH) researchers have primarily concentrated on the digitisation of source material, the automated analysis of these corpora presents significant challenges due to data heterogeneity and a scarcity of labels. Multimodality also emerges as a crucial consideration.\nNevertheless, the integration of Machine Learning (ML) and Explainable AI (XAI) holds substantial promise for scaling humanities research and fostering novel research directions. Foundation Models and Large Language Models (LLMs), coupled with prompting techniques, can effectively support intermediate tasks such as labelling, data curation, and error correction. However, their utility for addressing more complex research questions remains limited.\nSignificant challenges persist, notably the issue of low-resource data for ML, which impacts scaling laws. Furthermore, out-of-domain transfer, particularly for historical and small-scale datasets, necessitates rigorous evaluation, as current LLMs are predominantly trained and aligned for natural language tasks and code generation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modeling Science",
    "section": "",
    "text": "Overview\nThis chapter critically examines the current capabilities of Large Language Models (LLMs) within historical, philosophical, and sociological inquiry, with a particular focus on the history of science. The authors identify significant limitations inherent in existing LLM paradigms, notably their susceptibility to hallucination, the misinterpretation of embedding vectors as true meaning, and a fundamental inability to furnish justified, evidence-based answers or to formulate coherent plans for scientific inquiry. To address these profound deficiencies, the presenter proposes a novel framework centred on “validation” and introduces the nascent concept of “Computational Epistemology.”\nThe proposed solution establishes a robust research infrastructure, meticulously designed with several key components. This architecture incorporates a Scholarium system, which functions as a curated, editorially validated database of historical sources and structured content items, offering a direct alternative to conventional embedding-based approaches. The infrastructure seamlessly integrates multimodal LLMs, such as Gemini 2.5, Claude, and Llama, operating within a Cursor environment to facilitate complex historical queries. Furthermore, the project leverages a FAIR (Findable, Accessible, Interoperable, Reusable) data repository, specifically Zenodo, for the long-term preservation and dissemination of research data. Open Science Technology, a startup developing an MCP API server to standardise global AI access to scholarly knowledge, provides essential technical support. The overarching aim is to empower AI systems to deliver complete, validated, and historically accurate answers, thereby fundamentally transforming digital humanities research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#evolution-of-large-language-models",
    "title": "8  Modeling Science",
    "section": "8.1 Evolution of Large Language Models",
    "text": "8.1 Evolution of Large Language Models\n\n\n\nSlide 02\n\n\nLarge Language Models (LLMs) have evolved with remarkable rapidity. Initially, the foundational paradigm centred on the principle that “Attention is all you need.” This concept subsequently expanded, embracing the notion that “Context is all you need,” a shift exemplified by the integration of Retrieval-Augmented Generation (RAG) systems to furnish broader contextual understanding. Presently, the latest iterations of these models propose that “Thinking is all you need,” signifying a profound advancement in their cognitive capabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#critical-deficiencies-in-current-llm-capabilities",
    "href": "chapter_ai-nepi_008.html#critical-deficiencies-in-current-llm-capabilities",
    "title": "8  Modeling Science",
    "section": "8.2 Critical Deficiencies in Current LLM Capabilities",
    "text": "8.2 Critical Deficiencies in Current LLM Capabilities\n\n\n\nSlide 03\n\n\nDespite their rapid evolution, current Large Language Models (LLMs) exhibit several critical deficiencies. Crucially, they lack an inherent mechanism or “opponent” to effectively counter hallucination, a persistent challenge in generating reliable outputs. Furthermore, whilst useful for semantic similarity, embedding vectors do not inherently represent the true meanings of expressions, often leading to profound misinterpretations. These models frequently formulate statements that, whilst plausible in sound, prove factually incorrect; they often merely reiterate content from internet media rather than discerning and furnishing verified knowledge.\nBeyond these issues, LLMs demonstrate a profound inability to seek out the most robustly justified information or to formulate coherent plans for scientific inquiry. These capabilities constitute fundamental requirements for rigorous academic work. Presently, existing technologies offer no discernible hope for achieving these crucial goals, rendering current LLMs largely unsuitable for tasks demanding high levels of validation and epistemic rigour.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#computational-epistemology-and-validation-framework",
    "href": "chapter_ai-nepi_008.html#computational-epistemology-and-validation-framework",
    "title": "8  Modeling Science",
    "section": "8.3 Computational Epistemology and Validation Framework",
    "text": "8.3 Computational Epistemology and Validation Framework\n\n\n\nSlide 04\n\n\nA fundamental requirement for advancing LLM utility in scholarly domains centres on validation—a principle encapsulated by the assertion, “Validation is all you need.” This imperative necessitates systems capable of furnishing comprehensive reasons, robust arguments, and verifiable evidence, both for and against the truth of a given proposition. Moreover, such systems must offer clear justifications for or against the pursuit of specific actions.\nTo address this critical gap, the presenter proposes a new discipline: Computational Epistemology. This emerging field focuses on the methods and methodologies requisite for implementing the validation framework. Central to this discipline is the concept of epistemic agency, which demands the ability to identify propositions extending beyond simple sentences, to analyse complex argumentation structures within texts and historical sources, and to discern the intentions, plans, and actions of historical figures as meticulously documented in their records.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#interactive-historical-inquiry-environment",
    "href": "chapter_ai-nepi_008.html#interactive-historical-inquiry-environment",
    "title": "8  Modeling Science",
    "section": "8.4 Interactive Historical Inquiry Environment",
    "text": "8.4 Interactive Historical Inquiry Environment\n\n\n\nSlide 05\n\n\nThe research team has developed a sophisticated working environment specifically designed to facilitate historical inquiry. This intuitive interface displays an open historical source, exemplified by a book detailing the construction of Sanssouci castle under Frederick the Great. A dedicated panel, titled “Personen und Aufgaben in der Bibliothek,” empowers users to formulate precise queries. For instance, a user might pose the question: “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain).\nThe system aims to deliver a validated, qualified, and factually correct answer, relying exclusively on proven evidence rather than anecdotal information. The output is meticulously structured, presenting details such as the names of individuals, descriptions of their work, and the remuneration received—for example, “Nahl (sculptor) created the drawings and models in miniature, receiving 200 Thaler.” This capability proves invaluable for resolving long-standing historical disputes, such as the precise extent of Leonhard Euler’s involvement in the significant construction failures at Sanssouci during the 18th century. An AI agent, named Bernoulli, robustly supports these complex inquiries.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-source-repository",
    "href": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-source-repository",
    "title": "8  Modeling Science",
    "section": "8.5 Scholarium: Curated Scholarly Source Repository",
    "text": "8.5 Scholarium: Curated Scholarly Source Repository\n\n\n\nSlide 06\n\n\nThe proposed system necessitates five or six core components, commencing with a scholarly curated editorial board responsible for validating source material. A prime example of this rigorous approach is the Opera Omnia Euler, a monumental collection spanning 86 volumes. This work represents 120 years of dedicated scholarly effort by numerous academics, who finalised its comprehensive editing—encompassing all 866 publications and Euler’s complete correspondence—two years prior to this presentation. This foundational resource is further complemented by other significant scholarly editions, such as the Opera Bernoulli Euler, Kepler Gesammelte Werke, and Brahe Opera Omnia, collectively forming a robust evidence base.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-registry-based-content-management",
    "href": "chapter_ai-nepi_008.html#scholarium-registry-based-content-management",
    "title": "8  Modeling Science",
    "section": "8.6 Scholarium: Registry-Based Content Management",
    "text": "8.6 Scholarium: Registry-Based Content Management\n\n\n\nSlide 07\n\n\nA pivotal innovation within this framework is the Scholarium, which serves as a direct substitute for conventional embedding-based approaches. This component functions as a meticulously curated database of content items, systematically organising historical information. It captures a diverse array of data types, including chronologies of personal actions, various communication acts such as letters, publications, and reports, and specific statements. Furthermore, it meticulously documents implications, arguments, and inquiries, alongside the evolving use of language, terminology, and concepts by historical figures. The system also tracks the application of models, methods, tools, devices, data, information, evidence, and sources.\nThe project team rigorously validates each entry within this detailed inventory of historically proven activities against original sources, thereby ensuring unparalleled accuracy and reliability. Technical integration occurs seamlessly through an AI API and a Model Context Protocol (MCP) API, enabling programmatic access to this rich, structured knowledge base.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#multimodal-llm-integration-for-inquiry",
    "href": "chapter_ai-nepi_008.html#multimodal-llm-integration-for-inquiry",
    "title": "8  Modeling Science",
    "section": "8.7 Multimodal LLM Integration for Inquiry",
    "text": "8.7 Multimodal LLM Integration for Inquiry\n\n\n\nSlide 08\n\n\nOnce the project team establishes these meticulously curated records, researchers can conduct inquiries using readily accessible multimodal models. Experience indicates that multimodal models, particularly the latest iterations such as Gemini 2.5, prove most effective for this task, given their capacity to seamlessly combine information derived from both text and images. The system also integrates other prominent models, including Claude and Llama, all operating within the Cursor environment, specifically leveraging LettreAI on Cursor.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-data-infrastructure",
    "href": "chapter_ai-nepi_008.html#fair-data-infrastructure",
    "title": "8  Modeling Science",
    "section": "8.8 FAIR Data Infrastructure",
    "text": "8.8 FAIR Data Infrastructure\n\n\n\nSlide 09\n\n\nEstablishing a robust and enduring FAIR (Findable, Accessible, Interoperable, Reusable) data infrastructure constitutes another essential component of this initiative. For this purpose, the project team utilises Zenodo, a repository hosted by CERN in Geneva. Zenodo facilitates the long-term storage and publication of research data, thereby ensuring its sustained availability and discoverability for future scholarly endeavours.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-support-and-standardisation",
    "href": "chapter_ai-nepi_008.html#technical-support-and-standardisation",
    "title": "8  Modeling Science",
    "section": "8.9 Technical Support and Standardisation",
    "text": "8.9 Technical Support and Standardisation\n\n\n\nSlide 10\n\n\nOpen Science Technology, a startup specifically founded to manage the operational infrastructure, provides comprehensive technical support for the project. This support encompasses running the core systems and, crucially, furnishing an MCP (Model Context Protocol) API server. This server establishes a standardised API, enabling Artificial Intelligence models worldwide to access the project’s meticulously curated data. The initiative ultimately aims to standardise AI access APIs to knowledge across the global community, fostering an environment of open collaboration in the pursuit of advanced scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nThe authors embarked on an inquiry to assess biases within the classification of publications related to Sustainable Development Goals (SDGs) across prominent bibliometric databases, employing Large Language Models (LLMs) as a core analytical tool. This project critically examines the performative nature of databases such as Web of Science, Scopus, and OpenAlex. Despite their crucial role in scientific impact assessment, these platforms exhibit inconsistencies in SDG classification. Previous studies highlight minimal overlap in SDG labelling across different providers, potentially distorting perceptions of research priorities and influencing resource allocation and policy decisions.\nThis investigation specifically sought to understand the aggregate effects of introducing LLM-based tools on the representation of SDG-related research. The team leveraged LLMs both as a detector of inherent data biases and as a proof-of-concept for automating information extraction to inform research policy. Methodologically, the study focused on five selected SDGs pertinent to socio-economic inequalities, analysing a jointly indexed corpus of over 15 million publications from January 2015 to July 2023.\nA key technical approach involved fine-tuning DistilGPT2, a pre-trained, open-source LLM with limited prior knowledge, on subsets of publication titles and abstracts. The authors meticulously derived prompts for the LLM from the specific targets of each SDG, serving as a benchmark for assessing the completeness of information captured by the models. The research employed three distinct decoding strategies—top-k, nucleus, and contrastive search—to generate responses, which then underwent noun phrase extraction and direct content comparison.\nCrucially, the findings reveal a systematic overlook in the data concerning sensitive locations (e.g., African countries, Least Developed Countries), vulnerable demographic groups (e.g., persons with disabilities, indigenous peoples), and critical SDG-specific focuses (e.g., human trafficking, migration). Whilst the United States consistently featured in LLM responses, followed by South Africa and China, these omissions underscore significant biases. Furthermore, the analysis identified distinct methodological leanings amongst the databases: Web of Science demonstrated a more theoretical approach, whereas Scopus and OpenAlex exhibited a more empirical orientation. These insights highlight the sensitivity of LLMs to their training data and the broader implications for research policy and addressing socio-economic inequalities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#initial-research-aim",
    "href": "chapter_ai-nepi_009.html#initial-research-aim",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 Initial Research Aim",
    "text": "9.1 Initial Research Aim\n\n\n\nSlide 01\n\n\nThe authors initially aimed to utilise Large Language Models (LLMs) to assess inherent biases within publications classified across three major bibliometric databases. This foundational objective sought to uncover discrepancies in how these extensive repositories categorise scholarly output, thereby influencing academic and policy landscapes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 SDG Classification in Bibliometric Databases",
    "text": "9.2 SDG Classification in Bibliometric Databases\n\n\n\nSlide 01\n\n\nBibliometric databases function as critical digital infrastructures, underpinning bibliometric analyses and impact assessments within the scientific community. These platforms, however, possess a performative nature, shaping perceptions of the science system and attributing value based on specific, inherent understandings, as highlighted by Whitley (2000) and Winkler (1988). This study specifically considered three major bibliometric databases: Web of Science, Scopus, and OpenAlex. Each of these platforms has implemented bibliometric classifications designed to align publications with the Sustainable Development Goals (SDGs).\nPrevious research, notably by Armitage et al. (2020), revealed that SDG labelling by various providers—including Elsevier, Bergen, and Aurora—produces divergent results, demonstrating minimal overlap in their classifications. Consequently, such disparities in classification can lead to varied perceptions of research priorities, potentially influencing both resource allocation and critical policy decisions. These differences, moreover, often reflect underlying political and commercial interests.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "href": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Case Study: SDGs in Bibliometric Data",
    "text": "9.3 Case Study: SDGs in Bibliometric Data\n\n\n\nSlide 02\n\n\nThis case study, a collaborative effort by Ottaviani and Stahlschmidt (2024), sought to assess the aggregated effects on the representation of SDG-related research within bibliometric databases following the introduction of Large Language Model (LLM)-based tools. The authors employed a method involving the separate training of pre-trained, smaller LLMs, specifically DistilGPT2, on distinct subsets of publication abstracts. They meticulously curated these subsets based on the SDG classifications provided by various bibliometric databases.\nThe project conceptualised LLM technology in two primary roles: firstly, as a sophisticated detector of inherent biases within the data; and secondly, as a proof-of-concept exercise for automating information extraction to directly inform decision-making processes in research. Ultimately, the broader aim of this endeavour was to develop a generalisable framework for assessing the potential impact of such technologies on research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#partial-chain-of-dependencies",
    "href": "chapter_ai-nepi_009.html#partial-chain-of-dependencies",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 Partial Chain of Dependencies",
    "text": "9.4 Partial Chain of Dependencies\n\n\n\nSlide 03\n\n\nA conceptual flowchart delineates the intricate chain of dependencies considered within this research. Initially, the established SDG classification frameworks directly define the scope and nature of “Research” pertaining to these global goals. Subsequently, a diverse array of stakeholders—including researchers, Small and Medium-sized Enterprises (SMEs), governmental bodies, and various intermediate figures—actively process this “Research” on SDGs. This processed research, in turn, critically informs “Decision-making to align with SDGs,” which ultimately exerts an impact on existing “Socioeconomic inequalities.”\nCrucially, the model integrates the role of LLMs: “LLM as detector of ‘biases’” is positioned to analyse “Research” on SDGs, whilst the “Introduction of LLM in Research Policy” is depicted as a subsequent step, directly influencing “Socioeconomic inequalities.” Consequently, LLMs possess the capacity to alter the underlying metadata, specifically concerning “research on SDGs,” thereby influencing a multitude of advices, policy choices, performance indicators, and practical measures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methods-and-research-design",
    "href": "chapter_ai-nepi_009.html#methods-and-research-design",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Methods and Research Design",
    "text": "9.5 Methods and Research Design\n\n\n\nSlide 04\n\n\nThe research design carefully selected key bibliometric databases and Sustainable Development Goals for analysis. The investigators chose three prominent databases: the proprietary Web of Science and Scopus, alongside the open-source platform OpenAlex. Furthermore, the study focused on five specific SDGs, strategically chosen to facilitate the modelling of socio-economic inequalities, thereby providing a targeted lens for the inquiry.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data",
    "href": "chapter_ai-nepi_009.html#processed-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Processed Data",
    "text": "9.6 Processed Data\n\n\n\nSlide 04\n\n\nThe researchers meticulously processed a substantial dataset, comprising a jointly indexed subset of 15,471,336 publications. This comprehensive collection specifically included publications shared by all three bibliometric databases—Web of Science, Scopus, and OpenAlex—identified through exact Digital Object Identifier (DOI) matching. The data spanned a period from January 2015 to July 2023.\nThe study then evaluated the performance of three distinct classification standards applied to the five selected SDGs. For each SDG, the team generated three separate subsets of publications, each corresponding to one of the bibliometric databases, all derived from the initial shared corpus. This rigorous approach aimed to establish a robust comparative benchmark for subsequent analyses.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-socio-dimension",
    "href": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-socio-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 Comparing SDG-Classified Papers: Socio Dimension",
    "text": "9.7 Comparing SDG-Classified Papers: Socio Dimension\n\n\n\nSlide 05\n\n\nInitial findings concerning the socio dimension of SDG-classified papers demonstrated a remarkably small overlap in SDG labelling across the databases, a result entirely consistent with the observations of Armitage (2020). The analysis specifically focused on SDG 04 (Quality Education), SDG 05 (Gender Equality), and SDG 10 (Reduced Inequalities).\nNotably, Scopus, for instance, did not classify certain publications as SDG 5, even though these papers were present within its broader database. Conversely, Web of Science classified a significant 10% of its SDG 5 publications as originating from the field of mathematics, including topics such as geometrical differential equations. This particular finding strongly suggests either a misclassification or a notable absence of relevant information within the database’s tagging system.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#economic-dimension-and-llm-approach",
    "href": "chapter_ai-nepi_009.html#economic-dimension-and-llm-approach",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 Economic Dimension and LLM Approach",
    "text": "9.8 Economic Dimension and LLM Approach\n\n\n\nSlide 06\n\n\nThe study extended its comparative analysis to the economic dimension, examining SDG 08 (Decent Work and Economic Growth) and SDG 09 (Industry, Innovation, and Infrastructure). A novel approach to LLM modelling underpinned this phase of the research. The core idea involved training a Large Language Model exclusively on specific corpora of publications, each corpus having been classified by a particular bibliometric database for a distinct SDG.\nFor practical implementation, the researchers opted to fine-tune DistilGPT2, an existing open-source LLM. This model, characterised by its fundamental architecture and minimal prior knowledge compared to its commercial or larger open-source counterparts, offered a crucial advantage: it ensured the absence of pre-existing semantic knowledge concerning either the publications or the prompts. The fine-tuning process exclusively utilised publication titles and abstracts as training data. Subsequently, a new title served as the prompt input, prompting the LLM to generate a corresponding new abstract.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-targets-and-prompt-generation",
    "href": "chapter_ai-nepi_009.html#sdg-targets-and-prompt-generation",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.9 SDG Targets and Prompt Generation",
    "text": "9.9 SDG Targets and Prompt Generation\n\n\n\nSlide 08\n\n\nEach Sustainable Development Goal is meticulously structured around a series of specific targets; for instance, SDG 4 encompasses targets ranging from 4.1 to 4.6. The SDGs under consideration typically feature between eight and twelve such targets. The researchers meticulously drafted the prompts for the LLM directly from these precise SDG targets.\nThis approach simultaneously served as an attempt at validation and benchmarking. Should the LLM fail to generate relevant responses to prompts derived from the targets, it signals a deficiency in the information it has processed, given that these targets fundamentally articulate the core objectives of each SDG, effectively serving as a “ground zero” for expected content. For analytical simplification, the methodology involved noun phrase extraction, alongside direct textual searching within the actual LLM responses during the comparative phase.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-workflow",
    "href": "chapter_ai-nepi_009.html#research-design-workflow",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.10 Research Design Workflow",
    "text": "9.10 Research Design Workflow\n\n\n\nSlide 10\n\n\nThe research design implemented a precise workflow for processing and analysing data. Initially, a curated “Set of abstracts classified SDG# by DB#” directly informed the “Fine-Tuning” of the DistilGPT-2 model. Concurrently, a “Set of prompts specifically to SDG#” served as input for the “Fine-tuned DistilGPT-2 SDG# DB#”.\nCrucially, the fine-tuned model then employed three distinct decoding strategies—top-k, nucleus, and contrastive search—to generate diverse outputs. These outputs, labelled as “Responses SDG# DB# for top-k”, “Responses SDG# DB# for nucleus”, and “Responses SDG# DB# for contrastive search”, subsequently underwent a rigorous post-processing stage. A “prompts’ words filter” refined these responses, culminating in the extraction of “Noun phrases SDG# DB#”. The analytical phase involved both this noun phrase extraction for simplification and a direct, meticulous search through the actual generated responses during the comparative assessment.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "href": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.11 Illustrative Example: SDG 4 Analysis",
    "text": "9.11 Illustrative Example: SDG 4 Analysis\n\n\n\nSlide 10\n\n\nFor illustrative purposes, the analysis of LLM responses for SDG 4 exemplifies the broader methodology applied across all Sustainable Development Goals. This involved meticulously matching extracted noun phrases with the specific SDG targets. The analysis systematically examined four critical data dimensions: Locations, Actors, Data/Metrics, and Focuses.\nFor each SDG, the investigation assessed two primary aspects: its compliance with the stated targets and the identification of any inherent biases. Notably, distinct differences emerged amongst the bibliometric databases. The findings were structured into a comprehensive table, categorised by “Unique DBs,” “Addressed Targets,” “Not Addressed Targets,” and “Focuses.” Within these dimensions, it became evident that all SDG targets consistently reference locations, whilst actors represent another crucial category. Data and metrics primarily emerged from the LLM’s responses, and focuses were identified as inherently SDG-specific.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-llm-omissions-sdg-4-illustration",
    "href": "chapter_ai-nepi_009.html#systematic-llm-omissions-sdg-4-illustration",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.12 Systematic LLM Omissions: SDG 4 Illustration",
    "text": "9.12 Systematic LLM Omissions: SDG 4 Illustration\n\n\n\nSlide 11\n\n\nDespite explicit stimulation, the Large Language Model consistently failed to address several critical areas, as exemplified by its performance on SDG 4. The omissions were particularly notable across specific categories. In terms of locations, the LLM neglected to mention African countries (with the exception of South Africa), developing countries (with a question mark over China), least developed countries, and Small Island Developing States.\nRegarding actors, the model systematically overlooked vulnerable populations, including persons with disabilities, indigenous peoples, and children in vulnerable situations. Furthermore, a range of crucial focuses remained unaddressed: vocational training, scholarships, the establishment of safe, non-violent, inclusive, and effective environments, sustainable lifestyles, human rights, the promotion of a culture of peace and non-violence, global citizenship, the appreciation of cultural diversity, free primary and secondary education, and tertiary education. These persistent omissions are highly significant, as they pertain to sensitive categories and focuses explicitly articulated within the SDG targets themselves.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-key-findings",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-key-findings",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.13 Cross-SDG Considerations and Key Findings",
    "text": "9.13 Cross-SDG Considerations and Key Findings\n\n\n\nSlide 12\n\n\nAcross all five Sustainable Development Goals analysed, several recurrent results emerged, highlighting consistent patterns in the LLM’s performance and the underlying data biases. Regarding locations, least developed countries, such as South-Saharan Africa for SDG 8, received minimal attention. Conversely, the United States consistently featured in responses for every SDG, with South Africa and China following as the next most frequently cited locations, ahead of the UK and Australia.\nIn terms of metrics, the LLM frequently recalled various surveys as datasets, including the Demographic and Health Surveys (DHS) and the World Values Survey (WVS). The responses also encompassed a range of research methodologies, spanning theoretical, empirical, thematic analysis, market dynamics, and macroeconomics. However, a significant finding concerned actors: discriminated and vulnerable categories were systematically overlooked across different SDGs, with no overarching response addressing these groups. Similarly, whilst SDG-specific focuses were present, the most sensitive topics, such as human trafficking, human exploitation, and migration, were notably absent.\nFurthermore, the analysis revealed distinct methodological orientations among the bibliometric databases. Web of Science consistently exhibited a highly theoretical approach, often emphasising models and abstract concepts. In stark contrast, both Scopus and OpenAlex demonstrated a more empirical orientation in their classifications. Consequently, the primary finding of this research points to a systematic overlook within the data concerning critical actors, the poorest countries, and underrepresented topics.\nThe study acknowledges several limitations inherent in its general framework. Specific applied cases might yield different outcomes, though the researchers express some uncertainty regarding the extent of such variation. A high sensitivity of LLMs to their training data remains a significant factor, alongside potential variations in model architecture and calibration parameters. Nevertheless, the research endeavoured to account for these aspects by utilising training data from three distinct databases and employing three different decoding strategies drawn from existing literature.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "",
    "text": "Overview\nThe authors have addressed the persistent challenge of extracting citation data from complex footnotes prevalent in law and humanities scholarship. Historically, bibliometric databases offer inadequate coverage for these domains, primarily due to a lack of commercial interest, a focus on impact factors over intellectual history, and the inherent complexity of humanities footnotes. Traditional machine learning tools, moreover, demonstrate poor performance in parsing these intricate structures. Consequently, the authors explore the utility of Large Language Models (LLMs) and Vision Language Models (VLMs) as a more effective solution.\nA central tenet of this research involves the authors establishing a robust testing and evaluation framework. To this end, the project team is developing a high-quality gold standard dataset, meticulously annotated using TEI XML encoding. This standard, well-established within the digital humanities, facilitates comprehensive representation of citation phenomena, including contextual information. Furthermore, it ensures interoperability with existing tools such as Grobid, enabling direct performance comparisons.\nTo operationalise this approach, the developers crafted Llamore, a lightweight Python package. Llamore extracts citation data from raw text or PDFs, exporting it into TEI-formatted XML files. Crucially, it also evaluates extraction performance against gold standard references using an F1-score metric, which accounts for precision and recall through an unbalanced assignment problem. Initial evaluations reveal that whilst Llamore’s resource consumption exceeds that of traditional tools like Grobid for biomedical literature, it significantly outperforms Grobid when processing the challenging, footnoted humanities data. Future work will expand the training data, refine evaluation metrics, and enhance Llamore’s capabilities to capture contextual citation information and resolve complex stylistic variations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-citation-graph-generation-for-humanities",
    "href": "chapter_ai-nepi_010.html#challenges-in-citation-graph-generation-for-humanities",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.1 Challenges in Citation Graph Generation for Humanities",
    "text": "10.1 Challenges in Citation Graph Generation for Humanities\nThe authors confront a significant challenge: Large Language Models and other algorithms currently struggle with the intricate footnotes characteristic of law and humanities scholarship. Generating comprehensive citation graphs from these sources constitutes a primary objective for the authors. Such graphs prove invaluable for intellectual history, enabling scholars to discern patterns and relationships within knowledge production, trace intellectual influences, and quantify the reception of published ideas. For instance, one can readily identify the most cited authors within a specific journal over a defined period, as demonstrated by an analysis of the Journal of Law and Society between 1994 and 2003.\nA fundamental impediment arises for the authors from the extremely poor coverage of historical Social Sciences and Humanities (SSH) literature within existing bibliometric data sources. Leading platforms, including Web of Science, Scopus, and OpenAlex, exhibit substantial deficiencies. Web of Science and Scopus, moreover, impose prohibitive costs and restrictive licensing terms, hindering open research. Whilst OpenAlex offers an open-access alternative, it too lacks comprehensive coverage for many A-journals, pre-digital content, and non-English language publications. For example, the Zeitschrift für Rechtssoziologie, established in 1980, shows negligible citation data before the 2000s within these databases.\nSeveral factors contribute to this persistent data gap. Commercial entities demonstrate limited financial interest in humanities scholarship, unlike their engagement with STEM, medicine, and economics. Furthermore, these databases prioritise “impact factor” metrics for scientific evaluation, a focus that diverges from the needs of intellectual history research. Crucially, the pervasive use of complex footnotes within humanities literature presents a unique parsing challenge, which traditional systems have struggled to overcome.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#addressing-footnote-complexity-with-large-language-models",
    "href": "chapter_ai-nepi_010.html#addressing-footnote-complexity-with-large-language-models",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.2 Addressing Footnote Complexity with Large Language Models",
    "text": "10.2 Addressing Footnote Complexity with Large Language Models\nA second, equally pressing problem arises for the authors from the inherent complexity of humanities footnotes, often termed “footnotes from hell.” These structures frequently incorporate extensive commentary, extraneous content, and non-reference text, embedding the actual citations within considerable noise. Traditional instruments for extracting such information necessitate laborious manual annotation. Moreover, conventional machine learning tools, including those based on conditional random forests, consistently exhibit poor performance. For instance, the ExCite Performance study (Boulanger/Iurshina 2022) reported low extraction and segmentation accuracies across various training datasets, with combined data yielding an extraction accuracy of merely 0.22 and segmentation accuracy of 0.47.\nConsequently, the authors have turned to Large Language Models (LLMs) as a promising alternative. Initial experiments in 2022, utilising models such as text-davinci-003, demonstrated LLMs’ considerable capacity for extracting references from highly unstructured textual data. Newer models offer even greater potential, whilst Vision Language Models (VLMs) extend this capability to direct processing of PDF documents. The developers employ various methods, including prompt engineering, Retrieval-Augmented Generation (RAG), and fine-tuning, to optimise these models.\nNevertheless, a crucial concern persists regarding the trustworthiness of LLM-generated results, particularly the risk of hallucinations. A notable incident involved a lawyer who, relying on ChatGPT, submitted a federal court filing citing at least six non-existent cases. Addressing this fundamental issue, the authors contend, demands a robust testing and evaluation solution. Such a solution requires a high-quality gold standard dataset, a flexible framework capable of adapting to the rapidly evolving technology landscape, and solid testing algorithms to generate comparable performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-dataset",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-dataset",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.3 Developing a TEI-Annotated Gold Standard Dataset",
    "text": "10.3 Developing a TEI-Annotated Gold Standard Dataset\nTo address the need for reliable evaluation, the project team has embarked upon compiling a comprehensive training and evaluation dataset, employing TEI XML encoding. This choice rests upon several compelling reasons. TEI XML represents a well-established, precisely specified, and comprehensive standard for text interchange within the digital humanities. Crucially, it encompasses a far broader range of phenomena than more restrictive bibliographical standards, such as CSL or BibTeX. Indeed, TEI extends beyond mere reference management, allowing for the encoding of citations, cross-references, and other contextual markup, which proves vital for classifying citation intention. Furthermore, adopting this standard enables the authors to leverage existing digital editions, text collections, and corpora, thereby enhancing the generalisation and robustness of the developed mechanisms.\nNevertheless, the TEI standard presents its own set of challenges, both conceptual and technical. Conceptual difficulties arise in differentiating between pointers and references, whilst technical complexities involve managing constrained elements versus elliptic material. Despite these hurdles, the authors’ establishment of the dataset progresses steadily. The encoding process involves multiple stages: capturing PDF screenshots, segmenting reference strings to distinguish them from non-reference footnote text, and finally, generating parsed structured data. The dataset currently comprises 1,100 footnotes and endnotes, drawn from 25 articles across 10 Directory of Open Access Journals (DOAJ) titles. It specifically focuses on humanities scholarship, particularly legal and historical texts, and encompasses a diverse range of languages, including French, German, Spanish, Italian, and Portuguese, spanning the period from 1958 to 2018. The authors estimate the dataset will contain over 1,600 references, with individual occurrences encoded separately to preserve contextual information. Notably, the project team adjusted its strategy midway, shifting to Open Access journals and incorporating PDFs to facilitate Vision Language Model (VLM) mechanisms and enable the full publication of the dataset.\nThe interoperability afforded by the TEI XML standard offers a significant advantage, enabling seamless integration with existing tooling. Grobid, a widely recognised tool for reference and information extraction, notably utilises TEI XML for its training and evaluation processes. Consequently, this shared data format permits direct performance comparisons with Grobid and facilitates the exchange of training data, benefiting both the project and the broader research community.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction-and-evaluation",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction-and-evaluation",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.4 Llamore: A Python Package for Reference Extraction and Evaluation",
    "text": "10.4 Llamore: A Python Package for Reference Extraction and Evaluation\nThe project team has developed Llamore, a Python package acronym for “Large LANguage MOdels for Reference Extraction.” This tool facilitates two primary functions: extracting citation data from raw text or PDF inputs using multimodal Large Language Models, and subsequently evaluating the extraction performance. The workflow proceeds from text or PDF documents, through Llamore, to produce references in TEI XML format. The authors then compare these extracted references with gold standard references, yielding an F1-score as an evaluation metric.\nThe authors crafted Llamore with two key objectives. Firstly, the authors designed the package to remain lightweight, comprising fewer than 2,000 lines of code. Crucially, Llamore operates as an interface to a model of the user’s choosing, rather than embedding any specific model directly. Secondly, this design ensures broad compatibility with both open and closed Large Language Models and Vision Language Models.\nImplementing Llamore proves straightforward for users. Users can install the package directly from PyPI using pip install llamore. For extraction, users import the relevant extractor, such as GeminiExtractor or OpenaiExtractor, then instantiate it with an API key. The extractor processes either a PDF file path or a raw input string, returning a collection of references that can then be exported to a TEI XML file. Notably, the OpenaiExtractor provides compatibility with numerous open model serving frameworks, including Ollama and VLLM, which offer OpenAI-compatible API endpoints. For evaluation, users import the F1 class, configure it (e.g., levenshtein_distance=0 for exact matches), and compute the macro average F1-score by supplying both the extracted and gold references.\nLlamore employs the F1-score, a widely recognised metric for comparing structured data, to assess extraction performance. This score combines precision (the ratio of matches to predicted elements) and recall (the ratio of matches to gold elements) into a single harmonic mean. A perfect extraction yields an F1-score of 1, whilst an F1-score of 0 indicates no matches. For instance, in comparing an extracted reference to a gold standard, Llamore identifies matches for analytic_title, monographic_title, authors.surname, and publication_date, whilst noting a minor discrepancy in authors.forename due to an extraneous character in the gold reference. Furthermore, Llamore addresses the complex task of aligning extracted references with gold references by framing it as an unbalanced assignment problem. The tool computes F1 scores for every possible combination, constructs a matrix, and then maximises the total F1-score whilst ensuring a unique assignment, utilising SciPy’s solver for this optimisation. Significantly, the system penalises both missing and hallucinated references by assigning them an F1-score of zero.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "href": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.5 Performance Analysis and Future Directions",
    "text": "10.5 Performance Analysis and Future Directions\nInitial performance evaluations by the authors provide crucial insights into Llamore’s efficacy across diverse datasets. When tested on the PLOS 1000 Dataset, which comprises 1,000 biomedical PDFs and demands exact matches, Grobid achieved an F1 score of 0.61, whilst Llamore, utilising Gemini 2.0 Flash, attained a comparable F1 score of 0.62. However, for literature on which Grobid was specifically trained, it demonstrates superior efficiency, operating considerably faster and with fewer computational resources; Llamore’s compute requirements, conversely, are orders of magnitude greater.\nA more compelling distinction emerges when the authors evaluate performance on the project’s bespoke humanities dataset, which features complex footnotes and also requires exact matches. Here, Grobid struggles significantly, yielding an F1 score of only 0.14, largely due to its training data being out of distribution for such intricate structures. In stark contrast, Llamore (Gemini 2.0 Flash) achieves an F1 score of 0.45, representing a threefold improvement in performance. Nevertheless, this current performance metric pertains solely to pure reference extraction, excluding the capture of contextual information or cross-referencing.\nThe authors outline several key objectives for future work. The project team plans to generate additional training data and further refine the test metrics. Crucially, the authors aim to extend Llamore’s capabilities to support citations in context, discerning whether a work is cited approvingly or critically. Furthermore, the tool will incorporate features for resolving op cit. references, identifying specific pages cited, and quantifying multiple citations to the same work. Addressing these enhancements will necessitate overcoming several challenges, including the wide variation in citation styles (e.g., differentiating between volumes and pages, or first page versus cited page), the complexities of multilingual terminology (e.g., diverse contributor roles like “eds” or “hrsg. v.”, and special terms such as “passim”, “ibid”, or “n.d.”), the intricacies of canonical citations prevalent in fields like Bible studies or Roman law, and the accurate handling of ellipses, abbreviations, and cross-references.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview\nThis chapter elucidates the development of an innovative AI-driven solution, meticulously crafted to manage the escalating volume of scientific information and to bolster knowledge production. Researchers from DANS (Data Archiving and Networked Services) and GESIS (Leibniz Institute for the Social Sciences) collaborated to confront the pervasive challenge of information overload, which frequently impedes the effective review, evaluation, and selection of scholarly content. Their core objective involved engineering an AI system capable of engaging in a “chat” with papers drawn from specific collections, thereby significantly enhancing information retrieval and human-machine interaction.\nThe team conceptualised the system as a “local” or “tailored AI solution,” comprising two principal components: Ghostwriter, which serves as the intuitive user interface, and EverythingData, encompassing the intricate back-end processing pipelines. Its foundational methodology employs Retrieval-Augmented Generation (RAG), seamlessly integrating both vector spaces and knowledge graphs. The authors construct vector spaces from data file content, encoding them through embeddings derived from various machine learning algorithms and Large Language Models (LLMs). Concurrently, a graph represents a metadata layer, meticulously integrated with diverse ontologies and controlled vocabularies, including principles of responsible AI, and expressed via the Croissant ML standard.\nA key vision for this architecture, termed GraphRAG, seeks to unify graphs and vectors within a singular model, operating as a distributed AI. Within this framework, the LLM functions as both an interface and a sophisticated reasoning engine, connecting to a “RAG library”—essentially the graph—to navigate datasets and consume embeddings (vectors) as contextual information. The system demonstrably prevents hallucinations by strictly adhering to provided source material, offering precise, factual answers with direct references. It supports iterative query refinement and boasts robust multilingual capabilities, enabling queries in one language whilst processing documents in another.\nThe implementation involves ingesting articles, such as 100 papers from the method-data-analysis (mda) journal, into a vector store (Qdrant) and performing operations like term extraction, embedding construction, and enrichment. Crucially, the team expresses selected terms as structured data within a knowledge graph, often enriched with Wikidata, which contextualises embeddings and serves as a “ground truth” for validating LLM outputs. This decoupling of knowledge from the model facilitates benchmarking and ensures the sustainability of knowledge organisation systems for future scientific endeavours. The project ultimately aims to support human thought processes in formulating research questions, offering a controllable and cost-effective alternative to large, cloud-based LLMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#science-dynamics-and-ai-addressing-information-overload",
    "href": "chapter_ai-nepi_011.html#science-dynamics-and-ai-addressing-information-overload",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Science Dynamics and AI: Addressing Information Overload",
    "text": "11.1 Science Dynamics and AI: Addressing Information Overload\n\n\n\nSlide 01\n\n\nThis initiative represents a collaborative endeavour between DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, another prominent archive actively engaged in research. The project addresses a fundamental challenge within the evolving landscape of scientific disciplines: the relentless growth and increasing differentiation of knowledge. This expansion invariably complicates the processes of reviewing, evaluating, and selecting pertinent information.\nCrucially, the capacity to find and comprehend information remains a prerequisite for any form of knowledge creation, whether within individual cognitive processes or across broader academic communities. Modern machines, particularly the latest advancements in Artificial Intelligence, have undeniably accelerated this growth. Consequently, a pivotal question emerges: can these technologies also support the intricate knowledge production process, specifically within the domain of Information Retrieval?\nThe impetus for this work stemmed from extensive experimentation conducted by Slava Tikhonov, a senior research engineer at DANS, who pioneered the construction of various data pipelines. Rather than a straightforward pipeline, the system Tikhonov and his team developed is more accurately characterised as a complex “back of things”—a multifaceted architecture challenging to deconstruct and articulate. Ultimately, the project sought to apply and illustrate AI solutions to effectively manage the overwhelming deluge of information that increasingly submerges contemporary researchers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-objectives-and-system-architecture",
    "href": "chapter_ai-nepi_011.html#project-objectives-and-system-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.2 Project Objectives and System Architecture",
    "text": "11.2 Project Objectives and System Architecture\n\n\n\nSlide 02\n\n\nThe central research question guiding this project explored the feasibility of constructing an AI solution capable of facilitating interactive dialogue with scholarly papers drawn from a curated selection. This inquiry necessitated an exploration of several interconnected concepts: information retrieval, the dynamics of human-machine interaction, and the burgeoning field of Retrieval-Augmented Generation (RAG) within generative AI.\nThe researchers selected the method-data-analysis (mda) journal as a specific use case to demonstrate the system’s capabilities. They introduced a workflow underpinning a ‘local’ or ‘tailored AI solution’, distinguishing its primary components with internal project names: Ghostwriter, which functions as the user interface, and EverythingData, a comprehensive term encompassing all underlying back-end processes. The presentation subsequently provided illustrative examples of both front-end and back-end operations, culminating in a summary and outlook on the project’s implications.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-a-novel-information-retrieval-interface",
    "href": "chapter_ai-nepi_011.html#ghostwriter-a-novel-information-retrieval-interface",
    "title": "11  Science dynamics and AI",
    "section": "11.3 Ghostwriter: A Novel Information Retrieval Interface",
    "text": "11.3 Ghostwriter: A Novel Information Retrieval Interface\n\n\n\nSlide 03\n\n\nThe Ghostwriter interface represents a novel approach to information retrieval, designed to facilitate simultaneous interaction with both structured data and natural language inputs. The authors metaphorically describe this dual capability as “chatting with experts and librarians at the same time,” where the “librarian” embodies structured data and knowledge organisation systems, whilst the “expert” represents natural language.\nHistorically, traditional information retrieval, involving a query against a single database representation, necessitated prior knowledge of the database schema and its typical values to yield results. This scenario, likened to “Me and a database,” presented the classic information retrieval problem of formulating the precise query. More advanced information retrieval systems, operating on connected structured data or graphs, offered improvements. Here, the underlying machinery could suggest similar or superior queries based on schema interconnections, subsequently providing lists of potential results. This advancement, conceptualised as “Me and a librarian,” mirrors features found in Google and schema.org, though typically applied to web-scale interactions rather than local ones.\nThe advent of Large Language Models (LLMs) introduced another paradigm: a query against an LLM interprets natural language input and suggests results, also expressed in natural language. This interaction is akin to “Me and a library” or “Me and a round of experts.” Ghostwriter synthesises these approaches. It integrates a local LLM with target data collections and a network of additional data interpretation sources, accessible via Application Programming Interfaces (APIs). This sophisticated architecture enables the system to generate families of terms around a given query, identify related structured information, and ultimately return a comprehensive list of results. Crucially, applying this system iteratively empowers users to reformulate their questions, thereby gaining a deeper understanding of their actual query intent and the scope of the available data space.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-framework",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-framework",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Retrieval-Augmented Generation (RAG) Framework",
    "text": "11.4 Retrieval-Augmented Generation (RAG) Framework\n\n\n\nSlide 04\n\n\nScientifically, this system firmly situates itself within the broader academic discourse surrounding Retrieval-Augmented Generation (RAG). A foundational understanding of this topic is readily available through resources such as Philip Rattliff’s “The GraphRAG Manifesto: Adding Knowledge to GenAI,” published by Neo4j.\nThe system’s efficacy hinges upon two primary ingredients. Firstly, the authors meticulously construct a vector space from the content of data files. This content undergoes encoding into embeddings, which capture both properties and their associated attributes. Various machine learning algorithms, leveraging different Large Language Models, compute these embeddings. Secondly, a robust graph serves as a comprehensive metadata layer. This graph seamlessly integrates diverse ontologies and controlled vocabularies, notably incorporating principles of responsible AI, and adheres to the Croissant ML standard for its expression.\nA key strategic vision, termed GraphRAG, aims to unify both the graph and vector components into a singular, cohesive model. The authors designed this integration for local implementation, thereby fostering a form of Distributed AI. Within this architecture, the LLM assumes a dual role: it acts as the primary interface facilitating human-AI interaction and simultaneously functions as a sophisticated reasoning engine. The practical implementation involves the LLM connecting directly to a “RAG library,” which is essentially the graph, enabling it to navigate through datasets and consume the generated embeddings as contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-operational-workflow",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-operational-workflow",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Ghostwriter and EverythingData: Operational Workflow",
    "text": "11.5 Ghostwriter and EverythingData: Operational Workflow\n\n\n\nSlide 04\n\n\nThe operational workflow commences with an input comprising a collection of articles. Whilst the demonstration specifically utilised a small, scraped collection from the MDA journal, the system readily accommodates any document collection. This input then enters the “EverythingData” component, which orchestrates a series of intricate back-end operations.\nInitially, EverythingData stores the processed information within a vector store, specifically employing Qdrant. Subsequently, it executes a range of processes, including term extraction, the construction of embeddings, and various enrichments. A crucial step involves coupling this data with knowledge graphs: the team transforms selected terms into structured data within a graph and further enriches them, notably through integration with Wikidata. This strategic coupling serves to contextualise the embeddings, thereby imbuing them with enhanced semantic value.\nAll processed data converges into a unified “Vector Space RAG-Graph.” Users then interact with this comprehensive knowledge base via the query interface, formulating their questions in natural language. The system responds by providing two distinct outputs: a list of relevant documents, aligning with conventional information retrieval practices, and a concise explanatory summary text, intelligently generated by the system’s machinery in response to the user’s query. The Ghostwriter interface serves as the primary conduit for this seamless user interaction.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-indexing-and-factual-retrieval",
    "href": "chapter_ai-nepi_011.html#ghostwriter-indexing-and-factual-retrieval",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter: Indexing and Factual Retrieval",
    "text": "11.6 Ghostwriter: Indexing and Factual Retrieval\n\n\n\nSlide 06\n\n\nThe developer’s approach to Large Language Models (LLMs) stems from early engagement, commencing with GPT-2 testing in 2020. Rather than relying on monolithic models, the strategy involves deconstructing the LLM training process into smaller, more manageable components, enabling their targeted application. This modularity grants the system remarkable flexibility in content processing: whilst demonstrated with scholarly papers, it seamlessly handles any web content and even spreadsheets, facilitating precise queries regarding specific data values.\nCrucially, the system rigorously prevents hallucinations. Its responses are strictly factual and non-hallucinatory because it draws exclusively from the provided source material. Should information be unavailable within the designated source, the system transparently indicates “I don’t know.” This commitment to factual integrity is achieved by employing a relatively simple 1 billion parameter LLM, which, when synergistically combined with knowledge graphs, proves highly effective in answering complex questions.\nFor the presented use case, the researchers ingested 100 articles from the MDA (GESIS journal) website directly into Ghostwriter, establishing a dedicated collection. The system deliberately avoids reliance on any pre-ingested LLM knowledge; instead, it directly queries specific papers to extract factual information. The overarching goal remains to provide responses derived solely from the content present within the paper, without introducing extraneous details. This Ghostwriter instance, operating on MDA papers, is publicly accessible at https://gesis.now.museum.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#chatting-with-papers-the-male-breadwinner-model-example",
    "href": "chapter_ai-nepi_011.html#chatting-with-papers-the-male-breadwinner-model-example",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Chatting with Papers: The Male Breadwinner Model Example",
    "text": "11.7 Chatting with Papers: The Male Breadwinner Model Example\n\n\n\nSlide 08\n\n\nA practical demonstration of the Ghostwriter’s capabilities involved the query: “explain male breadwinner model to me.” In response, the system generated a comprehensive explanation of the Male Breadwinner Ideology, elucidating its societal concept, the expectation for men to serve as primary financial providers, and its observed influence on individual attitudes and entrepreneurial activities within Germany.\nCrucially, the system meticulously provides direct references to the original scholarly papers from which the information was extracted, including titles such as “The Past, Present and Future of Factorial Survey Experiments…” and “Gender and Survey Participation…”. This rigorous source referencing ensures transparency and validates the information presented. The system’s design inherently prevents hallucinations, as it precisely identifies and retrieves information directly from the source texts.\nTechnically, this precision is achieved by segmenting each paper into numerous small blocks, with a unique identifier assigned to every block. The system then employs advanced Large Language Model techniques to intelligently connect and retrieve these blocks, applying specific weights to prioritise their relevance. Furthermore, the integration of knowledge graphs significantly enhances this process by accurately predicting which particular text segments will most effectively address a given question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-approach-to-query-refinement-and-factual-integrity",
    "href": "chapter_ai-nepi_011.html#iterative-approach-to-query-refinement-and-factual-integrity",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Iterative Approach to Query Refinement and Factual Integrity",
    "text": "11.8 Iterative Approach to Query Refinement and Factual Integrity\n\n\n\nSlide 09\n\n\nThe system’s commitment to factual integrity extends to its handling of information gaps. When presented with a refined query, such as “explain how data was collected on male breadwinner model,” and the direct information is unavailable within its indexed sources, the system explicitly states “there is no direct information.”\nFor instance, in response to this particular query, the system noted a study that utilised German data and employed a mixed-methods research strategy, including a survey experiment by Hanhmueller et al. (2015). It also referenced another article by Haase et al. (2016) that examined the male breadwinner model, yet it transparently indicated the absence of data collection details for this specific study. Furthermore, the interface incorporates an “Add paper” button, empowering users to contribute new articles to the collection. Crucially, any information added through this feature will subsequently be incorporated into the system’s knowledge base, enriching future query responses.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-mechanics-entity-extraction-knowledge-graphs-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#system-mechanics-entity-extraction-knowledge-graphs-and-multilinguality",
    "title": "11  Science dynamics and AI",
    "section": "11.9 System Mechanics: Entity Extraction, Knowledge Graphs, and Multilinguality",
    "text": "11.9 System Mechanics: Entity Extraction, Knowledge Graphs, and Multilinguality\n\n\n\nSlide 09\n\n\nUnderpinning the system’s functionality lies a sophisticated entity extraction pipeline. This pipeline meticulously annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby transforming raw vector space data into a coherent knowledge graph. Beyond this initial transformation, the system actively links these entities to more extensive knowledge graph representations, notably leveraging Wikidata. This integration with Wikidata serves a crucial purpose: it establishes a “ground truth,” providing a reliable benchmark against which the accuracy of LLM-generated answers can be rigorously validated.\nA significant feature of the system is its immediate and robust multilinguality. This capability proves indispensable for processing scholarly papers published in diverse languages, such as Chinese or German, whilst enabling users to pose questions and receive answers in English. Ultimately, the Large Language Model orchestrates the synthesis of various text segments, culminating in the production of a concise, explanatory summary.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#fact-extraction-and-wikidata-integration-for-semantic-enrichment",
    "href": "chapter_ai-nepi_011.html#fact-extraction-and-wikidata-integration-for-semantic-enrichment",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Fact Extraction and Wikidata Integration for Semantic Enrichment",
    "text": "11.10 Fact Extraction and Wikidata Integration for Semantic Enrichment\n\n\n\nSlide 10\n\n\nThe fact extraction process commences by segmenting user queries into granular pieces. A sophisticated knowledge organisation system then processes these segments, systematically revealing new and deeper levels of associated terms. Crucially, all extracted information undergoes a rigorous linking process with Wikidata, transforming free-form strings into structured, canonical identifiers.\nThese Wikidata identifiers confer substantial benefits. They inherently enable multilingual translations, providing access to a comprehensive array of associated properties. Consequently, the system can comprehend and process questions posed in various languages. The determination of conceptual similarity for these linkages relies upon the precise measurements derived from Large Language Model embeddings.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#multilingual-capabilities-and-the-future-of-knowledge-organisation",
    "href": "chapter_ai-nepi_011.html#multilingual-capabilities-and-the-future-of-knowledge-organisation",
    "title": "11  Science dynamics and AI",
    "section": "11.11 Multilingual Capabilities and the Future of Knowledge Organisation",
    "text": "11.11 Multilingual Capabilities and the Future of Knowledge Organisation\n\n\n\nSlide 11\n\n\nThe system’s robust multilingual capabilities are exemplified by its treatment of core query concepts, such as “bread winner mo,” as abstract entities. The Gemma3 Large Language Model then generates translations for these concepts into hundreds of languages. A pivotal innovation involves establishing a “ground truth” by decoupling knowledge from specific questions and papers. This is achieved by storing knowledge as a comprehensive list of Wikidata identifiers, maintained externally to the model itself.\nThis externalisation of knowledge facilitates rigorous benchmarking. Researchers can test various models, including those not yet fully trained, by posing identical questions and comparing the consistency of their generated identifier lists. This comparative analysis effectively identifies models unsuitable for specific tasks. Furthermore, this methodological approach supports the creation of robust benchmarks and champions the utilisation of knowledge organisation systems for future generations of scientists. Collaborations with prominent industry partners, including Google and Meta, underscore a commitment to ensuring the long-term sustainability of this process. Ultimately, the developers firmly believe that knowledge organisation systems represent the future paradigm for information management.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-and-philosophical-implications",
    "href": "chapter_ai-nepi_011.html#live-demonstration-and-philosophical-implications",
    "title": "11  Science dynamics and AI",
    "section": "11.12 Live Demonstration and Philosophical Implications",
    "text": "11.12 Live Demonstration and Philosophical Implications\n\n\n\nSlide 13\n\n\nA live demonstration showcased the system’s capabilities via the GESIS Leibniz-Institut für Sozialwissenschaften website, specifically within its “Ask Questions” section. When queried with “Rational Choice Theory,” the system promptly retrieved pertinent information, synthesised a summary from various papers, and provided direct references to the original sources. A subsequent, more specific query, “explain utility in Rational Choice Theory,” prompted the system to select distinct pieces of information from the indexed papers, yielding varied results whilst consistently referencing the same source documents.\nAn Application Programming Interface (API) further extends the system’s utility, enabling an automatic mode suitable for agentic architectures. This allows for the construction of sophisticated pipelines, facilitating automated result collection and the identification of novel knowledge. Users also possess the ability to augment the collection by adding new pages or content, either via a webpage URL or an RSS feed, which the system seamlessly incorporates. A compelling demonstration of its multilingual prowess involved posing a question in English and successfully retrieving information from a source paper written entirely in German, save for its abstract.\nThe developers emphasised the significant benefits of a local system, which affords greater control over data and mitigates the considerable costs and inherent control limitations associated with large, cloud-based Large Language Models. The interaction with scholarly papers is evocatively likened to engaging with an “invisible college.” Crucially, the system’s fundamental purpose transcends merely providing definitive facts or ultimate answers. Instead, it aims to stimulate human thought processes, assist in comprehending complex questions, and support the precise formulation of research questions. Ultimately, these technological possibilities should be perceived as powerful tools designed to augment and support human intellectual activity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "",
    "text": "Overview\nThis presentation explores the application of Retrieval-Augmented Generation (RAG) systems within philosophical research, particularly addressing the unique challenges posed by the discipline’s emphasis on linguistic and semantic accuracy. The presenters highlight the limitations of standalone Large Language Models (LLMs) for philosophical inquiry, including their lack of direct access to full texts, restricted context windows, and inability to attribute claims reliably. Consequently, RAG systems emerge as an efficacious solution, enabling direct engagement with specific corpora and providing verifiable citations.\nThe authors detail a RAG system prototype developed using the Stanford Encyclopedia of Philosophy as its data source. This system endeavours to facilitate both didactic engagement with philosophical texts and advanced research tasks, such as fact-checking, corpus exploration, and passage identification for close reading. Crucially, the development process revealed that effective RAG implementation necessitates extensive “tweaking” and rigorous, domain-specific evaluation. The system’s architecture encompasses a frontend for user interaction, a backend for processing, and a comparative output display that benchmarks RAG performance against LLM-only responses.\nA key optimisation involved determining the optimal chunk size for text retrieval, revealing that main sections of the highly systematised Stanford Encyclopedia yielded superior results despite their length. These findings underscore RAG systems’ capacity to integrate verbatim corpora and domain knowledge, thereby reducing hallucinations and enabling citation. Nevertheless, the presenters caution that RAGs demand domain expertise for effective tweaking and evaluation, as their performance is highly sensitive to the specific corpus, question types, and evaluation criteria. A notable challenge identified involves RAGs’ tendency to provide less effective responses to broad overview questions, as their focus on local information can obscure broader perspectives. This observation suggests a future direction towards more flexible, agentic RAG systems capable of discerning question types.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.1 Addressing Philosophical Research Challenges with RAG Systems",
    "text": "12.1 Addressing Philosophical Research Challenges with RAG Systems\nPhilosophical inquiry often necessitates precise engagement with foundational texts, exemplified by questions such as Aristotle’s theory of matter in the Physics or the evolution of Einstein’s concept of locality from his early relativity works to his 1948 paper on “Quantenmechanik und Wirklichkeit”. Whilst Large Language Models (LLMs) like ChatGPT can offer decent, differentiated answers to such queries, they present several significant limitations for rigorous philosophical research.\nPrimarily, LLMs contend with an inherent problem of access. Although their training data may include full texts, they cannot directly quote specific passages or chapters verbatim without an online search, which itself is constrained by copyright. Crucially, LLM training mechanisms actively prevent verbatim memorisation, instead focusing on learning generalisable statistical rules for text production. Consequently, they cannot provide the deep, fine-grained textual analysis essential for philosophical work. Furthermore, LLMs face a limited context window; even models like ChatGPT 4o, with 128,000 tokens, quickly exhaust their capacity when processing extensive philosophical corpora. Finally, LLMs inherently struggle with attribution, failing to provide verifiable sources or citations for their claims, which can lead to unverified information or outright hallucinations.\nRetrieval-Augmented Generation (RAG) systems offer a robust solution to these challenges. A typical RAG setup integrates a specific data source—such as Aristotle’s or Einstein’s complete works—from which documents are retrieved, often via semantic search, though hybrid or classic search options also exist. The system then augments user prompts with relevant text chunks, directly addressing the limitations of standalone LLMs. This approach resolves the problem of access by providing the LLM with the original text sources, mitigates context window constraints by supplying only relevant segments, and crucially, enables the attribution of claims, offering verifiable citations akin to those found in platforms like Perplexity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.2 Potential Applications of RAG Systems in Philosophy",
    "text": "12.2 Potential Applications of RAG Systems in Philosophy\nRAG systems hold significant promise for philosophical applications, fundamentally transforming how scholars and students interact with complex texts. The overarching concept involves enabling users to “chat” with philosophical corpora, such as Locke’s complete works, in a manner akin to ChatGPT, yet with the crucial advantage of providing much more detailed domain knowledge and a verbatim text basis.\nDidactically, these systems prove invaluable. They facilitate student engagement with challenging texts, for instance, allowing students to initiate their exploration of Locke’s Essay by querying its general ideas before progressively delving into specific concepts like his epistemology or theory of matter. This iterative questioning fosters a deeper, more instructive understanding of the texts.\nBeyond pedagogy, RAG systems offer compelling applications for research. They enable reliable fact lookup in handbooks, effectively modernising the traditional process of manually consulting physical books for footnotes and remarks. Furthermore, the authors demonstrate how RAGs can be utilised to explore previously unexamined or newly digitised corpora, gaining comprehensive overviews of their contents. The systems also assist in identifying specific passages relevant for close reading, directly supporting focused scholarly analysis. Ultimately, the ambition extends to generating detailed answers for at least portions of complex research questions, painting a picture of future capabilities in philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-an-example-rag-system-the-stanford-encyclopedia-of-philosophy",
    "href": "chapter_ai-nepi_012.html#developing-an-example-rag-system-the-stanford-encyclopedia-of-philosophy",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.3 Developing an Example RAG System: The Stanford Encyclopedia of Philosophy",
    "text": "12.3 Developing an Example RAG System: The Stanford Encyclopedia of Philosophy\nTo explore the practical utility of RAG systems in philosophy, the authors developed an example implementation utilising the Stanford Encyclopedia of Philosophy (SEP), a widely recognised online handbook, as its primary data source. The content of the SEP was meticulously scraped and converted into Markdown format, forming the basis for the system.\nInitially, the project aimed simply to create a beneficial tool for the philosophical community. However, during the development and testing phases, a significant challenge emerged: the RAG system’s initial answers proved surprisingly poor, often performing worse than direct queries to a standalone ChatGPT instance. This necessitated a shift in the project’s focus, evolving into a qualitative study on the optimal setup of RAG systems for philosophical applications.\nImproving the system’s performance demanded extensive “tweaking,” involving adjustments to the underlying language models, optimisation of various hyperparameters, and the implementation of more complex algorithms, such as reranking. The methodology adopted for this iterative improvement was one of theoretically grounded trial and error, systematically identifying which measures enhanced answer quality. This process underscored the critical importance of sound evaluation standards, particularly given the nature of philosophical inquiry. Unlike historical research that might seek atomic facts, philosophical questions often elicit complex, unstructured textual propositions. Evaluating the factual accuracy and nuanced understanding within these propositions presents a distinct challenge, demanding robust and sophisticated evaluation criteria.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-overview-and-frontend-details",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-overview-and-frontend-details",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.4 SEP RAG System Overview and Frontend Details",
    "text": "12.4 SEP RAG System Overview and Frontend Details\nThe developed SEP RAG system comprises a user-friendly frontend and a robust backend, implemented in several thousand lines of Python code. The frontend provides a clear interface for user interaction, allowing for the configuration of various parameters.\nSpecifically, the SEP RAG Details 1 section of the frontend presents input fields for critical hyperparameters, including the choice of generative model, the prompt token limit (both model-defined and user-defined), and the number of texts to retrieve. Users can also define a “Persona” for the system and, crucially, input their “Philosophical Question,” exemplified by a query such as “What is priority monism?” A “Generate answer” button initiates the processing.\nFor comparative analysis, the SEP RAG Details 2 section displays the output in two distinct columns: an “Answer with LLM alone” column serves as a benchmark, whilst the “Answer with RAG” column showcases the system’s enhanced response. This side-by-side presentation facilitates a direct and immediate comparison of performance. Finally, the SEP RAG Details 3 section offers a comprehensive “Retrieved Texts Overview.” This detailed list includes file names, section headings, text lengths in tokens, total tokens, and an explicit indication of which texts were successfully included in the prompt and which were truncated due to token limitations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunk-size-a-philosophical-perspective",
    "href": "chapter_ai-nepi_012.html#optimising-chunk-size-a-philosophical-perspective",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.5 Optimising Chunk Size: A Philosophical Perspective",
    "text": "12.5 Optimising Chunk Size: A Philosophical Perspective\nOptimising the chunk size represents a critical hyperparameter adjustment in RAG system development. The authors explored three primary chunking options: a fixed number of words, typically around 500 tokens, a common and straightforward criterion in computer science; chunking by paragraphs; and chunking by sections, whether at a low or high level.\nRemarkably, the most effective results emerged from utilising the main sections of the Stanford Encyclopedia of Philosophy as the primary retrieval documents. This outcome proved surprising, particularly given that the average section length, at approximately 3,000 words, substantially exceeded the embedding model’s typical cutoff of just over 500 words. The authors hypothesise that this success stems from the highly systematised nature of the Stanford Encyclopedia; its main sections are so well-structured that their initial 500 words often convey the core ideas effectively.\nNevertheless, this finding carries a crucial generalisability caveat. This section-based chunking strategy might not prove effective for more heterogeneous texts that lack such rigorous internal organisation. Philosophical arguments, in particular, frequently “spread” across multiple pages, requiring extensive explanation beyond the confines of a single paragraph. Consequently, chunking by paragraphs can artificially fragment semantically coherent arguments, diminishing the quality of the retrieved information.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#results-cautions-and-open-challenges-in-rag-implementation",
    "href": "chapter_ai-nepi_012.html#results-cautions-and-open-challenges-in-rag-implementation",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.6 Results, Cautions, and Open Challenges in RAG Implementation",
    "text": "12.6 Results, Cautions, and Open Challenges in RAG Implementation\nRAG systems offer compelling advantages for scholarly applications. They seamlessly integrate verbatim corpora and highly specific domain knowledge, which dramatically reduces the occurrence of hallucinations whilst enabling the citation of relevant documents for generated answers. Consequently, RAG setups are inherently well-suited to assist with a diverse array of scientific tasks.\nNevertheless, the presenters highlight crucial cautions. Firstly, RAG systems fundamentally necessitate extensive “tweaking” to achieve optimal performance. Secondly, rigorous evaluation proves indispensable, demanding a representative set of questions and corresponding expected answers. This evaluation process, moreover, critically requires the involvement of domain experts, as no single RAG setup can universally claim superiority across all domains, corpus types, or question types. Each implementation demands specific tailoring.\nSeveral open challenges persist. A significant issue arises when the system fails to retrieve relevant documents, leading to a noticeable decrease in answer quality and necessitating prompt adjustments. Intriguingly, RAGs often yield inferior results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This counterintuitive outcome stems from RAGs’ inherent focus on the local information contained within retrieved chunks; this granular focus can inadvertently distract from the broader perspective required for comprehensive overview responses. Addressing this limitation necessitates the development of more flexible systems capable of discerning different question types, thereby paving the way for the emergence of more sophisticated, agentic RAG systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "",
    "text": "Overview\nThis chapter elucidates a novel methodology for addressing fundamental questions within the philosophy of science, particularly those concerning the inherent structure of research fields. The authors integrate advanced computational methods, including linguistic analysis and social network analysis, to meticulously reconstruct the intellectual and social landscape of scientific inquiry. Their work focuses on quantum gravity as a compelling case study, a field notably characterised by a plurality of theoretical approaches.\nThe methodology involves a two-pronged, bottom-up reconstruction. Firstly, the authors undertake a linguistic analysis of a substantial corpus of theoretical physics abstracts and titles, employing the Bertopic pipeline to identify intellectual topics. Secondly, they conduct a social network analysis of co-authorship data to delineate scientific communities. Recognising the scale-dependent nature of both topics and communities, their approach employs hierarchical clustering for both structures, utilising Ward agglomerative clustering for topics and a hierarchical stochastic block model for communities. A crucial adaptive topic coarse-graining strategy, based on the Minimum Description Length (MDL) criterion, refines the linguistic partition by prioritising information relevant to the social structure.\nThe findings reveal that this bottom-up reconstruction can either confirm or critically re-evaluate physicists’ intuitive understanding of their field’s structure. For instance, whilst some approaches align well with emergent topics, others, particularly those less conceptually autonomous, do not. Notably, the analysis suggests that string theory and supergravity, despite historical distinctions, coalesce into a single intellectual cluster when linguistic nuances without social consequences are removed. This convergence highlights the utility of computational methods in challenging long-standing philosophical intuitions, positing computation as a continuation of philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#investigating-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#investigating-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.1 Investigating Plural Pursuit in Quantum Gravity",
    "text": "13.1 Investigating Plural Pursuit in Quantum Gravity\nThis research endeavours to address fundamental questions within the philosophy of science by integrating social network analysis with established computational methodologies. The authors’ investigation centres on quantum gravity, a complex field serving as a compelling case study.\nTheir approach unfolds in three distinct stages. Initially, the authors introduce the quantum gravity case study, establishing its philosophical context. Subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape, meticulously detailing its intellectual and social contours. Finally, this empirically derived reconstruction undergoes rigorous comparison with the prevailing intuitions of physicists regarding their field’s inherent structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-plurality-of-approaches-to-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#the-plurality-of-approaches-to-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.2 The Plurality of Approaches to Quantum Gravity",
    "text": "13.2 The Plurality of Approaches to Quantum Gravity\nThe core of this inquiry focuses on quantum gravity and the concept of plural pursuit within scientific research. This area of fundamental physics presents a unique landscape for exploring how diverse approaches co-exist and evolve.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#problem-and-attempted-solutions-in-fundamental-physics",
    "href": "chapter_ai-nepi_015.html#problem-and-attempted-solutions-in-fundamental-physics",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.3 Problem and Attempted Solutions in Fundamental Physics",
    "text": "13.3 Problem and Attempted Solutions in Fundamental Physics\nA long-standing contemporary issue in fundamental physics concerns the formulation of a quantum theory of gravity. This formidable challenge necessitates reconciling our understanding of phenomena at minuscule scales with observations at vast cosmological dimensions.\nNumerous attempted solutions to this problem have been proposed. String theory stands as the most prominent amongst these, whilst other notable approaches include Supergravity, Loop Quantum Gravity, spin foams, Causal Set Theory, and Asymptotic Safety. To comprehensively account for this diverse landscape of concurrent research efforts, the concept of “plural pursuit” becomes indispensable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit-and-empirical-inquiry",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit-and-empirical-inquiry",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.4 Defining Plural Pursuit and Empirical Inquiry",
    "text": "13.4 Defining Plural Pursuit and Empirical Inquiry\nPlural pursuit designates situations where distinct yet concurrent instances of normal science converge on a singular, common problem-solving objective. In the context of fundamental physics, this objective involves reconciling quantum mechanics with gravitation.\nEach instance of normal science, as conceptualised here, finds articulation through a specific social community inextricably linked to an intellectual disciplinary matrix. This framework resonates with established accounts of research programmes, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes. Consequently, a pivotal empirical question arises: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently advancing distinct paradigms?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.5 Bottom-Up Reconstruction of Research Landscape",
    "text": "13.5 Bottom-Up Reconstruction of Research Landscape\nTo address the empirical question concerning plural pursuit, the authors first undertake a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This process meticulously delineates both the linguistic and intellectual architecture of the field, alongside its intricate social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#clustering-pipeline-data-acquisition-and-linguistic-analysis",
    "href": "chapter_ai-nepi_015.html#clustering-pipeline-data-acquisition-and-linguistic-analysis",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.6 Clustering Pipeline: Data Acquisition and Linguistic Analysis",
    "text": "13.6 Clustering Pipeline: Data Acquisition and Linguistic Analysis\nThe authors initiated their analysis by gathering a substantial dataset comprising approximately 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP. Their subsequent analytical pipeline unfolds in two principal stages: a linguistic analysis to reconstruct the intellectual framework, followed by a social network analysis to map the field’s social architecture.\nThe linguistic analysis, powered by the Bertopic pipeline, commences by spatialising the collected documents into an embedding space. Subsequently, an unsupervised clustering algorithm operates on this space, identifying 611 distinct, fine-grained topics. This granular resolution proves essential for capturing niche approaches within quantum gravity, some of which may encompass as few as one hundred papers. Finally, the system assigns a “specialty” to each physicist, determined by the predominant topic across their published works. This process ultimately yields a comprehensive partition of authors into topics, thereby illuminating the field’s underlying linguistic and intellectual structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#clustering-pipeline-social-network-analysis",
    "href": "chapter_ai-nepi_015.html#clustering-pipeline-social-network-analysis",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.7 Clustering Pipeline: Social Network Analysis",
    "text": "13.7 Clustering Pipeline: Social Network Analysis\nComplementing the linguistic analysis, the authors conducted a comprehensive social network analysis. This commenced with the construction of a co-authorship graph, wherein individual physicists represent the nodes and co-authorship relationships form the edges connecting them.\nApplying a community detection method to this extensive network of 30,000 physicists, their analysis successfully identified approximately 800 distinct communities. This process ultimately yields a robust partition of authors into these communities, thereby providing a clear reflection of the field’s intricate social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#operationalising-plural-pursuit-community-topic-mapping",
    "href": "chapter_ai-nepi_015.html#operationalising-plural-pursuit-community-topic-mapping",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.8 Operationalising Plural Pursuit: Community-Topic Mapping",
    "text": "13.8 Operationalising Plural Pursuit: Community-Topic Mapping\nOperationally, plural pursuit manifests as a direct, one-to-one correspondence between identified communities and their associated intellectual topics. Conceptually, this ideal scenario would appear as a block-diagonal correlation matrix, where communities align perfectly with distinct topics. Such a configuration would signify that each community dedicates itself entirely to a singular topic, thereby establishing a clear and unambiguous division of labour across the research landscape.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#challenges-in-interpreting-fine-grained-partitions",
    "href": "chapter_ai-nepi_015.html#challenges-in-interpreting-fine-grained-partitions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.9 Challenges in Interpreting Fine-Grained Partitions",
    "text": "13.9 Challenges in Interpreting Fine-Grained Partitions\nInitial attempts to directly apply the fine-grained partitions, comprising 611 topics and 800 communities, yielded a highly complex and largely indecipherable correlation matrix. This complexity stems from several inherent challenges.\nFirstly, the level of fine-graining for topics proves somewhat arbitrary; for instance, a major research programme like string theory might inadvertently scatter across numerous granular topics. Secondly, large-scale research programmes often involve parallel efforts undertaken by multiple distinct communities. Thirdly, and more fundamentally, the computational definitions of both “topic” and “community” exhibit inherent scale-dependency, meaning their delineation varies with the chosen level of granularity. Beyond these technical considerations, a deeper conceptual issue arises: research programmes are themselves hierarchically nested. String theory, for example, encompasses Superstring Theory, which further branches into Type II and Heterotic varieties, amongst others. Consequently, this inherent ambiguity across different scales mandates a strategic approach to effectively identify genuine instances of plural pursuit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-research-landscape",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-research-landscape",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.10 Hierarchical Reconstruction of Research Landscape",
    "text": "13.10 Hierarchical Reconstruction of Research Landscape\nTo overcome the challenges posed by scale-dependency, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, they employ Ward agglomerative clustering, systematically merging the initial 600 fine-grained topics one by one according to a defined objective function.\nConcurrently, for the community structure, the team implemented a hierarchical stochastic block model from the outset. This model intrinsically learns a multi-level partition, progressively grouping physicists into coarser communities. These resultant hierarchical structures collectively introduce a crucial notion of scale, enabling the observation and analysis of the research system at various levels of granularity. For example, a co-authorship network can be visualised where each physicist’s specialty, indicated by colour, reflects the linguistic structure at different levels of coarse-graining.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-scale-selection-for-optimal-interpretation",
    "href": "chapter_ai-nepi_015.html#adaptive-scale-selection-for-optimal-interpretation",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.11 Adaptive Scale Selection for Optimal Interpretation",
    "text": "13.11 Adaptive Scale Selection for Optimal Interpretation\nA significant challenge arises from the arbitrary nature of selecting an appropriate observation scale for both topic and community structures. To address this, the authors propose an adaptive topic coarse-graining strategy. This approach acknowledges that whilst linguistic topics capture subtle nuances, some of these distinctions hold no practical consequence for the collaborative dynamics amongst scientists.\nThe methodology systematically removes degrees of freedom from the fine-grained partition, but only when such removal does not diminish useful information pertinent to understanding the social structure. This process is governed by the Minimum Description Length (MDL) criterion, which seeks to minimise the quantity [- log P(G|sigma) - log P(sigma)]. This criterion effectively balances the model’s fit—the linguistic partition’s explanatory power regarding the social structure—with its complexity, ensuring the partition remains parsimonious. The process involves iteratively refining the topic dendrogram, halting when the increase in complexity no longer yields a commensurate gain in information about the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#resulting-topics-and-community-topic-correlations",
    "href": "chapter_ai-nepi_015.html#resulting-topics-and-community-topic-correlations",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.12 Resulting Topics and Community-Topic Correlations",
    "text": "13.12 Resulting Topics and Community-Topic Correlations\nThe adaptive coarse-graining strategy ultimately yielded 50 distinct topics, each clearly labelled by representative N-grams. To analyse their relationship with social structures, the authors constructed a correlation matrix, mapping these coarse-grained topics against community structures across various scales.\nTheir analysis revealed nuanced patterns. Some topics, such as a large purple cluster, demonstrated universal relevance, remaining untethered to specific communities. Conversely, other topics, notably string theory, exhibited strong correspondence with community structures at particular hierarchical levels, specifically the third level in this instance. Intriguingly, certain research programmes, such as loop quantum gravity, aligned with communities situated at much lower, more fine-grained levels within the hierarchy. Overall, the findings indicate the presence of nested structures and entangled scales, rather than a clear, simple division of labour. For example, a smaller community embedded within the broader string theory community showed a distinct connection to the intellectual topic of holography, thereby exemplifying these complex, nested relationships and the absence of a straightforward plural pursuit configuration.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuitions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.13 Top-Down Approach: Physicists’ Intuitions",
    "text": "13.13 Top-Down Approach: Physicists’ Intuitions\nTo validate their bottom-up reconstruction, the authors systematically confronted it with the prevailing intuitions of physicists themselves. This involved surveying the founding members of the International Society for Quantum Gravity, requesting them to enumerate the quantum gravity approaches they perceived as structuring the overall research landscape.\nThe collected responses yielded a detailed list of approaches, including asymptotic safety, causal sets, dynamical triangulations, group field theory, Loop Quantum Gravity (LQG), spin foams, noncommutative geometry, swampland conjectures, modified dispersion relations (DSR), quantum modified black holes, shape dynamics, tensor models, string theory, supergravity, and holography. A particular focus emerged on string theory, supergravity, and holography, primarily because some physicists expressed disagreement regarding whether these should be considered distinct entities, despite their historical and conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#comparing-top-down-and-bottom-up-perspectives",
    "href": "chapter_ai-nepi_015.html#comparing-top-down-and-bottom-up-perspectives",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.14 Comparing Top-Down and Bottom-Up Perspectives",
    "text": "13.14 Comparing Top-Down and Bottom-Up Perspectives\nThe authors trained a Support Vector Machine (SVM) classifier to predict the specific approach of individual papers, leveraging text embeddings derived from titles and abstracts (using all-MiniLM-L6-v2) and hand-coded labels for training. This supervised, top-down classification then underwent comparison with their bottom-up reconstruction.\nThe results demonstrated varied alignment. Certain top-down approaches exhibited strong correspondence with the emergent bottom-up topics. Conversely, other approaches, particularly those of a phenomenological nature or lacking a comprehensive conceptual framework, did not align effectively. The method proved most robust for well-defined, conceptually autonomous frameworks. Notably, the bottom-up analysis identified a substantial string theory cluster that encompassed both supergravity and string theory. This finding resonates with physicists’ intuitions, as articulated in a survey response: “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on”supergravity” and “string theory” is so large that I’m not sure the communities can be separated in a meaningful way.” This convergence suggests that when linguistic nuances without direct social consequences are disregarded, conceptually distinct areas may merge, a finding supported by both empirical data and expert perception.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#key-conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#key-conclusions-and-philosophical-implications",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.15 Key Conclusions and Philosophical Implications",
    "text": "13.15 Key Conclusions and Philosophical Implications\nThis research yields several significant conclusions. Firstly, socio-epistemic systems demonstrably operate across multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Secondly, effectively identifying configurations of plural pursuit—characterised by a one-to-one mapping between communities and their intellectual substrate—demands a meticulous alignment of these structures across their respective scales. Thirdly, the authors’ bottom-up reconstruction of the quantum gravity research landscape offers a robust mechanism to either confirm or critically re-assess the prevailing intuitions of physicists.\nFinally, and perhaps most profoundly, the increasing power of computational methods empowers scholars to revisit and challenge long-held philosophical insights and intuitions, particularly those concerning the nature of paradigms or communities within specific scientific contexts like quantum gravity. This work compellingly argues that “Computation is the continuation of philosophy by other means!”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThe authors conducted a comparative study, systematically evaluating the performance of Latent Dirichlet Allocation (LDA) and BERTopic across distinct textual granularities: titles, abstracts, and full texts. This investigation addresses a pivotal inquiry within topic modelling, a crucial analytical tool for scrutinising extensive volumes of scientific literature, particularly within the history, philosophy, and sociology of science. Topic modelling extracts themes from corpora, enabling the identification of research trends, paradigm shifts, substructures, interrelations of themes, and the evolution of scientific vocabulary.\nThe study’s core objective was to ascertain whether analysing titles or abstracts suffices for robust topic modelling, or if full-text analysis remains indispensable. This question arises given the substantial resources required for obtaining, preprocessing, and analysing comprehensive corpora. To achieve this, the research team constituted a corpus of scientific articles, meticulously identifying and segmenting title, abstract, and full-text sections. Subsequently, they applied both LDA and BERTopic approaches to each textual level. A dual analytical framework, encompassing both qualitative and quantitative methods, then facilitated the comparison of the resulting topic models. This rigorous methodology involved assessing model similarities, topic diversity, joint recall, and coherence, whilst leveraging a well-known astrobiology corpus for qualitative validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "href": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Study Design",
    "text": "14.1 Research Question and Study Design\nThis research addresses a pivotal question within the domain of topic modelling: does analysing titles or abstracts provide sufficient data, or does full-text analysis remain a prerequisite? Topic modelling, a technique for extracting thematic content from textual corpora, has emerged as an indispensable tool for scrutinising extensive scientific literature, particularly within the history, philosophy, and sociology of science. Indeed, scholars employ it for diverse tasks, including identifying research trends, discerning paradigm shifts, uncovering substructures, mapping thematic interrelations, and tracing the evolution of scientific vocabulary.\nCrucially, existing studies apply topic modelling across various textual structures, encompassing titles, abstracts, and complete articles. This practice, however, raises a significant concern: obtaining, preprocessing, and analysing full-text corpora demand considerable resources. Consequently, the efficiency of utilising shorter textual forms becomes a pressing inquiry.\nTo investigate this, the authors meticulously constituted a corpus of scientific articles. They then precisely identified and isolated the title, abstract, and full-text sections within each document. Subsequently, they applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. A comprehensive analytical framework, integrating both qualitative and quantitative methods, facilitated the systematic comparison of the resultant topic models. This rigorous design ensured a thorough evaluation of performance across different model types and textual granularities.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "href": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Topic Modelling Methodologies",
    "text": "14.2 Topic Modelling Methodologies\nThe study employed two principal topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches fundamentally postulate that documents can be represented as numerical vectors. Within this framework, topics become identifiable through the detection of linguistic regularities, specifically repetitions, whilst machine learning algorithms facilitate the automatic discovery of these patterns.\nLDA, a classical statistical technique, constructs simple vector representations by counting words within documents. In this established approach, topics manifest as latent variables, adhering to Dirichlet’s law. Crucially, LDA readily accommodates extensive textual content, allowing for its application to titles, abstracts, or full texts.\nConversely, BERTopic represents a more recent, modular methodology. It leverages Large Language Model (LLM)-based vector representations, originally drawing upon BERT, which lends the approach its name. Here, topics emerge as clusters of documents. Historically, BERTopic struggled with processing lengthy texts; however, for this investigation, the authors integrated a novel embedding technique. This advancement significantly enhanced BERTopic’s capacity, enabling it to process approximately 131,000 tokens, thereby facilitating its application to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison-framework",
    "href": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison-framework",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Corpus and Qualitative Comparison Framework",
    "text": "14.3 Corpus and Qualitative Comparison Framework\nThe study’s qualitative comparisons drew upon a meticulously analysed astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a comprehensive evaluation, the authors selected a full-text LDA model comprising 25 distinct topics to serve as a foundational reference.\nScholars meticulously analysed these 25 topics, examining their most representative words and documents. This process facilitated the generation of a concise label for each topic, derived directly from its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric based on the topics’ presence within individual documents. A community detection algorithm then identified four distinct thematic clusters, designated A, B, C, and D, and visually distinguished by red, green, yellow, and blue hues respectively.\nA graphical representation visually conveyed these findings, illustrating the correlations amongst the 25 topics, complete with their assigned labels and cluster affiliations. In this visualisation, the thickness of the connecting lines denoted the strength of the correlation between topics, whilst the size of each circular node indicated the topic’s overall prevalence across the entire document collection. This established analytical framework provided a robust basis for the qualitative assessment of the six topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Quantitative Analysis Metrics",
    "text": "14.4 Quantitative Analysis Metrics\nFor quantitative analysis, the authors employed four distinct metrics to rigorously compare the topic models.\n\nFirstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings, whilst correcting for chance agreement. This metric precisely assessed the degree to which documents tended to cluster together across different models.\nSecondly, Topic Diversity quantified the proportion of distinct top words within a given topic model, thereby evaluating whether individual topics were indeed characterised by unique vocabularies.\nThirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words. This metric critically assessed how effectively the top words collectively represented the documents assigned to each topic.\nFinally, Coherence CV, calculated as the average cosine relative distance between top words within topics, determined whether the constituent words of a topic exhibited a meaningful semantic relationship.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: Adjusted Rand Index Analysis",
    "text": "14.5 Results: Adjusted Rand Index Analysis\nThe Adjusted Rand Index (ARI) provided initial insights into the similarities amongst the six topic models. A score of zero on this metric signifies a random clustering, establishing a baseline for comparison. Analysis revealed that the LDA model applied to titles exhibited the most pronounced dissimilarity from all other models, consistently registering ARI values below 0.2 within the heatmap.\nConversely, the remaining models generally achieved a superior overall match, with ARI scores exceeding 0.2. Notably, BERTopic models demonstrated a stronger mutual fit, consistently yielding values above 0.35. Amongst these, the BERTopic abstract model emerged as particularly central, correlating effectively with every other model, save for the outlier LDA title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: LDA Model Inter-Comparisons",
    "text": "14.6 Results: LDA Model Inter-Comparisons\nA more granular analysis of the LDA models provided detailed insights into their inter-relationships. Comparing LDA Full-text with LDA Abstract (Table A) revealed a generally strong fit. A distinct reddish diagonal in the table indicated that each topic from one model largely corresponded to a topic in the other, sharing a high proportion of common documents. Despite this overall alignment, some dynamic shifts occurred: three full-text LDA topics entirely disappeared, whilst another three split into multiple topics within the abstract model. Concurrently, three novel abstract topics emerged, and three abstract topics resulted from the merger of others. Furthermore, one small class within the abstract topics contained fewer than 50 documents.\nIn stark contrast, the comparison between LDA Full-text and LDA Title (Table B) demonstrated a poor fit, necessitating substantial reorganisation. This disparity manifested as numerous full-text topics vanishing and a proliferation of new topics appearing within the title model, underscoring the limited correspondence between these two textual granularities for LDA.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: BERTopic Model Inter-Comparisons",
    "text": "14.7 Results: BERTopic Model Inter-Comparisons\nAnalysis of the BERTopic models, particularly in comparison with LDA Full-text, revealed varied levels of correspondence. Comparing LDA Full-text with BERTopic Full-text (Table C) indicated an average overall fit. Within this comparison, eight topics from the LDA model disappeared, whilst six topics split into the BERTopic model. Conversely, five new topics emerged within the BERTopic model, and one topic resulted from mergers. Furthermore, the document distribution showed four small classes alongside one notably large class.\nThe comparison between LDA Full-text and BERTopic Abstract (Table D) demonstrated a relatively good fit. Here, four topics disappeared, six topics split, two new topics appeared, and four topics resulted from mergers.\nFinally, examining LDA Full-text against BERTopic Title (Table E) again indicated an average overall fit. In this instance, seven topics disappeared, whilst one topic split. Simultaneously, seven new topics emerged, and one topic resulted from a merger. The document distribution for this comparison revealed three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: LDA Top-Word Correspondence",
    "text": "14.8 Results: LDA Top-Word Correspondence\nAn examination of the top words within the LDA models revealed that topics generally maintained a relatively well-formed structure across all iterations. The authors identified several robust topics exhibiting strong correspondence across every LDA model; “A-Radiation spore” serves as a prime example of such consistency.\nConversely, certain topics from the full-text model fragmented across the abstract and title models. For instance, “A-Life civilization” split into multiple sub-topics, a division that logically aligned with the broader theme of research in astrobiology. However, the fragmentation of “B-Chemistry” proved more challenging to interpret without deeper investigation.\nFurthermore, the analysis uncovered instances where topics from the full-text model merged into new, consolidated topics within the abstract and title models. The fusion of “B-Amino-acid” and “B-Protein-gene-RNA” exemplified this phenomenon, forming a more generalised and coherent thematic unit.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results: BERTopic Top-Word Correspondence",
    "text": "14.9 Results: BERTopic Top-Word Correspondence\nContinuing the assessment of top words, the three BERTopic models consistently yielded relatively well-formed topics. Notably, “A-Radiation spore” again demonstrated remarkable robustness, maintaining its coherence across all BERTopic iterations. Similarly, “A-Life civilization” remained comparatively stable across the models, albeit with some observed splitting.\nThis fragmentation of “A-Life civilization” specifically led to the emergence of narrower topics, focusing precisely on extraterrestrial life. Furthermore, the splitting of “B-Chemistry” across the BERTopic models also resulted in more specialised, narrower thematic categories.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-coherence-performance",
    "href": "chapter_ai-nepi_016.html#results-coherence-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results: Coherence Performance",
    "text": "14.10 Results: Coherence Performance\nAn evaluation of the models’ coherence, a metric assessing the meaningfulness of topic top words, revealed distinct performance patterns across a range of 5 to 50 topics. Titles consistently yielded the poorest coherence scores, indicating a less meaningful grouping of their constituent words. Conversely, abstract models generally demonstrated superior coherence compared to their full-text counterparts.\nAcross the board, BERTopic models exhibited better coherence than LDA, particularly for abstract and title analyses. However, this performance gap narrowed as the number of topics increased. Ultimately, the BERTopic Abstract model emerged as the unequivocal leader in terms of coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-diversity-performance",
    "href": "chapter_ai-nepi_016.html#results-diversity-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results: Diversity Performance",
    "text": "14.11 Results: Diversity Performance\nAssessing the diversity of top words representing the topics, a clear trend emerged: diversity generally diminished as the number of topics increased. Titles, surprisingly, offered the highest diversity amongst all models, suggesting a broader range of unique words characterising their topics.\nFurthermore, BERTopic consistently outperformed LDA in terms of diversity. Ultimately, the BERTopic Title model secured the top position for diversity, with BERTopic Full-text closely trailing.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "href": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Results: Joint Recall Performance",
    "text": "14.12 Results: Joint Recall Performance\nThe joint recall metric, which evaluates the efficacy of top words in collectively representing documents classified within each topic, revealed distinct performance hierarchies. Titles consistently yielded the poorest recall scores, indicating a limited ability of their top words to capture the full scope of associated documents. Conversely, full-text models demonstrated superior recall compared to both their abstract and title counterparts.\nBetween the two primary approaches, LDA generally exhibited better joint recall than BERTopic. Ultimately, LDA Full-text and BERTopic Full-text emerged as joint leaders in this category, with BERTopic Abstract following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "href": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Overall Model Performance Summary",
    "text": "14.13 Overall Model Performance Summary\nThe authors compiled the comprehensive results into a summary table, visually representing each model’s performance across various assessments using a graded circle system: black denoted the highest score, whilst white indicated the lowest. Crucially, this synthesis underscored the absence of an absolute “best” model, as varying research objectives inherently dictate differing needs and, consequently, distinct model choices.\nConsider, for instance, an objective focused solely on discovering main topics, where precise document classification is not paramount. In such a scenario, issues like poor recall or significant class imbalance might prove negligible. Here, full-text BERTopic performed commendably, despite exhibiting some class imbalance. Similarly, whilst far from optimal, title BERTopic nonetheless generated several robust topics that consistently appeared across other models. Conversely, the authors strongly advise against employing LDA Title, given its consistently poor performance across nearly all assessment criteria.\nUltimately, the study recommends conducting topic modelling on either abstract or full-text data, utilising both LDA and BERTopic, provided such an approach does not result in the misclassification of documents pertinent to the identified topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.14 Discussion and Future Directions",
    "text": "14.14 Discussion and Future Directions\nThis research yielded several crucial findings, informing future approaches to topic modelling. Firstly, title models consistently demonstrated poor performance. This deficiency likely stems from the inherent lack of information within titles, which can lead to the false classification of documents. Nevertheless, the BERTopic title model surprisingly revealed several meaningful topics, suggesting a potential balance between well-defined topics and comprehensive document coverage remains achievable.\nSecondly, full-text models, whilst offering comprehensive data, sometimes struggle to process vast quantities of information effectively. With LDA, topics can become more loosely defined and broader in scope, occasionally encompassing secondary themes such as methodology. Conversely, BERTopic, when applied to full text, can generate overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently performed well with summary information. Notably, the results obtained from LDA full text exhibited strong consistency with both abstract models, underscoring their utility. Fourthly, the study revealed a remarkable robustness of topics across all models. The authors identified very similar topics across the board, a consistency that facilitates the application of meta-analytic methods to pinpoint the most robust thematic elements. Moreover, leveraging the relative distance across models could enable the identification of an optimal solution, as exemplified by the BERTopic abstract model in this study, which performed exceptionally well across numerous metrics.\nFinally, the findings prompt consideration of new model paradigms. It appears feasible to exploit the inherent structural information—encompassing full text, abstracts, and titles—to extract more semantically meaningful sets of topics or top words, thereby advancing the precision and utility of topic modelling.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "",
    "text": "Overview\nThis chapter outlines a novel approach for imbuing large language models (LLMs) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, LLMs derive their understanding of time implicitly from statistical patterns within training texts; however, this method proves insufficient for tasks requiring precise temporal context. The authors propose the “Time Transformer”, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities arising from temporally contradictory information within training data.\nTo validate this concept, the research team developed a modest Transformer model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the Time Transformer’s efficacy. The experiments involved injecting synthetic temporal drifts—both synonymic succession (e.g., replacing “rain” with “liquid sunshine”) and co-occurrence changes (e.g., rain becoming rain and snow)—into the training data. The Time Transformer consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.\nBeyond this proof of concept, the Time Transformer holds significant implications for historical analysis, offering a foundation for downstream tasks on historical data and enabling instruction-tuned models to “talk to a specific time.” Whilst the architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Future research will explore benchmarking against explicit time-token approaches and investigate the utility of a modest, targeted encoder model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding-in-large-language-models",
    "href": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding-in-large-language-models",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.1 Addressing Implicit Temporal Understanding in Large Language Models",
    "text": "15.1 Addressing Implicit Temporal Understanding in Large Language Models\nCurrent large language models (LLMs) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora. Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.\nA critical challenge emerges when training data contains temporally contradictory information. For instance, consider two sentences: “The primary architectures for processing text through NNs are LSTMs” (true in 2017) and “The primary architectures for processing text through NNs are Transformers” (true in 2025). Without explicit temporal context, an LLM treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a “recency bias,” favouring more recent information in next-token prediction. Existing workarounds, such as prompt engineering—where users insert explicit temporal cues like “In 2017”—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge. Consequently, a more robust solution necessitates an architecture that enables LLMs to explicitly learn and reproduce these evolving patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-and-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-and-the-time-transformer",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.2 Formalising Time-Dependent Probabilities and the Time Transformer",
    "text": "15.2 Formalising Time-Dependent Probabilities and the Time Transformer\nTo formalise the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, x_n, given a sequence of preceding tokens, x_1, ..., x_{n-1}. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as p(x_n | x_1, ..., x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a specific time t emerges as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing LLMs largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.\nTo overcome this limitation, the authors propose a direct approach involving explicit modelling of the time-dependent probability distribution p(x_n | x_1, ..., x_{n-1}, t). Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, which the authors term the “Time Transformer”, introduces a simple yet profound modification: an additional dimension, φ(t), is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, E(x, t), which then serves as input to the Transformer architecture. Consequently, the Transformer processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution p_θ(x_n | x_1, ..., x_{n-1}, t). The training objective remains the standard maximisation of log likelihood across all sequences. This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-data-and-model-architecture",
    "href": "chapter_ai-nepi_017.html#empirical-validation-data-and-model-architecture",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.3 Empirical Validation: Data and Model Architecture",
    "text": "15.3 Empirical Validation: Data and Model Architecture\nTo empirically validate the Time Transformer concept, the authors required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. The team identified UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, as an ideal choice. They scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The authors intentionally simplified the tokenisation process, neglecting sub-word tokenisation, case, and interpunctuation, resulting in a compact vocabulary of only 3,395 unique words across the entire seven-year period. The team also considered TinyStories as an alternative dataset for its similar characteristics.\nA modest Transformer architecture, termed the “Vanilla model”, underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output. Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like GPT-4, which boasts 1.8 trillion parameters distributed across 120 layers. The team conducted training on an HPC cluster in Munich, utilising two H100 GPUs, and remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The authors have made the code for this implementation publicly available on GitHub, noting its development primarily for foundational understanding rather than optimal performance. Crucially, the trained model perfectly reproduced the language of weather reports; generated texts, initiated from a seed sequence such as “During the night, a band …”, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "href": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.4 Time Transformer Implementation and Results",
    "text": "15.4 Time Transformer Implementation and Results\nImplementing the Time Transformer required only a minimal architectural adjustment to the previously described Vanilla model. The authors reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This non-trainable time dimension employs a min-max normalised representation of the day of the year, calculated as (day of year - 1) / (365 - 1). The authors chose this specific encoding to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.\nThe first experiment aimed to demonstrate the Time Transformer’s capacity for learning synthetic temporal drift through synonymic succession. The team injected a time-dependent replacement rule into the training data: the system replaced rain with liquid sunshine according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of rain versus liquid sunshine. The Time Transformer flawlessly reproduced the injected sigmoid pattern, exhibiting rain predominantly early in the year and liquid sunshine towards the end, with the transition occurring precisely mid-year.\nThe second experiment explored the Time Transformer’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the “fixation of a collocation.” Here, the authors synthetically replaced instances of rain not immediately followed by and with rain and snow. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of rain and snow against rain only. The model successfully acquired this pattern, generating rain and snow almost exclusively in the latter part of the year, whilst early-year occurrences of rain (sometimes accompanied by snow) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed that specific heads had specialised in learning these temporal patterns, conditioning early-year rain and snow on the presence of a “cold system,” underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-future-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-future-challenges",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.5 Proof of Concept, Applications, and Future Challenges",
    "text": "15.5 Proof of Concept, Applications, and Future Challenges\nThe authors’ research unequivocally establishes a proof of concept: Transformer-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. For instance, a foundation Time Transformer could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to “talk to a specific time,” potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.\nSeveral promising avenues for future research emerge. Benchmarking the Time Transformer against explicit time-token approaches will quantify its performance advantages. Crucially, the authors consider investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, a significant next step.\nNevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained LLMs remains feasible or efficient. This often necessitates training models from scratch, a process demanding prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, thereby plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to BERT, which the authors propose building upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves, Sergei Bagdasarov, and Badr M. Abdullah have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe research team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996. This extensive collection comprises nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity, with 99.81% of outputs conforming to the specified format and 94% of discipline predictions aligning with predefined categories.\nFor the diachronic analysis of chemical knowledge, the team focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” Badr M. Abdullah, an expert in Large Language Models, also contributed significantly to this work.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, the authors also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\nThe research leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, scholars applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nThe authors specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nThe predefined primary disciplines included:\n\nPhysics\nChemistry\nEnvironmental & Earth Sciences\nAstronomy\nBiology & Life Sciences\nMedicine & Health Sciences\nMathematics & Statistics\nEngineering & Technology\nSocial Sciences & Humanities\n\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAn analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\nFor the diachronic analysis of the chemical space, the authors concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nFurther investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. The authors observed similar shifts for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\nIn conclusion, the authors successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nSophia Aguilar, a PhD student within the CASCADE project—a Marie Curie doctoral network—leads an investigation into the computational analysis of semantic change. Her work meticulously models diverse contextual factors and their intricate interplay. Building upon earlier studies that modelled distinct context types in isolation, this current endeavour seeks to integrate these approaches, thereby illuminating their complex interactions. The chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory, provides a compelling pilot study, drawing extensively from the Royal Society Corpus (RSC). Linguists collaborating on this research examine how language adapts to real-world transformations, grounding their inquiry in register theory and principles of rational communication. The project aims to detect periods of significant linguistic change, analyse the specific lexical and grammatical shifts, identify influential figures, and elucidate the underlying linguistic mechanisms and communicative drivers of these transformations. To address limitations in capturing the interaction between contextual signals, the team proposes a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics, positioning context as a central signal.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot\nWithin the CASCADE project, a Marie Curie doctoral network, Sophia Aguilar spearheads the computational analysis of semantic change. She meticulously models context comprehensively, examining the intricate interplay between its various dimensions. This work extends previous studies that modelled distinct context types in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution, documented within the Royal Society Corpus (RSC), serves as a compelling pilot study for these methodological explorations. This pivotal historical period witnessed the profound conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation vividly captured in resources such as chemistryworld.com and contemporary art, including the iconic painting of Lavoisier and his wife. The research team aims to model a broad spectrum of contextual factors:\n\nSituational (where)\nTemporal (when)\nExperiential (what)\nInterpersonal (who)\nTextual (how)\nCausal (why)\n\nFrom a linguistic perspective, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks underpin this inquiry. Firstly, Halliday (1985) and Biber (1988) articulate language variation and register theory, which posits that situational context directly influences language use. Concurrently, the linguistic system itself offers inherent variation, enabling concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”.\nSecondly, principles of rational communication and information theory, championed by the IDeaL SFB 1102 research centre and drawing on the work of Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation modulates information content. Such modulation optimises communication for efficiency whilst maintaining a reasonable level of cognitive effort.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint the precise timing of linguistic transformations, the research team employs Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, expressed as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal substantially higher divergence due to profound linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time—for example, from 1725 to 1845—reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns frequently signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 precisely coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, the analysts can meticulously track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models\nBeyond temporal detection, the research team delves into paradigmatic context and the intricate dynamics of conceptual change, referencing the foundational work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves that illustrate the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, vividly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data frequently sourced from repositories such as corpora.ids-mannheim.de.\nTo discern who spearheads and propagates these linguistic and conceptual shifts, Yuri Bizzoni, Katrin Menzel, and Elke Teich, associated with the IDeaL SFB 1102, employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that powerfully illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such meticulous analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, within the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal\nThe inquiry further extends to how linguistic change manifests and the communicative pressures that might drive it, drawing upon the insightful research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with significant contributions from Elke Teich. A pivotal concept in this analytical strand is “surprisal,” originating from Shannon’s (1949) information theory and subsequently developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is directly proportional to its unexpectedness or improbability within a given context; for example, the word completing “Jane bought a ____” might possess a different surprisal value than one completing “Jane read a ____.”\nApplying this principle to linguistic change, the authors examine shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, vividly visualised through graphs plotting surprisal against year, robustly supports this proposition. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) frequently reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community employing that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics\nSofía Aguilar, an Early Career Researcher funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the profound understanding that language change intrinsically links to shifts in social context, encompassing evolving goals, social structures, and domain-specific conventions. Whilst current methodologies—such as semantic change studies, KLD applications, and static network approaches—effectively track shifts, they often fall short in modelling the intricate interactions between various contextual signals. Aguilar’s proposed framework positions context as a central signal for modelling language dynamics, identifying Graph Convolutional Networks (GCNs) as a particularly promising technological direction due to their powerful capacity for modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four meticulously designed stages:\n\nData Sampling: This initial stage employs KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: The team begins by creating word- and time-aware feature vectors. BERT generates the word vectors, whilst one-hot encoding captures temporal and other pertinent features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the authors refine network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, whilst the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering profound insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research team acknowledges several profound questions that delineate the current limitations and chart compelling future directions for this field. A primary concern involves the very nature of computationally tracing conceptual change: can current and future models transcend the capture of mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nThe authors also give further consideration to defining the fundamental ‘unit’ of language change. They question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that these models generate genuinely meaningful explanations, rather than merely plausible ones, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#overview",
    "href": "chapter_ai-nepi_020.html#overview",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Researchers investigate the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a prime case study, owing to its pivotal role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, analyses the institute's extensive born-physical archive, which contains over two million pages of internal documents such as meeting notes, handwritten correspondence, presentations, and spreadsheets. To manage and interpret this vast dataset, investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks like entity extraction and synthetic document generation, crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction. Case studies demonstrate the power of these methods: one reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the \"Kitchen Cabinet\" and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena like the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, seeking partners to engage with their newly funded initiative: \"Born Physical, Studied Digitally.\"",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\n    State-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\n\n    Nevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. To equate bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. Researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\n\n    Delving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\n    The Human Genome Project (HGP) stands as a seminal example of \"big science\" in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\n\n    Its legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\n\n    Two principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH's most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high \"disruption\" scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI's success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\n    An interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\n\n    Central to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP's historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\n\n    The content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive's content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\n    The analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. Researchers acknowledge the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\n\n    Beyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\n    A critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like 'PERSON' and 'ORGANIZATION' even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\n\n    To showcase the analytical power derived from these processed documents, researchers reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\n    Network analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. Investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP's focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\n\n    Employing community detection algorithms like stochastic block models, researchers identified distinct interacting groups within the HapMap Project's communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, the analysis computationally uncovered a previously undocumented informal leadership group, termed the \"Kitchen Cabinet.\" This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\n\n    Further analysis of brokerage roles within these communication networks revealed distinct operational styles. The \"Kitchen Cabinet,\" for instance, predominantly exhibited a \"consultant\" brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\n    The rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI's decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\n\n    To understand these decisions, researchers developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism's genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers' centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\n\n    When all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a \"Matthew Effect\" at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\n    The methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself is part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\n\n    A strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named \"Born Physical, Studied Digitally,\" supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\n\n    This work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI's history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore sophisticated methods for transforming unstructured biographical sources into structured knowledge graphs. Their work directly addresses the challenge of computationally accessing rich information contained within traditional formats, such as printed books and archives, which inherently lack digital structure. The team’s core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks (e.g., Wer war wer in der DDR?). It then proceeds to extract entities—persons, places, countries, and works—and their relationships, representing them as nodes and edges in a knowledge graph. The team visualises these graphs using tools such as Neo4j. This structured representation facilitates complex queries, enabling investigations into network formations amongst professionals in specific periods or tracing the evolution of ideas. Their methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs encompass validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. Ultimately, the team aims to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nAcademics confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its inherent lack of digital structure. Whilst earlier tools, such as Get Grasso, aimed to digitise and process printed materials, the present investigation centres upon biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this inherent limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but rather as specialised tools within a controllable pipeline. This approach facilitates the construction of knowledge graphs from otherwise unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, with the relationships between them depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms such as Neo4j. Crucially, the project seeks the most efficacious LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach enables complex queries, such as mapping an individual’s birth and work locations, or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe authors’ transformation of raw biographical text into a structured knowledge graph adheres to a meticulously designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments, or ‘chunks’. From these chunks, an extraction pipeline identifies key entities and their interrelations, subsequently assembling them into a knowledge graph. For instance, a biographical entry for ‘Bartsch Henryk’ might yield entities such as his name (PERSON), role (‘ks. ewang.’ – evangelical priest, ROLE), birth date (DATE), and birthplace (‘Władysławowo’, LOCATION). This process also identifies relationships, such as ‘born in’ or ‘travelled to’ various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples; for example, ‘(Bartsch Henryk, is a, ks. ewang.)’.\nThis entire process unfolds within a two-stage pipeline, meticulously crafted by the team. The first stage, ‘Ontology-agnostic Open Information Extraction’ (OIE), focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them. A crucial quality check then determines their sufficiency. Subsequently, the second stage, ‘Ontology-driven Knowledge Graph’ (KG) building, commences with formulating Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial ‘Human-in-the-Loop’ layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe pipeline’s initial stage, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. This stage commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (‘Biografie’), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an ‘Extraction Call’. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text such as ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a ‘Validation Call’ produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a ‘Gold Standard’—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline, or if further refinement of the OIE steps is necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with formulating Competency Questions (CQs), which the team manually refines based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance: ‘Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?’\nGuided by these CQs and the sample triples, an Ontology Creation step, via a ‘Generation Call’, produces an ontology definition. This definition specifies classes (e.g., ‘Person’ as an owl:Class, ‘doctorate’ as a class of ‘academicEvent’) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, SHACL (Shapes Constraint Language) shapes are created to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a ‘Mapping Call’, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This process results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping ‘Robert Havemann’ to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nThe research team illustrates the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying the knowledge-graph approach to this corpus, the authors can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide XX\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by academics and journalists; the presentation displays sample entries for Gustav Hertz and Robert Havemann. An analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR. Further comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures such as the Politbüro or Ministerrat.\n\n\n\nSlide XX",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method for progressing from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, the authors identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the team intends to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater for more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks such as ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]