[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held April 2-4, 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThe workshop, “Large Language Models for the History, Philosophy, and Sociology of Science,” convened to delve into novel AI-assisted methodologies within these disciplines. Adrian Wüthrich, Arno Simons, Michael Zich, and Gerd Grasshoff meticulously organised the event, which drew over 220 registered participants, both in-person and online, from more than 50 initial submissions. This significant gathering stemmed from two distinct initiatives. Firstly, the “Network Epistemology in Practice” (NEPI) project saw Arno Simons pioneer the training of large language models (LLMs) on physics texts, whilst Michael Zich applied LLMs to analyse conceptual issues in physics. Secondly, Gerd Grasshoff’s long-standing advocacy for AI in the history and philosophy of science, particularly for analysing scientific discovery processes, provided a crucial impetus.\nThe European Research Council (ERC) Grant NEPI provides funding for this workshop. The NEPI project specifically investigates the internal communication dynamics of the ATLAS collaboration at CERN, the renowned particle physics laboratory. Its primary aim is to understand how this prominent research collaboration collectively generates new knowledge. This endeavour involves employing network analysis to discern intricate communication structures and utilising semantic tools, including LLMs, to trace the flow of ideas within these complex networks.\nThe organisers, recognising the importance of comprehensive documentation, ensured the recording of all Zoom sessions. A single camera focused on the presenter, whilst four microphones and a backup iPhone captured high-fidelity audio. Subject to presenter consent, videos of talks, including their associated discussions, will be uploaded to NEPI’s YouTube Channel. Notably, discussion segments will feature only the presenter’s audio and video, thereby safeguarding audience privacy. Svenja Götz, Lea Stengel, and Julia Kim provided invaluable administrative and organisational support for the workshop. Furthermore, Oliver Ziegler and his Unicam team offered expert technical assistance, facilitating the recording of keynotes and ensuring seamless Zoom operations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-logistics-and-recording-protocols",
    "href": "chapter_ai-nepi_001.html#workshop-logistics-and-recording-protocols",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.1 Workshop Logistics and Recording Protocols",
    "text": "2.1 Workshop Logistics and Recording Protocols\n\n\n\nSlide 02\n\n\nThe workshop established a comprehensive recording protocol to meticulously document its proceedings. As specified during registration, all Zoom sessions were recorded. A single camera captured the presenter, whilst four microphones and an iPhone served as a backup audio recorder, collectively ensuring high-fidelity capture of all spoken content. Following the workshop, and subject to presenter consent, the videos of individual talks—including their associated discussions—will be uploaded to the NEPI YouTube Channel. Crucially, during discussion segments, only the presenter’s audio and video will be featured, thereby safeguarding audience privacy. Participants seeking further information or wishing to withhold consent were encouraged to contact the organisers directly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-interaction-and-social-engagements",
    "href": "chapter_ai-nepi_001.html#workshop-interaction-and-social-engagements",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.2 Workshop Interaction and Social Engagements",
    "text": "2.2 Workshop Interaction and Social Engagements\n\n\n\nSlide 04\n\n\nTo foster dynamic engagement amongst a large participant group, the organisers established a streamlined question-and-answer protocol. Attendees were asked to keep their questions and comments concise and germane. Following each presentation, the chair collected approximately four questions, enabling the presenter to respond collectively and thereby maximise temporal efficiency. Beyond the live sessions, an Etherpad or Cryptpad, accessible via a provided QR code, offered an asynchronous platform. Here, participants could post comments or questions, allowing presenters to respond at their convenience. During active sessions, the Zoom chat provided an immediate channel for both online and in-person attendees to submit queries. Recognising the importance of informal scholarly exchange, the workshop incorporated ample opportunities for networking. These included dedicated lunch breaks, coffee breaks, a modest reception, and a workshop dinner, though seating for the latter remained limited. Specifically, coffee breaks and refreshments took place in room H 3005, whilst lunch breaks and the reception were hosted in room H 2051.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-address-large-scale-text-analysis-for-cultural-and-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-address-large-scale-text-analysis-for-cultural-and-societal-change",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.3 Keynote Address: Large-scale Text Analysis for Cultural and Societal Change",
    "text": "2.3 Keynote Address: Large-scale Text Analysis for Cultural and Societal Change\n\n\n\nSlide 05\n\n\nThe workshop’s inaugural keynote address, “Large-scale text analysis for the study of cultural and societal change,” was delivered by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi directs the “Change is Key” research programme at the institution, a programme primarily dedicated to investigating semantic change detection. Their collaborative work spans both technical aspects, such as the development of robust benchmarks, and broader methodological considerations, including the application of data science methods to address complex questions within the humanities. This dual focus rendered their contribution especially pertinent to the workshop’s overarching interdisciplinary aims.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-address-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-address-elevating-nlp-to-the-cross-document-level",
    "title": "2  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "2.4 Keynote Address: Elevating NLP to the Cross-Document Level",
    "text": "2.4 Keynote Address: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 06\n\n\nThe second keynote address, presented on the subsequent day, featured Iryna Gurevych, who directs the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University Darmstadt. Her extensive research portfolio covers critical areas such as information extraction, semantic text processing, and machine learning. Crucially, her work also involves the practical application of Natural Language Processing (NLP) techniques within the social sciences and humanities, thereby aligning precisely with the workshop’s core themes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: Architectures, Adaptation, and Applications",
    "section": "",
    "text": "Overview\nThis chapter systematically explores Large Language Models (LLMs), detailing their foundational architectures, methods for domain adaptation, and specific applications within the History, Philosophy, and Sociology of Science (HPSS). The discussion commences by establishing a primer on LLMs, tracing their evolution from the seminal Transformer architecture, pioneered by Vaswani and colleagues in 2017, to specialised models such as BERT (Devlin et al., 2018) and GPT (Radford et al., 2018). The authors meticulously differentiate between encoder-based models, which offer bidirectional, full-context understanding, and decoder-based models, characterised by their unidirectional, generative capabilities, thereby highlighting their distinct strengths.\nSubsequently, the chapter elucidates various strategies for adapting these models to specific scientific domains and tasks. These methods encompass pre-training, continued pre-training, fine-tuning for classification, prompt-based approaches, and contrastive learning, exemplified by Sentence BERT. The discussion then turns to advanced LLM systems, including Retrieval-Augmented Generation (RAG) pipelines and multi-LLM agents, which integrate multiple models and external tools to enhance performance and broaden their utility.\nThe chapter then delineates a comprehensive taxonomy of LLM applications pertinent to HPSS research, categorising them into four key areas: handling data and sources, analysing knowledge structures, understanding knowledge dynamics, and examining knowledge practices. Finally, the authors address critical challenges inherent in applying LLMs to HPSS, such as the historical evolution of concepts, the imperative for reconstructive and critical perspectives, and issues of data sparsity and multilingualism. The chapter concludes by advocating for increased LLM literacy, the development of shared datasets and benchmarks, and the strategic integration of LLM capabilities whilst preserving core HPSS methodologies, thereby fostering new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: Architectures, Adaptation, and Applications</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#additional-visual-materials",
    "href": "chapter_ai-nepi_003.html#additional-visual-materials",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: Architectures, Adaptation, and Applications",
    "section": "3.1 Additional Visual Materials",
    "text": "3.1 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n This slide presents the chapter’s agenda, whimsically titled ‘Today’s Menu’. The title and the subsequent list of topics appear on a dark, chalkboard-like image, framed by a wooden easel, lending a rustic, inviting aesthetic. The listed sections of the presentation are: ‘Primer on LLMs’, ‘Applications in HPSS’, and ‘Reflections’. This structure indicates that the chapter will first introduce Large Language Models (LLMs), then discuss their specific applications within the History, Philosophy, and Sociology of Science (HPSS), and conclude with broader insights.\n This slide displays a comprehensive diagram of the Transformer model architecture, a foundational component of modern Large Language Models. Labelled ‘Figure 1: The Transformer - model architecture’, the diagram attributes this seminal work to Vaswani and colleagues (2017), referencing their paper, Attention is all you need. The architecture divides into two primary sections: an Encoder on the left and a Decoder on the right, both enclosed within a rounded rectangular outline. Arrows indicate data flow, commencing with ‘Words’ converting to ‘Numbers’ before entering the Encoder, and similarly from ‘Numbers’ to ‘Word(s)’ upon exiting the Decoder. Both Encoder and Decoder sections feature stacks of identical layers, denoted by ‘Nx’. Each layer in both sections incorporates ‘Multi-Head Attention’ mechanisms, ‘Add & Norm’ operations, and ‘Feed Forward’ networks. The Decoder specifically includes ‘Masked Multi-Head Attention’ and accepts ‘Outputs (shifted right)’ as input, leading to ‘Output Probabilities’ via ‘Linear’ and ‘Softmax’ layers. ‘Input Embedding’ and ‘Positional Encoding’ are shown at the input stages for both Encoder and Decoder. The diagram visually elucidates how the Transformer processes input sequences and generates output sequences through attention mechanisms.\n This slide overlays the previous Transformer architecture diagram, maintaining the ‘Transformer’ title at the top. The primary alteration involves fading out the Decoder section of the diagram, along with its associated text labels, rendering it less prominent. Conversely, the Encoder section, encompassing its components such as ‘Input Embedding’, ‘Positional Encoding’, ‘Nx’ layers with ‘Multi-Head Attention’, ‘Add & Norm’, and ‘Feed Forward’ blocks, remains fully visible and highlighted. The labels ‘Words -&gt; Numbers’ and ‘Inputs’ also retain their clarity. The caption ‘Figure 1: The Transformer - model architecture.’ and the citation ‘Vaswani et al. 2017: Attention is all you need’ persist at the bottom. This visual emphasis directs attention specifically to the Encoder’s pivotal role and internal structure within the Transformer model.\n This slide continues the progressive revelation of the Transformer diagram, maintaining its focus on the Encoder section. A new, prominent BERT block now appears on the left, conceptually representing the Encoder. This BERT block is labelled ‘bidirectional full-context’ and incorporates internal components such as ‘LLMs for HPSS ?’ and ‘Vocab’, suggesting how domain-specific knowledge or applications might integrate. The BERT block also displays a grid-like structure, likely representing its internal layers or attention mechanisms. The original Transformer Encoder diagram, with its ‘Input Embedding’, ‘Positional Encoding’, and ‘Nx’ layers, remains visible to the right of the new BERT block, albeit slightly faded, indicating that the BERT block serves as a conceptual representation of the Encoder’s function. The Decoder section of the main Transformer diagram remains faded from the previous slide. A new citation, ‘Devlin et al. 2018. BERT: Pre-training of…’, is added to the bottom left, alongside the original ‘Vaswani et al. 2017’ citation, acknowledging the BERT model’s origin. The slide title ‘Transformer’ remains at the top, and the figure caption ‘Figure 1: The Transformer - model architecture.’ is positioned at the bottom centre.\n This slide duplicates the immediately preceding slide, preserving the identical visual state and textual content. The ‘Transformer’ title remains at the top. The BERT block, labelled ‘bidirectional full-context’ and containing ‘LLMs for HPSS ?’ and ‘Vocab’, is prominently displayed on the left, representing the Encoder. The detailed internal structure of the Encoder from the original Transformer diagram persists, albeit faded, positioned to the right of the BERT block. The Decoder section of the Transformer diagram remains faded. The citations ‘Devlin et al. 2018. BERT: Pre-training of…’ and ‘Vaswani et al. 2017: Attention is all you need’ are present at the bottom, alongside the figure caption ‘Figure 1: The Transformer - model architecture.’. This repetition suggests a deliberate pause or continued discussion on the BERT model’s role within the Encoder context before further modifications are introduced.\n This slide continues the incremental build-up of the Transformer diagram. The BERT block, representing the Encoder and labelled ‘bidirectional full-context’ with ‘LLMs for HPSS ?’ and ‘Vocab’, remains prominently displayed on the left. The detailed internal structure of the Encoder from the original Transformer diagram persists, albeit faded, to the right of the BERT block. The significant change on this slide is the full visibility and highlighting of the Decoder section of the main Transformer diagram, which was previously faded. This includes all its components: ‘Masked Multi-Head Attention’, ‘Add & Norm’, ‘Feed Forward’, ‘Linear’, ‘Softmax’, ‘Output Probabilities’, ‘Outputs (shifted right)’, and the ‘Numbers -&gt; Word(s)’ flow. The slide title ‘Transformer’ remains at the top, and both citations, ‘Devlin et al. 2018. BERT: Pre-training of…’ and ‘Vaswani et al. 2017: Attention is all you need’, along with the figure caption ‘Figure 1: The Transformer - model architecture.’, are present at the bottom. This step re-introduces the full Transformer architecture whilst still emphasising the BERT-like Encoder.\n This slide presents the final, comprehensive comparison within the Transformer architecture, illustrating both BERT and GPT models. The ‘Transformer’ title remains at the top. On the left, the BERT block, representing the Encoder, is fully visible and labelled ‘bidirectional full-context’, including ‘LLMs for HPSS ?’ and ‘Vocab’ components. The original detailed Encoder diagram persists, albeit faded. On the right, a new GPT block is introduced, representing the Decoder, and is labelled ‘unidirectional generative’. This GPT block also includes ‘LLMs for HPSS ?’ and ‘Vocab’ components, along with a similar internal grid structure. The original detailed Decoder diagram persists, albeit faded, to the left of the new GPT block. The data flow arrows ‘Words -&gt; Numbers’ and ‘Numbers -&gt; Word(s)’ are clearly depicted for both sides. Three citations are now present at the bottom: ‘Devlin et al. 2018. BERT: Pre-training of…’ for BERT, ‘Vaswani et al. 2017: Attention is all you need’ for the core Transformer, and ‘Radford et al. 2018. Improving Language…’ for GPT. The figure caption ‘Figure 1: The Transformer - model architecture.’ remains in the centre. This slide effectively contrasts the Encoder-only (BERT) and Decoder-only (GPT) applications of the Transformer, explaining their respective ‘bidirectional full-context’ and ‘unidirectional generative’ capabilities.\n This slide duplicates the previous one, displaying the complete comparative diagram of the Transformer architecture with specific implementations of BERT and GPT. The title ‘Transformer’ remains at the top. The left side features the BERT block, representing the Encoder, labelled ‘bidirectional full-context’ and including ‘LLMs for HPSS ?’ and ‘Vocab’. The original detailed Encoder diagram is faded behind it. The right side features the GPT block, representing the Decoder, labelled ‘unidirectional generative’ and also including ‘LLMs for HPSS ?’ and ‘Vocab’. The original detailed Decoder diagram is faded behind it. The data flow ‘Words -&gt; Numbers’ and ‘Numbers -&gt; Word(s)’ is shown. All three citations are present at the bottom: ‘Devlin et al. 2018. BERT: Pre-training of…’, ‘Vaswani et al. 2017: Attention is all you need’, and ‘Radford et al. 2018. Improving Language…’. The figure caption ‘Figure 1: The Transformer - model architecture.’ is also present. This slide serves to reinforce the visual comparison of BERT’s encoder-based, bidirectional processing versus GPT’s decoder-based, unidirectional generative capabilities.\n This slide presents a detailed timeline chart, aptly titled ‘Scientific LLMs’, which showcases the development and release of various language models specifically designed for scientific text processing from 2018 to 2024. The y-axis delineates years, from 2018 at the bottom to 2024 at the top, whilst the x-axis categorises models by their architectural type: ‘Others’, ‘Enc-Dec.’ (Encoder-Decoder), ‘Decoders’, and ‘Encoders’. Each model is represented by a coloured bubble or label, with a legend indicating ‘Open-Source’ (white text on coloured background) and ‘Closed-Source’ (black text on white background). Numerous model names populate the timeline, including, but not limited to, FLAIR, BERT, GPT2, RoBERTa, BioBERT, Clinical Flair, Galactica, BioGPT, OpenGPT, SciBERT, MatSciBERT, ChemBERTa, K2, Lightweight, BioOptimus, Med-PaLM, PubMedBERT, ClinicalLongformer, PubMedELECTRA, ChemBERTa-2, BioELECTRA, and ClinicalBERT. This chart, sourced from Ho and colleagues’ 2024 survey, A Survey of Pre-trained Language Models for Processing Scientific Text, visually demonstrates the rapid proliferation of scientific LLMs and their architectural diversity over time.\n This slide, titled ‘Domain and task adaptation via training’, presents a conceptual overview of four distinct methods for adapting Large Language Models (LLMs) to specific domains and tasks. The slide is structured into four quadrants, each depicting a different adaptation strategy using simplified LLM block diagrams:\n\nPretraining: This section illustrates two LLM blocks, each featuring ‘Vocab’ and ‘LLMs for HPSS ?’ components, connected by a dotted line, implying a continuous learning process.\nExtra Parameters: This quadrant depicts two LLM blocks, each with ‘LLMs for HPSS ?’ and ‘Vocab’, augmented by additional ‘Extra Parameters’ boxes above them. These are labelled ‘Positive’, ‘Tool’, ‘Sentiment’, ‘NER’, and ‘Field’, suggesting the integration of task-specific layers.\nPrompt Based: This section features two LLM blocks with ‘Vocab’ and ‘LLMs for HPSS ?’, demonstrating how prompts such as ‘LLMs for HPSS [mask] / ?’ and ‘LLMs stands for’ guide the model, yielding outputs like ‘Yes’ and ‘Large Language Model’.\nContrastive: This quadrant shows two ‘Pooling’ layers connected to LLM blocks, with a ‘Similarity’ score of ‘0.85’ indicated, representing methods that learn by comparing inputs.\n\nEach LLM block consistently includes ‘LLMs for HPSS ?’ and ‘Vocab’ labels, emphasising the application context. The diagrams employ simplified representations of neural network layers to convey the architectural modifications or input strategies for adaptation.\n This slide, titled ‘Domain and task adaptation via RAG’, illustrates the process of Retrieval-Augmented Generation. The diagram divides into two main conceptual phases: ‘Retrieval’ on the left and ‘Generation’ on the right. The ‘Retrieval’ section commences with ‘Documents’, represented by two LLM-like blocks labelled ‘Expert interviews are…’ and ‘Language models are…’, each connected to a ‘Pooling’ layer. A ‘Query’, originating from a thinking emoji icon and asking ‘What are LLMs?’, enters a ‘Pooling’ layer. This query then undergoes a ‘Similarity’ comparison (with values ‘0.1’ and ‘0.8’ indicated) against the pooled document representations, leading to a ‘Retrieved Document’. The ‘Generation’ section displays an LLM block with ‘Vocab’ and ‘LLMs are tools for…’ components. Both the ‘Retrieved Document’ and the original ‘Query’ (‘What are LLMs?’) feed into this generation block, which subsequently produces an output, indicated by ‘What are Language models are…’. The overall flow demonstrates how external knowledge, derived from documents, is retrieved based on a query and then utilised to augment the LLM’s generation process, thereby providing more informed and accurate responses. The thinking emoji at the bottom represents the user’s initial query.\n This slide, titled ‘Key distinctions’, provides a concise overview of important differentiating factors among Large Language Models (LLMs). This text-only slide organises these distinctions into four main categories, each accompanied by a brief explanation or examples:\n\nArchitecture/Pretraining: This distinction elaborates on the fundamental structural designs of Transformer models, encompassing ‘Encoder-based vs. Decoder-based vs. Encoder-Decoder-based’ architectures.\nFine-tuning: Described as ‘Various strategies’, this category indicates the diverse approaches employed to adapt pre-trained models to specific tasks or datasets.\nEmbeddings: This category differentiates between ‘Word vs. Sentence’ embeddings, highlighting the granularity at which text is numerically represented.\nLevel of abstraction: This distinction considers LLMs at different operational scales, from ‘LLMs vs. Pipelines vs. Agents’, suggesting that LLMs can function as standalone models, integrate into multi-step workflows, or act as autonomous agents.\n\n This slide, titled ‘Applications in HPSS’ (History, Philosophy, and Sociology of Science), provides a detailed breakdown of how Large Language Models can be applied within these academic fields. The applications are organised into four main categories, each with specific examples:\n\nDealing with data and sources: This includes ‘Parsing and extracting (publication types, acknowledgements, citations)’ for automated information retrieval, and ‘Interacting with sources (summarisation, RAG-type chatting)’ for dynamic engagement with textual data.\nKnowledge structures: This covers ‘Entity extraction (scientific instruments, celestial bodies, chemicals)’ for identifying specific concepts or objects, and ‘Mappings (disciplines, interdisciplinary fields, science-policy discourses)’ for understanding relationships between academic domains.\nKnowledge dynamics: This section focuses on ‘Conceptual histories (“theory” in DH, “virtual” and “Planck” in physics)’ for tracking concept evolution, and ‘Novelty (breakthrough papers, emerging technologies)’ for identifying innovation.\nKnowledge practices: This final category includes ‘Argument reconstruction (premises & conclusions, causality)’ for analysing logical structures, ‘Citation context analysis (purpose, sentiment)’ for understanding scholarly referencing, and ‘Discourse analysis (hedge sentences, jargon, boundary work)’ for examining social and rhetorical aspects of scientific communication.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: Architectures, Adaptation, and Applications</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila have developed OpenAlex Mapper, an innovative tool designed to mitigate critical generalisation and validation challenges pervasive in History, Philosophy, and Sociology of Science (HPSS) research. This presentation introduces the tool, clarifies its technical underpinnings, demonstrates its interactive capabilities, and examines its manifold applications within transdisciplinary contexts.\nThe methodology centres on fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, the team sampled 300,000 English abstracts from the OpenAlex database, a comprehensive and openly accessible repository of scholarly material. Engineers embedded these abstracts and reduced their dimensionality to two dimensions using Uniform Manifold Approximation and Projection (UMAP), thereby creating a foundational 2D base map. OpenAlex Mapper then allows users to submit arbitrary queries, downloading and embedding the corresponding records before projecting them onto this pre-trained UMAP model.\nThe interactive map facilitates in-depth investigation of specific terms, authors, temporal distributions, and citation networks. Significantly, the tool provides a rigorous quantitative framework that grounds qualitative, heuristic investigations, enabling researchers to trace the diffusion of models, map the distribution of concepts, and analyse method usage patterns across vast interdisciplinary samples. Examples include tracking the Hopfield model’s adoption, visualising model templates like Ising and Sherrington-Kirkpatrick, and contrasting the spatial distribution of concepts such as “phase transition” and “emergence,” alongside methods like Random Forest and Logistic Regression.\nDespite its utility, the system acknowledges several qualifications. It relies on the OpenAlex database, which, whilst robust, is not without imperfections, particularly concerning disciplinary representation. The current language model processes English-only sources, and the embedding step necessitates the presence of abstracts or well-formed titles. Furthermore, the UMAP algorithm, a stochastic process, introduces inherent trade-offs in dimensionality reduction, meaning the 768 dimensions of the Specter model cannot be perfectly represented in two, leading to potential misalignments. A working paper, Philosophy at Scale: Introducing OpenAlex Mapper, offers more detailed technical insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper Architecture and Workflow",
    "text": "4.1 OpenAlex Mapper Architecture and Workflow\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila crafted OpenAlex Mapper, a novel tool funded by an ERC grant focused on “possible life.” This innovation aims to introduce the tool, clarify its high-level technical operations, demonstrate its practical application, and ultimately discuss its utility for research within the History, Philosophy, and Sociology of Science (HPSS).\nThe core workflow of OpenAlex Mapper comprises several distinct stages. Initially, the team fine-tuned the Specter 2 embedding model, specifically to enhance its recognition of disciplinary boundaries. This process involved training the model on a dataset of articles originating from highly similar disciplinary backgrounds, with UMAP dimensionality reduction providing a visualisation of this training. Notably, these adjustments constituted minor modifications to the language model, rather than a comprehensive retraining effort.\nSubsequently, for base-map preparation, the researchers leveraged the OpenAlex database, a vast and inclusive repository of scholarly material that surpasses the scale of Web of Science or Scopus. OpenAlex offers fully open data, facilitating easy batch querying and free accessibility, which distinguishes it from many proprietary alternatives. From this extensive database, the team sampled 300,000 random articles, imposing minimal restrictions beyond requiring reasonably well-formed English abstracts. These abstracts then underwent embedding using the previously fine-tuned Specter 2 model. Engineers further reduced these embeddings to two dimensions through Uniform Manifold Approximation and Projection (UMAP), yielding both a 2D base map and a trained UMAP model.\nFor individual user queries, OpenAlex Mapper allows submission of arbitrary searches to the OpenAlex database. The tool downloads the relevant records—for instance, the first 1,000 for demonstration purposes—and embeds their abstracts using the identical fine-tuned language model. These new embeddings are then projected through the pre-trained UMAP model, ensuring that the queried articles acquire positions on the two-dimensional map consistent with their hypothetical presence during the original layout process. The resulting interactive map is accessible online and available for download via data mappers, offering features such as temporal distributions and citation graph overlays. Users can access the slides and interactive tool via maxnoichl.eu/talk, whilst a version with a higher latency GPU setup is also available for processing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration of OpenAlex Mapper",
    "text": "4.2 Interactive Demonstration of OpenAlex Mapper\nThe OpenAlex Mapper tool, accessible via https://m7n-openalex-mapper.hf.space, offers a straightforward user experience. Users initiate their investigation by searching the OpenAlex database directly through its comprehensive search interface, for example, by entering a query such as “scale-free network models.”\nIn the backend, the system efficiently downloads the initial 1,000 records pertinent to the search query, a limit imposed to optimise processing time. Subsequently, it embeds all abstracts from these downloaded records. If the user enables the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as a projection of the search results onto a pre-existing grey base map, visually representing the disciplinary landscape.\nCrucially, the map is fully interactive, empowering users to delve into specific data points. For instance, one can investigate the presence of a term like “coriander” within unexpected fields such as epidemiology or public health, gaining nuanced insights into interdisciplinary connections. The demonstration showcased queries for both “coriander,” a standard OpenAlex example, and “scale-free network models.” Furthermore, the developers have made an alternative setup available, featuring a higher latency GPU, which accommodates larger and more complex queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper primarily addresses the persistent challenges of generalisation and validation that arise from the reliance on small samples and case studies within History, Philosophy, and Sociology of Science (HPSS). Whilst traditional HPSS methods—such as the close reading of scholarly papers, direct interaction with scientists, and studies conducted by researchers with scientific training—offer invaluable detailed, close-up views of scientific processes, they often struggle to scale. Generalising these granular insights to the vast, global, and rapidly evolving landscape of contemporary science presents a significant hurdle.\nOpenAlex Mapper contributes by providing rigorous quantitative methods that effectively ground qualitative, heuristic investigations. A key feature of the tool is its capacity to trace all analytical results directly back to their original textual sources, ensuring transparency and scholarly rigour.\nThe tool supports several specific applications, offering compelling examples of its utility. Researchers can trace the diffusion of particular models, such as the Hopfield model, to ascertain where it genuinely “stuck” or achieved widespread adoption and sustained reference across diverse scientific domains. Furthermore, the system facilitates the investigation of “model templates”—conceptual frameworks defining models of similar structure that emerge in disparate scientific fields, potentially structuring science in ways orthogonal to established disciplines. Examples like the Ising, Hopfield, and Sherrington-Kirkpatrick models often appear at specific, non-continuous locations on the base map, providing crucial insights for ongoing debates concerning model transfer in science.\nBeyond models, OpenAlex Mapper enables the mapping of concept distribution. For instance, it can visually contrast the spread of “phase transition” (depicted in blue) with “emergence” (in orange), broadening such analyses into interdisciplinary contexts and circumventing common problems associated with acquiring specific datasets. Finally, the tool proves invaluable for analysing method usage. It reveals distinguishable patterns of specific methods within interdisciplinary contexts; for example, neuroscientists frequently employ Random Forest algorithms, whilst researchers in psychiatry or mental health often utilise Logistic Regression. This observation prompts profound philosophical questions regarding the underlying reasons for these patterns and their implications for debates on machine learning in science versus classical statistics, and the concept of “theory-free science.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Considerations",
    "text": "4.4 Limitations and Future Considerations\nWhilst OpenAlex Mapper offers significant analytical capabilities, its application is subject to several important qualifications. The system’s efficacy inherently depends on the OpenAlex database, which, despite its overall reasonable data quality compared to other available sources, is not without imperfections. Notably, certain disciplines, such as law, may exhibit underrepresentation within the database, potentially skewing comprehensive analyses.\nThe current language model processes English-only sources, which somewhat limits the tool’s global scope. Nevertheless, this constraint poses less of a problem for investigations focused on the more recent history of science. In principle, the integration of multilingual models could remedy this limitation, although the availability of high-quality, science-trained multilingual models remains scarce. Furthermore, the embedding step of the methodology necessitates that sources include either abstracts or well-formed titles, thereby restricting the range of processable data.\nCrucially, the method relies heavily on the Uniform Manifold Approximation and Projection (UMAP) algorithm, which presents its own set of imperfections. As a stochastic algorithm, UMAP generates one specific output amongst many possible configurations. Moreover, the algorithm must make inherent trade-offs during dimensionality reduction; the 768 dimensions of the Specter language model cannot be perfectly compressed into two, inevitably leading to some degree of “pushing and pulling and misaligning” of data points.\nFor those seeking further information, the presentation slides are available online at maxnoichl.eu/talk. Additionally, a working paper, titled Philosophy at Scale: Introducing OpenAlex Mapper, provides more exhaustive technical details regarding the tool’s development and operation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThis report systematically documents the research conducted within the ActDisease project at Uppsala University, focusing on genre classification for historical medical periodicals. Researchers aimed to develop automated methods for categorising diverse text types found in these publications, thereby enhancing their accessibility for digital humanities research. The project, funded by the ERC, investigates how patient organisations influenced modern medicine in 20th-century Europe, primarily utilising their periodicals as source material. The methodology encompassed rigorous data collection, addressing significant digitisation challenges such as Optical Character Recognition (OCR) errors and complex layouts. Experiments explored both zero-shot and few-shot classification approaches, employing multilingual encoder models like XLM-RoBERTa, mBERT, and historical mBERT, alongside instruction-tuned generative models such as Llama-3.1 8b Instruct. Key findings highlight the efficacy of genre classification in making heterogeneous historical sources amenable to text mining, demonstrating that leveraging modern datasets and few-shot learning with pre-trained multilingual encoders significantly improves performance, particularly for historical BERT models. Future work involves applying these methods to specific historical hypotheses, refining annotation schemes, generating synthetic data, and implementing active learning strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#presentation-outline-and-scope",
    "href": "chapter_ai-nepi_005.html#presentation-outline-and-scope",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 Presentation Outline and Scope",
    "text": "5.1 Presentation Outline and Scope\n\n\n\nSlide 02\n\n\nThis presentation systematically outlines the research on genre classification for historical medical periodicals. It commences by introducing the ActDisease project, detailing its objectives, the characteristics of its unique dataset, and the inherent challenges encountered during data digitisation. Subsequently, the report transitions to the core experimental work, elucidating the motivation behind genre classification, the methodologies employed for zero-shot and few-shot classification, and specific experiments involving the Llama-3.1 8b Instruct model. The presentation concludes by summarising key findings and discussing future research directions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-scope",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-scope",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Project: Objectives and Scope",
    "text": "5.2 The ActDisease Project: Objectives and Scope\n\n\n\nSlide 03\n\n\nThe ActDisease project, formally known as ‘Acting out Disease - How Patient Organizations Shaped Modern Medicine’, constitutes an ERC-funded research initiative. This endeavour primarily investigates the historical role of patient organisations in 20th-century Europe. Its central purpose involves scrutinising how these organisations influenced the evolution of disease concepts, the lived experience of illness, and prevailing medical practices. The project specifically focuses on ten European patient organisations across Sweden, Germany, France, and Great Britain, spanning the period from approximately 1890 to 1990. Patient organisation periodicals, predominantly magazines, serve as the main source material. Notably, the Hay Fever Association of Heligoland, founded in 1897, provides a historical anchor for the project’s scope.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "href": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 ActDisease Dataset Composition",
    "text": "5.3 ActDisease Dataset Composition\n\n\n\nSlide 04\n\n\nThe ActDisease project leverages a substantial, recently digitised private collection of patient organisation magazines, collectively comprising 96,186 pages. This comprehensive dataset encompasses periodicals from several European countries, addressing a range of diseases. German contributions include two allergy/asthma magazines (10,926 pages, 1901-1985), one diabetes magazine (19,324 pages, 1931-1990), and one multiple sclerosis magazine (5,646 pages, 1954-1990). From Sweden, the collection features one allergy/asthma magazine (4,054 pages, 1957-1990), one diabetes magazine (7,150 pages, 1949-1990), and one lung diseases magazine (16,790 pages, 1938-1991). French sources comprise one diabetes magazine (6,206 pages, 1947-1990) and three rheumatism/paralysis magazines (9,317 pages, 1935-1990). Finally, the UK contributes one diabetes magazine (11,127 pages, 1935-1990) and one rheumatism magazine (5,646 pages, 1950-1990). Visual examples, such as the ‘BRA Review’ and ‘Allergia’ (5/1991), illustrate the diverse nature of these historical medical periodicals.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitization-challenges-and-ocr-issues",
    "href": "chapter_ai-nepi_005.html#digitization-challenges-and-ocr-issues",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Digitization Challenges and OCR Issues",
    "text": "5.4 Digitization Challenges and OCR Issues\n\n\n\nSlide 05\n\n\nThe digitisation process for the ActDisease dataset encountered several significant challenges, primarily stemming from Optical Character Recognition (OCR). Whilst ABBYY FineReader Server 14 generally performed well on common layouts and fonts, complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality persistently challenged its accuracy. Consequently, the digitised texts exhibited remaining issues, notably OCR errors, particularly prevalent in German and French materials, and disrupted reading order. Researchers addressed these limitations by conducting experiments on post-OCR correction of German texts, employing instruction-tuned generative models. They observed frequent OCR errors within creative text types, such as advertisements, humour pages, and poems. This work aligns with ongoing research, including a publication by Danilova and Aangenendt on post-OCR correction of historical German periodicals using Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#challenges-in-analysing-historical-periodicals",
    "href": "chapter_ai-nepi_005.html#challenges-in-analysing-historical-periodicals",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Challenges in Analysing Historical Periodicals",
    "text": "5.5 Challenges in Analysing Historical Periodicals\n\n\n\nSlide 06\n\n\nAnalysing historical medical periodicals presents distinct challenges due to the inherent diversity of their textual content. These materials consistently feature a wide array of text types, a characteristic observed across all magazines within the collection. Critically, different genres frequently co-occur on a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This textual heterogeneity poses a significant problem for conventional analytical methods. Standard yearly and decade-based topic models or term counts fail to account for such intricate textual variations. Consequently, this unclassified mixture of genres introduces a likely bias into topic models and term counts, potentially distorting analytical outcomes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Rationale for Genre Classification",
    "text": "5.6 Rationale for Genre Classification\n\n\n\nSlide 07\n\n\nGenre has emerged as a particularly useful concept within language technology, offering a robust framework for textual analysis. Researchers define genre as a class of documents united by a shared communicative purpose, a definition supported by scholars such as Petrenz (2004) and Kessler (1997). This classification approach directly supports the project’s key objective: to explore the historical material from diverse perspectives, thereby enabling more nuanced historical arguments. Crucially, genre classification facilitates the study of communicative strategies as they evolve over time, allowing for comparative analysis across different countries, diseases, and publications, as highlighted by Broersma (2010). Furthermore, it enables a fine-grained analysis of term distributions and topic models specifically within identified genres, offering deeper insights than broad, unclassified analyses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrating-genre-diversity-poetry-example",
    "href": "chapter_ai-nepi_005.html#illustrating-genre-diversity-poetry-example",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Illustrating Genre Diversity: Poetry Example",
    "text": "5.7 Illustrating Genre Diversity: Poetry Example\n\n\n\nSlide 08\n\n\nTo illustrate the inherent genre diversity within the historical medical periodicals, researchers present a scanned page featuring multiple text columns and an illustration of a human figure. This visual example clearly demonstrates how various text types coexist within a single publication. A specific section of the text is highlighted as ‘Poetry’, encased within a distinctive white box with a purple border. Surrounding this poetic content, other snippets are visible, including a discussion on ‘Människokroppen’ (The Human Body), a recipe for ‘Cadbury sugar-free chocolate for diabetics’, and practical instructions such as ‘Put oil in saucepan’. This composite layout underscores the challenge of analysing such materials without explicit genre classification.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrating-genre-diversity-patient-experiences",
    "href": "chapter_ai-nepi_005.html#illustrating-genre-diversity-patient-experiences",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Illustrating Genre Diversity: Patient Experiences",
    "text": "5.8 Illustrating Genre Diversity: Patient Experiences\n\n\n\nSlide 09\n\n\nFurther illustrating the textual heterogeneity within historical medical periodicals, another scanned page displays multiple text columns and an illustration. A specific text block is prominently labelled ‘Patient Experiences’, demarcated by a white box with a purple border. The embedded text vividly describes a personal narrative of awakening from a diabetic coma, recounting semi-conscious ramblings that were so realistic they remained imprinted on the memory. This example clearly demonstrates a distinct personal narrative genre. Other visible text snippets on the page include a continuation of the ‘Cadbury sugar-free chocolate’ recipe and instructions for ‘Put oil in saucepan’, reinforcing the co-occurrence of diverse content types.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels-for-classification",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels-for-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Defining Genre Labels for Classification",
    "text": "5.9 Defining Genre Labels for Classification\n\n\n\nSlide 11\n\n\nResearchers meticulously defined the genre labels employed in this project under the direct supervision of the main historian, an expert in patient organisations. This interdisciplinary approach ensured that the genre categories possessed both historical relevance and practical utility. The labels proved instrumental in segmenting the diverse content within the materials, thereby facilitating more focused and granular historical analysis. Furthermore, the design principle prioritised general applicability, aiming for labels that could serve broad analytical purposes beyond the immediate dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#operational-genre-definitions",
    "href": "chapter_ai-nepi_005.html#operational-genre-definitions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Operational Genre Definitions",
    "text": "5.10 Operational Genre Definitions\n\n\n\nSlide 12\n\n\nFor the purpose of genre classification, researchers established nine distinct categories, each with a precise operational definition. ‘Academic’ texts encompass research-based reports or explanations of scientific ideas, such as research articles or formal reports. ‘Administrative’ documents pertain to organisational activities, including meeting minutes, financial reports, or announcements. ‘Advertisements’ specifically promote products or services for commercial gain. ‘Guides’ offer step-by-step instructions, exemplified by health tips, legal advice, or recipes. ‘Fiction’ aims to entertain and emotionally engage, comprising stories, poems, humour, or myths. ‘Legal’ texts explain terms and conditions, such as contracts or amendments. ‘News’ reports recent events and developments. ‘Nonfiction Prose’ narrates real events or describes cultural and historical topics, encompassing memoirs, essays, or documentary prose. Finally, ‘QA’ (Question and Answer) refers to texts structured as questions followed by expert answers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-annotation-methodology",
    "href": "chapter_ai-nepi_005.html#dataset-annotation-methodology",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Dataset Annotation Methodology",
    "text": "5.11 Dataset Annotation Methodology\n\n\n\nSlide 13\n\n\nResearchers meticulously annotated the dataset to establish ground truth for genre classification. The annotation unit was defined at the paragraph level, specifically ABBYY-generated paragraphs merged based on consistent font patterns, including type, size, bolding, and italics, within a single page. The annotation effort focused on two key periodicals: the Swedish ‘Diabetes’ and the German ‘Diabetiker Journal’, specifically their first and mid-year issues for each year. A multidisciplinary team comprising four historians and two computational linguists, all native or proficient in Swedish and German, performed the annotations. To ensure robustness and reliability, two independent annotations were collected for each paragraph. This rigorous process yielded a remarkably high average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha, indicating strong consistency across annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-annotation-example",
    "href": "chapter_ai-nepi_005.html#illustrative-annotation-example",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Illustrative Annotation Example",
    "text": "5.12 Illustrative Annotation Example\n\n\n\nSlide 14\n\n\nAn illustrative example from ‘Der Diabetiker’ vividly demonstrates the annotation process. Researchers organised the annotation data within a table format, featuring columns for ‘Year’, ‘Volume’, ‘Issue_Nr’, ‘Title’, and ‘Paragraph’, alongside binary indicators for each genre label: ‘academic’, ‘administrative’, ‘advertisement’, ‘fiction’, ‘guide’, ‘nf_prose’, ‘legal’, ‘QA’, and ‘news’. For instance, a paragraph from 1958, titled ‘THE ISLAND (Diabetes was not an obstacle)’, received a ‘1’ under ‘nf_prose’, whilst all other genre categories were marked ‘0’. Another paragraph from the same source similarly received a ‘1’ for ‘nf_prose’. Annotators utilised a .numbers file for this task. Although the original sentences were in German, they were translated using Google Translate for presentation purposes, ensuring clarity of the annotation assignments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-for-experiments",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-for-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Dataset Splitting for Experiments",
    "text": "5.13 Dataset Splitting for Experiments\n\n\n\nSlide 15\n\n\nResearchers meticulously partitioned the ActDisease annotated dataset to facilitate various experimental setups. The main split comprised a training set of 1182 paragraphs and a held-out set of 552 paragraphs, constituting approximately 30% of the total data, with stratification applied by genre label to maintain distribution. For few-shot experiments, six distinct training set sizes—100, 200, 300, 400, 500, and the full 1182 paragraphs—were randomly sampled from the main training set, ensuring label balance within each subset. The held-out set was subsequently divided into equally sized validation and test sets, also balanced by label. Notably, the ‘legal’ and ‘news’ genres were excluded from these validation and test sets due to insufficient training data. For zero-shot experiments, the entire test set was utilised, allowing for evaluation without direct training on the ActDisease data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-distribution-in-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#genre-distribution-in-actdisease-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Genre Distribution in ActDisease Dataset",
    "text": "5.14 Genre Distribution in ActDisease Dataset\n\n\n\nSlide 16\n\n\nThe distribution of genres within the ActDisease dataset, across both training and held-out samples, is visually represented through two distinct bar charts. The left chart illustrates the number of training paragraphs categorised by language and genre, whilst the right chart presents the corresponding distribution for the held-out sample. Both charts employ a y-axis indicating the number of instances and an x-axis denoting the language, specifically German and Swedish. A comprehensive legend maps colours to each of the nine genre labels: ‘QA’, ‘academic’, ‘administrative’, ‘advertisement’, ‘fiction’, ‘guide’, ‘legal’, ‘news’, and ‘non_fiction_prose’. These visualisations reveal varying frequencies; for instance, ‘advertisement’ appears as a dominant genre within the German training set, whilst ‘guide’ shows prominence in the Swedish held-out set, highlighting the heterogeneous nature of the dataset’s composition.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-classification",
    "href": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 External Datasets for Zero-Shot Classification",
    "text": "5.15 External Datasets for Zero-Shot Classification\n\n\n\nSlide 17\n\n\nFor zero-shot classification experiments, researchers leveraged several publicly available external datasets. Two primary Automatic Genre Identification datasets, annotated at the document level, included the Corpus of Online Registers of English (CORE), developed by Egbert et al. (2015), which primarily features English texts but also includes main categories in Swedish, Finnish, and French. The Functional Text Dimensions (FTD) dataset of web genres, introduced by Sharoff (2018), provides a balanced collection of English and Russian texts, having previously served in automatic web genre classification, as noted by Kuzman et al. (2023). Additionally, the UD-MULTIGENRE (UDM) dataset, a subset of Universal Dependencies (de Marneffe et al., 2021), offered sentence-level genre annotations across 38 languages, with recovered annotations by Danilova and Stymne (2023). These diverse datasets provided crucial external knowledge for cross-dataset genre classification.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Cross-Dataset Genre Mapping",
    "text": "5.16 Cross-Dataset Genre Mapping\n\n\n\nSlide 18\n\n\nTo enable zero-shot learning across diverse datasets, researchers meticulously mapped genre labels between the ActDisease project’s categories and those found in CORE, UDM, and FTD. For instance, ‘Academic’ in ActDisease corresponds to ‘research article (RA)’ in CORE, ‘academic’ in UDM, and ‘academic (A14)’ in FTD. ‘Administrative’ uniquely maps to ‘parliament’ in UDM, lacking direct equivalents in CORE or FTD. ‘Advertisement’ aligns with ‘advertisement (AD)’ and ‘description with intent to sell (DS)’ in CORE, and ‘commercial (A12)’ in FTD. ‘Guide’ finds multiple mappings in CORE, including ‘how-to (HT)’ and ‘recipe (RE)’, whilst directly mapping to ‘guide’ in UDM and ‘instruct (A7)’ in FTD. ‘Fiction’ is represented by ‘poem (PO)’ and ‘short story (SS)’ in CORE, ‘fiction’ in UDM, and ‘fictive (A4)’ and ‘poetic (A19)’ in FTD. ‘Legal’ maps to ‘legal terms and conditions (LT)’ in CORE, ‘legal’ in UDM, and ‘legal (A9)’ in FTD. ‘News’ corresponds to ‘news report/blog (NE)’ in CORE, ‘news’ in UDM, and ‘reporting (A8)’ in FTD. ‘Nonfiction’ encompasses a broad range, including various blog types and articles in CORE, ‘nonfiction prose’ and ‘blog’ in UDM, and ‘personal (A11)’ and ‘argumentative (A1)’ in FTD. Finally, ‘QA’ maps to ‘question/answer forum (QA)’ and ‘advice (AV)’ in CORE, and ‘QA’ in UDM, with no direct FTD equivalent. This comprehensive mapping ensures comparability across the disparate datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-creation-and-configuration",
    "href": "chapter_ai-nepi_005.html#training-data-creation-and-configuration",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Training Data Creation and Configuration",
    "text": "5.17 Training Data Creation and Configuration\n\n\n\nSlide 19\n\n\nResearchers meticulously crafted the training data through a multi-stage pipeline, beginning with a ‘Genre Map’ and subsequent ‘Mapping’ process that populated a ‘DataFrame’ designated ‘new genre mapping’. This DataFrame then underwent a crucial ‘Pre-process’ step, which systematically removed web addresses, emails, XML-tags, and emojis to clean the data. Following this, the data entered a ‘SAMPLING’ stage. The process integrated three external datasets—CORE, FTD, and UDM—as inputs to the initial mapping. Four distinct configuration options guided the data preparation: ‘[G+]’ restricted the dataset to only Germanic languages, whilst ‘[G-]’ included all language families. Concurrently, ‘[B1]’ implemented balancing based on the ActDisease project’s defined labels, and ‘[B2]’ combined balancing by ActDisease labels with the original labels from the source datasets. These language scope and balancing methods were combined, as indicated by connections between [G+] and [G-] with [B1] and [B2]. Visual representations of the sampled data, including four FTD training samples, four CORE training samples (specifically G+ B1, G+ B2, G- B1, G- B2), and UDM training samples, illustrate the comprehensive nature of this data preparation pipeline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models-utilised",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models-utilised",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Multilingual Encoder Models Utilised",
    "text": "5.18 Multilingual Encoder Models Utilised\n\n\n\nSlide 20\n\n\nResearchers employed a selection of multilingual encoder models for the genre classification task. The primary models included XLM-Roberta, introduced by Conneau et al. (2020), mBERT, developed by Devlin et al. (2019), and a specialised historical mBERT, presented by Schweter et al. (2022). BERT-like models, such as these, have seen extensive application in prior work for web register and genre classification, as evidenced by studies from Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). XLM-RoBERTa stands out as a state-of-the-art web genre classifier, according to Kuzman et al. (2023). The historical mBERT model, specifically, benefits from pretraining on a substantial corpus of multilingual historical newspapers, rendering it particularly relevant for this project. Conversely, mBERT served as a comparative baseline against hmBERT, given their non-direct comparability with XLM-Roberta.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#model-fine-tuning-process",
    "href": "chapter_ai-nepi_005.html#model-fine-tuning-process",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Model Fine-tuning Process",
    "text": "5.19 Model Fine-tuning Process\n\n\n\nSlide 21\n\n\nResearchers systematically fine-tuned the selected models through a structured process. Each training set was prepared in four distinct configurations, providing varied data inputs. These datasets included FTD (Functional Text Dimensions), CORE (Corpus of Online Registers of English), UDM (UD-MULTIGENRE), and a combined Merged dataset. A central ‘finetuning’ step applied uniformly across all three chosen models: XLM-Roberta, mBERT, and hmBERT. This comprehensive experimental setup ultimately yielded a total of 48 fine-tuned models, representing the permutations of dataset configurations and model types, ready for subsequent evaluation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-phase",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-phase",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.20 Zero-Shot Learning Evaluation Phase",
    "text": "5.20 Zero-Shot Learning Evaluation Phase\n\n\n\nSlide 22\n\n\nThis section initiates the evaluation phase for the zero-shot learning experiments, presenting the methodologies and results derived from the classification tasks performed without direct training on the target dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-prediction-evaluation-methodology",
    "href": "chapter_ai-nepi_005.html#zero-shot-prediction-evaluation-methodology",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.21 Zero-Shot Prediction Evaluation Methodology",
    "text": "5.21 Zero-Shot Prediction Evaluation Methodology\n\n\n\nSlide 23\n\n\nEvaluating zero-shot predictions presented a significant challenge due to the imperfect overlap of label sets across different datasets, which precluded direct comparison of overall performance metrics. Consequently, researchers adopted a granular approach, assessing the performance of each genre independently and meticulously analysing confusion matrices. The X-GENRE web genre classifier, as described by Kuzman et al. (2023), served as a crucial baseline. The evaluation specifically focused on predictions made on labels most similar to and directly mappable to the ActDisease categories. Furthermore, the experiments inherently involved cross-lingual scenarios: the FTD and X-GENRE datasets were entirely cross-lingual, lacking German or Swedish training data, whilst the UDM and CORE datasets exhibited partial cross-lingual characteristics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#summary-of-zero-shot-classification-results",
    "href": "chapter_ai-nepi_005.html#summary-of-zero-shot-classification-results",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.22 Summary of Zero-Shot Classification Results",
    "text": "5.22 Summary of Zero-Shot Classification Results\n\n\n\nSlide 24\n\n\nThe zero-shot classification experiments yielded several key findings. Models fine-tuned on the FTD dataset demonstrated superior performance when employing the ActDisease genre mapping. Conversely, other datasets exhibited distinct class-specific biases. In the UDM dataset, a bias towards ‘news’ emerged, primarily because the news training data contained the highest number of Germanic instances, predominantly German. For the CORE dataset, a bias towards ‘guide’ was observed, as only the guide training data was truly multilingual. Intriguingly, XLM-Roberta, when trained on UDM, achieved an average of 32% more correct predictions in the ‘QA’ genre compared to mBERT and hmBERT. Conversely, hmBERT, also on UDM, showed an average of 16% more correct predictions in ‘Administrative’ texts than XLM-Roberta and mBERT. Furthermore, models trained on CORE consistently performed well in predicting the ‘legal’ genre.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-classification-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#zero-shot-classification-confusion-matrices",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.23 Zero-Shot Classification Confusion Matrices",
    "text": "5.23 Zero-Shot Classification Confusion Matrices\n\n\n\nSlide 25\n\n\nFour distinct confusion matrices visually represent the performance of various models and dataset configurations in the zero-shot classification task. Each matrix plots ‘True Genre’ against ‘Predicted Genre’, with a colour scale indicating the count of instances. The genres consistently include ‘QA’, ‘academic’, ‘administrative’, ‘advertisement’, ‘fiction’, ‘guide’, ‘legal’, ‘news’, and ‘nf_prose’. For instance, the ‘hmBERT_UDM_True_True’ matrix shows 43 instances of ‘academic’ and 31 instances of ‘guide’ were correctly classified, whilst 3 instances of ‘news’ were incorrectly predicted as ‘administrative’. The ‘xlmr_CORE_True_False’ matrix highlights 85 correct ‘guide’ predictions, but also 30 ‘academic’ instances misclassified as ‘administrative’. In the ‘xlmr_UDM_False_False’ matrix, 35 ‘administrative’ instances were correctly predicted, yet 8 ‘QA’ instances were misclassified as ‘administrative’. Finally, the ‘xlmr_FTD_False_False’ matrix demonstrates 108 correct ‘advertisement’ predictions, alongside 30 ‘news’ instances incorrectly classified as ‘administrative’. These matrices provide granular insights into the models’ strengths and specific misclassification patterns across different datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-per-category-f1-scores",
    "href": "chapter_ai-nepi_005.html#zero-shot-per-category-f1-scores",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.24 Zero-Shot Per-Category F1 Scores",
    "text": "5.24 Zero-Shot Per-Category F1 Scores\n\n\n\nSlide 26\n\n\nA comprehensive table presents the F1 scores for various genres and models across four distinct dataset configurations: FTD, CORE, UDM, and a merged dataset. The data is organised by dataset, then by model, including X-GENRE, hmBERT, mBERT, and XLM-RoBERTa, with columns representing each genre category. Notably, for the FTD dataset, XLM-RoBERTa consistently demonstrated strong performance, achieving an F1 score of 0.89 for ‘guide’, 0.82 for ‘legal’, and 0.74 for ‘advertisement’. Within the CORE dataset, both hmBERT and XLM-RoBERTa exhibited high F1 scores for ‘legal’, reaching 0.80 and 0.84 respectively. For the UDM dataset, XLM-RoBERTa secured the highest F1 for ‘QA’ at 0.53, whilst hmBERT led for ‘administrative’ at 0.43. Across the board, the ‘merged’ configuration generally yielded lower F1 scores for all models and genres when compared to individual dataset configurations. Bolded values within the table highlight the peak F1 score for each specific genre within its respective dataset configuration, providing clear quantitative insights into model performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#impact-of-configuration-on-zero-shot-performance",
    "href": "chapter_ai-nepi_005.html#impact-of-configuration-on-zero-shot-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.25 Impact of Configuration on Zero-Shot Performance",
    "text": "5.25 Impact of Configuration on Zero-Shot Performance\n\n\n\nSlide 27\n\n\nAnalysis of the average F1 performance across different configurations reveals distinct impacts on zero-shot classification. For the FTD dataset, configurations [B 1] and [G -] yielded the highest F1 scores of 0.56, whilst [B 2] and [G +] resulted in a decrease in performance. In the CORE dataset, F1 scores remained consistently around 0.32 across most configurations, with a marginal decrease to 0.31 for [G -]; the presence of a small number of Finnish and French instances, particularly within the ‘guide’ genre, marginally reduced overall performance. Conversely, for the UDM dataset, the presence of other language families and balancing strategies generally improved macro F1 performance, with [G -] achieving the highest score of 0.22 and [B 2] also performing strongly at 0.19. These findings underscore how data balancing and language family inclusion critically influence model efficacy in zero-shot settings.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-evaluation-phase",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-evaluation-phase",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.26 Few-Shot Learning Evaluation Phase",
    "text": "5.26 Few-Shot Learning Evaluation Phase\n\n\n\nSlide 28\n\n\nThis section marks the commencement of the evaluation phase for few-shot learning experiments, detailing the performance and insights derived from models trained on limited data samples.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-trends",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-trends",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.27 Few-Shot Learning Performance Trends",
    "text": "5.27 Few-Shot Learning Performance Trends\n\n\n\nSlide 29\n\n\nA line graph visually depicts the performance of various models in a few-shot setting, illustrating the impact of Masked Language Model (MLM) fine-tuning. The x-axis represents dataset size, ranging from 200 to 1200 instances, whilst the y-axis indicates the F1 score, spanning from 0.2 to 0.8. Six distinct lines trace the performance of ‘hmbert’, ‘hmbert-mlm’, ‘mbert’, ‘mbert-mlm’, ‘xlmr’, and ‘xlmr-mlm’. Consistently, all models demonstrate an increasing trend in F1 score as the dataset size expands, underscoring the benefit of more training data. Crucially, further training on the ActDisease dataset, particularly with MLM fine-tuning, proves distinctly advantageous. Whilst F1 scores continue to rise with an increasing number of training instances, they remain below 0.8 even at the largest size of 1182. Notably, the hmBERT-MLM model consistently outperforms its counterparts, achieving the highest F1 scores.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#detailed-few-shot-learning-performance-metrics",
    "href": "chapter_ai-nepi_005.html#detailed-few-shot-learning-performance-metrics",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.28 Detailed Few-Shot Learning Performance Metrics",
    "text": "5.28 Detailed Few-Shot Learning Performance Metrics\n\n\n\nSlide 30\n\n\nA comprehensive table details the performance of six distinct models—XLMR, XLMR-MLM, hmBERT, hmBERT-MLM, mBERT, and mBERT-MLM—across two training sizes: 500 and 1182 instances. The table presents F1 scores for individual genre categories, including ‘QA’, ‘academic’, ‘administrative’, ‘advertisement’, ‘fiction’, ‘guide’, and ‘nf_prose’, alongside overall metrics such as ‘accuracy’ and ‘macro_F1’. For ‘QA’, XLMR-MLM at 1182 instances achieved the highest F1 of 0.84. Both XLMR-MLM and mBERT-MLM reached an F1 of 0.81 for ‘academic’ at the largest training size. In ‘administrative’ classification, hmBERT-MLM and mBERT-MLM both attained an F1 of 0.86. XLMR-MLM demonstrated exceptional performance for ‘advertisement’, securing an F1 of 0.93. For ‘fiction’, mBERT-MLM achieved 0.62, whilst hmBERT-MLM reached 0.51. XLMR-MLM led in ‘guide’ classification with 0.79, and hmBERT-MLM achieved 0.82 for ‘nf_prose’. Overall accuracy ranged from 0.63 to 0.81, whilst macro_F1 scores spanned 0.61 to 0.77, with hmBERT-MLM at 1182 instances achieving the highest macro_F1 of 0.77. Shaded cells and bolded values visually highlight superior performance within each category and size, underscoring the benefits of MLM fine-tuning and increased training data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-classification-confusion-matrix-and-insights",
    "href": "chapter_ai-nepi_005.html#few-shot-classification-confusion-matrix-and-insights",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.29 Few-Shot Classification Confusion Matrix and Insights",
    "text": "5.29 Few-Shot Classification Confusion Matrix and Insights\n\n\n\nSlide 31\n\n\nA confusion matrix, specifically illustrating the XLM-Roberta-MLM classification results with a full-sized training dataset, provides granular insights into few-shot performance. The matrix plots ‘True Genre’ against ‘Predicted Genre’, encompassing ‘administrative’, ‘academic’, ‘fiction’, ‘nf_prose’, ‘QA’, ‘guide’, and ‘advertisement’ genres, with a colour scale indicating instance counts up to 120. For example, the model accurately predicted 59 ‘administrative’, 96 ‘nf_prose’, and 120 ‘advertisement’ instances. Qualitative observations derived from this analysis highlight several critical points. Researchers suggest that ‘fiction’ and ‘nonfictional prose’ may exhibit increasing similarity, posing a classification challenge. This phenomenon likely stems from the domain-specific nature of the dataset, as all genres are confined to patient organisation magazines primarily focused on diabetes. Consequently, both fictional and (auto)biographical texts frequently revolve around the experiences of diabetes patients, leading them to share common themes and narrative structures, which can blur genre boundaries.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation-phase",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation-phase",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.30 Few-Shot Prompting Evaluation Phase",
    "text": "5.30 Few-Shot Prompting Evaluation Phase\n\n\n\nSlide 32\n\n\nThis section introduces the evaluation phase dedicated to few-shot prompting experiments, focusing on the performance of large language models when guided by specific instructions and limited examples.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-instruct-model-prompting",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-instruct-model-prompting",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.31 Llama-3.1 8b Instruct Model Prompting",
    "text": "5.31 Llama-3.1 8b Instruct Model Prompting\n\n\n\nSlide 33\n\n\nResearchers employed the Llama-3.1 8b Instruct model for few-shot prompting, providing it with a precise instruction: ‘Label the text with one of the following genres based on their purpose and content:’. This instruction was accompanied by explicit definitions for each genre. ‘Academic’ encompassed research-based articles and popular science. ‘Administrative’ included documents like meeting minutes and financial reports. ‘Advertisement’ covered promotions and invitations. ‘Guide’ provided dietary advice, exercise instructions, or recipes. ‘Fiction’ comprised poems, short stories, and humour. ‘Legal’ referred to contracts and terms. ‘News’ denoted daily reports. ‘Nonfictional_prose’ covered autobiographies, memoirs, and essays. ‘QA’ described question-answer formatted texts. An ‘Examples:’ section further guided the model, whilst the input format was specified as ‘[INST] test[i]“text” [/INST]’, with an expected ‘Genre:’ output. This detailed prompt engineering aimed to optimise the model’s genre classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-instruct-performance",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-instruct-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.32 Llama-3.1 8b Instruct Performance",
    "text": "5.32 Llama-3.1 8b Instruct Performance\n\n\n\nSlide 34\n\n\nThe Llama-3.1 8b Instruct model demonstrated varied performance across genres in the few-shot prompting setup. Its F1-scores included 0.62 for ‘QA’, 0.72 for ‘academic’, 0.60 for ‘administrative’, 0.73 for ‘advertisement’, 0.64 for ‘fiction’, and 0.61 for ‘guide’. Notably, the model achieved its highest F1 score of 0.84 for ‘legal’ texts. Conversely, its performance on ‘news’ was exceptionally low, yielding an F1 of merely 0.08, whilst ‘nonfiction_prose’ scored 0.49. Overall metrics indicated an accuracy of 0.62, a macro average F1 of 0.59, and a weighted average F1 of 0.63. A visual confusion matrix, depicting few-shot prediction performance on the entire held-out set (used as a zero-shot test set), further illuminated these results. It showed successful classification for many genres, with 18 ‘QA’, 54 ‘academic’, 58 ‘administrative’, 77 ‘advertisement’, 30 ‘fiction’, 28 ‘guide’, 27 ‘legal’, and 49 ‘nf_prose’ instances correctly predicted. However, the matrix starkly highlighted the model’s struggle with ‘news’, correctly predicting only 1 instance, whilst misclassifying 8 instances as ‘nf_prose’, underscoring a significant area for improvement.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#challenges-of-popular-magazine-text-mining",
    "href": "chapter_ai-nepi_005.html#challenges-of-popular-magazine-text-mining",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.33 Challenges of Popular Magazine Text Mining",
    "text": "5.33 Challenges of Popular Magazine Text Mining\n\n\n\nSlide 35\n\n\nA primary conclusion drawn from this research highlights the inherent complexity of text mining popular magazines. Unlike more homogeneous scientific journals and books, these periodicals frequently contain a multitude of genres. This textual diversity significantly increases the challenge of extracting meaningful information and patterns through automated text mining techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#enabling-text-mining-through-genre-classification",
    "href": "chapter_ai-nepi_005.html#enabling-text-mining-through-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.34 Enabling Text Mining through Genre Classification",
    "text": "5.34 Enabling Text Mining through Genre Classification\n\n\n\nSlide 36\n\n\nWhilst popular magazines inherently present a multitude of genres, rendering text mining more challenging than for scientific journals and books, a crucial finding emerges: genre classification offers a viable solution. By systematically categorising the diverse content, genre classification effectively renders these complex historical sources accessible for advanced text mining methodologies, thereby unlocking their rich informational potential.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#key-conclusions-and-performance-insights",
    "href": "chapter_ai-nepi_005.html#key-conclusions-and-performance-insights",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.35 Key Conclusions and Performance Insights",
    "text": "5.35 Key Conclusions and Performance Insights\n\n\n\nSlide 37\n\n\nThe research yields five pivotal conclusions regarding genre classification in historical medical periodicals. Firstly, popular magazines, unlike more uniform scientific publications, contain a multitude of genres, which inherently complicates text mining efforts. Secondly, the implementation of genre classification effectively addresses this challenge, rendering these rich historical sources accessible for advanced text mining. Thirdly, even in the absence of specific training data, researchers successfully demonstrated the feasibility of leveraging existing modern datasets for effective classification. Fourthly, open generative models proved capable of achieving a decent quality of classification. Finally, and most significantly, few-shot learning applied to multilingual encoders, particularly when combined with prior Masked Language Model (MLM) fine-tuning, exhibited superior performance. This approach yielded particularly strong gains for the historical multilingual BERT model, achieving a 24% improvement, notably surpassing the 14.5% gain for mBERT-MLM and 16.9% for XLM-RoBERTa.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#future-research-directions",
    "href": "chapter_ai-nepi_005.html#future-research-directions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.36 Future Research Directions",
    "text": "5.36 Future Research Directions\n\n\n\nSlide 38\n\n\nFuture research within the ActDisease project will pursue several promising directions. Researchers plan to engage more deeply with specific historical hypotheses, leveraging the genre classification framework to address nuanced questions. A key methodological development involves crafting a new annotation scheme designed to capture more fine-grained genre distinctions. To support this, an annotation project, financed by Swe-CLARIN, is currently underway. Furthermore, the team intends to explore advanced data augmentation techniques, specifically through synthetic data generation, to expand the training corpus. Finally, the implementation of active learning strategies will optimise the annotation process, ensuring efficient and targeted data collection for continuous model improvement.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  The VERITRACE Project",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant (2023-2028), operates from the Vrije Universiteit Brussel (VUB) under the distinguished leadership of Research Professor Cornelis J. Schilt. This ambitious initiative meticulously investigates the profound influence of early modern ‘ancient wisdom’ traditions upon natural philosophy. Professor Schilt and his team aim to trace the enduring impact of seminal texts such as the Chaldean Oracles, Sibylline Oracles, Orphic Hymns, and the Corpus Hermeticum on intellectual giants like Isaac Newton and Johannes Kepler. Concurrently, they endeavour to uncover a broader, frequently overlooked network of related works, aptly termed the ‘great Unread’.\nThe VERITRACE team employs sophisticated computational methods to facilitate large-scale multilingual exploration. Their approach identifies textual reuse, discerning both direct lexical overlaps and subtle semantic similarities, whilst also revealing hidden networks of texts, passages, themes, topics, and authors. This rigorous investigation further seeks to illuminate novel patterns within the intellectual history and philosophy of science. Utilising a diverse multilingual dataset of approximately 430,000 printed texts from 1540 to 1728, meticulously sourced from Early English Books Online (EEBO), Gallica, and the Bavarian State Library, the team applies state-of-the-art digital techniques. These include advanced keyword search, precise text matching, nuanced topic modelling, and insightful sentiment analysis.\nCore challenges for the project encompass the variable quality of Optical Character Recognition (OCR), the complexities of early modern typography and semantics across at least six languages, and the sheer volume of data. To address these hurdles, the team leverages Large Language Models (LLMs) for both metadata enrichment, employing GPT-based LLMs as discerning ‘judges’, and semantic encoding, utilising BERT-based LLMs for robust vector embeddings. A newly developed web application, currently in its alpha version, provides essential functionalities for corpus exploration, advanced search, and text matching. Future plans for this application include the integration of powerful analytical tools such as topic modelling and diachronic analysis. Comprising five members, including Professor Schilt as PI, a classicist, and historians, the VERITRACE team adeptly navigates significant computational and methodological complexities in its pursuit of comprehensive historical analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-overview-and-core-objectives",
    "href": "chapter_ai-nepi_006.html#project-overview-and-core-objectives",
    "title": "6  The VERITRACE Project",
    "section": "6.1 Project Overview and Core Objectives",
    "text": "6.1 Project Overview and Core Objectives\n\n\n\nSlide 03\n\n\nThe VERITRACE project, a five-year ERC Starting Grant awarded from 2023 to 2028, operates from the Vrije Universiteit Brussel (VUB) under the leadership of Research Professor Cornelis J. Schilt. This initiative comprises a five-member team, including a classicist, historians, and a digital humanities specialist, who collectively aim to trace the profound influence of an early modern ‘ancient wisdom’ tradition on the evolution of natural philosophy and science.\nProfessor Schilt and his colleagues specifically focus on a close-reading corpus of 140 works, encompassing seminal texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence confirms the significant impact of these works; for instance, Isaac Newton engaged with the Sibylline Oracles, whilst Johannes Kepler demonstrated familiarity with the Corpus Hermeticum. Beyond these well-known connections, the project endeavours to uncover a much broader, often overlooked network of texts and intellectual relationships within this tradition, collectively termed the ‘great Unread’. These works, frequently authored by lesser-known figures, typically remain outside the primary focus of historical scholarship, yet they offer crucial insights into the intellectual landscape of the early modern period.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-to-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_006.html#computational-approaches-to-history-philosophy-and-sociology-of-science-hpss",
    "title": "6  The VERITRACE Project",
    "section": "6.2 Computational Approaches to History, Philosophy, and Sociology of Science (HPSS)",
    "text": "6.2 Computational Approaches to History, Philosophy, and Sociology of Science (HPSS)\n\n\n\nSlide 04\n\n\nThe VERITRACE project fundamentally aims to advance the field of History, Philosophy, and Sociology of Science (HPSS) through the application of sophisticated computational methodologies. Professor Schilt and his team are developing tools for large-scale multilingual exploration, primarily through advanced keyword search capabilities. Crucially, the initiative seeks to identify textual re-use within an extensive, multilingual corpus, distinguishing between direct lexical overlaps and more subtle indirect semantic similarities. This capability effectively functions as an ‘Early Modern Plagiarism Detector’, enabling the detection of unacknowledged textual appropriation.\nFurthermore, the project strives to uncover previously ignored networks of texts, passages, themes, topics, and authors, thereby illuminating hidden intellectual connections. Ultimately, these computational investigations are poised to reveal novel patterns and insights into the intellectual history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-set-characteristics-and-analytical-techniques",
    "href": "chapter_ai-nepi_006.html#data-set-characteristics-and-analytical-techniques",
    "title": "6  The VERITRACE Project",
    "section": "6.3 Data Set Characteristics and Analytical Techniques",
    "text": "6.3 Data Set Characteristics and Analytical Techniques\n\n\n\nSlide 05\n\n\nTo achieve its ambitious objectives, the VERITRACE team has assembled a substantial and diverse multilingual dataset. They focus exclusively on printed works from approximately 1540 to 1728, concluding shortly after Isaac Newton’s death. This extensive corpus comprises around 430,000 texts in at least six different languages.\nThe researchers meticulously gathered these digital texts from three primary sources: Early English Books Online (EEBO), Gallica (the digital library of the French National Library), and the Bavarian State Library, which constitutes the largest single contributor to the dataset. Leveraging this rich collection, the team applies state-of-the-art digital techniques, including advanced keyword search functionalities, sophisticated text matching algorithms, topic modelling for thematic discovery, and sentiment analysis to discern emotional tones, amongst other analytical methods.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-initial-llm-applications",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-initial-llm-applications",
    "title": "6  The VERITRACE Project",
    "section": "6.4 Core Challenges and Initial LLM Applications",
    "text": "6.4 Core Challenges and Initial LLM Applications\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several formidable challenges inherent in processing historical texts at scale. Firstly, the team contends with highly variable Optical Character Recognition (OCR) quality, as libraries provide texts in raw formats—including XML, HOCR, and HTML files—without corresponding ground truth page images. This initial data quality significantly impacts all subsequent processing stages.\nSecondly, the project navigates the complexities of early modern typography and the evolving semantics of at least six distinct languages, presenting considerable linguistic hurdles. Thirdly, the sheer volume of data—hundreds of thousands of texts printed across Europe over approximately two centuries—necessitates robust computational strategies. To address these challenges, the team currently employs Large Language Models (LLMs) in two principal capacities. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, effectively acting as ‘judges’ to refine bibliographic information. Concurrently, on the encoder side, BERT-based LLMs generate vector embeddings, encoding the semantic meaning of sentences and short passages within the textual corpus to facilitate precise text matching.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment-motivation-and-challenges",
    "href": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment-motivation-and-challenges",
    "title": "6  The VERITRACE Project",
    "section": "6.5 LLMs as Judges for Metadata Enrichment: Motivation and Challenges",
    "text": "6.5 LLMs as Judges for Metadata Enrichment: Motivation and Challenges\n\n\n\nSlide 07\n\n\nA fundamental motivation for the VERITRACE project involves leveraging the Universal Short Title Catalogue (USTC) as a high-quality source for metadata enrichment. The primary objective is to map VERITRACE’s internal records onto USTC’s comprehensive entries, thereby generating ‘enriched’ metadata that demands significantly less manual cleaning.\nHowever, this process presents a considerable challenge: whilst some mapping can be automated using existing external identifiers, the vast majority of VERITRACE records currently lack such identifiers and remain uncleaned. Consequently, the project faces the complex task of matching these records at scale, a process that proves exceedingly tedious for human review. The Universal Short Title Catalogue, accessible at https://www.ustc.ac.uk, serves as the authoritative reference for this enrichment endeavour.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#automating-bibliographic-record-matching",
    "href": "chapter_ai-nepi_006.html#automating-bibliographic-record-matching",
    "title": "6  The VERITRACE Project",
    "section": "6.6 Automating Bibliographic Record Matching",
    "text": "6.6 Automating Bibliographic Record Matching\n\n\n\nSlide 08\n\n\nThe VERITRACE team seeks to automate the arduous task of comparing bibliographic metadata pairs to ascertain whether they represent the same underlying printed text. Previously, each team member undertook the extremely tedious manual review of 10,000 such pairs. To mitigate this, the researchers initially generate potential matches using a fuzzy matching algorithm, which assigns a match score to each pair.\nThe ultimate aim is for Large Language Models (LLMs) to perform these yes/no decisions at scale, critically providing detailed reasoning for each determination. However, this LLM-based approach has not yet achieved full functionality. The models frequently produce hallucinations, and whilst requesting more structured output can reduce these, it often results in generic, less helpful reasoning, indicating an ongoing challenge in balancing precision with informative responses.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llm-bench-for-match-evaluation",
    "href": "chapter_ai-nepi_006.html#llm-bench-for-match-evaluation",
    "title": "6  The VERITRACE Project",
    "section": "6.7 LLM Bench for Match Evaluation",
    "text": "6.7 LLM Bench for Match Evaluation\n\n\n\nSlide 09\n\n\nTo address the challenge of automated bibliographic record matching, the VERITRACE project proposes ‘The LLM Bench’, a sophisticated panel of Large Language Models designed to evaluate potential matches. This system employs a tiered model configuration: llama3:8b serves as the primary model, prized for its power and accuracy; qwen2:5.7b acts as a secondary model, offering architectural diversity; mixtral:8x7b functions as a tiebreaker, possessing greater power than the initial two; and llama3.3:latest is designated as an expert model, reserved exclusively for complex edge cases requiring human review.\nThe process involves feeding pairs of bibliographic records—one from a low-quality metadata source and the other from a high-quality source—through this chain of LLMs. Each model provides a judgment (match or non-match), accompanied by detailed reasoning and confidence levels. Subsequently, the team validates these LLM decisions against ground truth data, with the VERITRACE team conducting a final review to ensure accuracy and refine the system.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prompt-guidelines-and-llm-output-analysis",
    "href": "chapter_ai-nepi_006.html#prompt-guidelines-and-llm-output-analysis",
    "title": "6  The VERITRACE Project",
    "section": "6.8 Prompt Guidelines and LLM Output Analysis",
    "text": "6.8 Prompt Guidelines and LLM Output Analysis\n\n\n\nSlide 10\n\n\nThe VERITRACE team has established comprehensive prompt guidelines, termed ‘MATCHING_GUIDELINES’, to direct the LLMs in determining bibliographic record matches. These guidelines prioritise title content, whilst accounting for minor formatting differences, and emphasise author alignment, publication dates within a one-year window, and corroborating place/printer information.\nSpecific match criteria mandate identical core work in titles, precise or near-precise dates, matching or equivalent publication places, recognisable printer variations, and substantial author overlap. Conversely, non-match indicators include significantly divergent titles, dates exceeding a one-year difference, unexplained discrepancies in places or printers, and distinct authors or edition variations. An illustrative example demonstrates a ‘Ground Truth’ of ‘NON-MATCH’ yet a ‘Final Decision’ of ‘MATCH’ with high confidence (87.7%), driven by the tiebreaker model. The reasoning provided highlights factors such as title similarity, identical authors and dates, and equivalent publication places, even when language discrepancies exist. A significant ongoing challenge, however, stems from hallucinations in the output of the open-source LLMs employed. Whilst requesting more structured output mitigates these hallucinations, it often results in generic, less informative reasoning, presenting a delicate balance for the researchers to refine.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-technical-approach",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-technical-approach",
    "title": "6  The VERITRACE Project",
    "section": "6.9 VERITRACE Web Application: Alpha Version and Technical Approach",
    "text": "6.9 VERITRACE Web Application: Alpha Version and Technical Approach\n\n\n\nSlide 11\n\n\nThe VERITRACE project has developed an ‘alpha’ version of its web application, which remains in its nascent stages and is not yet publicly accessible, currently residing on the presenter’s local machine. This application represents a future promise of the project’s capabilities, serving primarily as a testing and development platform.\nThe engineers are currently evaluating a BERT-based Large Language Model, specifically LaBSE, for generating vector embeddings that represent every passage within the extensive textual corpus. However, initial assessments suggest that whilst LaBSE functions in certain scenarios, it will likely prove insufficient for the project’s comprehensive requirements. Due to its early developmental stage, the application is demonstrated through static screenshots rather than a live, interactive website.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-processing-pipeline-for-web-application",
    "href": "chapter_ai-nepi_006.html#data-processing-pipeline-for-web-application",
    "title": "6  The VERITRACE Project",
    "section": "6.10 Data Processing Pipeline for Web Application",
    "text": "6.10 Data Processing Pipeline for Web Application\n\n\n\nSlide 12\n\n\nThe VERITRACE web application relies on an Elasticsearch database as its backend, necessitating a complex and extensive data processing pipeline to transform raw text into a usable format. This pipeline involves numerous critical steps, each demanding significant optimisation.\nInitially, the system extracts text into standardised text files, subsequently generating precise mappings of all character positions. Further stages include segmenting documents into manageable units and rigorously assessing the Optical Character Recognition (OCR) quality, a particularly challenging task given the raw nature of the input. Each of these stages, whilst seemingly straightforward, requires considerable time and effort—potentially a week per stage—to optimise fully, underscoring the intricate background work underpinning the application’s functionality.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-data-processing-pipeline-details",
    "href": "chapter_ai-nepi_006.html#veritrace-data-processing-pipeline-details",
    "title": "6  The VERITRACE Project",
    "section": "6.11 VERITRACE Data Processing Pipeline Details",
    "text": "6.11 VERITRACE Data Processing Pipeline Details\n\n\n\nSlide 13\n\n\nThe VERITRACE project employs a sophisticated 15-stage data processing pipeline, currently running with 40% overall progress, indicating six completed stages. A detailed status summary reveals eight pending stages, one actively running, and no failures or skips. The pipeline’s configuration specifies parameters such as a working directory, file and error policies, input directory, dashboard port (8080), and settings for dependency verification and browser interaction.\nCompleted stages include batch processing (1.3s), character position generation (5.3s), page extraction (7.3s), language analysis (4m 35s), language map generation (0.7s), and OCR quality assessment (12.7s). The ‘segment documents’ stage is presently active, whilst subsequent stages, such as filtering segments, tracking relationships, unifying JSON, enriching MongoDB, and enriching sequences, remain pending. This complex pipeline transforms raw XML, HOCR, and HTML files into a format suitable for Elasticsearch, with vector embeddings generated towards its conclusion, each stage demanding meticulous optimisation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-explore-section-and-metadata",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-explore-section-and-metadata",
    "title": "6  The VERITRACE Project",
    "section": "6.12 VERITRACE Web Application: Explore Section and Metadata",
    "text": "6.12 VERITRACE Web Application: Explore Section and Metadata\n\n\n\nSlide 14\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Search, Match, Analyse, and Read. The ‘Explore’ section serves as a comprehensive hub for corpus statistics and metadata exploration, currently presenting 427,305 metadata records directly from a Mongo database.\nThis section offers various visualisations, including pie charts for language distribution and data sources, a bar chart detailing documents by decade, and a donut chart illustrating top publication places. Furthermore, the ‘Metadata Explorer’ enables users to browse and filter documents based on their metadata attributes. Crucially, the system performs granular language identification on every text, down to approximately 50 characters, to accurately account for multilingual content. For instance, a text might be identified as 15% Greek and 85% Latin, classifying it as substantively multilingual. The application also attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-search-functionality",
    "href": "chapter_ai-nepi_006.html#veritrace-search-functionality",
    "title": "6  The VERITRACE Project",
    "section": "6.13 VERITRACE Search Functionality",
    "text": "6.13 VERITRACE Search Functionality\n\n\n\nSlide 15\n\n\nFor most scholars, the primary point of engagement with the VERITRACE web application will be its robust search functionality. Whilst the current prototype operates on a limited corpus of 132 files, rather than the full 400,000-plus, its index already occupies 15 gigabytes, indicating that the complete online corpus will span many terabytes.\nA basic keyword search, for example, for ‘Hermes’, rapidly retrieves 22 documents with 332 total matches in 107 milliseconds, including works like ‘Hermes Trismegisti Erkännuß…’ by Hermes (1706) and ‘Hermes theologus…’ by Wodenote (1649). Conversely, a more refined, structured query such as ‘author:kepler ’hermes’’ significantly narrows the results, yielding just one document with two total matches in a mere 17 milliseconds. This specific result points to Kepler’s ‘Prodromus Dissertationum Cosmographicarum…’ from 1621, which contains the phrase ‘Hermes tuus.’. This comparison effectively demonstrates the enhanced precision attainable through the use of structured queries within the digital humanities search environment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-search-interface-and-query-specificity",
    "href": "chapter_ai-nepi_006.html#veritrace-search-interface-and-query-specificity",
    "title": "6  The VERITRACE Project",
    "section": "6.14 VERITRACE Search Interface and Query Specificity",
    "text": "6.14 VERITRACE Search Interface and Query Specificity\n\n\n\nSlide 16\n\n\nThe VERITRACE search interface consistently displays corpus statistics, indicating 132 unique files, over 16.9 million total segments, and an index size of 15.37 gigabytes for the ‘veritrace_2025_a2’ dataset. A general keyword search for ‘hermes’, for example, rapidly retrieves 22 documents with 332 total matches in 107 milliseconds, including works like ‘Hermes Trismegisti Erkännuß…’ by Hermes (1706) and ‘Hermes theologus…’ by Wodenote (1649).\nConversely, a more refined, structured query such as ‘author:kepler ’hermes’’ significantly narrows the results, yielding just one document with two total matches in a mere 17 milliseconds. This specific result points to Kepler’s ‘Prodromus Dissertationum Cosmographicarum…’ from 1621, which contains the phrase ‘Hermes tuus.’. This comparison effectively demonstrates the enhanced precision attainable through the use of structured queries within the digital humanities search environment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-analytical-capabilities-the-analyse-module",
    "href": "chapter_ai-nepi_006.html#future-analytical-capabilities-the-analyse-module",
    "title": "6  The VERITRACE Project",
    "section": "6.15 Future Analytical Capabilities: The ‘Analyse’ Module",
    "text": "6.15 Future Analytical Capabilities: The ‘Analyse’ Module\n\n\n\nSlide 17\n\n\nWhilst currently unimplemented, the ‘Analyse’ section of the VERITRACE website is slated to host a suite of advanced analytical tools. Future functionalities will include Topic Modelling, enabling users to discover prevalent themes across the entire corpus or within selected documents. Additionally, the platform will integrate Latent Semantic Analysis (LSA) for assessing document similarity and Diachronic Analysis, designed to visualise linguistic and conceptual shifts over extended historical periods. The development team actively incorporates insights from contemporary research and community feedback to inform the meticulous implementation of these sophisticated analytical capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-read-section-digital-facsimiles-and-metadata",
    "href": "chapter_ai-nepi_006.html#veritrace-read-section-digital-facsimiles-and-metadata",
    "title": "6  The VERITRACE Project",
    "section": "6.16 VERITRACE Read Section: Digital Facsimiles and Metadata",
    "text": "6.16 VERITRACE Read Section: Digital Facsimiles and Metadata\n\n\n\nSlide 18\n\n\nThe VERITRACE platform incorporates a dedicated ‘Read’ section, designed to provide scholars with access to high-quality digital facsimiles of historical texts, moving beyond raw OCR output. This section integrates a Mirador viewer, enabling users to read PDFs of every text within the corpus, much like a conventional library website.\nAlongside the visual facsimile, comprehensive metadata is readily available. For instance, a document such as ‘Mercurii Trismegisti Pymander…’ by Hermes, Trismegistus, published in 1534, is presented with extensive bibliographic details including its creator, full title, printer, publication place, and date. Furthermore, granular language information, OCR quality assessments, and detailed document statistics enhance the scholarly utility of each entry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-match-tool-textual-similarity-and-customisation",
    "href": "chapter_ai-nepi_006.html#veritrace-match-tool-textual-similarity-and-customisation",
    "title": "6  The VERITRACE Project",
    "section": "6.17 VERITRACE Match Tool: Textual Similarity and Customisation",
    "text": "6.17 VERITRACE Match Tool: Textual Similarity and Customisation\n\n\n\nSlide 19\n\n\nThe VERITRACE Match Tool is meticulously designed to identify textual reuse and similarities between documents, primarily leveraging vector embeddings. This versatile tool supports various comparison modes: users can compare a single document against another, conduct multi-document comparisons (e.g., Newton’s Latin Opticks against all of Kepler’s works), or even attempt a full corpus comparison, though the latter presents significant computational challenges regarding processing power and user waiting times.\nCrucially, the tool offers extensive customisation through a multitude of parameters, allowing users to fine-tune the matching algorithms. Lexical matching, for instance, includes adjustable parameters such as minimum similarity (0.65), maximum results (100), and Jaccard/BM25 weights. Semantic matching, conversely, features settings for minimum similarity (0.85), short passage thresholds, and vector normalisation. A hybrid matching option combines both approaches with adjustable weighting. Additional parameters govern corpus matching batch sizes and query settings. A key case study involves comparing Newton’s Latin and English Opticks to evaluate both lexical and semantic matches, demonstrating the tool’s capacity for cross-lingual analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-methodologies-lexical-semantic-and-hybrid",
    "href": "chapter_ai-nepi_006.html#text-matching-methodologies-lexical-semantic-and-hybrid",
    "title": "6  The VERITRACE Project",
    "section": "6.18 Text Matching Methodologies: Lexical, Semantic, and Hybrid",
    "text": "6.18 Text Matching Methodologies: Lexical, Semantic, and Hybrid\n\n\n\nSlide 20\n\n\nThe VERITRACE Match Tool employs two fundamental types of text matching: lexical and semantic, with a hybrid option combining both. Lexical matching identifies textually similar passages based on keyword overlap and vocabulary similarity. However, this approach proves ineffective across different languages, as direct lexical matches are unlikely. Consequently, for the project’s multilingual corpus, semantic matching becomes indispensable. This method utilises vector embeddings to identify conceptually similar passages, even when they share no common linguistic vocabulary, thereby enabling cross-lingual conceptual comparisons. A hybrid approach allows for the combination of both methods with adjustable weighting.\nFurthermore, the tool offers distinct matching modes: ‘Standard’ for balanced performance, ‘Comprehensive’ for maximum recall (albeit slower), and ‘Selective’ for higher precision (faster). A crucial ‘sanity check’ involves a lexical match between Newton’s Latin Opticks (1719) and its English counterpart (1718). As anticipated, the ‘Standard’ mode yields no matches, confirming the ineffectiveness of lexical matching across languages. Interestingly, the ‘Comprehensive’ mode reveals three matches, likely indicating instances of English text, such as a preface, embedded within the Latin edition.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-match-results-and-semantic-match-expectations",
    "href": "chapter_ai-nepi_006.html#lexical-match-results-and-semantic-match-expectations",
    "title": "6  The VERITRACE Project",
    "section": "6.19 Lexical Match Results and Semantic Match Expectations",
    "text": "6.19 Lexical Match Results and Semantic Match Expectations\n\n\n\nSlide 21\n\n\nThe VERITRACE system visually presents lexical match results with automatic highlighting of shared terms, displaying the source passage on the left and the comparison passage on the right, alongside a similarity score. A ‘sanity check’ involving a lexical comparison of Newton’s English Opticks (1718) against itself demonstrates the system’s precision: it yields a 100% normalised match score, a 99.7% coverage score, and a perfect 100% quality score. This comparison, involving 1,140 passages and over 1.2 million individual comparisons, completes in 23 seconds, identifying 1,137 estimated matches, all falling within the 90-100% similarity range.\nCrucially, when performing a semantic match between a text and its translation, such as the Latin and English versions of Opticks, the researchers anticipate reasonable matches. Despite significant lexical differences, the conceptual similarity between a text and its translation should be accurately captured by semantic embeddings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-match-highlighting-and-semantic-match-results",
    "href": "chapter_ai-nepi_006.html#lexical-match-highlighting-and-semantic-match-results",
    "title": "6  The VERITRACE Project",
    "section": "6.20 Lexical Match Highlighting and Semantic Match Results",
    "text": "6.20 Lexical Match Highlighting and Semantic Match Results\n\n\n\nSlide 22\n\n\nThe VERITRACE system provides automatic highlighting for lexical matches, identifying terms based on overlap and BM25 relevance ranking. This process successfully analyses 1,137 passages, skipping only three deemed too short for matching. Examples demonstrate precise 100% similarity scores for identical or near-identical passages, such as descriptions of ‘Mercury’ in various forms or discussions of chemical solutions and physical phenomena.\nWhen conducting semantic matches between the Latin and English versions of Newton’s Opticks, the results generally appear reasonable, even in the presence of OCR issues, with conceptually similar passages, such as those discussing ‘colors’, aligning effectively. However, the match score functionality requires further development; whilst the quality score remains high, the coverage score is inconsistent. This discrepancy might be partially explained by the Latin edition being considerably longer than its English counterpart. Nevertheless, broader queries suggest that the current embedding model, LaBSE, is ultimately inadequate for the project’s complex requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-matching-interface-and-progress",
    "href": "chapter_ai-nepi_006.html#semantic-matching-interface-and-progress",
    "title": "6  The VERITRACE Project",
    "section": "6.21 Semantic Matching Interface and Progress",
    "text": "6.21 Semantic Matching Interface and Progress\n\n\n\nSlide 23\n\n\nThe VERITRACE interface for semantic matching offers clear configuration options. Users select from ‘Lexical’, ‘Semantic’ (the default for conceptual comparisons), or ‘Hybrid’ match types, whilst also choosing a ‘Standard’, ‘Comprehensive’, or ‘Selective’ matching mode to balance performance and accuracy. A prominent ‘Run Text Matching’ button initiates the analysis. Results are then presented across three distinct tabs: ‘Match Summary’, ‘Match Details’, and ‘Visualization’. During the comparison process, a progress bar indicates the current status, such as ‘Comparing documents… (20%)’, alongside an instruction to run the matching to view results. This module specifically aims to evaluate translation quality through rigorous text comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-match-results-and-model-adequacy",
    "href": "chapter_ai-nepi_006.html#semantic-match-results-and-model-adequacy",
    "title": "6  The VERITRACE Project",
    "section": "6.22 Semantic Match Results and Model Adequacy",
    "text": "6.22 Semantic Match Results and Model Adequacy\n\n\n\nSlide 24\n\n\nWhen conducting semantic matches between the Latin and English versions of Newton’s Opticks, the results generally appear reasonable, even in the presence of OCR issues, with corresponding paragraphs discussing concepts like ‘Mercury’ and ‘colours’ aligning effectively. The system reports a combined match score of 58%, with a coverage score of 36.9% and a high quality score of 91.2%. Analysis of the 421 estimated matches reveals that whilst some achieve 90-100% similarity, the majority fall within the 70-90% range. The Latin query document contains 1,996 passages, compared to 1,140 in the English comparison document, and the query completes 2.2 million comparisons in 25 seconds.\nDespite these promising indicators, the project team expresses concern that the current embedding model, LaBSE, is likely inadequate for the task. This inadequacy may stem from ‘out-of-domain model collapse’, a phenomenon observed when models trained on modern languages are applied to historical, multilingual texts, leading to suboptimal performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-model-selection",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-model-selection",
    "title": "6  The VERITRACE Project",
    "section": "6.23 Future Challenges and Model Selection",
    "text": "6.23 Future Challenges and Model Selection\n\n\n\nSlide 25\n\n\nThe VERITRACE project faces several critical challenges on the horizon, particularly concerning model selection and data quality. Whilst LaBSE serves as an initial choice for vector embeddings, the researchers acknowledge the potential superiority of other models, such as XLM-Roberta, intfloat multilingual-e5-large, or historical mBERT, each presenting distinct trade-offs in terms of storage requirements and inference time. A key strategic question remains whether to fine-tune a base model specifically on the project’s unique historical corpus, given its distinct characteristics, or to persist with existing pre-trained models.\nFurthermore, the evolving nature of semantic meaning across centuries poses a significant hurdle; ensuring that texts from 1540 and 1700, written in different languages, occupy a coherent vector space presents a complex problem. Poor OCR quality, a pervasive issue, fundamentally impacts all downstream processes, including the crucial segmentation of texts into sentences and passages. Re-OCR’ing the entire corpus is not feasible, prompting considerations of re-OCR’ing only the lowest quality texts or actively seeking out existing high-quality versions. Finally, scaling and performance represent substantial future concerns; current queries on a mere 132 texts take 15 seconds, implying considerable challenges when expanding to the full 430,000-text corpus, necessitating robust solutions for managing computational resources and query times.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#additional-visual-materials",
    "href": "chapter_ai-nepi_006.html#additional-visual-materials",
    "title": "6  The VERITRACE Project",
    "section": "6.24 Additional Visual Materials",
    "text": "6.24 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n The slide is titled ‘Issues on the Horizon,’ indicating a discussion of future challenges and considerations. The background is a light beige, framed by a thick black border. In the top right corner, a decorative element features stylized plant branches with leaves, rendered in a light purple or lavender color, extending from the corner into the slide area. The content is presented as a series of bullet points addressing various technical and methodological concerns. The first point discusses model selection, stating that ‘LaBSE is just a starting choice – there are other embedding models that might work better: XLM-Roberta, intfloat multilingual-e5-large, historical mBERT, or others. All have trade-offs.’ This highlights the need to evaluate different embedding models for specific tasks. The second point poses a strategic question: ‘Or is this a losing battle and we ought to fine-tune one of these base models on our historical corpus?’ This suggests a debate between using off-the-shelf models versus domain-specific fine-tuning. The third issue addresses diachronic semantic change: ‘Semantic meaning changes over time – how to handle that across centuries?’ This points to the challenge of applying modern language models to historical texts where word meanings may have evolved. The fourth bullet focuses on data quality, specifically OCR errors: ‘Poor-quality OCR would seem to affect everything downstream. Not feasible to re-OCR our entire corpus. Re-OCR the very poor-quality texts? Invest time to find existing high-quality versions?’ This outlines the significant problem of noisy historical data and potential strategies for mitigation. The fifth point notes that ‘Scaling and performance will increasingly become an issue,’ indicating future challenges with computational resources as data volumes grow. Finally, the slide concludes with an open invitation for collaboration or guidance: ‘Advice very welcome!’",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nOliver Eberle, a postdoctoral senior researcher at the Berlin Institute for Learning and Data, presented a comprehensive exploration of Explainable AI (XAI) and its profound applications within the digital humanities. He structured his compelling presentation into two principal sections: firstly, a meticulous examination of XAI and the interpretability of Large Language Models (LLMs); secondly, a compelling demonstration of AI-based scientific insights within humanities research.\nEberle’s initial discourse focused on XAI 1.0, which primarily involved feature attributions for classification models. Researchers often visualised these attributions through heatmaps to identify pixel responsibility in image data. This approach aimed to verify predictions, identify model flaws and biases, facilitate learning about underlying problems, and ensure compliance with emerging legislation such as the European AI Act. Subsequently, Eberle shifted his focus to the complexities introduced by generative AI and multi-task foundation models, necessitating more advanced interpretability methods beyond simple heatmaps, such as feature interactions and mechanistic perspectives (XAI 2.0).\nHis empirical examples highlighted biases in Transformer LLMs’ sentiment predictions and their tendency to prioritise later context in long-range dependencies. The discussion then progressed to second and higher-order interactions, explaining sentence similarities through dot products of embeddings and employing Graph Neural Networks (GNNs) for structured predictions. Eberle and his team utilised ‘walk-based’ explanations to uncover complex language structures.\nThe latter part of the presentation showcased practical applications in the humanities, including extracting visual definitions from corpora of mathematical instruments and conducting corpus-level analyses of early modern astronomical tables, specifically the Sacrobosco Corpus. A key development Eberle presented was the XAI-Historian workflow, which he designed to aid historians in generating data-driven hypotheses at scale. This involved developing statistical models for bigram representations and utilising cluster entropy analysis to investigate the spread of innovation across historical European print locations, revealing insights into publishing programmes and political controls. Eberle concluded his presentation by acknowledging the significant challenges in applying AI to humanities data, particularly concerning heterogeneity, limited annotations, and the limitations of current foundation models for complex research questions involving low-resource or out-of-domain historical data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#introduction-to-explainable-ai-and-humanities-applications",
    "href": "chapter_ai-nepi_007.html#introduction-to-explainable-ai-and-humanities-applications",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Introduction to Explainable AI and Humanities Applications",
    "text": "7.1 Introduction to Explainable AI and Humanities Applications\n\n\n\nSlide 02\n\n\nOliver Eberle, a postdoctoral senior researcher at the Berlin Institute for Learning and Data, commenced his presentation by highlighting his background in machine learning and his recent engagement with digital humanities through collaborations with historians. He outlined the talk’s dual focus: firstly, on Explainable AI (XAI) and the interpretability of Large Language Models (LLMs); and secondly, on the application of AI to generate scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Explainable AI 1.0: Feature Attributions",
    "text": "7.2 Explainable AI 1.0: Feature Attributions\n\n\n\nSlide 03\n\n\nEberle introduced Explainable AI (XAI) 1.0, focusing specifically on ‘Feature attributions’, a concept central to the machine learning community’s understanding of model explanations. Historically, this field predominantly applied to visual data, particularly images. Over the past decade, however, a significant shift has occurred, with a burgeoning interest in language-based applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#understanding-black-box-ai-systems-and-the-need-for-explainability",
    "href": "chapter_ai-nepi_007.html#understanding-black-box-ai-systems-and-the-need-for-explainability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Understanding Black Box AI Systems and the Need for Explainability",
    "text": "7.3 Understanding Black Box AI Systems and the Need for Explainability\n\n\n\nSlide 04\n\n\nUnderstanding the internal workings of ‘Black Box AI Systems’ presents a significant challenge, particularly in classification tasks. For instance, when an input image of a rooster yields a ‘Rooster’ prediction, the user typically lacks insight into the basis of this classification. Consequently, the field of Explainable AI (XAI) has dedicated substantial research to tracing the origins of these predictions. XAI 1.0 primarily employed post-hoc explainability methods, such as heatmaps, which visually highlight the specific pixels responsible for a model’s prediction, thereby clarifying, for example, why a rooster was recognised. Beyond this specific application, the broader imperative for explainability encompasses several critical objectives: verifying predictions to ensure the model’s logical operation; identifying flaws and biases to facilitate error correction; learning about the underlying problem space, as models can uncover unexpected solutions; and, increasingly, ensuring compliance with legislative frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#transition-to-generative-ai-and-multi-task-foundation-models",
    "href": "chapter_ai-nepi_007.html#transition-to-generative-ai-and-multi-task-foundation-models",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Transition to Generative AI and Multi-Task Foundation Models",
    "text": "7.4 Transition to Generative AI and Multi-Task Foundation Models\n\n\n\nSlide 06\n\n\nThe landscape of AI has significantly evolved from traditional classification models to the era of Generative AI (Gen AI), where models exhibit a diverse array of capabilities. These advanced systems can now perform classification, retrieve similar images, generate novel images, and answer complex questions across various topics. This expansion of functionality, however, introduces a substantial challenge: grounding the predictions or answers from Large Language Models (LLMs) back to their specific inputs becomes considerably more difficult. Consequently, the future of Explainable AI (XAI) necessitates moving beyond simple heatmap representations towards understanding intricate ‘feature interactions’ and adopting more ‘mechanistic views’ of model behaviour. Crucially, contemporary foundation models operate as both ‘multi-task’ and ‘world models’, offering profound insights into societal dynamics, the evolution of textual data, and inherent textual features.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#common-mistakes-in-ai-models-object-detection-and-multi-step-planning",
    "href": "chapter_ai-nepi_007.html#common-mistakes-in-ai-models-object-detection-and-multi-step-planning",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 Common Mistakes in AI Models: Object Detection and Multi-Step Planning",
    "text": "7.5 Common Mistakes in AI Models: Object Detection and Multi-Step Planning\n\n\n\nSlide 07\n\n\nAI models, despite their sophistication, frequently exhibit surprising errors. Two prominent examples illustrate these limitations. Firstly, in object detection, a standard classifier might erroneously base its ‘boat’ prediction on the surrounding water, a correlated textural feature, rather than the boat itself, indicating a reliance on superficial cues. Secondly, in multi-step planning, Large Language Models (LLMs) demonstrate failures in complex tasks such as the Tower of Hanoi puzzle. Here, an LLM might attempt to move the largest, inaccessible disk directly to the final peg, thereby violating the inherent physical constraints of the problem. This highlights a fundamental lack of understanding regarding the physical rules governing such planning scenarios.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 XAI 2.0: Structured Interpretability",
    "text": "7.6 XAI 2.0: Structured Interpretability\n\n\n\nSlide 08\n\n\nEberle then shifted his focus to ‘Structured Interpretability’, representing XAI 2.0. This advanced approach aims to transcend the limitations of traditional heatmap representations, seeking deeper insights into model decisions. Whilst acknowledging the potential for improved reasoning in more recent models, such as Llama 3.something, the core objective remains to develop more sophisticated methods for understanding AI behaviour.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-explanations-for-classifier-predictions",
    "href": "chapter_ai-nepi_007.html#first-order-explanations-for-classifier-predictions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 First-Order Explanations for Classifier Predictions",
    "text": "7.7 First-Order Explanations for Classifier Predictions\n\n\n\nSlide 09\n\n\nFirst-order explanations prove particularly useful for understanding classifier behaviour, typically by generating heatmaps over classifications. For instance, in an application involving historical data tables, Eberle and his team aimed to distinguish specific subgroups. By employing heatmaps, they verified that the classifier’s predictions were based on meaningful features. Crucially, the model correctly focused on the numerical content within the tables, which served as an effective proxy for identifying numerical tables, thereby validating its intended operation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-order-explanations-pairwise-relationships-and-similarity",
    "href": "chapter_ai-nepi_007.html#second-order-explanations-pairwise-relationships-and-similarity",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 Second-Order Explanations: Pairwise Relationships and Similarity",
    "text": "7.8 Second-Order Explanations: Pairwise Relationships and Similarity\n\n\n\nSlide 10\n\n\nBeyond individual features, the authors extended their analysis to second-order explanations, concentrating on pairwise relationships. They investigated similarity by computing the dot product of embeddings derived from two inputs, such as images. This process yielded a similarity score, which required further explanation. Eberle and his team discovered that interaction scores provided an appropriate method for representing these similarity predictions. For example, in the context of historical tables, interactions between specific digits indicated that two tables were identical. This approach successfully verified that the model functioned as intended, accurately identifying meaningful similarities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-explanations-graph-structures-and-mechanistic-understanding",
    "href": "chapter_ai-nepi_007.html#higher-order-explanations-graph-structures-and-mechanistic-understanding",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.9 Higher-Order Explanations: Graph Structures and Mechanistic Understanding",
    "text": "7.9 Higher-Order Explanations: Graph Structures and Mechanistic Understanding\n\n\n\nSlide 11\n\n\nIn more recent work, Eberle and his colleagues explored higher-order interactions within graph structures, finding that these provide more meaningful insights. This approach applies to various network types, such as citation networks or networks of books and entities, where models are trained on classification tasks. Their explanation method involves identifying ‘feature subgraphs’ or ‘feature walks’, which represent sets of features that become relevant collectively. This technique aims to yield more complex insights into model behaviour, ultimately progressing towards a ‘circuit level understanding’ of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biased-sentiment-predictions",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biased-sentiment-predictions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.10 First-Order Attributions in LLMs: Biased Sentiment Predictions",
    "text": "7.10 First-Order Attributions in LLMs: Biased Sentiment Predictions\n\n\n\nSlide 12\n\n\nApplying interpretability techniques to language and humanities contexts, Eberle and his team investigated first-order attributions for sentiment predictions within Transformer Large Language Models (LLMs). Utilising a common dataset of movie reviews, they ranked sentences and generated heatmaps with a newly proposed method tailored for Transformers. Their findings revealed significant biases in sentiment predictions: male Western names, such as Lee, Barry, Raphael, or the Cohen Brothers, correlated more strongly with positive sentiment, whilst foreign-sounding names like Saddam, Castro, or Chan were more likely to elicit negative scores. This research underscores the utility of Explainable AI in detecting such fine-grained biases within models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#long-range-dependencies-in-llms-context-prioritisation",
    "href": "chapter_ai-nepi_007.html#long-range-dependencies-in-llms-context-prioritisation",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.11 Long-Range Dependencies in LLMs: Context Prioritisation",
    "text": "7.11 Long-Range Dependencies in LLMs: Context Prioritisation\n\n\n\nSlide 14\n\n\nEberle and his team investigated Large Language Models (LLMs) in a standard scenario involving long context windows, such as 8,000-token Wikipedia articles, where the task involved generating a summary from provided text. Their primary research question concerned the models’ capacity to utilise long-range information. By analysing the origin of information within the generated summaries, they discovered that LLMs predominantly focus on the later parts of the context, exhibiting a strong tendency to prioritise information presented closer to the prompt. Whilst models can retrieve information from the very beginning of the context, this occurs significantly less frequently, as indicated by a log scale of counts. Consequently, summaries produced by LLMs may not offer a balanced representation of the entire input text, instead favouring more recently presented information.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text-explaining-sentence-similarities",
    "href": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text-explaining-sentence-similarities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.12 Second & Higher-Order Interactions in Text: Explaining Sentence Similarities",
    "text": "7.12 Second & Higher-Order Interactions in Text: Explaining Sentence Similarities\n\n\n\nSlide 16\n\n\nThe investigation extended to second and higher-order interactions within textual data, specifically addressing the explanation of sentence similarities and embeddings. Eberle and his colleagues employed an unsupervised analysis of representations extracted from a pre-trained foundation model, such as a Sentence-BERT model. This involved computing similarity scores between sentence pairs using dot products of their embeddings, denoted as ⟨ϕ(x), ϕ(x’)⟩. The core challenge lay in understanding the underlying reasons for a given similarity score. Second-order explanations provided a solution by yielding interaction scores between individual tokens. For instance, the high similarity between ‘A cat I really like.’ and ‘It is a great cat!’ was attributed to specific token interactions. Findings often revealed reliance on straightforward noun matching strategies, including synonyms or identical noun tokens, alongside some noun-verb and separator token interactions. This suggested that models, whilst compressing vast amounts of information, frequently resort to surprisingly simplistic strategies. This observation, though not immediately intuitive, holds significant relevance for those embedding data and computing rankings, as the features contributing to high similarity scores can be remarkably simple.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions-and-llm-interpretability",
    "href": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions-and-llm-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.13 Graph Neural Networks for Structured Predictions and LLM Interpretability",
    "text": "7.13 Graph Neural Networks for Structured Predictions and LLM Interpretability\n\n\n\nSlide 19\n\n\nThe discussion then turned to Graph Neural Networks (GNNs) and their utility for structured predictions. GNNs offer attributions in terms of ‘walks’, which represent interactions between features within a graph. Crucially, the structural information encoded by GNNs can be conceptually framed as Large Language Models (LLMs), given that LLM attention networks effectively determine which tokens can ‘message pass’, a process analogous to GNN interactions. This conceptual alignment allows for the application of GNN-based interpretability methods to analyse complex language structures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-for-complex-language-structure",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-for-complex-language-structure",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.14 Higher-Order Interactions for Complex Language Structure",
    "text": "7.14 Higher-Order Interactions for Complex Language Structure\n\n\n\nSlide 21\n\n\nStandard explanation methods, such as Bag of Words (BoW), often fail to capture the inherent complexity of natural language, particularly nuances like negation. For instance, a BoW model might incorrectly assign a positive score to the phrase ‘First I didn’t like the boring pictures’ due to the presence of ‘like’, overlooking the crucial negation. In contrast, higher-order explanation methods significantly improve this capability, accurately interpreting complex language structures. These methods can correctly assign a negative score to the entire negated clause and precisely interpret the hierarchical structure of subsequent positive clauses, such as ‘but it is certainly one of the best movies I have ever seen.’ This enhanced understanding stems from the inherent suitability of hierarchical natural language structures for graph representations. The methodology involves training a Graph Neural Network (GNN) or Large Language Model (LLM) on sentiment analysis tasks, such as movie reviews, and subsequently extracting ‘walks’ to explain their predictions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities-visual-definitions",
    "href": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities-visual-definitions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.15 AI-based Scientific Insights in the Humanities: Visual Definitions",
    "text": "7.15 AI-based Scientific Insights in the Humanities: Visual Definitions\n\n\n\nSlide 23\n\n\nThe presentation transitioned to AI-based scientific insights within the humanities, commencing with heatmap-based approaches. Eberle and his team utilised a corpus of mathematical instruments, specifically the Sphaera Cor. corpus, to classify images into categories such as ‘machine’ or ‘mathematical instrument’. In close collaboration with historians, including Matteo Valleriani and Jochen Büttner, they sought to establish more objective criteria for visual definitions. A key finding revealed that the fine-grained scales present on mathematical instruments were highly relevant features for the model’s classification decisions. This work underscored the critical necessity of continuous engagement with domain experts to ensure the meaningfulness and accuracy of these AI-derived definitions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables",
    "href": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.16 Corpus-Level Analysis of Early Modern Astronomical Tables",
    "text": "7.16 Corpus-Level Analysis of Early Modern Astronomical Tables\n\n\n\nSlide 25\n\n\nIn their most extensive digital humanities collaboration to date, Eberle and his colleagues undertook a corpus-level analysis of numerical tables from the Sphaera Corpus, an early modern text spanning from 1472 to 1650. Historians Matteo Valleriani and Jochen Büttner presented this challenging dataset, which initially appeared intractable for automated analysis. The primary objective was to develop an automated method for matching and identifying semantic similarities between these historical tables, a task previously unfeasible at scale. This initiative aimed to unlock new avenues for historical research by enabling comprehensive analysis of this rich, yet previously inaccessible, data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#historical-insights-at-scale-the-xai-historian-workflow",
    "href": "chapter_ai-nepi_007.html#historical-insights-at-scale-the-xai-historian-workflow",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.17 Historical Insights at Scale: The XAI-Historian Workflow",
    "text": "7.17 Historical Insights at Scale: The XAI-Historian Workflow\n\n\n\nSlide 26\n\n\nEberle and his team developed the XAI-Historian workflow, a collaborative project designed to empower historians with scalable insights and facilitate data-driven hypothesis generation. This initiative centred on the Sacrobosco Corpus, an extensive collection of 76,000 pages of university textbooks from 1472 to 1650. The project faced significant machine learning challenges, including highly heterogeneous data, severely limited annotations, and the inadequacy of standard OCR and foundation models for this out-of-domain historical material. Consequently, the authors implemented a bespoke three-stage methodology. Firstly, data collection involved curating the Sacrobosco Corpus. Secondly, an atomisation-recomposition process transformed input tables into bigram maps, subsequently generating histograms (Φ(x)). Thirdly, corpus-level analysis involved embedding these historical tables into a spatial representation, where data similarity was computed as the inner product of their feature representations, y(x, x’) = ⟨Φ(x), Φ(x’)⟩. Crucially, the team developed a specific statistical model for bigram representations, as generic foundation models proved unsuitable. The model’s efficacy was verified by its consistent detection of identical bigrams across different inputs, such as ‘38’, thereby establishing its reliability for historical analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#cluster-entropy-analysis-for-investigating-innovation-spread",
    "href": "chapter_ai-nepi_007.html#cluster-entropy-analysis-for-investigating-innovation-spread",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.18 Cluster Entropy Analysis for Investigating Innovation Spread",
    "text": "7.18 Cluster Entropy Analysis for Investigating Innovation Spread\n\n\n\nSlide 28\n\n\nEberle and his team applied cluster entropy analysis in case studies to investigate the historical spread of innovation across Europe. This involved examining the publication output of specific historical print locations, or cities, each of which produced a distinct ‘programme’ of publication types. Previously, analysing the diversity or focus of these print programmes at scale proved impossible. To overcome this, the authors devised a novel clustering approach, leveraging representations derived from their model. Entropy (H(p)) served as the key metric to quantify diversity: a low entropy score indicated a city’s tendency to reproduce the same content, signifying a less diverse print programme, whilst a higher entropy score denoted a more varied and diverse output. The primary objective was to ascertain the number of distinct clusters a given city produced.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#identifying-historical-anomalies-through-cluster-entropy",
    "href": "chapter_ai-nepi_007.html#identifying-historical-anomalies-through-cluster-entropy",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.19 Identifying Historical Anomalies Through Cluster Entropy",
    "text": "7.19 Identifying Historical Anomalies Through Cluster Entropy\n\n\n\nSlide 29\n\n\nEmploying a distance-based clustering method, Eberle and his team quantified the diversity of print programmes produced by various historical cities using entropy as a metric. A low entropy score indicated a city’s propensity to reproduce identical content, signifying a less diverse output, whereas a higher score denoted a more varied print programme. This analysis led to the identification of two particularly interesting cases exhibiting the lowest entropy scores. Frankfurt/Main emerged as a known historical centre for the repeated reprinting of editions. More notably, Wittenberg presented a compelling case where the political control exerted by Protestant reformers actively restricted the print programme, dictating the curriculum content. This finding represented a significant historical anomaly, which, crucially, aligned with and supported existing historical intuition.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.20 Conclusion: AI-based Methods for the Humanities – Challenges and Opportunities",
    "text": "7.20 Conclusion: AI-based Methods for the Humanities – Challenges and Opportunities\n\n\n\nSlide 33\n\n\nIn conclusion, whilst humanities and Digital Humanities (DH) researchers have historically concentrated on digitising source material, the automated analysis of these corpora presents significant challenges due to their inherent heterogeneity and scarcity of labels. Nevertheless, the integration of Machine Learning (ML) with Explainable AI (XAI) offers a promising pathway to scale humanities research and cultivate novel research directions. Foundation Models and Large Language Models (LLMs) can automate various intermediate tasks, including labelling, data curation, and error correction; however, their utility remains limited for more complex research questions. A significant roadblock persists in the challenges posed by low-resource data for ML, particularly concerning scaling laws. Consequently, the out-of-domain transfer of models to historical and small-scale data necessitates rigorous evaluation. Crucially, current LLMs are primarily trained and aligned for natural language tasks and code generation, indicating a fundamental mismatch with the specific data requirements of humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#additional-visual-materials",
    "href": "chapter_ai-nepi_007.html#additional-visual-materials",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.21 Additional Visual Materials",
    "text": "7.21 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n This slide, titled ‘Post-Hoc Explainability’, presents a conceptual diagram illustrating how AI predictions can be interpreted. The top section depicts the standard classification process: an ‘input x’ image of a rooster is fed into a ‘Black Box AI System’ to ‘classify image’, which then outputs a ‘Rooster’ ‘prediction f(x)’. Below this, the diagram introduces the concept of explainability. An arrow labelled ‘Explanation methods’ extends from the ‘prediction f(x)’ box, leading to a stylised neural network diagram. This network diagram, labelled ‘explain prediction’, shows interconnected nodes with some paths highlighted in red, suggesting the internal workings or important pathways within the explanation process. An arrow from this network points to a ‘heatmap’, which is a blank square with a small, irregular red area circled on its top right. An annotation next to the heatmap clarifies that the ‘AI system’s decision is based on these pixels’, indicating that the heatmap visually highlights the most relevant parts of the input image for the AI’s prediction. A curved arrow from the heatmap explanation leads to a dashed-line box titled ‘Why explainability ?’. This box lists four key reasons: ‘Verify predictions’, ‘Identify flaws and biases’, ‘Learn about the problem’, and ‘Ensure compliance to legislation’. The slide number ‘5’ is in the bottom centre, and ‘Samek et al. (2017)’ is cited in the bottom right, indicating the source or a key reference for the presented concepts.\n This slide, titled ‘Ex 1a: Biased Sentiment Predictions in Transformer LLMs’, details an experiment setup and its results. The ‘Setup’ section outlines two objectives: first, ‘Understanding feature importance in LLMs using Explainable AI’, and second, to ‘Analyze what names are most / least relevant to change the prediction towards a positive / negative review’. A diagram in the top right corner visually represents feature importance, showing a dashed circle containing four smaller circles labelled x1, x2, x3, x4, with x1 highlighted in a light red colour, indicating it as the most important feature amongst others. The ‘Results’ section displays a vertical axis labelled ‘name relevance’, with a ‘+’ sign at the top indicating higher relevance and a ‘-’ sign at the bottom indicating lower relevance. Several example sentences, each enclosed by [CLS] and [SEP] tokens, are listed along this axis. Words within these sentences are highlighted in either red/pink or blue/purple to denote their relevance. Names highlighted in red/pink are positioned towards the top of the ‘name relevance’ axis, indicating high relevance. These include ‘malcolm d. lee’, ‘dave barry’, ‘raphael atmosphere’ (from ‘the sally jesse raphael atmosphere’), ‘coe n brothers’ (from ‘the coe n brothers and’), ‘jackie chan’ (from ‘a jackie chan movie’), and ‘cuban leader fide l castro’ (from ‘of cuban leader fide l castro’). Names highlighted in blue/purple are positioned towards the bottom, indicating lower relevance. These include ‘martha stewart’ and ‘program run am ok’ (from ‘some sort of martha stewart decor ating program run am ok’), and ‘comedy equivalent’ and ‘saddam hussein’ (from ‘the comedy equivalent of saddam hussein’). The ellipsis ‘…’ indicates more examples are omitted. This visualisation demonstrates how specific names disproportionately influence the sentiment predictions of LLMs, revealing potential biases. The slide is numbered ‘13’ and attributes the work to ‘Ali et al., XAI for Transformers. (ICML ‘22)’.\n This slide, titled ‘Ex 1b: Long-range dependencies in LLMs’, illustrates the experimental setup and findings regarding how Large Language Models (LLMs) process information across long contexts. The ‘Setup’ section describes the task: ‘Given a long context (e.g. 8k tokens Wikipedia article), generate a summary from the provided text’. The core research question posed is: ‘Can LLMs utilise long-range information?’ The slide visually represents a long context window, showing a sequence of tokens from ‘Token 1’ to ‘Token 8000’. A red arrow labelled ‘Attention’ points from the later tokens (e.g., ‘Token 7999’, ‘Token 8000’) towards the ‘Summary’, indicating that the model’s attention is concentrated on the end of the input. The ‘Results’ section presents a bar chart with a logarithmic y-axis labelled ‘Counts’ (ranging from 10^0 to 10^5) and an x-axis labelled ‘Position in context’ (ranging from 0 to 8000). The bars show that information from the end of the context (e.g., positions 7000-8000) is retrieved significantly more often than information from the beginning (e.g., positions 0-1000). A red dashed line highlights the sharp decline in counts for earlier positions. This demonstrates that LLMs exhibit a strong ‘recency bias’, predominantly focusing on the later parts of the context when generating summaries. The slide number ‘15’ is in the bottom centre, and the work is attributed to ‘Eberle et al., Explaining Long-Range Dependencies in Transformers. (NeurIPS ’23)’.\n This slide, titled ‘Ex 2: Explaining sentence similarities and embeddings’, focuses on understanding how sentence embeddings capture semantic relationships. The ‘Setup’ section states the core methodology: ‘Unsupervised analysis of the representations extracted from a pretrained foundation model using dot-products ⟨ϕ(x), ϕ(x’)⟩‘. This implies that the similarity is computed as the dot product of the embeddings of two sentences, x and x’. Section ‘I. Forward prediction / encoder’ visually demonstrates this process. Two input sentences, ‘A cat I really like.’ and ‘It is a great cat!’, are shown entering separate encoder models, represented as trapezoidal shapes. Each encoder is depicted with internal layers labelled ϕ1, ϕ2, …, ϕL, signifying the transformation of the input sentence into a high-dimensional representation or embedding. The outputs of these encoders are then fed into a dot product operation, indicated by ‘⟨.,⟩’, which yields a ‘similarity score’. An example score of ‘0.92’ is given, followed by the question ‘but why?’, highlighting the need for interpretability. Section ‘II. Token interactions’ addresses this ‘why’ question by visualising the contributions of individual tokens to the overall similarity. This diagram shows the two sentences, ‘A cat I really like.’ and ‘It is a great cat!’, with lines connecting words between them. The thickness and colour of these lines indicate the strength of interaction or contribution. For instance, a thick red line connects ‘cat’ from the first sentence to ‘cat’ from the second, and another thick red line connects ‘like’ from the first sentence to ‘great’ from the second, suggesting these word pairs are highly influential in the high similarity score. Thinner, lighter lines connect other words, indicating weaker interactions. Two mathematical equations are presented. The first, y(x, x') = ⟨ϕ(x), ϕ(x')⟩, formally defines the similarity score y between sentences x and x' as the dot product of their embeddings ϕ(x) and ϕ(x'). The second, BiLRP(y, x, x') = Σ_{m=1}^h LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x) ⊗ LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x'), introduces ‘BiLRP’ (Bilinear Layer-wise Relevance Propagation), a method for explaining the similarity score by decomposing it into contributions from individual tokens. This equation involves a summation over h components and a tensor product (⊗) of LRP scores for each sentence’s tokens, indicating how relevance is propagated back through the model layers (ϕ1 to ϕL) to the input tokens. The method is attributed to ‘Eberle et al. (TPAMI ’22)’. In the top right corner, a small circular diagram shows four nodes (x1, x2, x3, x4) within a dashed circle. Nodes x1 and x4 are highlighted in red and connected by a thick red line, visually reinforcing the concept of identifying strong relationships or similarities between specific elements within a set. The slide number ‘17’ is visible at the bottom centre.\n This slide, titled ‘Ex 2: Explaining sentence similarities and embeddings’, continues the discussion on understanding how sentence embeddings capture semantic relationships. The ‘Results’ section presents key findings. The first point states: ‘Models compress vast amounts of information, but often resort to surprisingly simple strategies’. This suggests that despite their complexity, models may rely on straightforward mechanisms for certain tasks. The second point elaborates on these strategies: ‘Often simple noun matching strategies (synonyms, identical noun tokens)’. This indicates that direct word matches, or their synonyms, frequently drive similarity predictions. The third point adds: ‘Some noun-verb and separator token interactions’. This suggests that beyond simple noun matching, models also consider relationships between nouns and verbs, and the role of punctuation or other separator tokens. The slide includes two example sentences: ‘A cat I really like.’ and ‘It is a great cat!’. Below these, a diagram visually represents the ‘token interactions’ contributing to their similarity. Words are connected by lines, with thickness and colour indicating interaction strength. For instance, a thick red line connects ‘cat’ to ‘cat’, and ‘like’ to ‘great’, highlighting their strong contribution to similarity. Thinner lines connect other words, showing weaker interactions. The slide concludes with a crucial implication: ‘This is important for those embedding data and computing rankings, as the features contributing to high similarity scores can be remarkably simple’. This advises practitioners to be aware that seemingly complex model behaviours might stem from surprisingly basic underlying features. The slide number ‘18’ is in the bottom centre, and the work is attributed to ‘Eberle et al. (TPAMI ’22)’.\n This slide, titled “Using walks to explain GNN (and LLM) predictions,” presents a framework for model interpretability. It is divided into two main sections: “Model” at the top and “Walk-based relevance” at the bottom. The “Model” section begins with an “input graph,” depicted as a complex, multi-layered graph structure resembling two interconnected hexagonal prisms. This input graph feeds into a series of processing blocks labelled H₀, H₁, and H_T, which represent interaction layers within the model. These layers culminate in a “prediction” output, indicated by a triangular symbol. The entire sequence from input graph to prediction is labelled “interaction.” A red vertical line extends from the “prediction” output, turning left to connect to the “explaining the prediction” process. In the top right, a smaller illustrative graph shows four nodes (x₁, x₂, x₃, x₄) within a dashed circle. Nodes x₁, x₂, and x₄ are highlighted in red, along with edges connecting x₁ to x₄ and x₂ to x₄, whilst x₃ remains unhighlighted, likely demonstrating a simple example of relevant nodes and edges. The “Walk-based relevance” section visually explains how the prediction is interpreted. It starts with a large, detailed version of the input graph, where a prominent, thick red path, explicitly labelled “walk,” traces a specific sequence of nodes and edges through the graph. Many other edges and nodes in this graph are also highlighted with varying shades of red and purple, indicating different degrees of relevance. An arrow points from this detailed graph to a simplified version, where only a subset of the graph’s structure and edges are highlighted in red, suggesting a relevant subgraph. Another arrow then points to a further simplified representation where only the nodes are shown, with relevant nodes encircled in red, indicating the most critical elements for the prediction. The red line from the “prediction” output connects to these visual explanations, signifying that these walk-based visualisations are the result of “explaining the prediction.” The bottom right of the slide includes a citation: “Schnake et al., Higher-Order Explanations of Graph Neural Networks via Relevant Walks. (TPAMI ’22)”, and the page number “20” is in the bottom left.\n This slide, titled “Ex 3: Interaction of nodes learns complex language structure,” begins with a “Setup” section outlining two key points: first, that “Hierarchical structure in natural language is well suited to graph structures,” implying that language’s inherent organisation aligns well with graph representations. Second, it states the methodology: “Train a GNN on movie review sentiment task and extract walks,” indicating that a GNN is trained for sentiment classification, and then paths (walks) through the learned graph are used for explanation. The slide then presents an example sentence: “First I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen.” This sentence contains both negative and positive sentiment. Under the heading “high-order interactions” on the left, a graph diagram illustrates the GNN’s learned structure. Words are represented as nodes (ovals), and connections (edges) between them are coloured and weighted. Blue edges indicate a contribution to negative sentiment, whilst red edges indicate a contribution to positive sentiment, with thicker lines denoting stronger connections. The diagram clearly shows the sentence’s two clauses: the first clause (“First I didn’t like the boring pictures,”) is predominantly connected by blue edges, highlighting the negative sentiment associated with “didn’t like” and “boring pictures.” The second clause (“but it is certainly one of the best movies I have ever seen.”) is predominantly connected by red edges, emphasising the positive sentiment from phrases like “certainly one of the best movies” and “have ever seen.” This visualisation demonstrates how the GNN captures the structural interplay of words to determine sentiment. In contrast, the bottom section, labelled “standard explanations” and “BoW” (Bag of Words), shows the same sentence with individual words highlighted. Blue highlights indicate negative sentiment, and red highlights indicate positive sentiment. Words like “didn’t,” “like,” “boring,” and “pictures” are highlighted in blue, whilst words such as “certainly,” “best,” “movies,” and “seen” are highlighted in red. This Bag-of-Words approach attributes sentiment to individual words without explicitly showing their interdependencies. To the right of the BoW text, another graph diagram is presented, focusing on the positive part of the sentence (“of movies the best seen I have ever”). This diagram uses only red edges, reinforcing the positive sentiment of this phrase and possibly representing a more traditional parse tree. The overall comparison highlights that GNNs, by learning high-order interactions and graph structures, can provide more nuanced and context-aware explanations of sentiment compared to simpler Bag-of-Words models. The slide number “22” is in the bottom centre, and the citation “Schnake et al. (TPAMI ’22)” is in the bottom right corner.\n This slide, titled ‘Ex 4: Extracting visual definitions from corpora,’ with a subtitle ‘Math. instruments; Sphaera Cor. (Valleriani+’19)’, illustrates the application of AI in visual humanities. The left side of the slide displays three historical engravings, labelled A, B, and C. Image A depicts a ‘time measurement instrument (Cortés, 1556),’ which appears to be an ornate sundial or astrolabe. Image B shows a ‘device to measure ang. height of celestial bodies (Finé, 1587),’ illustrated as a geometric quadrant. Image C presents a ‘Machine to strike gold medals (Branca,1629),’ a detailed engraving of a large, gear-driven machine operated by a person in a workshop setting. The right side of the slide, under the heading ‘Class-specific Heatmap explanations,’ demonstrates how machine learning models interpret these images. Three smaller heatmaps at the top show abstract representations for ‘math. instrument,’ ‘machine,’ and ‘scientific illustration,’ indicating salient features for each category. Below these, a larger heatmap, specifically for the ‘machine’ class, is overlaid on a faded version of Image C. This heatmap uses red and blue gradients to highlight regions of the image that are most influential in its classification as a ‘machine.’ Three light blue circles on this heatmap point to specific components of the machine, labelled with their identified functions: ‘scaffolding’ for the upper structure, ‘support pole’ for a vertical element, and ‘footstand’ for its base. This visual analysis provides an explanation of the model’s reasoning. The footer indicates the source as ‘El-Hajj & Eberle+, Explainability and transparency in the realm of DH. (Int J Digit Humanities ’23)’ and the page number ‘24’.\n This slide, titled ‘Verifying modelling and features using XAI and Historians’, is structured into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section outlines three key points: First, ‘Historical tables are carriers of scientific knowledge processes’, emphasising their importance as data sources. Second, the approach is to ‘Represent tables using bag of bigrams (e.g. ’01’ or ‘21’)‘, indicating a method of feature extraction based on sequences of two characters or elements. Third, the system uses ’Limited annotations: learned feature extractor + hard-coded structure’, suggesting a hybrid approach combining machine learning with predefined rules. Visually, in the top right, a diagram illustrates a concept of feature interaction or relationship. It shows a dashed circle enclosing four nodes labelled ‘x₁’, ‘x₂’, ‘x₃’, and ‘x₄’. Nodes ‘x₁’ and ‘x₄’ are highlighted in red and connected by a red line, implying a detected relationship or a significant feature pair. The ‘Results’ section presents findings through various visual aids. On the bottom left, a scatter plot or network diagram shows a dense cluster of points, with arrows pointing from it to a small, old-looking table image. An arrow labelled ‘x’ points from the cluster to the table, and another labelled ‘x’’ points from the cluster to a specific region within the table. An ‘s’ label points to the overall cluster, suggesting the source data or a spatial representation. The small table image is a snippet from a historical document, with text ‘384 Comment. in III. Cap.’, ‘TABVLA ASCENS’, and ‘Obliquation.’. To the right of this, under the heading ‘Bigram Model’, two larger images of historical tables are displayed side-by-side. These tables contain columns with labels like ‘V’, ‘M’, ‘G. M.’, and ‘G.’, and rows of numbers including ‘0’, ‘II’, ‘38’, ‘22’, ‘12’, ‘4’, ‘44’, ‘30’, ‘61’. Red lines connect specific numerical entries across the two tables, visually demonstrating the bigram model’s ability to identify correspondences or relationships between elements in different tables or different parts of the same table. On the far right, two data visualisations present quantitative results. The first is a table titled ‘Histogram correlation’, with columns ‘Density’, ‘ρ’ (correlation coefficient), ‘Nbig’ (number of bigrams), and ‘Nuni’ (number of unigrams). It shows results for different data densities: ‘Low (≤150)’ with ρ=0.84, Nbig=493, Nuni=916; ‘Dense (150–300)’ with ρ=0.88, Nbig=786, Nuni=1501; and ‘Very dense (&gt;300)’ with ρ=0.93, Nbig=982, Nuni=1764. This indicates that correlation and the number of extracted features increase with data density. The second visualisation is a horizontal bar chart labelled ‘Cluster classification’ on the y-axis and ‘Cluster purity’ on the x-axis, ranging from 0.3 to 0.9. It compares the purity of clusters obtained using different methods: ‘Bigram’ (blue bar, highest purity, close to 0.9), ‘Pooled’ (grey bar), ‘Unigram’ (grey bar), and ‘VGG-16’ (grey bar, lowest purity, around 0.6). Error bars are shown for each method. This chart highlights the superior performance of the ‘Bigram’ model in achieving higher cluster purity. Two citations are listed at the bottom right: ‘Eberle et al. (TPAMI ’22)’ and ‘Eberle et al. (Sci Adv ’24)’, crediting the research to the authors. The slide number ‘27’ is in the bottom centre.\n This slide, titled ‘Cluster entropy analysis to investigate innovation’, is divided into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section describes the methodology: ‘- Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This indicates that the analysis quantifies the deviation of actual innovation clusters from a theoretical maximum dispersion, likely to understand the concentration or diffusion of innovation. The ‘Results’ section presents visual findings. On the left, a diagram illustrates the concept of ‘clustering’. It shows three distinct, layered clusters, each representing a geographic region associated with historical printing or publication. The top layer, depicted with black dots, is labelled ‘Lisbon’. The middle layer, shown with magenta dots, is labelled ‘Venice’. The bottom layer, with blue/purple dots, is labelled ‘Wittenberg’. An arrow points from the ‘clustering’ process to a bar chart labelled ‘table clusters’, which displays a series of vertical bars of varying heights, suggesting a quantitative representation of the characteristics or distribution of these clusters. On the right, a detailed geographical map of Europe displays numerous ‘Sphaera publication cities’ marked with red dots. The map spans from approximately 5.000°W to 15.000°E longitude and 40.000°N to 50.000°N latitude. Key cities labelled include: London, Leiden, Antwerp, Leuven, Cologne, Mainz, Frankfurt (Main), Neustadt an der Weinstraße, Paris, Strasbourg, Dijon, Basel, Lyon, Geneva, Saint Gervais, Avignon, Milan, Bologna, Ferrara, Florence, Perugia, Rome, Seville, Madrid, Alcalá de Henares, Salamanca, Valladolid, Coimbra, Lisbon, Wittenberg, Leipzig, Kraków, Vienna, Ingolstadt, Dillingen an der Donau, Heidelberg, Nuremberg, and Lemgo. A legend in the bottom right corner clarifies that the dots represent ‘Sphaera publication cities’ and notes the coordinate system ‘EPSG:4326’, along with a North arrow. This map provides the spatial context for the ‘print locations’ analysed for cluster entropy, visually connecting the abstract clusters to their real-world geographic distribution. The slide also includes a page number ‘30’ and a citation ‘Eberle et al. (Sci Adv ’24)’ at the bottom right, attributing the research to a publication by Eberle and colleagues in Science Advances from 2024.\n This slide, titled ‘Cluster entropy analysis to investigate innovation’, is divided into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section defines the core metric used in the analysis: ‘Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This indicates that the study quantifies how far a given cluster’s entropy deviates from its theoretical maximum, likely as a measure of its structure or predictability. The ‘Results’ section presents the findings through two interconnected visual elements. On the left, a small bar chart labelled ‘table clusters’ is shown, depicting a distribution of cluster sizes or counts. An arrow labelled ‘clustering’ points from this bar chart towards a series of three layered 2D scatter plots, each representing a different city. The top layer is labelled ‘Lisbon’ and shows a cluster of black dots. The middle layer is labelled ‘Venice’ and displays a cluster of magenta dots. The bottom layer is labelled ‘Wittenberg’ and shows a cluster of purple dots. These scatter plots likely visualise the spatial distribution or characteristics of the ‘clusters’ being analysed. On the right, a bar chart displays the calculated entropy difference for numerous cities. The vertical y-axis is labelled ‘H(p) - H(p_max)’ and ranges from 0 down to -3. The horizontal x-axis lists various historical print locations vertically, including ‘Alcalá Hen.’, ‘Antwerp’, ‘Avignon’, ‘Basel’, ‘Coimbra’, ‘Cologne’, ‘Florence’, ‘Frankfurt/Main’, ‘Geneva’, ‘Heidelberg’, ‘Kraków’, ‘Leiden’, ‘Leipzig’, ‘Lemgo’, ‘Lisbon’, ‘London’, ‘Lyon’, ‘Madrid’, ‘Mainz’, ‘Mexico City’, ‘Neustadt/Wstr.’, ‘Paris’, ‘Rome’, ‘Seville’, ‘Siena’, ‘Sine loco’, ‘St. Gervais’, ‘Strasbourg’, ‘Valladolid’, ‘Venice’, ‘Vienna’, and ‘Wittenberg’. The bars are coloured either black or light grey, suggesting a categorisation or distinction between the cities based on their entropy difference values. For example, Lisbon, Florence, London, and Rome show large negative differences (black bars), whilst Venice and Wittenberg show smaller negative differences (grey bars). The footer indicates the source of the work as ‘Eberle et al. (Sci Adv ’24)’ and the slide number ‘31’.\n This slide, titled ‘Cluster entropy analysis to investigate innovation’, presents the methodology and key findings. The ‘Setup’ section defines the metric used: ‘Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This implies a measure of how much the actual clustering deviates from a state of maximum possible disorder or randomness. The ‘Results’ section begins with a visual explanation of ‘clustering’. On the left, a small bar chart labelled ‘table clusters’ shows varying bar heights, likely representing the distribution or size of different clusters. Below this, three stacked 2D scatter plots illustrate different clustering patterns for specific cities: the top layer shows black points labelled ‘Lisbon’, the middle layer shows magenta points labelled ‘Venice’, and the bottom layer shows purple points labelled ‘Wittenberg’. An arrow points from the ‘clustering’ text to the top scatter plot, indicating these are examples of the clustering being analysed. The main visual element is a bar chart on the right, displaying ‘H(p) - H(p_max)’ on the y-axis, ranging from 0 down to -3. The x-axis lists numerous cities and locations, including ‘Alcalá Hen.’, ‘Antwerp’, ‘Avignon’, ‘Basel’, ‘Coimbra’, ‘Cologne’, ‘Florence’, ‘Frankfurt/Main’, ‘Geneva’, ‘Heidelberg’, ‘Kraków’, ‘Leiden’, ‘Leipzig’, ‘Lemgo’, ‘Lisbon’, ‘London’, ‘Lyon’, ‘Madrid’, ‘Mainz’, ‘Mexico City’, ‘Neustadt/Wstr.’, ‘Paris’, ‘Rome’, ‘Seville’, ‘Siena’, ‘Sine loco’, ‘St. Gervais’, ‘Strasbourg’, ‘Valladolid’, ‘Venice’, ‘Vienna’, and ‘Wittenberg’. Most bars are light grey, but two cities, ‘Frankfurt/Main’ and ‘Wittenberg’, are highlighted with black bars and blue outlines, indicating they are points of interest. These two cities exhibit the most negative values on the y-axis, suggesting their observed cluster entropy is significantly lower than the maximum attainable entropy, implying a high degree of structure or distinct clustering. Corresponding text below the chart provides context for these highlighted cities: ‘Frankfurt/Main: Centre for reprinting editions’ and ‘Wittenberg: Political control of the reformers’. This suggests a correlation between the entropy measure and the historical roles or characteristics of these locations. The slide includes a citation ‘Eberle et al. (Sci Adv ’24)’ and the page number ‘32’ in the bottom right.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  LLM: Evolution of competence",
    "section": "",
    "text": "Overview\nThe authors meticulously chart the rapid evolution of Large Language Models (LLMs), progressing from foundational ‘attention’ mechanisms to advanced ‘thinking’ capabilities. Their work critically examines the current limitations inherent in LLMs, particularly concerning factual accuracy, semantic understanding, and the capacity for rigorous scientific inquiry. Proposing ‘validation’ as the crucial next frontier, the authors introduce computational epistemology as a novel discipline specifically designed to address these identified gaps. A key focus of their research involves cultivating an ‘epistemic agency’ within LLMs, enabling these models to furnish reasoned arguments, provide robust evidence, and identify propositions that transcend mere sentences.\nThe practical application of these concepts manifests within a bespoke working environment, the AI Cockpit. This innovative system leverages multimodal LLMs, including Gemini 2.5, Claude, and Llama, alongside an AI agent named Bernoulli, to meticulously analyse historical documents. The underlying infrastructure relies upon a Scholarium—a meticulously curated, scholarly-edited database of primary sources, such as the Opera Omnia Euler. This Scholarium serves as a validated alternative to conventional embedding-based approaches. Furthermore, the authors underscore the paramount importance of a FAIR (Findability, Accessibility, Interoperability, Reusability) data infrastructure, exemplified by Zenodo. They also advocate for open science principles through OpenScienceTechnology, a dedicated initiative that provides technical support, including an MCP API Server for standardised global AI access to knowledge.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-of-large-language-model-competence",
    "href": "chapter_ai-nepi_008.html#evolution-of-large-language-model-competence",
    "title": "8  LLM: Evolution of competence",
    "section": "8.1 Evolution of Large Language Model Competence",
    "text": "8.1 Evolution of Large Language Model Competence\n\n\n\nSlide 03\n\n\nLarge Language Models have undergone a remarkably swift evolution, progressing through distinct phases of competence. The authors observe that initially, their foundational capability was encapsulated by the principle ‘Attention is all you need’, enabling basic conversational interactions. Subsequently, this capability evolved to incorporate ‘Context is all you need’, necessitating a larger contextual understanding, often facilitated by techniques such as Retrieval-Augmented Generation (RAGs). The latest models now propose ‘Thinking is all you need’, indicating an advancement towards more sophisticated reasoning, whether with or without a predefined plan. This trajectory clearly illustrates a progression from fundamental conversational AI to advanced reasoning capabilities. Nevertheless, a critical question persists: do these advancements truly fulfil all requirements, or do essential components still remain elusive?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#addressing-critical-limitations-in-large-language-models",
    "href": "chapter_ai-nepi_008.html#addressing-critical-limitations-in-large-language-models",
    "title": "8  LLM: Evolution of competence",
    "section": "8.2 Addressing Critical Limitations in Large Language Models",
    "text": "8.2 Addressing Critical Limitations in Large Language Models\nCurrent Large Language Models exhibit several critical limitations that necessitate urgent attention. The authors identify a crucial unmet need for an ‘opponent’ mechanism to effectively counter hallucinations, thereby ensuring factual accuracy in generated content. Furthermore, a fundamental technical and philosophical limitation persists: embedding vectors, whilst powerful, do not encapsulate the true meanings of expressions. Consequently, the research team asserts that models must prioritise truthfulness over mere fluency, refraining from formulating statements that sound plausible but are factually incorrect. Beyond this, LLMs should not simply repeat unverified content from internet media; rather, they must critically evaluate information sources. The imperative, the authors contend, is for models to actively seek and present only the best-justified information. Moreover, a significant missing capability involves the LLM’s ability to formulate optimal plans for scientific inquiry. Regrettably, existing technologies offer no discernible path towards achieving these vital goals. Therefore, the authors propose a new paradigm: ‘Validation is all you need’, positing validation as the indispensable next step in LLM development.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#computational-epistemology-and-epistemic-agency-for-llms",
    "href": "chapter_ai-nepi_008.html#computational-epistemology-and-epistemic-agency-for-llms",
    "title": "8  LLM: Evolution of competence",
    "section": "8.3 Computational Epistemology and Epistemic Agency for LLMs",
    "text": "8.3 Computational Epistemology and Epistemic Agency for LLMs\n\n\n\nSlide 04\n\n\nThe ‘Validation is all you need’ framework, proposed by the authors, establishes stringent requirements for advanced Large Language Models. Such models must demonstrably provide comprehensive reasons, robust arguments, and verifiable evidence both for and against the truth of any given proposition. Furthermore, they should offer reasoned justifications for or against the pursuit of specific actions. To address these complex demands, the research team has introduced a novel discipline: Computational Epistemology. This emerging field focuses on developing the methods and methodologies necessary to bridge the existing validation gap in LLM capabilities. A central tenet of this discipline involves cultivating ‘epistemic agency’ within LLMs. This agency necessitates the ability to identify propositions that transcend simple sentences, to discern arguments embedded within diverse texts and historical sources, and crucially, to comprehend the intentions, plans, and actions of individuals as documented in historical records. The authors have developed a working environment to illustrate these capabilities, exemplified by its application to historical inquiries, such as the contentious construction of Sanssouci castle and Leonhard Euler’s debated involvement.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#ai-powered-historical-inquiry-the-euler-workspace",
    "href": "chapter_ai-nepi_008.html#ai-powered-historical-inquiry-the-euler-workspace",
    "title": "8  LLM: Evolution of competence",
    "section": "8.4 AI-Powered Historical Inquiry: The Euler Workspace",
    "text": "8.4 AI-Powered Historical Inquiry: The Euler Workspace\n\n\n\nSlide 05\n\n\nThe authors presented a live demonstration showcasing the Manger1789.pdf - Euler (Workspace), an advanced AI-powered research tool crafted for historical document analysis. The interface prominently displays a scanned historical document, such as Manger1789.pdf, on its left pane, enabling users to navigate through extensive primary sources. Conversely, the right pane hosts a dynamic query-response interface, which facilitates direct interaction. Users can formulate complex inquiries, exemplified by the question: ‘Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten’ (Reconstruct which persons performed which work on the water fountain). The system then generates validated, qualified answers, meticulously derived from proven evidence rather than mere hearsay. The output typically presents a structured list detailing individuals, their specific contributions, periods of work, remuneration, and both their successes and failures. This entire working environment operates within the Cursor platform, which enables the deployment of AI agents capable of executing a diverse range of tasks, including automated processes. A compelling historical context for this tool involves the long-standing academic dispute surrounding the 18th-century mathematician Leonhard Euler’s precise role and potential culpability in significant construction failures, such as the water fountain at Sanssouci.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-a-curated-scholarly-evidence-base",
    "href": "chapter_ai-nepi_008.html#scholarium-a-curated-scholarly-evidence-base",
    "title": "8  LLM: Evolution of competence",
    "section": "8.5 Scholarium: A Curated Scholarly Evidence Base",
    "text": "8.5 Scholarium: A Curated Scholarly Evidence Base\n\n\n\nSlide 06\n\n\nFor rigorous historical inquiry, the authors developed an AI agent, presumptuously named Bernoulli, to interrogate diverse scholarly sources. The challenge extends beyond merely reading individual PDF documents; it necessitates searching and integrating all available sources, which require indexing and content concentration beyond simple tokens. To achieve this, the research team deemed a robust system comprising five or six core components essential. Foremost amongst these is a scholarly curated editorial board, responsible for the meticulous validation of sources. A prime example is the Opera Omnia Euler itself, a monumental 86-volume collection representing 120 years of scholarly endeavour, with all 866 publications and correspondence having been fully edited two years prior.\nThis Scholarium serves as a sophisticated substitute for conventional embeddings, functioning as a curated database of content items. These items encompass detailed chronologies of actions, expressions, the evolution of terminology, and the use of tools and materials by historical figures. Crucially, every entry undergoes rigorous validation against original historical sources, thereby creating an exhaustive inventory of historically proven activities. Advanced multimodal models, such as the latest Gemini 2.5, can then interrogate these meticulously compiled records, adeptly combining information derived from both text and images. The Scholarium initiative also incorporates other seminal works, including the Kepler Gesammelte Werke and Brahe Opera Omnia, all accessible via a structured digital library environment like the Euler Opera Omnia Viewer, which facilitates navigation through collections, series, volumes, and indices.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-ai-cockpit-user-interface",
    "href": "chapter_ai-nepi_008.html#the-ai-cockpit-user-interface",
    "title": "8  LLM: Evolution of competence",
    "section": "8.6 The AI Cockpit User Interface",
    "text": "8.6 The AI Cockpit User Interface\n\n\n\nSlide 08\n\n\nThe authors explicitly identify the previously demonstrated software interface as an AI Cockpit. This user interface, which they designed for comprehensive document analysis, features a scanned historical document prominently displayed on its left pane. Conversely, the right pane hosts an AI-powered analysis and chat interface, which adeptly extracts structured information directly from the document. Several prominent Large Language Models, including Claude, Gemini, and LLama, power this AI Cockpit. Furthermore, the system incorporates LettreAI on Cursor, indicating a more specialised or integrated AI tool operating seamlessly within the user’s interactive environment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-infrastructure-for-research-data",
    "href": "chapter_ai-nepi_008.html#fair-infrastructure-for-research-data",
    "title": "8  LLM: Evolution of competence",
    "section": "8.7 FAIR Infrastructure for Research Data",
    "text": "8.7 FAIR Infrastructure for Research Data\n\n\n\nSlide 09\n\n\nThe project, spearheaded by the authors, rigorously adheres to the FAIR principles—Findability, Accessibility, Interoperability, and Reusability—for all research data. Zenodo, a prominent example of such a FAIR infrastructure, serves as the primary long-term repository for storing and publishing the project’s extensive datasets. Hosted by CERN in Geneva, Zenodo ensures the enduring availability and preservation of these critical research outputs. A visual representation of the Zenodo website highlights its functionalities, including ‘Featured communities’ like the American Astronomical Society Journals, and showcases ‘Recent uploads’, demonstrating its role as an open repository for diverse research outputs, from datasets to publications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#open-science-technology-and-standardised-ai-access",
    "href": "chapter_ai-nepi_008.html#open-science-technology-and-standardised-ai-access",
    "title": "8  LLM: Evolution of competence",
    "section": "8.8 Open Science Technology and Standardised AI Access",
    "text": "8.8 Open Science Technology and Standardised AI Access\n\n\n\nSlide 10\n\n\nOpenScienceTechnology, a dedicated startup, provides technical support for the project’s infrastructure. This support is firmly rooted in the principles of open science, encompassing Open Source software, Open Access to research outputs, Open Data, and Open Collaboration. A crucial technical component, the MCP API Server (Model Context Protocol API Server), facilitates worldwide access to the project’s data via artificial intelligence models through a standardised API. This initiative, championed by the authors and OpenScienceTechnology, represents a concerted effort to standardise AI access APIs to knowledge within the global community, fostering widespread interoperability and collaborative research endeavours.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#additional-visual-materials",
    "href": "chapter_ai-nepi_008.html#additional-visual-materials",
    "title": "8  LLM: Evolution of competence",
    "section": "8.9 Additional Visual Materials",
    "text": "8.9 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n The slide, titled ‘LLM: Evolution of competence’, outlines a progression in the capabilities of Large Language Models. It presents three distinct phases or aspects, each introduced by a phrase echoing the title of the seminal Transformer* paper, ‘Attention is all you need’. The first point, ‘“Attention is all you need”’, associates with ‘Chat with GPT’, implying the foundational capability of attention mechanisms for conversational AI. The second point, ‘“Context is all you need”’, links to ‘Larger Context with RAG’ (Retrieval-Augmented Generation), suggesting an evolution towards models that can leverage broader external information for more informed responses. The third and final point, ‘“Thinking is all you need”’, describes ‘Reasoning with/without a plan’, indicating a future or advanced capability of LLMs to perform complex logical operations, potentially with or without explicit pre-defined steps. This slide conceptualises the development trajectory of LLMs from basic conversational ability to advanced reasoning.*",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>LLM: Evolution of competence</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nThis chapter details a conceptual inquiry into the representation of Sustainable Development Goal (SDG)-related research within bibliometric databases, employing Large Language Models (LLMs) to detect inherent biases. Ottaviani and Stahlschmidt aimed to utilise LLMs as a technological tool for assessing biases in publications classified across three prominent bibliometric databases: Web of Science, Scopus, and OpenAlex. Their study highlights the critical, yet performative, role of these databases in the sociology of science, acknowledging their influence on academic behaviour, funding, and policy, whilst also noting their susceptibility to political and commercial interests.\nThe research team’s methodology involved training a pre-existing, open-source LLM, DistilGPT2, on a shared corpus of over 15 million publications classified by the three databases. This approach circumvented the resource-intensive process of training an LLM from scratch, whilst ensuring minimal prior semantic knowledge. The project specifically focused on five SDGs related to socio-economic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure).\nA key component of the research design involved crafting 80-120 specific prompts for each SDG, derived from their respective targets, to serve as a benchmark for compliance and bias detection. The fine-tuned LLM processed these prompts using various decoding strategies (top-k, nucleus, contrastive search), generating responses from which noun phrases were extracted. Analysis of these noun phrases across four dimensions—locations, actors, data/metrics, and focuses—revealed a systematic overlook in the bibliometric data. The LLM’s responses consistently neglected disadvantaged categories of individuals, the poorest countries, and sensitive, underrepresented topics explicitly addressed by SDG targets, instead concentrating on economically powerful and highly developed nations. This finding underscores the decisive impact of seemingly objective science-informed practices, such as bibliometric classification, on research representation and, consequently, on policy and resource allocation. The authors acknowledge limitations, including the general framework and the high sensitivity of LLMs to architectural choices, training data, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-background-and-aims",
    "href": "chapter_ai-nepi_009.html#research-background-and-aims",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 Research Background and Aims",
    "text": "9.1 Research Background and Aims\n\n\n\nSlide 02\n\n\nOttaviani and Stahlschmidt embarked on this project with the primary aim of employing Large Language Models (LLMs) as a technological instrument to discern biases. Specifically, they sought to identify biases originating from publications classified within three prominent bibliometric databases. This foundational objective guided their subsequent methodological development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 SDG Classification in Databases",
    "text": "9.2 SDG Classification in Databases\n\n\n\nSlide 06\n\n\nBibliometric databases function as critical digital infrastructures, enabling extensive bibliometric analyses and impact assessments across the scientific community. These systems, however, exhibit a performative nature, fundamentally shaped by particular understandings of the scientific landscape and inherent value attributions, as scholarly works by Whitley (2000) and Winkler (1988) demonstrate. Consequently, they exert considerable influence over the conduct of academics, researchers, funding institutions, and policymakers alike. Crucially, these databases also reflect and respond to diverse political and commercial interests.\nIn recent years, leading bibliometric platforms, including Web of Science, Scopus, and OpenAlex, have implemented classifications designed to align publications with the United Nations Sustainable Development Goals (SDGs). Nevertheless, prior research, notably by Armitage et al. (2020), has revealed that SDG labelling practices across different providers—such as Elsevier, Bergen, and Aurora—produce markedly divergent results, exhibiting very limited overlap. These inconsistencies in classification can distort perceptions of research priorities, thereby potentially affecting resource allocation and policy decisions, often influenced by underlying political and commercial agendas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "href": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Case Study: SDGs in Bibliometric Data",
    "text": "9.3 Case Study: SDGs in Bibliometric Data\n\n\n\nSlide 08\n\n\nThis case study, conducted by Ottaviani and Stahlschmidt (2024), specifically investigates the aggregated effects on the representation of SDG-related research within bibliometric databases following the introduction of LLM-based tools. The authors adopted a methodological approach that involved deploying “little” pre-trained Large Language Models, notably DistilGPT2. They trained these models independently on distinct subsets of publication abstracts, each corresponding to the SDG classifications derived from various bibliometric databases. This LLM technology served a dual purpose: firstly, as a detector of inherent biases within the data; and secondly, as a proof-of-concept exercise demonstrating its potential for automating information extraction to inform research-related decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "href": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 Chain of Dependencies",
    "text": "9.4 Chain of Dependencies\n\n\n\nSlide 10\n\n\nThe research delineates a partial chain of dependencies, illustrating the intricate relationships within the science-policy interface. Initially, SDG classification directly defines the scope and nature of “Research” on SDGs. Subsequently, a diverse array of stakeholders, including researchers, Small and Medium-sized Enterprises (SMEs), governments, and various intermediate figures, actively process this SDG-focused research. This processed research then critically informs “Decision-making to align with SDGs,” which, in turn, directly impacts “Socioeconomic inequalities.”\nCrucially, the study positions the LLM as a “detector of ‘biases’” operating at the level of “Research” on SDGs. The “Introduction of LLM in Research Policy” emerges as a direct consequence of this bias detection, and this intervention ultimately influences “Socioeconomic inequalities.” Fundamentally, LLMs possess the capacity to alter the metadata associated with SDG research, thereby profoundly influencing the advice provided, choices made, indicators developed, and measures implemented within policy frameworks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Actors and SDG Selection",
    "text": "9.5 Actors and SDG Selection\nThe study focused on three principal bibliometric databases: Web of Science, a proprietary platform from Clarivate (US); Scopus, another proprietary service provided by Elsevier (UK); and OpenAlex, an open-access resource formerly associated with Microsoft (US). The authors strategically selected five Sustainable Development Goals to specifically model socio-economic inequalities. These included three SDGs representing the equity or socio dimension: SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities). Additionally, two SDGs were chosen to represent the economic and technological development dimension: SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data",
    "href": "chapter_ai-nepi_009.html#processed-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Processed Data",
    "text": "9.6 Processed Data\nThe research team processed a substantial dataset, comprising a jointly indexed subset of 15,471,336 publications. They meticulously collected these publications by identifying those shared across all three bibliometric databases—Web of Science, Scopus, and OpenAlex—through precise DOI matching. This collection spanned a timeframe from January 2015 to July 2023. The project then assessed the performance of the three distinct classification standards for the five selected SDGs. Consequently, for each SDG, three separate publication subsets emerged, each attributed to a specific bibliometric database. This rigorous approach, applying classification solely to the shared corpus, established a robust benchmark for subsequent comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 SDG Classification: Socio Dimension",
    "text": "9.7 SDG Classification: Socio Dimension\nInitial results from the comparative analysis of SDG-classified papers, focusing on the socio dimension, strongly corroborate the findings of Armitage (2020), demonstrating a consistently minimal overlap in SDG labelling across the bibliometric databases.\nFor SDG 04, pertaining to Quality Education:\n\nWeb of Science identified 124,359 publications (19.1%).\nOpenAlex identified 218,907 (33.6%).\nScopus identified 339,063 (52.2%).\n\nThe overlaps comprised:\n\n59,002 (9.0%) between Web of Science and OpenAlex.\n35,733 (5.5%) between Web of Science and Scopus.\n35,733 (5.5%) between OpenAlex and Scopus.\nA mere 46,711 (7.2%) across all three platforms.\n\nRegarding SDG 05, Gender Equality:\n\nWeb of Science classified 37,324 publications (57.4%).\nOpenAlex classified 71,727 (9.4%).\nScopus classified 82,273 (12.7%).\n\nThe overlaps were similarly low:\n\n31,210 (4.8%) for Web of Science and OpenAlex.\n26,377 (4.1%) for Web of Science and Scopus.\n34,898 (5.4%) for OpenAlex and Scopus.\n38,066 (12.1%) for all three.\n\nA notable observation emerged for SDG 05: whilst Scopus contained publications relevant to this goal, it often did not classify them as such. Furthermore, Web of Science classified approximately 10% of its SDG 05 publications from the field of mathematics, including topics such as geometrical differential equations, suggesting potential misclassification or a failure to capture relevant content comprehensively.\nFor SDG 10, focused on Reducing Inequalities:\n\nWeb of Science identified 95,460 publications (12.2%).\nOpenAlex identified 213,419 (43.3%).\nScopus identified 236,665 (30.2%).\n\nThe overlaps were:\n\n25,277 (3.2%) for Web of Science and OpenAlex.\n22,540 (2.9%) for Web of Science and Scopus.\n18,850 (2.4%) for OpenAlex and Scopus.\nOnly 10,853 (1.3%) across all three databases.\n\nThese figures consistently highlight the significant discrepancies in how different bibliometric databases classify SDG-related research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 SDG Classification: Economic Dimension",
    "text": "9.8 SDG Classification: Economic Dimension\nThe comparative analysis extended to the economic dimension of SDG-classified papers, revealing similar patterns of limited overlap.\nFor SDG 08, focusing on Decent Work and Economic Growth:\n\nWeb of Science identified 82,366 publications (19.0%).\nOpenAlex identified 121,106 (28.0%).\nScopus identified 183,641 (37.8%).\n\nThe overlaps comprised:\n\n16,268 (3.8%) for Web of Science and OpenAlex.\n8,020 (1.9%) for Web of Science and Scopus.\n39,071 (9.0%) for OpenAlex and Scopus.\n10,853 (2.5%) across all three databases.\n\nSimilarly, for SDG 09, concerning Industry, Innovation, and Infrastructure:\n\nWeb of Science classified 230,883 publications (30.3%).\nOpenAlex classified 217,822 (28.6%).\nScopus classified 200,566 (26.4%).\n\nOverlaps included:\n\n25,679 (3.4%) for Web of Science and OpenAlex.\n26,501 (3.5%) for Web of Science and Scopus.\n44,702 (5.9%) for OpenAlex and Scopus.\n15,186 (2.0%) for all three.\n\nThese figures consistently underscore the significant discrepancies in how different bibliometric databases classify SDG-related research, even within the economic dimension.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "href": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.9 LLM Technology and Key Findings",
    "text": "9.9 LLM Technology and Key Findings\nOttaviani and Stahlschmidt modelled the Large Language Model (LLM) technology with a specific objective: to develop an LLM whose knowledge base originated exclusively from publications classified under a particular Sustainable Development Goal by a given bibliometric database. Recognising the substantial resources required to train an LLM from scratch, they adopted a pragmatic compromise: fine-tuning an existing, pre-trained, open-source LLM. DistilGPT2 emerged as the chosen model, primarily due to its fundamental architecture and minimal prior knowledge, which ensured the absence of pre-existing semantic understanding concerning either publications or prompts.\nThe primary finding from this investigation highlights a systematic overlook within the data, specifically regarding certain actors, the world’s poorest countries, and various underrepresented topics. However, the authors acknowledge several limitations. Their framework remains general, implying that specific applied cases might yield different outcomes. Moreover, LLMs inherently demonstrate high sensitivity to their underlying model architecture, the characteristics of their training data, the chosen (hyper-)parameters, and the decoding strategies employed. Whilst Ottaviani and Stahlschmidt partially accounted for training data variability by incorporating three distinct databases, and for decoding strategies by utilising three methods from existing literature, the need for more advanced LLM architectures persists.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "href": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.10 SDG 4: Quality Education Targets",
    "text": "9.10 SDG 4: Quality Education Targets\nSustainable Development Goal 4 mandates the assurance of inclusive and equitable quality education, alongside the promotion of lifelong learning opportunities for all. A series of specific targets underpins this overarching goal, each outlining a crucial aspect of educational development.\nFor instance:\n\nTarget 4.1 stipulates that by 2030, all girls and boys must complete free, equitable, and high-quality primary and secondary education, leading to demonstrably relevant and effective learning outcomes.\nTarget 4.2 focuses on ensuring universal access to quality early childhood development, care, and pre-primary education by the same year, preparing children for primary schooling.\nTarget 4.3 aims for equal access to affordable and quality technical, vocational, and tertiary education, including university, for all women and men by 2030.\nTarget 4.4 seeks to substantially increase the number of young people and adults possessing relevant skills, particularly technical and vocational proficiencies, for employment, decent work, and entrepreneurship.\nTarget 4.5 addresses the elimination of gender disparities in education and guarantees equal access to all educational levels and vocational training for vulnerable populations, including individuals with disabilities, indigenous peoples, and children in precarious situations.\nTarget 4.6 commits to ensuring that all youth and a significant proportion of adults, irrespective of gender, achieve literacy and numeracy by 2030.\n\nThese targets collectively form the framework of the 2030 Agenda for SDGs, as defined by the United Nations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.11 Prompts for Benchmarking",
    "text": "9.11 Prompts for Benchmarking\nTo establish a robust benchmarking system, Ottaviani and Stahlschmidt meticulously crafted prompts for each Sustainable Development Goal. Each SDG comprises a comprehensive list of between eight and twelve specific targets. For every individual target, the authors systematically developed ten diverse questions, or prompts, with each question designed to explore a distinct facet of that target. This rigorous process yielded a specific set of 80 to 120 prompts for each SDG, collectively serving as a critical benchmark or standard for both defining compliance with the SDGs and identifying potential biases within the data. For instance, prompts such as, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” interrogated Target 4.1, which aims to ensure that all girls and boys complete free, equitable, and quality primary and secondary education by 2030.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "href": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.12 Research Design: Database and SDG Specificity",
    "text": "9.12 Research Design: Database and SDG Specificity\nThe research design meticulously outlines the process for analysing SDG-related content within bibliometric databases. The workflow commenced with an initial input comprising a set of abstracts, each classified according to a specific SDG and originating from a particular database. The authors then applied a crucial fine-tuning process to this dataset, utilising the DistilGPT-2 model. The output of this fine-tuning was a specialised “Fine-tuned DistilGPT-2” model, tailored to each specific SDG and database combination.\nSubsequently, a dedicated set of prompts, crafted specifically for each SDG, served as input to the fine-tuned LLM. To generate diverse and relevant outputs, Ottaviani and Stahlschmidt applied three distinct decoding strategies: top-k, nucleus, and contrastive search. The LLM produced corresponding responses for each SDG and database, categorised by the decoding strategy employed. A final post-processing step involved applying a “prompts’ words filter” to these responses, ultimately yielding a refined set of noun phrases pertinent to each SDG and database.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "href": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.13 Illustrative Example: SDG 4 Analysis",
    "text": "9.13 Illustrative Example: SDG 4 Analysis\nAs an illustrative example, the analysis of SDG 4 involved a systematic method of matching noun phrases, extracted from the LLM’s responses, directly against the specific SDG targets. Ottaviani and Stahlschmidt employed four critical data dimensions for this analysis: Locations, Actors, Data/Metrics, and Focuses. For each SDG, this comprehensive approach facilitated a dual assessment: firstly, evaluating the degree of compliance with its stated targets; and secondly, identifying any inherent biases within the data. Notably, the analysis consistently revealed distinct differences across the various bibliometric databases. A structured table organised these findings, categorising them by unique databases and indicating which targets were addressed or not addressed across the dimensions of locations, actors, data/metrics, and focuses.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "href": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.14 Systematic Overlooks by the LLM",
    "text": "9.14 Systematic Overlooks by the LLM\nDespite the LLM’s stimulation, its responses consistently overlooked several critical areas, revealing systematic biases within the underlying data. Geographically, the model rarely addressed African countries, with the notable exception of South Africa, and largely ignored other developing nations, including China (though this observation carried a degree of uncertainty). The model also largely overlooked least developed countries and small island developing states.\nIn terms of human actors, the LLM systematically overlooked vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Furthermore, the model failed to adequately focus on several key thematic areas explicitly outlined in the SDGs. These included vocational training, scholarships, the establishment of safe, non-violent, inclusive, and effective educational environments, the promotion of sustainable lifestyles, human rights, the cultivation of a culture of peace and non-violence, global citizenship, and the appreciation of cultural diversity. Crucially, the LLM also neglected to address the fundamental concepts of free primary and secondary education, and tertiary education.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.15 Cross-SDG Considerations",
    "text": "9.15 Cross-SDG Considerations\nAcross the five Sustainable Development Goals analysed, several consistent patterns emerged. Geographically, the LLM’s responses rarely addressed least developed countries, with South-Saharan Africa, for instance, receiving minimal attention in relation to SDG8. The United States maintained an undeniable dominance in location mentions, followed by South Africa and China as the most frequently cited countries, with the UK and Australia also featuring prominently.\nRegarding metrics, the LLM’s output included references to various data sources, such as the Demographic and Health Surveys (DHS) and the World Values Survey (WVS), alongside mentions of general metrics, indicators, and benchmarks. The discussed research methodologies spanned theoretical frameworks, empirical studies, thematic analysis, market dynamics, and macroeconomics. However, concerning human actors, the analysis consistently revealed a systematic overlook of discriminated and vulnerable categories across all SDGs. Furthermore, whilst the LLM addressed some SDG-specific focuses, it notably omitted the most sensitive topics, such as human trafficking, human exploitation, and migration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-summary",
    "href": "chapter_ai-nepi_009.html#case-study-summary",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.16 Case Study Summary",
    "text": "9.16 Case Study Summary\nThe case study’s findings reveal a critical insight: the integration of Large Language Models as an analytical AI tool, positioned between SDG classification and the policymaking process, exposes a systematic oversight within the scientific publications classified by SDGs. This oversight consistently neglects the most disadvantaged categories of individuals, the world’s poorest countries, and crucial underrepresented topics explicitly targeted by the SDGs. Conversely, the analysis demonstrates that economic superpowers and highly developing nations receive disproportionate attention. These results unequivocally highlight the profound and decisive impact of seemingly objective, science-informed practices, such as the bibliometric classification of SDGs, on the representation of global challenges.\nThe authors acknowledge several limitations. The LLM’s performance exhibits high sensitivity to its model architecture, the characteristics of its training data (though partially mitigated by incorporating three distinct databases), the chosen (hyper-)parameters, and the decoding strategies employed (which were also partially accounted for). Furthermore, the research operates within a general framework, suggesting that its applicability to highly specific contexts may require further investigation. Future work should therefore explore the development of more sophisticated LLM architectures to address these sensitivities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "",
    "text": "Overview\nThis chapter details an innovative approach to extracting citation data from law and humanities scholarship, a domain historically underserved by conventional bibliometric databases. The research team has developed a specialised methodology to overcome the challenges posed by complex, often multilingual, footnotes and the poor coverage of non-STEM fields in existing data sources. At the core of this solution lies the strategic leverage of Large Language Models (LLMs) and Vision Language Models (VLMs), underpinned by a meticulously crafted, TEI XML-encoded gold standard dataset.\nThe project directly addresses critical limitations inherent in current bibliometric tools, such as Web of Science, Scopus, and OpenAlex. These platforms frequently lack comprehensive coverage for pre-digital, non-English, or non-“A-journal” content, whilst also imposing prohibitive costs and restrictive licences. Furthermore, the intricate nature of humanities footnotes, often laden with commentary and varied citation styles, renders traditional machine learning tools largely ineffective.\nTo ensure the reliability of LLM-extracted data, the team has established a robust testing and evaluation framework. This framework relies on a high-quality gold standard dataset, comprising over 1,000 footnotes from 20 open-access articles across multiple languages and historical periods, yielding more than 1,500 references. This dataset is encoded in TEI XML, a well-established standard in digital humanities, which facilitates detailed contextual markup beyond mere reference management.\nA key technological contribution from the authors is “Llamore”, a lightweight Python package engineered for LLM-based reference extraction and performance evaluation. Llamore extracts citation data from raw text or PDFs, outputting TEI XML, and assesses extraction accuracy using the F1-score. It employs an unbalanced assignment problem solver to align extracted references with gold standard data, thereby maximising the total F1-score for robust evaluation.\nInitial evaluations demonstrate Llamore’s efficacy. When tested against the PLOS 1000 biomedical dataset, Llamore (using Gemini 2.0 Flash) achieved an F1-score of 0.62, comparable to Grobid’s 0.61. Crucially, on the specialised footnoted humanities dataset, Llamore significantly outperformed Grobid, achieving an F1-score of 0.45 compared to Grobid’s 0.14. Whilst Grobid remains more resource-efficient for its trained literature, Llamore proves three times more effective for complex footnoted content. This research paves the way for generating comprehensive citation graphs, enabling deeper insights into intellectual history, influence reconstruction, and the reception of ideas within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "href": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.1 Citation Graphs in Intellectual History",
    "text": "10.1 Citation Graphs in Intellectual History\n\n\n\nSlide 02\n\n\nScholars primarily employ citation graphs to illuminate patterns and relationships within knowledge production, particularly in the realm of intellectual history. These analytical tools prove invaluable for reconstructing influences and meticulously measuring the reception of published ideas. Consequently, researchers can identify, for instance, the most-cited authors across specific timeframes. An illustrative example involves an analysis of the Journal of Law and Society, for which an interactive web application provides detailed insights.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "href": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.2 Deficiencies in Existing Bibliometric Databases for SSH",
    "text": "10.2 Deficiencies in Existing Bibliometric Databases for SSH\n\n\n\nSlide 03\n\n\nA significant challenge arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) within existing bibliometric data sources. The research team deems these databases, including prominent platforms such as Web of Science, Scopus, and OpenAlex, fundamentally unusable for the specific domain under investigation, primarily owing to their pervasive lack of relevant data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "href": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.3 Limitations of Commercial and Open Bibliometric Databases",
    "text": "10.3 Limitations of Commercial and Open Bibliometric Databases\n\n\n\nSlide 04\n\n\nBeyond the general inadequacy, specific limitations plague both commercial and open bibliometric databases. Web of Science and Scopus, for instance, impose exorbitant costs and operate under highly restrictive licences; consequently, the authors advocate for a decisive shift away from reliance on these proprietary systems. Whilst OpenAlex offers the significant advantage of open access, it nonetheless presents considerable shortcomings for Social Sciences and Humanities research. Specifically, it frequently lacks comprehensive coverage for numerous “A-journals,” provides insufficient data from the pre-digital era, and typically omits content published in languages other than English.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "href": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity",
    "text": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity\n\n\n\nSlide 05\n\n\nThe Zeitschrift für Rechtssoziologie, a German Journal for Law and Society established in 1980, exemplifies the pervasive issue of poor data coverage within the humanities. Its citation data, as observed in bibliometric databases, only demonstrates significant improvement after the year 2000, with minimal records available for the preceding decades. This deficiency stems from several factors. Primarily, the humanities attract less commercial interest compared to STEM, medicine, and economics, which typically dominate large bibliometric databases. Moreover, these databases prioritise the “impact factor” for scientific evaluation, a metric largely irrelevant to research in intellectual history.\nCrucially, the literature of interest in humanities scholarship frequently features highly complex footnotes, colloquially termed “footnotes from hell.” These are not mere citations; they often incorporate extensive commentary and various “messy” elements, all embedded within a considerable amount of non-citation “noise,” making automated extraction profoundly challenging.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "href": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.5 Limitations of Traditional Tools and Promise of LLMs",
    "text": "10.5 Limitations of Traditional Tools and Promise of LLMs\nCreating training data presents a formidable challenge, necessitating a labour-intensive annotation process that demands substantial time investment. Furthermore, existing tools, particularly those reliant on traditional machine learning methods such as Conditional Random Forests, exhibit poor performance when confronted with complex footnotes. For instance, ExCite’s performance, as documented by Boulanger and Iurshina in 2022, reveals consistently low extraction and segmentation accuracies across various training datasets.\nNevertheless, Large Language Models (LLMs) offer a promising avenue for resolution. Early experiments conducted in 2022 with models like text-davinci-003 already demonstrated the significant power of LLMs in extracting references from highly unstructured textual data. Newer models promise even better results, whilst Vision Language Models (VLMs) extend this capability to direct PDF processing. The research team is currently exploring various methods to harness these advanced models effectively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "href": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.6 Ensuring Trustworthiness and Robust Evaluation",
    "text": "10.6 Ensuring Trustworthiness and Robust Evaluation\n\n\n\nSlide 13\n\n\nA fundamental concern centres on the trustworthiness of results generated by Large Language Models. Instances of LLMs fabricating non-existent citations, as exemplified by a lawyer’s disastrous use of ChatGPT in federal court, underscore this critical issue. Consequently, a guiding principle dictates against attempting to solve problems for which no validation data exists.\nTo address this, the authors necessitate a robust testing and evaluation solution. This solution comprises three essential components:\n\nA high-quality Gold Standard dataset.\nA flexible framework capable of adapting to the rapidly evolving technological landscape.\nSolid testing and evaluation algorithms designed to produce comparable metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "href": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.7 TEI-Annotated Gold Standard Dataset Development",
    "text": "10.7 TEI-Annotated Gold Standard Dataset Development\n\n\n\nSlide 16\n\n\nThe research team has embarked upon compiling a comprehensive training dataset, specifically designed for dual utility as evaluation data, employing TEI XML encoding. This choice stems from TEI’s status as a well-established, meticulously specified, and comprehensive standard for text interchange within the humanities and digital editorics. Unlike more constrained bibliographical standards such as CSL or BibTeX, TEI encompasses a broader array of phenomena, extending beyond mere reference management to facilitate the encoding of contextual information, including citation intention. Furthermore, its adoption enables the integration of existing TEI XML corpora from various digital editorics projects, thereby supporting the testing of generalisation and robustness features.\nDespite its advantages, the TEI standard presents certain challenges, both conceptual—concerning the distinction between pointers and references—and technical—regarding constrained elements versus elliptic material. The dataset development process involves several stages: initially, capturing PDF screenshots; subsequently, segmenting the reference string to isolate the citation from surrounding non-reference text within footnotes; and finally, parsing the content into a structured data format, utilising TEI elements such as biblStruct, analytic, monogr, author, title, imprint, date, biblScope, and biblRef. This dataset is currently under active development.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "href": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.8 Dataset Strategy Evolution and Tooling Integration",
    "text": "10.8 Dataset Strategy Evolution and Tooling Integration\nThe strategy for constructing this dataset has evolved significantly. Initially, the focus centred on compiling data directly relevant to the primary research question. More recently, however, the authors made a strategic decision to incorporate PDFs, thereby enabling the utilisation of Vision Language Model (VLM) mechanisms. The overarching aim now involves publishing the complete dataset, encompassing everything from the raw PDFs to the meticulously parsed references. To achieve this, the team is sourcing material from open-access journals. The current scope involves coding over 1,000 footnotes derived from 20 articles, spanning several languages and a broad historical timeframe, which are expected to yield more than 1,500 references. Notably, even multiple occurrences of the same work are encoded separately to capture their distinct contexts.\nA significant benefit of adopting the TEI XML standard lies in the extensive tooling available for this interoperable format. Grobid, a widely recognised tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. This alignment facilitates direct performance comparisons with Grobid and enables the provision of new training data to the Grobid team, fostering collaborative advancement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "href": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package",
    "text": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package\nThe research team has developed “Llamore”, an acronym for Large LANguage MOdels for Reference Extraction, as a dedicated Python package. This tool serves a dual purpose: it extracts citation data from either raw text or PDF documents, leveraging multimodal Large Language Models, and subsequently evaluates the performance of this extraction. Specifically, Llamore processes textual or PDF inputs to generate references in TEI XML format, whilst also accepting gold standard references to produce an F1-score as an evaluation metric. The design of Llamore prioritises two key objectives: it remains lightweight, comprising fewer than 2,000 lines of code, and ensures broad compatibility with both open and closed Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "href": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.10 Llamore Implementation and Workflow",
    "text": "10.10 Llamore Implementation and Workflow\nLlamore is readily available on Pypi, enabling straightforward installation via pip. The extraction workflow commences with defining an extractor, which is contingent upon the specific Large Language Model selected. For instance, the OpenAI extractor offers broad compatibility, as many open model serving frameworks, including Olama and VLLM, provide OpenAI-compatible APIs. Subsequently, users supply either a PDF or raw text as input to this extractor, which then yields the extracted references. These references can then be exported conveniently to an XML file. For evaluation purposes, users import the dedicated F1 class and provide both the gold standard references and the extracted references to compute the macro average of performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "href": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment",
    "text": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment\nLlamore employs the F1-score as its primary evaluation metric, a well-established standard for comparing structured data. This score represents the harmonic mean of Precision and Recall, where Precision is calculated as the ratio of matches to predicted elements, and Recall as the ratio of matches to gold elements. An F1-score of 1 signifies perfect extraction, whilst a score of 0 indicates no matches whatsoever. For instance, an extracted reference might align perfectly with a gold reference on fields such as analytic_title, monographic_title, surname, and publication_date, yet exhibit a mismatch in the forename due to a minor discrepancy like an extraneous dot in the gold standard.\nA more complex challenge arises in aligning multiple extracted references with their corresponding gold references. Llamore addresses this as an unbalanced assignment problem. The system computes F1-scores for every possible combination of extracted and gold references, subsequently constructing a matrix from these scores. It then leverages SciPy’s solver to maximise the total F1-score whilst ensuring a unique assignment between the extracted and gold references. Following this alignment, the individual F1-scores are macro-averaged to provide an overall performance metric.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "href": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.12 Performance Evaluation of Llamore",
    "text": "10.12 Performance Evaluation of Llamore\nPerformance evaluations of Llamore reveal distinct capabilities across different datasets. When tested against the PLOS 1000 dataset, comprising 1,000 PDFs from the biomedical field, Llamore (utilising Gemini 2.0 Flash) achieved an exact match F1-score of 0.62, closely aligning with Grobid’s score of 0.61. This indicates comparable performance on literature for which Grobid was specifically trained. However, it is crucial to note that Grobid maintains a significant advantage in resource efficiency, requiring orders of magnitude less computational power than Gemini.\nConversely, on the custom humanities dataset, which features complex footnoted literature, Llamore demonstrated a marked superiority. Grobid struggled considerably, yielding an F1-score of merely 0.14, whilst Llamore (Gemini 2.0 Flash) achieved a substantially higher score of 0.45. This represents approximately a threefold improvement in performance for the challenging domain of footnoted humanities scholarship.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "href": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.13 Conclusion and Takeaways",
    "text": "10.13 Conclusion and Takeaways\nGrobid remains the preferred choice for literature upon which it has been specifically trained, primarily owing to its significantly faster processing speed and reduced resource intensity. Conversely, Llamore, when paired with Gemini, demonstrates approximately three times better performance for the challenging domain of footnoted literature. This performance specifically pertains to pure reference extraction, excluding contextual or cross-referencing information.\nA critical aspect of utilising open-source databases, such as OpenAlex, involves the burden of quality assurance falling directly upon the user. The authors advise a cautious approach towards both OpenAlex and commercial databases like Web of Science and Scopus, as their quality assurance priorities may not align with specific research questions. Large Language Models, whilst powerful, can produce false positives by inventing citations, thus necessitating the attainment of reliable results before any large-scale application. The ultimate ambition extends beyond mere reference string extraction to obtaining reliable results for nuanced contextual information, such as whether a citation is approving or not.\nAnalysis reveals that Grobid’s poor performance on humanities data stems from its training data being out of distribution for this domain. Large Language Models, however, exhibit their own distinct failure modes. These include difficulty discerning ambiguous elements, such as whether a number represents a volume or a page, and being misled by capitalisation. They frequently misidentify personal names appearing within titles as authors and struggle with specialised terminology like Idem, Derselbe, passim, ibid, or n.d.. Furthermore, canonical citations found in fields such as Bible studies, Roman law, and classical literature, along with ellipses, abbreviations, and cross-references, present considerable challenges.\nThe requisite F1-score for a gold standard dataset is contingent upon the analytical ambition; a lower score might suffice for identifying broad tendencies, whilst high accuracy demands a much higher score. For less precise needs, such as fuzzy searching in bibliographical databases using only a title and author, an exact match may not be essential. Nevertheless, the current stage of research prioritises achieving highly reliable results to facilitate the extraction of richer contextual information. A human-in-the-loop approach has been considered for the dataset establishment workflow, where Llamore could pre-annotate data for subsequent human correction. However, intermediate stages, such as merely marking the referring string without full parsing, still necessitate substantial manual effort.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#overview",
    "href": "chapter_ai-nepi_011.html#overview",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "This presentation details the development of an AI solution designed to facilitate interaction with scholarly papers, addressing the increasing volume of scientific information. Researchers from DANS, the data archive of the Royal Netherlands Academy of Arts and Sciences, and GESIS, an archive also engaged in research, collaborated on this project. Their primary objective involved constructing an AI system that enables users to \"chat\" with selected academic texts, thereby enhancing information retrieval processes.\n\n  The solution comprises two principal components: Ghostwriter, serving as the user interface, and EverythingData, which manages the underlying backend workflow. Ghostwriter functions as a novel information retrieval interface, allowing simultaneous interaction with both structured data (metaphorically, a \"librarian\" representing knowledge organisation systems) and natural language content (an \"expert\"). This approach aligns with the broader scientific discourse surrounding Retrieval Augmented Generation (RAG), specifically advocating for a GraphRAG methodology.\n\n  The system's core ingredients include a vector space, constructed from data file content encoded as embeddings via various machine learning algorithms and Large Language Models (LLMs), and a graph, which represents a metadata layer integrated with diverse ontologies and controlled vocabularies, including those for responsible AI, expressed using the Croissant ML standard. The vision centres on unifying these graph and vector representations within a single model, creating a local, distributed AI where the LLM acts as both an interface and a reasoning engine. This engine connects to a \"RAG library\" (the graph), navigates datasets, and consumes embeddings (vectors) as contextual information.\n\n  Implementation begins with ingesting a collection of articles, such as those from the *methods, data, analyses* (MDA) journal. EverythingData processes these articles, storing information in a vector store (Qdrant) and performing operations like term extraction, embedding construction, and enrichment. Crucially, the system couples these processes with knowledge graphs, thereby contextualising embeddings and enhancing their value. Users formulate natural language queries within the Ghostwriter interface, prompting the system to return a list of relevant documents and an explanatory summary.\n\n  To prevent hallucinations, the system explicitly states when information is not directly available within the provided text, offering users the option to add new papers. Backend operations involve an entity extraction pipeline that annotates terms with semantic meaning by mapping them to controlled vocabularies, transitioning from vector space to the knowledge graph. Entities are further linked to the Wikidata ontology, providing immediate multilingual support by enabling queries in various languages. The LLM then synthesises these extracted text pieces to produce the final explanatory output.\n\n  The pipeline demonstrates versatility, functioning with any collection, including webpages, RSS feeds, and trusted data repositories like Dataverse instances. This capability enables localised interaction with papers, supporting close reading and containing the search space by situating questions within specific areas of scientific knowledge. The system extracts information beyond explicit text and metadata, forging associations at both natural language and Knowledge Organisation System levels. The Ghostwriter interface specifically facilitates the identification of related documents and the refinement of queries.\n\n  A key innovation lies in decoupling knowledge from questions and papers, storing this knowledge externally (e.g., as Wikidata identifiers). This separation allows for benchmarking different models against established Knowledge Organisation Systems, fostering future advancements in scientific inquiry. The project actively collaborates with industry partners, including Google and Meta, to ensure the sustainability of this KOS-based approach. During a live demonstration, the Ghostwriter interface successfully processed queries regarding \"rational choice theory\" and \"utility in rational choice theory,\" providing accurate summaries and references, whilst showcasing its multilingual capabilities by processing a German-language paper. The system employs a 1 billion parameter LLM (Llama), capable of local execution, and segments papers into small, identifiable blocks, leveraging LLM techniques and knowledge graphs to predict relevant text segments. This local processing capability offers greater control and cost-effectiveness compared to reliance on large, external models, ultimately supporting human thought processes in formulating research questions rather than providing definitive answers. The system has already demonstrated scalability, handling approximately 300,000 documents for Harvard University.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#presentation-objectives-and-core-research-question",
    "href": "chapter_ai-nepi_011.html#presentation-objectives-and-core-research-question",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Presentation Objectives and Core Research Question",
    "text": "11.1 Presentation Objectives and Core Research Question\n\n\n\nSlide 02\n\n\n    This presentation addresses a pivotal research question: whether an AI solution can be effectively constructed to facilitate interactive \"chatting\" with a curated selection of academic papers. The discussion commences by introducing fundamental concepts in information retrieval, exploring the intricate dynamics of human-machine interaction, and detailing the principles of Retrieval-Augmented Generation (RAG) within generative AI.\n\n    A specific use case, drawing upon articles from the *methods, data, analyses* (MDA) journal, illustrates the practical application of the developed system. The presentation then introduces the underlying workflow of a \"local\" or \"tailored AI solution,\" which comprises two key components: Ghostwriter, serving as the interface, and EverythingData, encompassing the comprehensive backend operations. Subsequent sections provide detailed illustrations of both front-end and back-end functionalities, culminating in a summary and an outlook on future developments.\n\n    The broader context for this endeavour stems from the pervasive challenge of managing the overwhelming flood of information in contemporary science. This initiative represents a collaborative effort between DANS, the data archive of the Royal Netherlands Academy of Arts and Sciences, and GESIS, an institution that combines archiving with active research.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-new-paradigm-for-information-retrieval",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-new-paradigm-for-information-retrieval",
    "title": "11  Science dynamics and AI",
    "section": "11.2 The Ghostwriter Interface: A New Paradigm for Information Retrieval",
    "text": "11.2 The Ghostwriter Interface: A New Paradigm for Information Retrieval\n\n\n\nSlide 03\n\n\n    The Ghostwriter approach introduces a novel interface for information retrieval, fundamentally altering how users interact with data. This system conceptualises information access through a series of evolving metaphors, each representing increasing sophistication in interaction. Initially, querying a single database, akin to \"Me and a database,\" necessitates explicit knowledge of the schema and its typical values to yield results, embodying a classic information retrieval challenge.\n\n    Progressing beyond this, the interaction with \"Me and a librarian\" signifies engagement with a single data collection or space, where connected structured databases or graphs operate in the background. This \"librarian\" metaphor specifically refers to structured data, encompassing knowledge organisation systems (KOS) and pre-existing classifications. Further still, the advent of Large Language Models (LLMs) transforms the interaction into \"Me and a library\" or \"Me and a round of experts,\" where natural language becomes the primary mode of engagement.\n\n    Crucially, the Ghostwriter interface claims to enable simultaneous \"chatting with experts and librarians.\" This advanced interaction is facilitated by a local LLM, integrated with a target data collection or space, and embedded within a network of additional data interpretation sources via APIs. This innovative design allows users to leverage both structured knowledge and the nuanced understanding provided by natural language processing in a unified environment.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-and-graphrag-architecture",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-and-graphrag-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.3 Retrieval Augmented Generation (RAG) and GraphRAG Architecture",
    "text": "11.3 Retrieval Augmented Generation (RAG) and GraphRAG Architecture\n\n\n\nSlide 04\n\n\n    Scientifically, this project situates itself within the broader discourse of Retrieval Augmented Generation (RAG). A particularly insightful resource on this topic is Philip Rustle's paper from Neo4j, which offers a comprehensive introduction to the field. The system's architecture relies on two main ingredients: a vector space and a graph.\n\n    The vector space is meticulously constructed from the content of data files, with information encoded into embeddings. Various Machine Learning (ML) algorithms and diverse Large Language Models (LLMs) compute these embeddings. Concurrently, a graph component represents a sophisticated metadata layer. This graph integrates seamlessly with a range of ontologies and controlled vocabularies, notably including those pertaining to responsible AI, and adheres to the Croissant ML standard for its expression.\n\n    The overarching vision for this system involves the integration of both graph and vector representations into a singular, cohesive model, termed GraphRAG. This model is designed for implementation as a \"local\" Distributed AI, where the LLM performs a dual function: serving as the primary interface between human users and the AI, and operating as a robust reasoning engine. In practice, the LLM connects to a \"RAG library,\" which embodies the graph structure. Through this connection, the LLM navigates extensive datasets and consumes embeddings, which represent the vector component, as crucial contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-workflow",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-workflow",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Ghostwriter and EverythingData Workflow",
    "text": "11.4 Ghostwriter and EverythingData Workflow\n    The Ghostwriter and EverythingData workflow commences with a designated collection of articles, exemplified by those from the *methods, data, analyses* (MDA) journal. This system, however, demonstrates versatility, accepting any collection of documents as its input. The \"EverythingData\" component, representing the upper part of the workflow, orchestrates a series of operations.\n\n    Initially, this component stores information within a vector store, specifically utilising Qdrant. Subsequently, it executes various processes, including term extraction, the construction of embeddings, and further enrichments. A critical aspect involves the integration with knowledge graphs, which couples the processed information to contextualise the embeddings. This integration significantly enhances the value of specific words, phrases, and embeddings by providing a richer context.\n\n    All processed data then converges into a unified vector space, termed the RAG-Graph. The Ghostwriter interface directly interacts with this RAG-Graph vector space, allowing users to formulate queries as natural language questions. In response, the system delivers a list of relevant documents and an explanatory summary, generated by the underlying machinery. A key design principle ensures that the system does not hallucinate; it precisely identifies the source of its information.\n\n    From an implementation perspective, the system meticulously splits each paper into small, manageable blocks, assigning a unique identifier to every block. Leveraging advanced LLM techniques, the system intelligently connects and retrieves these blocks, employing weights in conjunction with knowledge graphs to accurately predict which pieces of text will best respond to a specific question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-document-indexing-and-collection-management",
    "href": "chapter_ai-nepi_011.html#ghostwriter-document-indexing-and-collection-management",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Ghostwriter: Document Indexing and Collection Management",
    "text": "11.5 Ghostwriter: Document Indexing and Collection Management\n\n\n\nSlide 06\n\n\n    Ghostwriter functions as a sophisticated tool for indexing documents and webpages, enabling their organisation into distinct collections. The system initially ingested articles from the *methods, data, analyses* (MDA) journal, creating a test collection of approximately 100 articles scraped directly from its website. This demonstrates the system's capacity to process and manage specific datasets.\n\n    Crucially, Ghostwriter exhibits considerable input flexibility, capable of ingesting virtually any content from the web, including spreadsheets, individual webpages, entire websites via crawling, and RSS feeds. A core design principle ensures that the system relies exclusively on the ingested source data, thereby preventing hallucinations. Rather than employing large, complex LLMs, the system leverages a more modest 1 billion parameter LLM to address intricate questions, achieving accuracy through its integration with knowledge graphs. The primary objective remains to furnish only factual information present within the paper, explicitly stating \"I don't know\" if the requested information is unavailable in the source material.\n\n    The user interface, accessible via the \"Ask Questions\" section on the GESIS website, provides an intuitive input field for queries. Users can readily create new collections, manage available collections (such as the pre-selected \"mda\" collection), and add new content through options like \"Single Webpage,\" \"Website Crawler,\" or \"RSS Feed.\" The Ghostwriter system, specifically for MDA papers, is accessible online via `https://gesis.now.museum`.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-chatting-with-papers-and-hallucination-prevention",
    "href": "chapter_ai-nepi_011.html#ghostwriter-chatting-with-papers-and-hallucination-prevention",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter: Chatting with Papers and Hallucination Prevention",
    "text": "11.6 Ghostwriter: Chatting with Papers and Hallucination Prevention\n    Ghostwriter's core functionality centres on enabling users to \"chat\" directly with academic papers. For instance, when presented with the query \"explain male breadwinner model to me,\" the system delivers a concise explanation alongside precise references. These references, unlike those often generated by general-purpose LLMs, are accurate and directly traceable to the original sources, such as \"The Past, Present and Future of Factorial Survey Experiments\" and \"Gender and Survey Participation.\"\n\n    A fundamental design principle ensures that the system does not hallucinate; it maintains an exact awareness of its information sources. This precision is achieved through a meticulous implementation strategy: each paper undergoes a process of segmentation into small, discrete blocks, with a unique identifier assigned to every block. The underlying LLM then employs sophisticated techniques to connect and retrieve these blocks, applying weights in conjunction with knowledge graphs to accurately predict which text segments are most pertinent to a given question. The interface clearly presents the answer, followed by a \"Sources\" section that lists the contributing papers, complete with a \"Chat\" button, title, reference URL, and a relevance score.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-query-refinement-and-content-addition",
    "href": "chapter_ai-nepi_011.html#iterative-query-refinement-and-content-addition",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Iterative Query Refinement and Content Addition",
    "text": "11.7 Iterative Query Refinement and Content Addition\n\n\n\nSlide 08\n\n\n    The Ghostwriter system incorporates an iterative approach to prevent hallucinations, explicitly stating when information is unavailable. For example, if a user poses the question \"explain how data was collected on male breadwinner model,\" and the direct information is not present in the indexed texts, the system will respond by stating, \"According to the provided text, there is no direct information about how data was collected on the male breadwinner model,\" whilst potentially referring to other related articles.\n\n    This design encourages users to actively participate in refining the knowledge base. Should a user locate an article containing the missing information, they can utilise the \"Add Paper\" button to ingest it into the system. Consequently, upon subsequent queries regarding the same topic, the system will then be able to provide a comprehensive response, drawing upon the newly added content. The sources section continues to list relevant papers, such as \"The Past, Present and Future of Factorial Survey Experiments\" and \"Gender of Interviewer Effects,\" even when direct answers are not found for specific sub-queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#behind-the-screens-entity-extraction-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#behind-the-screens-entity-extraction-and-multilinguality",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Behind the Screens: Entity Extraction and Multilinguality",
    "text": "11.8 Behind the Screens: Entity Extraction and Multilinguality\n\n\n\nSlide 09\n\n\n    Behind the Ghostwriter interface, a sophisticated entity extraction pipeline operates, meticulously annotating terms with semantic meaning. This process involves mapping terms to controlled vocabularies, effectively transitioning data from the vector space into a structured knowledge graph. Crucially, the system links these extracted entities to broader knowledge graph representations, notably leveraging Wikidata. This linking is paramount as it establishes a \"ground truth,\" enabling the validation of LLM-generated answers and ensuring their accuracy.\n\n    Furthermore, the system provides immediate and robust multilinguality support. This capability is critical; it allows users to pose questions in one language, such as English, and receive reliable answers even when the source papers are written in entirely different languages, like Chinese or German. Ultimately, the LLM synthesises these disparate pieces of text to produce a coherent, explanatory summary.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#fact-extraction-and-knowledge-organisation-systems",
    "href": "chapter_ai-nepi_011.html#fact-extraction-and-knowledge-organisation-systems",
    "title": "11  Science dynamics and AI",
    "section": "11.9 Fact Extraction and Knowledge Organisation Systems",
    "text": "11.9 Fact Extraction and Knowledge Organisation Systems\n\n\n\nSlide 10\n\n\n    The fact extraction process within Ghostwriter meticulously dissects a user's query into smaller, manageable pieces. The system then maps this query to a comprehensive graph representation, whilst simultaneously annotating the query strings with specific \"facts.\" For instance, a query such as \"explain male breadwinner model to me\" prompts the extraction of pertinent facts, including \"gender roles societal expectations,\" \"male breadwinner model economic systems,\" and \"male breadwinner model social structures,\" amongst others.\n\n    The resulting graph representation encapsulates key concepts like \"gender roles,\" \"male breadwinner model,\" \"economic systems,\" \"patriarchal society,\" and \"feminine role,\" complete with their interrelationships and assigned importance scores. This entire mechanism operates as a dynamic Knowledge Organisation System (KOS), capable of repeated querying. Each iteration can generate new, deeper levels of detail beneath the initial terms, providing an increasingly granular understanding of the subject matter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#linking-entities-to-wikidata-for-multilingual-support",
    "href": "chapter_ai-nepi_011.html#linking-entities-to-wikidata-for-multilingual-support",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Linking Entities to Wikidata for Multilingual Support",
    "text": "11.10 Linking Entities to Wikidata for Multilingual Support\n\n\n\nSlide 11\n\n\n    A crucial step in the Ghostwriter workflow involves linking all extracted entities directly to Wikidata. This process transforms free-form strings into standardised identifiers, which are inherently connected to a wealth of multilingual translations. Consequently, the system gains the profound ability to comprehend questions posed in various languages, ensuring that the underlying knowledge remains accessible regardless of the query's linguistic origin. The similarity measure, which quantifies the relevance of a link, is derived directly from the LLM embeddings. For example, the term \"Male\" might be linked to the Wikidata entity `Q12308941`, representing a male given name, with an associated similarity score of 0.3429.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#multilingual-query-translation",
    "href": "chapter_ai-nepi_011.html#multilingual-query-translation",
    "title": "11  Science dynamics and AI",
    "section": "11.11 Multilingual Query Translation",
    "text": "11.11 Multilingual Query Translation\n    The system's robust multilingual support treats the core query, such as \"bread winner model,\" as a conceptual entity rather than a fixed string. An integrated LLM, specifically Gemma3, then generates translations of this concept into hundreds of languages. This comprehensive set of translations subsequently forms the complete query submitted to the LLM. For instance, the concept is translated into Chinese as \"男性主要收入模式 (nánxì zhǔyào shōurù móshì),\" alongside translations into Czech, Danish, Dutch, English, French, German, Greek, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Slovak, Spanish, Swedish, and Ukrainian, ensuring broad linguistic coverage.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#summary-of-pipeline-capabilities-and-future-vision",
    "href": "chapter_ai-nepi_011.html#summary-of-pipeline-capabilities-and-future-vision",
    "title": "11  Science dynamics and AI",
    "section": "11.12 Summary of Pipeline Capabilities and Future Vision",
    "text": "11.12 Summary of Pipeline Capabilities and Future Vision\n\n\n\nSlide 13\n\n\n    The developed pipeline demonstrates remarkable versatility, functioning effectively with any collection of documents, whether sourced from webpages, websites, RSS feeds, or trusted data repositories such as Dataverse instances. This capability enables the creation of a \"semantic index\" and a localised chatbot for diverse collections, thereby supporting \"close reading\" by human researchers.\n\n    The system strategically contains the search space by situating user questions and their associated collections within specific, relevant areas of the networked scientific knowledge domain. Crucially, it facilitates the acquisition of information that extends beyond what is explicitly stated in the text or annotated in the metadata. This is achieved by forging intricate associations at both the natural language level and within Knowledge Organisation Systems (KOS), effectively simulating a dialogue with the \"experts\" or \"invisible colleges\" that underpin the scholarly papers.\n\n    The Ghostwriter interface itself offers dual functionalities: it assists users in identifying related documents and provides mechanisms for refining questions or queries. A significant innovation lies in the system's ability to decouple knowledge from specific questions and papers, storing this knowledge externally, for instance, as Wikidata identifiers. This separation establishes a robust \"ground truth,\" allowing researchers to benchmark different models by comparing the identifier lists they produce in response to identical questions. This method readily identifies models unsuitable for particular tasks, whilst simultaneously leveraging KOS for the benefit of future generations of scientists. The project actively collaborates with industry leaders, including Google and Meta, to ensure the long-term sustainability of this KOS-centric approach, which the developers firmly believe represents the future of information management in science.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-querying-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#live-demonstration-querying-and-multilingual-capabilities",
    "title": "11  Science dynamics and AI",
    "section": "11.13 Live Demonstration: Querying and Multilingual Capabilities",
    "text": "11.13 Live Demonstration: Querying and Multilingual Capabilities\n\n\n\nSlide 13\n\n\n    A live demonstration showcased the \"Ask Questions\" web interface developed by the GESIS Leibniz-Institut für Sozialwissenschaften. The interface displayed the \"mda\" collection, comprising 37,637 vectors, as the selected dataset.\n\n    When presented with the query \"what is the role of utility in the Rational Choice Theory,\" the system promptly provided a detailed explanation, citing various researchers and studies. The sources section accurately listed the contributing papers, including \"The measurement of utility and subjective probability\" and \"Responding to Socially Desirable and Undesirable Topics,\" complete with their respective reference URLs and scores. A subsequent query, \"explain utility in rational choice theory,\" yielded different, yet relevant, pieces of information, consistently pointing back to the same source papers.\n\n    The system also offers an API, enabling an automatic mode for building agentic architectures, where results can be collected and new insights from papers identified. Users can contribute new content via an \"Add page\" button, ensuring the knowledge base remains current. A compelling aspect of the demonstration involved the system's multilingual capability: whilst the query was posed in English, the primary source paper, \"Die Messung von Nutzen und subjektiven Wahrscheinlichkeiten,\" was entirely in German, save for its abstract. The system successfully processed this, demonstrating its linguistic versatility.\n\n    Technically, the system operates efficiently on a local computer, even accommodating paper training processes, utilising a compact 1 billion parameter LLM. This local deployment offers significant advantages in terms of control and cost-effectiveness, particularly for handling private or sensitive information. The system has already proven its scalability, having been developed for Harvard University to manage approximately 300,000 documents. The developers advocate for this local, controlled approach over a full reliance on external models like ChatGPT, emphasising its role in supporting the human thought process and aiding in the formulation of research questions, rather than merely providing definitive answers. The team actively seeks collaborations with external parties who have concrete research questions, offering resources for try-outs and facilitating the handover of the system for further tinkering and refinement.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "",
    "text": "Overview\nThis report systematically documents the application of Retrieval-Augmented Generation (RAG) systems within philosophy, particularly addressing the discipline’s stringent requirements for linguistic and semantic accuracy. Paul Näger presents a compelling perspective on RAG, highlighting its capacity to resolve core limitations inherent in Large Language Models (LLMs), such as restricted access to full texts, finite context windows, and challenges in attribution. Whilst LLMs excel at generating generalisable statistical rules for text production, they are not designed for verbatim text learning. This poses a critical necessity for philosophical inquiry, which demands deep engagement with original sources and their fine-grained formulations. Consequently, RAG systems emerge as a vital solution.\nNäger’s presentation explores diverse applications, ranging from pedagogical tools to advanced research functionalities. For instance, RAG enables students to interact conversationally with philosophical corpora, such as Locke’s Oeuvre, fostering deeper textual understanding. For researchers, RAG facilitates efficient fact-finding in handbooks, exploration of previously unexamined corpora, identification of passages for close reading, and the precise answering of specific research questions.\nA practical RAG implementation, utilising the Stanford Encyclopedia of Philosophy (SEP) as its data source, demonstrates these capabilities. Initially conceived as a community tool, the project evolved into a qualitative study investigating optimal RAG system configurations for philosophical contexts. This research meticulously examines model choices, including generative LLMs (e.g., gpt-4o-mini) and embedding models, alongside the intricate tuning of hyperparameters such as the number of retrieved documents (top-k), input/output token limits, generation temperature, and chunk size and overlap.\nThe methodology employs a theoretically grounded trial-and-error approach, emphasising the criticality of robust evaluation standards for assessing complex, unstructured philosophical propositions. A key finding reveals that chunking content into main sections, despite their average length (approximately 3000 words) exceeding the embedding model’s cutoff (512 words), yields superior results. This efficacy largely stems from the highly systematised nature of the SEP. Furthermore, the system incorporates reranking as an additional step to enhance retrieval accuracy. It leverages a generative LLM for more advanced semantic differentiation, albeit at increased computational cost.\nUltimately, RAG systems offer significant advantages by integrating verbatim corpora and specialised domain knowledge, thereby reducing hallucinations and enabling precise citation. Their effective deployment, however, necessitates careful tweaking, rigorous evaluation with representative question sets, and the indispensable involvement of domain experts, particularly when exploring unfamiliar corpora. Challenges persist, notably the degradation of answer quality when relevant documents are scarce. Paradoxically, RAGs tend to perform less effectively on broad overview questions, suggesting a need for more flexible, potentially agentic RAG architectures in future developments.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-core-llm-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-core-llm-challenges-with-rag-systems",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.1 Addressing Core LLM Challenges with RAG Systems",
    "text": "12.1 Addressing Core LLM Challenges with RAG Systems\n\n\n\nSlide 03\n\n\nPhilosophical inquiry frequently poses complex questions. One might seek to elucidate Aristotle’s theory of matter within his Physics or trace the evolution of Einstein’s concept of locality across his works, from early relativity papers to his 1948 publication on ‘Quantenmechanik und Wirklichkeit’. Whilst Large Language Models (LLMs) like ChatGPT can furnish reasonably differentiated responses to these queries, they exhibit several fundamental limitations. Retrieval-Augmented Generation (RAG) systems specifically address these challenges, offering a robust framework for enhancing LLM performance in knowledge-intensive domains.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-system-architecture-and-workflow",
    "href": "chapter_ai-nepi_012.html#rag-system-architecture-and-workflow",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.2 RAG System Architecture and Workflow",
    "text": "12.2 RAG System Architecture and Workflow\n\n\n\nSlide 04\n\n\nA RAG system fundamentally integrates external data sources to augment the capabilities of Large Language Models. This architecture necessitates a dedicated data source, which, for philosophical research, might comprise a specific corpus such as the complete works of Aristotle or Einstein. Researchers retrieve relevant documents from this corpus, typically employing semantic search, though hybrid or classic search methods also remain viable options. Subsequently, these retrieved text chunks dynamically augment the prompts directed to the LLM. Crucially, this augmentation process directly resolves a significant limitation of standalone LLMs: their lack of direct access to full, original texts. Whilst LLMs may have encountered these texts during their training, they cannot reliably quote specific passages or avoid factual inaccuracies, often leading to ‘hallucinations’.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overcoming-llm-limitations-access-and-verbatim-learning",
    "href": "chapter_ai-nepi_012.html#overcoming-llm-limitations-access-and-verbatim-learning",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.3 Overcoming LLM Limitations: Access and Verbatim Learning",
    "text": "12.3 Overcoming LLM Limitations: Access and Verbatim Learning\n\n\n\nSlide 05\n\n\nLarge Language Models, despite their sophisticated conversational abilities, inherently lack direct access to the complete texts they discuss. Consequently, when prompted to quote specific sections from a paper, an LLM may either admit its inability or, more problematically, generate fabricated content. This limitation stems from their training methodology; LLMs are not engineered to memorise texts verbatim. Instead, their design explicitly prevents rote learning, compelling them to acquire generalisable statistical rules for text production. Philosophical research, however, with its profound emphasis on linguistic and semantic accuracy, critically depends upon direct engagement with original textual sources and their precise, fine-grained formulations. RAG systems, by providing explicit access to these corpora, directly address this fundamental disparity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-context-window-limitations-and-attribution",
    "href": "chapter_ai-nepi_012.html#addressing-context-window-limitations-and-attribution",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.4 Addressing Context Window Limitations and Attribution",
    "text": "12.4 Addressing Context Window Limitations and Attribution\n\n\n\nSlide 06\n\n\nBeyond facilitating direct textual access, RAG systems adeptly navigate two further critical challenges posed by Large Language Models. Firstly, they mitigate the issue of a limited context window. Although models like ChatGPT 4.0 boast a substantial context of 128,000 tokens, extensive corpora can rapidly exhaust this capacity. RAG systems circumvent this by intelligently retrieving and supplying only the most pertinent text chunks, thereby ensuring that the LLM operates within its operational limits whilst still receiving highly relevant information. Secondly, RAG systems inherently resolve the attribution problem. They furnish explicit citations for the information provided, mirroring the functionality observed in platforms like Perplexity, where numbered references link claims directly to their source documents. This capability is paramount for academic rigour, ensuring the verifiability and trustworthiness of generated content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-system-workflow-query-retrieval-augmentation-generation",
    "href": "chapter_ai-nepi_012.html#rag-system-workflow-query-retrieval-augmentation-generation",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.5 RAG System Workflow: Query, Retrieval, Augmentation, Generation",
    "text": "12.5 RAG System Workflow: Query, Retrieval, Augmentation, Generation\n\n\n\nSlide 07\n\n\nThe operational workflow of a RAG system follows a systematic, multi-stage process. Initially, a user submits a query to a dedicated application. This application then initiates a retrieval query, directing it towards various data sources, which may include vector databases or APIs. Upon receiving this query, the data sources return relevant text chunks to the application. Crucially, the application then combines the original user query with these newly retrieved chunks, forming an ‘augmented’ query. This enriched input is subsequently transmitted to a Large Language Model (LLM) for processing. The LLM, leveraging both the query and the contextual chunks, generates a comprehensive answer, which it relays back to the application. Finally, the application delivers this refined answer to the user, ensuring that responses are both informative and grounded in specific, verifiable sources.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#philosophical-applications-of-rag-systems-didactics-and-research",
    "href": "chapter_ai-nepi_012.html#philosophical-applications-of-rag-systems-didactics-and-research",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.6 Philosophical Applications of RAG Systems: Didactics and Research",
    "text": "12.6 Philosophical Applications of RAG Systems: Didactics and Research\n\n\n\nSlide 08\n\n\nRAG systems offer transformative potential across various facets of philosophical engagement. Fundamentally, they enable conversational interaction with extensive philosophical corpora, such as Locke’s complete works, mirroring the intuitive style of ChatGPT whilst providing significantly more detailed domain knowledge and a verifiable verbatim text basis. This capability proves invaluable for didactics; repeated questioning becomes a highly instructive method for students to progressively deepen their understanding of complex texts, moving from general concepts to intricate details. For instance, students can explore Locke’s epistemology or his theory of matter.\nMoreover, RAG systems hold considerable promise for research. They facilitate efficient fact-finding within handbooks, streamlining the process of locating specific information for orientation, remarks, or footnotes. Researchers can also employ these systems to explore previously unexamined corpora, provided the texts are first digitised, gaining a comprehensive overview of their contents. Furthermore, RAG aids in identifying precise passages for close reading that directly pertain to specific research questions. Ultimately, these systems aspire to furnish detailed answers to at least components of complex research questions, painting a compelling vision for future philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-for-philosophical-corpora-enhanced-domain-knowledge",
    "href": "chapter_ai-nepi_012.html#rag-for-philosophical-corpora-enhanced-domain-knowledge",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.7 RAG for Philosophical Corpora: Enhanced Domain Knowledge",
    "text": "12.7 RAG for Philosophical Corpora: Enhanced Domain Knowledge\n\n\n\nSlide 09\n\n\nThe overarching concept driving the application of RAG systems in philosophy centres on enabling conversational interaction with extensive philosophical corpora, such as the complete works of John Locke. This approach aims to replicate the intuitive user experience of platforms like ChatGPT. Crucially, however, it significantly enhances the interaction by providing a far more detailed domain-specific knowledge base and, critically, a verifiable verbatim textual foundation, ensuring scholarly rigour and precision.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#pedagogical-utility-deepening-textual-engagement",
    "href": "chapter_ai-nepi_012.html#pedagogical-utility-deepening-textual-engagement",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.8 Pedagogical Utility: Deepening Textual Engagement",
    "text": "12.8 Pedagogical Utility: Deepening Textual Engagement\n\n\n\nSlide 10\n\n\nFor pedagogical purposes, RAG systems offer a remarkably effective instructional approach. Students can engage with challenging philosophical texts, such as Locke’s Essay Concerning Human Understanding, by initiating a conversational dialogue. This allows them to begin with broad inquiries, like Locke’s general philosophical tenets, and then progressively delve into more specific areas, such as his ideas on epistemology or his theory of matter. Through this iterative questioning, RAG systems provide a dynamic and instructive pathway for students to achieve a profound understanding of complex textual content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#research-applications-fact-finding-and-corpus-exploration",
    "href": "chapter_ai-nepi_012.html#research-applications-fact-finding-and-corpus-exploration",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.9 Research Applications: Fact-Finding and Corpus Exploration",
    "text": "12.9 Research Applications: Fact-Finding and Corpus Exploration\n\n\n\nSlide 11\n\n\nIn the realm of research, RAG systems prove indispensable for tasks such as fact-finding within handbooks, providing essential orientation, facilitating remarks, and generating accurate footnotes. Historically, scholars manually consulted physical books; now, whilst LLMs can offer information, its reliability remains questionable, often leading to hallucination. Consequently, robust RAG systems become critical for ensuring the veracity of retrieved facts. Furthermore, these systems enable the exploration of previously unexamined corpora. Once digitised, such texts can be interrogated conversationally, yielding deeper overviews of their content and unlocking new avenues for scholarly investigation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advanced-research-applications-close-reading-and-question-answering",
    "href": "chapter_ai-nepi_012.html#advanced-research-applications-close-reading-and-question-answering",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.10 Advanced Research Applications: Close Reading and Question Answering",
    "text": "12.10 Advanced Research Applications: Close Reading and Question Answering\n\n\n\nSlide 12\n\n\nBeyond basic fact-finding, RAG systems significantly advance philosophical research by enabling the precise identification of passages for close reading that directly pertain to a specific research question. Ultimately, these systems hold the potential to furnish detailed answers, at least to components of complex research questions. This capability paints a compelling vision for the future of philosophical inquiry, promising to streamline and deepen scholarly engagement with intricate textual and conceptual challenges.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#example-rag-implementation-stanford-encyclopedia-of-philosophy",
    "href": "chapter_ai-nepi_012.html#example-rag-implementation-stanford-encyclopedia-of-philosophy",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.11 Example RAG Implementation: Stanford Encyclopedia of Philosophy",
    "text": "12.11 Example RAG Implementation: Stanford Encyclopedia of Philosophy\n\n\n\nSlide 13\n\n\nPaul Näger and his team developed an illustrative RAG system, leveraging the Stanford Encyclopedia of Philosophy (SEP) as its foundational data source. They systematically scraped the content of this well-regarded online handbook and converted it into markdown format, preparing it for integration into the RAG architecture.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#initial-aim-community-tool",
    "href": "chapter_ai-nepi_012.html#initial-aim-community-tool",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.12 Initial Aim: Community Tool",
    "text": "12.12 Initial Aim: Community Tool\n\n\n\nSlide 14\n\n\nInitially, Näger’s project aimed to develop a practical and beneficial tool specifically for the academic community, providing a valuable resource for philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#evolving-aims-from-tool-to-qualitative-study",
    "href": "chapter_ai-nepi_012.html#evolving-aims-from-tool-to-qualitative-study",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.13 Evolving Aims: From Tool to Qualitative Study",
    "text": "12.13 Evolving Aims: From Tool to Qualitative Study\n\n\n\nSlide 16\n\n\nPaul Näger and his team implemented a RAG system utilising the Stanford Encyclopedia of Philosophy as its data source. However, initial attempts to configure the system using conventional textbook approaches for retrieval and generation produced unsatisfactory results; indeed, the answers proved inferior to those generated by ChatGPT alone. This unexpected outcome prompted a significant re-evaluation of the project’s objectives, shifting its primary aim from merely developing a functional tool to undertaking a comprehensive qualitative study on the optimal setup of RAG systems specifically tailored for philosophical applications.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#qualitative-study-focus-model-choices-and-hyperparameter-tuning",
    "href": "chapter_ai-nepi_012.html#qualitative-study-focus-model-choices-and-hyperparameter-tuning",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.14 Qualitative Study Focus: Model Choices and Hyperparameter Tuning",
    "text": "12.14 Qualitative Study Focus: Model Choices and Hyperparameter Tuning\n\n\n\nSlide 17\n\n\nNäger’s qualitative study meticulously investigates two critical areas for optimising RAG system performance in philosophy. Firstly, it scrutinises model choices, specifically evaluating the efficacy of various generative Large Language Models and their corresponding embedding models. Secondly, the study delves into the intricate process of hyperparameter tuning. This involves systematically adjusting parameters such as the number of documents retrieved (top-k), the maximum input and output token lengths, the temperature or top-p settings for text generation, and the optimal chunk size and overlap for document segmentation. Each of these parameters profoundly influences the quality and relevance of the generated responses.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#methodological-challenges-retrieval-semantic-mismatch-and-reranking",
    "href": "chapter_ai-nepi_012.html#methodological-challenges-retrieval-semantic-mismatch-and-reranking",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.15 Methodological Challenges: Retrieval Semantic Mismatch and Reranking",
    "text": "12.15 Methodological Challenges: Retrieval Semantic Mismatch and Reranking\n\n\n\nSlide 18\n\n\nBeyond model selection and hyperparameter tuning, Näger and his team confronted additional methodological challenges, notably the issue of retrieval semantic mismatch. To address this, they implemented reranking, an advanced technique designed to refine the relevance of retrieved documents. Their overarching methodology employs a theoretically grounded trial-and-error approach, systematically assessing how various adjustments improve answer quality. Crucially, sound evaluation remains paramount, particularly given the complex nature of philosophical propositions, which rarely reduce to simple, atomic facts. Evaluating the factual accuracy of these nuanced statements presents a significant challenge, demanding rigorous and context-sensitive assessment criteria.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#frontend-overview-configuration-and-comparative-answers",
    "href": "chapter_ai-nepi_012.html#frontend-overview-configuration-and-comparative-answers",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.16 Frontend Overview: Configuration and Comparative Answers",
    "text": "12.16 Frontend Overview: Configuration and Comparative Answers\n\n\n\nSlide 19\n\n\nThe system’s frontend, as demonstrated by Näger, provides a comprehensive interface for configuration and comparative analysis. Users can specify the Generative Model, currently set to gpt-4o-mini, and define prompt token limits, with a model capacity of 128,000 tokens and a user-defined limit of 15,000. Furthermore, the interface allows setting the number of texts to retrieve, typically 15. A ‘Persona’ text area meticulously instructs the model to act as an ‘expert philosopher’, ensuring ‘meticulous and precise’ answers. Users input their philosophical questions, such as ‘What is priority monism?’, into a dedicated field. The system then presents a comparative output, displaying both an ‘Answer with LLM alone’—serving as a benchmark—and an ‘Answer with RAG’, facilitating direct qualitative assessment. Complementing these answers, a ‘Retrieved Texts Overview’ table details the source ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and whether each text was ‘included’ in the prompt, offering full transparency into the retrieval process.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#backend-code-and-output-details",
    "href": "chapter_ai-nepi_012.html#backend-code-and-output-details",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.17 Backend Code and Output Details",
    "text": "12.17 Backend Code and Output Details\n\n\n\nSlide 20\n\n\nNäger’s team crafted the system’s backend in Python, orchestrating the intricate processes of text handling and retrieval. Functions such as print_section_overview and print_paragraph_overview provide detailed insights into how text sections and paragraphs are processed, whilst print_overview_intro displays the total document count within the dataset. Conditional calls throughout the code exemplify a modular design, facilitating flexible text processing and retrieval logic. The system’s output meticulously lists the retrieved texts, detailing article names, specific section headings, and crucially, indicating which texts were fully included in the prompt and which were truncated due to token limitations, thereby ensuring transparency in the information provided to the LLM.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-corpora",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.18 Optimising Chunk Size for Philosophical Corpora",
    "text": "12.18 Optimising Chunk Size for Philosophical Corpora\n\n\n\nSlide 21\n\n\nOptimising chunk size represents a critical hyperparameter in RAG system development. Näger and his team explored three primary chunking strategies: employing a fixed number of words (e.g., 500), segmenting by paragraphs, or dividing content into main sections, whether at a lower or higher hierarchical level. Surprisingly, chunking into main sections, inclusive of their headings, yielded the most favourable results. This outcome proved counter-intuitive, given that the average section length of approximately 3000 words substantially exceeded the embedding model’s typical cutoff of 512 words. This efficacy, however, likely stems from the highly systematised structure of the Stanford Encyclopedia of Philosophy, where the initial 500 words of a section often encapsulate its core ideas. Such a strategy might prove less effective for more heterogeneous or less rigorously structured texts. Consequently, future work plans to investigate embedding models with extended context windows, such as Cohere Embed 3, to better accommodate these longer semantic units.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#reranking-addressing-retrieval-semantic-mismatch",
    "href": "chapter_ai-nepi_012.html#reranking-addressing-retrieval-semantic-mismatch",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.19 Reranking: Addressing Retrieval Semantic Mismatch",
    "text": "12.19 Reranking: Addressing Retrieval Semantic Mismatch\n\n\n\nSlide 22\n\n\nReranking constitutes a crucial additional step within the retrieval pipeline, specifically designed to mitigate the problem of false positives, where initially retrieved texts lack true relevance to the query. The primary aim of reranking involves reordering documents based on their actual pertinence. To achieve this, Näger’s team employs a generative Large Language Model (gLLM) to evaluate the relevance of the texts. This approach leverages the gLLM’s superior semantic differentiation capabilities, which significantly surpass those of simpler embedding models. The evaluation process incorporates specific scoring categories, including the informativeness of the text and the length of its relevant passages, culminating in a comprehensive total score. Whilst reranking demonstrably yields highly effective results, it concurrently incurs a substantial increase in computational costs.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advantages-of-rag-systems-in-scientific-tasks",
    "href": "chapter_ai-nepi_012.html#advantages-of-rag-systems-in-scientific-tasks",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.20 Advantages of RAG Systems in Scientific Tasks",
    "text": "12.20 Advantages of RAG Systems in Scientific Tasks\n\n\n\nSlide 23\n\n\nRAG systems offer compelling advantages for scientific tasks. They seamlessly integrate verbatim corpora alongside specialised domain knowledge, thereby furnishing more detailed answers and significantly reducing the incidence of hallucinations. Furthermore, these systems inherently facilitate the precise citation of relevant documents, a critical feature for academic integrity. Consequently, the RAG architecture proves exceptionally well-suited for assisting across a wide spectrum of scientific endeavours, enhancing both the accuracy and trustworthiness of AI-generated content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#cautions-and-challenges-in-rag-implementation",
    "href": "chapter_ai-nepi_012.html#cautions-and-challenges-in-rag-implementation",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.21 Cautions and Challenges in RAG Implementation",
    "text": "12.21 Cautions and Challenges in RAG Implementation\n\n\n\nSlide 24\n\n\nWhilst RAG systems offer substantial benefits, their effective deployment necessitates careful consideration of several cautions and challenges. Firstly, RAG systems inherently demand extensive tweaking; optimal settings are highly contingent upon the specific corpus and the nature of the questions posed. Secondly, rigorous evaluation remains paramount, requiring a representative set of questions and meticulously prepared expected answers. When exploring previously unexamined corpora, the indispensable involvement of domain experts, such as philosophers, becomes critical for accurate assessment. A significant challenge arises when no relevant documents are retrieved, leading to a marked decrease in answer quality, which then necessitates prompt adjustment. Paradoxically, RAG systems often yield inferior results for widely discussed overview questions, such as ‘What are the central arguments against scientific realism?’ This phenomenon occurs because RAGs, by design, concentrate on local information, which can inadvertently distract from the broader perspective required for comprehensive overview responses. Consequently, future developments must focus on crafting more flexible systems, particularly agentic RAG architectures, capable of discerning between question types and adapting their approach accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#additional-visual-materials",
    "href": "chapter_ai-nepi_012.html#additional-visual-materials",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.22 Additional Visual Materials",
    "text": "12.22 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n\n\n\nSlide 25\n\n\nThis slide, titled “SEP RAG Overview”, presents a detailed view of the system’s frontend. A “Configuration” section, which can be initialised, includes an “Options” subsection. Here, the “Generative Model” is set to gpt-4o-mini, with a “Prompt Token Limit (Model)” of 128,000 and a user-defined “Prompt Token Limit (here)” of 15,000. The system is configured to retrieve 15 texts. A “Persona” text area instructs the model to act as an “expert philosopher”, ensuring “meticulous and precise” answers. The “Philosophical Question” field contains the query: “What is priority monism?”. Below the “Generate answer” button, the slide presents two comparative answers. The “Answer with LLM alone” defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with pluralism and mereological nihilism, and citing examples like the universe and philosophers such as Spinoza. The “Answer with RAG”, titled “Response to ‘What is priority monism?’”, offers a similar but more nuanced definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative. It notes the contrast with existence monism, discusses mathematical expressions, ontological priority, and relevance to quantum mechanics (entangled systems). Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. Finally, a “Retrieved Texts Overview” table lists five documents, detailing their ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and ‘included’ status. The table indicates that the RAG system retrieved relevant sections from ‘monism’ and ‘disability-care-rationing’ files, with the first two ‘monism’ entries fully included, and a third ‘monism’ entry partially included. The ‘distances’ column suggests lower values indicate higher relevance. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.\n\n\n\nSlide 26\n\n\nThis slide, also titled “SEP RAG Overview”, visually divides into “Frontend” on the left and “Backend: Python Code” on the right. The frontend section displays a collapsed “Configuration” panel and an expanded “Options” panel, allowing selection of gpt-4o-mini as the “Generative Model” and setting token limits (128,000 model, 15,000 user-defined). The “Persona” is set to “expert philosopher”, and 15 texts are to be retrieved. The “Philosophical Question” is “What is priority monism?”. The interface then presents comparative answers from an “LLM alone” and “RAG”, with the RAG answer providing a more detailed, source-attributed philosophical definition. A “Retrieved Texts Overview” table details source files, section headings, distances, token lengths, and inclusion status. The “Backend: Python Code” section displays Python function definitions, including print_section_overview, print_paragraph_overview, and print_overview_intro, illustrating the modular and configurable nature of the backend’s text processing and retrieval logic. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.\n\n\n\nSlide 27\n\n\nTitled “SEP RAG Details 1”, this slide focuses on the “Frontend: Input section”. The interface features a “Configuration” section with an “Initialize” button. The “Options” section provides detailed settings: “Generative Model” is gpt-4o-mini, “Prompt Token Limit (Model)” is 128,000, and “Prompt Token Limit (here)” is 15,000. The “Persona” instruction is “You are an expert philosopher. You answer meticulously and precisely.” The system is set to retrieve 15 texts. The “Philosophical Question” input box contains “What is priority monism?”. A “Generate answer” button is present at the bottom. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “7”.\n\n\n\nSlide 28\n\n\nThis slide, “SEP RAG Details 2”, presents the “Frontend: Output-section answers”. It features a side-by-side comparison of answers to ‘What is priority monism?’. The ‘Answer with LLM alone’ defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with mereological nihilism and citing examples like the universe and philosophers such as Spinoza. The ‘Answer with RAG’, titled “Response to ‘What is priority monism?’”, provides a more detailed, augmented definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative and dependent on the fundamental whole (Text 0). It contrasts this with existence monism, discusses mathematical expressions, and highlights its relevance to emergent properties in quantum mechanics. Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. The RAG answer text is truncated, indicated by a scrollbar. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 29\n\n\nContinuing the “SEP RAG Details 2” theme, this slide focuses on a “Comparative setup for qualitative evaluation” of output answers. It displays the “Answer with LLM alone” on the left, serving as a ‘benchmark’, which defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its derived parts, contrasting it with pluralism or mereological nihilism. On the right, the “Answer with RAG”, titled “Response to ‘What is priority monism?’”, provides a definition augmented by retrieved information, citing ‘Text 0’ multiple times. This RAG answer highlights the existence of exactly one basic concrete object (the universe), contrasts it with existence monism, discusses its mathematical expression, and notes its relevance to quantum mechanics and emergent properties. It also attributes the view to Plato and Spinoza and contrasts it with priority pluralism and nihilism. The RAG answer text is partially visible, with a scroll bar indicating more content. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 30\n\n\nThis slide, “SEP RAG Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The left section, “Answer with LLM alone”, explains priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its parts, which derive their existence from the whole. It contrasts this with pluralism and mereological nihilism, using the universe as an example and mentioning Spinoza. The right section, “Answer with RAG”, presents the “Response to ‘What is priority monism?’”. This augmented answer defines priority monism as the existence of one basic concrete object (the universe), with parts being derivative. It highlights phrases marked with ‘(Text 0)’ for source attribution, contrasts it with ‘existence monism’, mentions its mathematical expression, and its crucial role in understanding emergent properties in quantum mechanics. It also associates the view with ‘Plato and Spinoza’ and contrasts it with ‘priority pluralism’ and ‘nihilism’. The text on the right is truncated. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 31\n\n\nThe slide, “SEP RAG Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The “Answer with LLM alone” on the left defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its constituent parts, citing Spinoza and contrasting it with pluralism. The “Answer with RAG” on the right, titled “Response to ‘What is priority monism?’”, provides a definition with specific phrases highlighted and marked with ‘(Text 0)’, indicating retrieved information. This RAG-generated answer highlights ‘existence monism’ as a contrast, states ‘there exists exactly one basic entity’ (the cosmos), and notes its ‘crucial for understanding emergent properties found in quantum mechanics’. It associates ‘Plato’ and Spinoza with this view and contrasts it with ‘priority pluralism’ and ‘nihilism’. The RAG answer ends abruptly, suggesting an interactive or truncated output. The footer indicates “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 32\n\n\nTitled “SEP RAG Details 3”, this slide presents the “Frontend: Output section retrieved texts” under the heading “Retrieved Texts Overview:”. It features a table with six columns: ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/_token’, ‘total_token’, and ‘included’. The table lists 15 retrieved text segments. For instance, the first entry is from ‘monism’, section ‘## 3. Priority Monism’, with a distance of 0.448, length of 12515 tokens, total tokens 12515, and marked ‘Yes’ for inclusion. The ‘distances’ column likely represents a similarity score, with lower values indicating higher relevance. The ‘included’ column shows that only the top two most relevant sections were fully included, and the third was partially included, suggesting a cutoff based on relevance or a maximum token limit for the context provided to the language model. The ‘total_token’ column tracks the cumulative token count of all retrieved sections. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.\n\n\n\nSlide 33\n\n\nThis slide, “SEP RAG Details 3”, continues to detail the “Frontend: Output section retrieved texts” under “Retrieved Texts Overview:”. It displays a comprehensive table of 15 retrieved text segments, indexed from 0 to 14. The table’s columns are ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/token’, ‘total_token’, and ‘included’. The ‘file_names’ include ‘monism’, ‘disability-care-rationing’, ‘neutral-monism’, and others. The ‘sec_heading’ specifies the section (e.g., ‘## 3. Priority Monism’, ‘## Abstract’). ‘Distances’ range from 0.448 to 1.241, indicating relevance. ’length/_token’ shows segment lengths, and ‘total_token’ tracks cumulative token count. The ‘included’ column indicates ‘Yes’, ‘No’, or ‘Partially’. Several rows are highlighted: the first two ‘monism’ entries are yellow-highlighted and marked ‘Yes’ for inclusion. The third ‘monism’ entry is yellow-highlighted and ‘Partially’ included. A ‘disability-care-rationing’ entry is highlighted in reddish-pink and marked ‘No’. This highlighting likely draws attention to specific retrieval outcomes and inclusion criteria. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.\n\n\n\nSlide 34\n\n\nThis slide introduces “Chunk size” as a “hyperparameter”, visually indicated by a green arrow pointing from the term to the label. The main body of the slide is blank, suggesting it serves as an introductory concept or a placeholder for further discussion on this key configurable parameter in RAG methodology. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 35\n\n\nThe slide, titled “Chunk size” and labelled as a “hyperparameter”, presents “Options:” for determining chunk size. These options are listed as:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThis outlines different strategies for segmenting text or data. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 36\n\n\nThis slide, titled “Chunk size” and identified as a “hyperparameter”, reiterates the “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nIt then states the “Best result: chunking into main sections (including headings).” A rationale is provided: “philosophical facts are rarely short and isolated, they need some space for presentation”. This leads to the conclusion: “best to stick to longer semantic units”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 37\n\n\nThe slide, titled “Chunk size” and labelled as a “hyperparameter”, lists “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nIt states the “Best result: chunking into main sections (including headings).” The rationale provided is that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) section highlights a key observation: “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this discrepancy, the slide notes “Nevertheless best results.” Another rationale explains that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 38\n\n\nThis slide, titled “Chunk size” and identified as a “hyperparameter”, details the “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThe “Best result:” was “chunking into main sections (including headings)”. The rationale is that “philosophical facts are rarely short and isolated, they need some space for presentation”, thus “best to stick to longer semantic units”. A “NB:” (Nota Bene) section highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this, it notes “Nevertheless best results.” Another rationale states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. A “planned:” section outlines future work: “try emb. models with longer context window like Cohere Embed 3”, suggesting experimentation with larger input sizes. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 39\n\n\nThe slide, titled “Chunk size” and explicitly labelled as a “hyperparameter”, begins by listing “Options” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThe “Best result” is stated as “chunking into main sections (including headings)”. An observation notes that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) point highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words). Nevertheless best results.” A “planned:” section indicates future work to “try emb. models with longer context window like Cohere Embed 3”. Another observation states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The slide concludes with a crucial lesson: “effective chunking highly depends on the specifics of the corpus and the kind of questions”. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 40\n\n\nThis slide introduces “Reranking”, prominently displayed and clarified as an “additional step to retrieval”. This visual element emphasises the sequential nature of reranking within an information processing pipeline. The main body of the slide is blank, indicating that further details or diagrams related to reranking would likely be presented incrementally or on subsequent slides. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.\n\n\n\nSlide 41\n\n\nThis presentation slide focuses on “Reranking” as an “additional step to retrieval”. The main content area identifies the core problem that reranking aims to solve: “not all retrieved texts are relevant to the question (false positives)”. This highlights a common challenge in information retrieval where an initial search might yield documents that are not truly pertinent to the user’s query, necessitating a refinement step. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "",
    "text": "Overview\nL. Gautheron and Mike Schneider, in a collaborative endeavour with Schneider from the University of Missouri, have embarked upon a developing project. This initiative addresses fundamental questions within the philosophy of science, integrating advanced computational methods, including those discussed in recent academic discourse, with sophisticated social network analysis techniques. Gautheron and Schneider systematically explore the concept of ‘plural pursuit’ within the context of quantum gravity research.\nTheir approach outlines three distinct steps. First, the authors establish the philosophical framework and introduce the case study. Second, they construct a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, they confront this empirical reconstruction with physicists’ ‘top-down’ intuitions regarding their field’s structure. The project ultimately seeks to determine whether quantum gravity research exemplifies plural pursuit, characterised by independent communities pursuing distinct paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.1 Introduction to Plural Pursuit in Quantum Gravity",
    "text": "13.1 Introduction to Plural Pursuit in Quantum Gravity\n\n\n\nSlide 02\n\n\nGautheron and Schneider’s developing project, a collaborative endeavour with Mike Schneider from the University of Missouri, directly addresses fundamental questions in the general philosophy of science. The authors integrate computational methods, previously discussed in academic forums, with social network analysis techniques to achieve their objectives. Their presentation systematically unfolds in three distinct steps.\nInitially, Gautheron and Schneider establish the philosophical framing and introduce quantum gravity as the central case study. Subsequently, they propose a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, the authors confront this empirically derived reconstruction with physicists’ own ‘top-down’ intuitions concerning the field’s inherent structure. The overarching themes, prominently displayed on the accompanying slide, encompass ‘Quantum gravity and plural pursuit in science’, ‘Plural pursuit across scales: a bottom-up approach’, and ‘The physicists’ intuition: a top-down approach’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#presentation-structure-and-core-themes",
    "href": "chapter_ai-nepi_015.html#presentation-structure-and-core-themes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.2 Presentation Structure and Core Themes",
    "text": "13.2 Presentation Structure and Core Themes\n\n\n\nSlide 03\n\n\nGautheron and Schneider systematically unfold their presentation in three distinct steps. Initially, they establish the philosophical framing and introduce quantum gravity as the central case study. Subsequently, the authors propose a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, they confront this empirically derived reconstruction with physicists’ own ‘top-down’ intuitions concerning the field’s inherent structure.\nThe accompanying slide visually reinforces these core themes, prominently highlighting ‘Quantum gravity and plural pursuit in science’. It also presents ‘Plural pursuit across scales: a bottom-up approach’ and ‘The physicists’ intuition: a top-down approach’ as integral, albeit visually de-emphasised, components.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-fundamental-problem-of-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#the-fundamental-problem-of-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.3 The Fundamental Problem of Quantum Gravity",
    "text": "13.3 The Fundamental Problem of Quantum Gravity\n\n\n\nSlide 04\n\n\nA central, enduring challenge in fundamental physics involves formulating a quantum theory of gravity. This profound problem necessitates reconciling our understanding of phenomena at very small scales, governed by quantum mechanics, with our knowledge of very large scales, described by general relativity. Physicists have attempted numerous solutions, with string theory emerging as the most prominent amongst them.\nTo comprehensively account for this multifaceted situation, Gautheron and Schneider introduce the concept of ‘plural pursuit’. The slide explicitly titles this section ‘A plurality of approaches to quantum gravity’, framing the core problem as ‘how to formulate a quantum theory of gravity?’ and prompting a discussion on ‘Attempted solutions:’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit-in-scientific-inquiry",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit-in-scientific-inquiry",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.4 Defining Plural Pursuit in Scientific Inquiry",
    "text": "13.4 Defining Plural Pursuit in Scientific Inquiry\n\n\n\nSlide 05\n\n\nMike Schneider defines ‘plural pursuit’ as ‘distinct yet concurrent instances of normal science, dedicated to a common problem-solving goal’. In this specific context, the shared objective involves reconciling quantum theory with gravitation. Each instance of normal science, crucially, finds its articulation through a social community intrinsically linked to an intellectual disciplinary matrix. This concept draws parallels with Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes.\nConsequently, Gautheron and Schneider pose an empirical question: does quantum gravity research exemplify plural pursuit, meaning it comprises independent communities concurrently pursuing distinct paradigms? The accompanying slide reiterates the problem and enumerates specific attempted solutions, including String theory, Supergravity, Loop quantum gravity (encompassing spin foams), Causal set theory, and Asymptotic safety.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-methodology",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-methodology",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.5 Bottom-Up Reconstruction Methodology",
    "text": "13.5 Bottom-Up Reconstruction Methodology\n\n\n\nSlide 06\n\n\nGautheron and Schneider systematically gathered a substantial data corpus, comprising approximately 200,000 abstracts and titles from fundamental physics literature. Their methodology then proceeded in two distinct steps. First, they reconstructed the intellectual structure of the field through linguistic analysis, employing the Bertopic pipeline. This process involved spatialising documents into an embedding space, followed by unsupervised clustering at a highly fine-grained level.\nThis meticulous approach yielded 600 distinct topics, a granularity deemed necessary for accurately capturing niche research areas, some of which might involve as few as 100 papers. Consequently, the authors assigned each physicist a ‘specialty’, determined by the most prevalent topic across their publications, thereby partitioning authors according to the field’s inherent linguistic and intellectual structure. Second, Gautheron and Schneider conducted a comprehensive social network analysis. This involved constructing a co-authorship graph, where individual physicists functioned as nodes and co-authorship relationships formed the edges. Applying a community detection method to this network, the authors recovered approximately 800 communities from a total of 30,000 physicists, providing an alternative partition of authors that directly reflects the field’s social structure. The accompanying slide visually reinforces the problem and attempted solutions, whilst explicitly linking them to the concept of ‘plural pursuit’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-as-a-one-to-one-mapping",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-as-a-one-to-one-mapping",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.6 Conceptualising Plural Pursuit as a One-to-One Mapping",
    "text": "13.6 Conceptualising Plural Pursuit as a One-to-One Mapping\n\n\n\nSlide 07\n\n\nWithin this framework, Gautheron and Schneider define plural pursuit as a precise one-to-one mapping between distinct communities and their corresponding intellectual topics. The authors visualise this relationship using a correlation matrix, where communities align with topics. An ideal scenario, indicative of a clear division of labour, would manifest as a block-diagonal matrix, signifying that each community dedicates itself entirely to a single topic. The accompanying slide, titled ‘Normal science and plural pursuit’, explicitly defines plural pursuit as ‘distinct yet concurrent instances of normal science, dedicated to a common problem-solving goal’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-dependency-in-research-landscapes",
    "href": "chapter_ai-nepi_015.html#addressing-scale-dependency-in-research-landscapes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.7 Addressing Scale-Dependency in Research Landscapes",
    "text": "13.7 Addressing Scale-Dependency in Research Landscapes\n\n\n\nSlide 08\n\n\nGautheron and Schneider observe that directly applying the fine-grained partitions, comprising 600 topics and 800 communities, results in a highly complex and unintelligible correlation matrix. This challenge, the authors explain, stems from several issues inherent in fine-graining. Firstly, the level of topic fine-graining often proves arbitrary; for instance, string theory might appear scattered across numerous distinct topics. Secondly, micro-social processes significantly shape communities, enabling multiple communities to undertake large research programmes concurrently.\nMore fundamentally, the computational notions of ‘topic’ and ‘community’ are inherently scale-dependent. Conceptually, research programmes themselves exhibit nested structures, allowing for the division of, for example, string theory into families and subfamilies. Consequently, Gautheron and Schneider argue that identifying instances of plural pursuit demands addressing this inherent ambiguity arising from observations at different scales. The accompanying slide reiterates the definition of plural pursuit whilst elaborating that ‘Each instance of normal science is articulated by community × intellectual disciplinary matrix’, citing Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes as examples.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-the-research-landscape",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-the-research-landscape",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.8 Hierarchical Reconstruction of the Research Landscape",
    "text": "13.8 Hierarchical Reconstruction of the Research Landscape\n\n\n\nSlide 09\n\n\nTo address the challenges of scale-dependency, Gautheron and Schneider propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, the authors implement a hierarchical clustering approach, commencing with 600 fine-grained topics and progressively merging them using an agglomerative clustering technique. Concurrently, for the community structure, they employ a hierarchical stochastic block model from the outset, which learns a multi-level partition into progressively coarser communities.\nThese hierarchical structures collectively induce a notion of scale, enabling observation of the system at various levels; for instance, one can visualise the co-authorship network with scientists’ specialties indicated by colour at different linguistic coarse-graining levels. Nevertheless, Gautheron and Schneider note that a persistent challenge remains: the chosen scale for observation is still arbitrary, complicating the selection of an appropriate level for either the topic or community structure. The accompanying slide reiterates the core definitions whilst posing the central empirical question: ‘Is quantum gravity research an instance of plural pursuit? (i.e., independent communities pursuing different paradigms in parallel)’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#necessity-of-adaptive-topic-coarse-graining",
    "href": "chapter_ai-nepi_015.html#necessity-of-adaptive-topic-coarse-graining",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.9 Necessity of Adaptive Topic Coarse-Graining",
    "text": "13.9 Necessity of Adaptive Topic Coarse-Graining\n\n\n\nSlide 11\n\n\nCrucially, the selection of scale significantly impacts the resulting correlation matrix, leading to markedly different interpretations of the research landscape. Consequently, Gautheron and Schneider propose an adaptive topic coarse-graining strategy to address this variability. The rationale underpinning this approach is that whilst fine-grained topics capture subtle linguistic nuances, some of these distinctions hold no practical consequence for scientists’ collaborative capacities.\nTheir primary objective, therefore, involves systematically removing degrees of freedom from the initial fine-grained partition without compromising any information essential for comprehending the field’s social structure. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the ‘Linguistic analysis’ process, whilst also indicating the presence of a ‘Social network analysis’ component.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-methodology",
    "href": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-methodology",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.10 Adaptive Topic Coarse-Graining Methodology",
    "text": "13.10 Adaptive Topic Coarse-Graining Methodology\n\n\n\nSlide 12\n\n\nGautheron and Schneider employed the Minimum Description Length (MDL) criterion to select an appropriate scale for topic coarse-graining. This criterion aims to minimise a quantity that judiciously balances two critical requirements: the linguistic partition’s efficacy in explaining the field’s social structure, and the imperative for a partition that avoids excessive complexity or fine-graining. The authors iteratively refined the 600-topic hierarchical tree, continuing as long as this refinement improved the MDL criterion.\nThe procedure ceased when further increases in complexity no longer yielded sufficient information gain regarding the social structure. This adaptive strategy successfully reduced the initial 600 topics to a more manageable 50 coarse-grained topics. Notably, whilst many topics were aggregated, Gautheron and Schneider meticulously preserved certain small-scale linguistic topics, underscoring their crucial role in comprehending the social structure. This outcome, the authors contend, validated the initial fine-grained classification, as some of these smaller topics proved indispensable for the social analysis. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘2287748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the ‘Linguistic analysis’ process, whilst also indicating the presence of a ‘Social network analysis’ component.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#labelling-and-focusing-coarse-grained-topics",
    "href": "chapter_ai-nepi_015.html#labelling-and-focusing-coarse-grained-topics",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.11 Labelling and Focusing Coarse-Grained Topics",
    "text": "13.11 Labelling and Focusing Coarse-Grained Topics\n\n\n\nSlide 13\n\n\nThe coarse-graining process ultimately yielded 50 distinct topics. To enhance their interpretability, Gautheron and Schneider assigned labels to these topics by extracting representative engrams. For the purpose of this study, the authors’ analysis specifically focused on those topics directly involving quantum gravity. The accompanying slide, titled ‘Clustering pipeline’, details the ‘Linguistic analysis’ pipeline, which commences with the spatialisation of documents into an embedding space (L.1), proceeds to unsupervised clustering (L.2) that identified K=611 clusters, and culminates in the derivation of individual ‘Specialty σ_i’ (L.3).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#matching-intellectual-topics-to-social-communities",
    "href": "chapter_ai-nepi_015.html#matching-intellectual-topics-to-social-communities",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.12 Matching Intellectual Topics to Social Communities",
    "text": "13.12 Matching Intellectual Topics to Social Communities\n\n\n\nSlide 14\n\n\nGautheron and Schneider employed a correlation matrix to systematically match the coarse-grained topics with the field’s community structures across various scales. For each topic, the authors meticulously identified the community that most effectively explained its presence across different levels of the community hierarchy. Their analysis yielded several key observations: some expansive topics, such as a prominent purple cluster, exhibited no clear ties to specific communities, suggesting a broad, pervasive interest across the entire field.\nConversely, other topics, notably string theory, demonstrated a robust correspondence with a community structure situated at the third level of the hierarchy. Interestingly, certain quantum gravity research programmes, such as loop quantum gravity, aligned with communities found at much lower, more fine-grained levels of the hierarchy. Furthermore, the study revealed intricate nested structures; for instance, a smaller community, whilst embedded within a larger string theory community, simultaneously maintained strong ties to a distinct intellectual topic, holography. Ultimately, this comprehensive analysis indicated an absence of a clear division of labour, instead demonstrating a complex entanglement between different scales, as Gautheron and Schneider observe. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1: Community detection C=819).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuition-on-field-structure",
    "href": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuition-on-field-structure",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.13 Top-Down Approach: Physicists’ Intuition on Field Structure",
    "text": "13.13 Top-Down Approach: Physicists’ Intuition on Field Structure\n\n\n\nSlide 15\n\n\nTo gather a ‘top-down’ perspective, Gautheron and Schneider surveyed the founding members of the International Society for Quantum Gravity. The authors specifically asked respondents to ‘Provide a list of quantum gravity approaches that come to your mind as structuring the total research landscape in quantum gravity.’ Despite some disagreement amongst the physicists, Gautheron and Schneider successfully compiled a comprehensive and detailed list of approaches that partition the field.\nFor further analysis, the authors specifically focused on String theory, Supergravity, and Holography. This particular focus arose from Gautheron and Schneider’s observation that physicists themselves expressed uncertainty regarding whether these three approaches should be considered distinct, with some arguing for their fundamental connection to string theory, notwithstanding their clear historical and conceptual differences. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-top-down-intuition",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-top-down-intuition",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.14 Confronting Bottom-Up Reconstruction with Top-Down Intuition",
    "text": "13.14 Confronting Bottom-Up Reconstruction with Top-Down Intuition\n\n\n\nSlide 16\n\n\nGautheron and Schneider trained a classifier to predict the specific approach of papers from their text embeddings, utilising the all-MiniLM-L6-v2 model applied to titles and abstracts, alongside hand-coded labels. This classifier enabled the authors to directly confront the supervised, ‘top-down’ list of approaches with the ‘bottom-up’ reconstruction. Their results demonstrated varied effectiveness: whilst the classifier performed commendably for certain approaches, exhibiting a strong correspondence to topics emergent from the bottom-up analysis, it proved less effective for approaches that were either phenomenological or lacked a ‘full-fledged conceptual framework’. Conversely, it achieved high accuracy for ‘well-defined and conceptually autonomous’ frameworks.\nA particularly significant finding emerged from the authors’ bottom-up approach: a large string theory cluster that comprehensively encompassed both supergravity and string theory. This empirical observation notably converged with the uncertainty expressed by physicists regarding the true separation of these two areas, despite their distinct historical trajectories and conceptual underpinnings, as Gautheron and Schneider highlight. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#implications-for-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#implications-for-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.15 Implications for Plural Pursuit in Quantum Gravity",
    "text": "13.15 Implications for Plural Pursuit in Quantum Gravity\n\n\n\nSlide 17\n\n\nGautheron and Schneider observe that the extensive overlap between communities engaged in supergravity and string theory research suggests a challenging meaningful separation between them, even if a small contingent of individuals continues to pursue supergravity independently. This observation aligns with the authors’ bottom-up assessment, which, by systematically removing linguistic nuances devoid of social structural consequences, consolidates these areas. This convergence occurs despite the initial linguistic clusters accurately acknowledging their conceptual distinctions.\nIn conclusion, Gautheron and Schneider contend that socio-epistemic systems are demonstrably observable at multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Consequently, identifying genuine configurations of plural pursuit necessitates a meticulous matching of these structures across various scales, as their work demonstrates. Furthermore, Gautheron and Schneider’s bottom-up reconstruction of the quantum gravity research landscape possesses the capacity to either confirm or critically re-assess physicists’ established intuitions. Crucially, the increasing power of computational methods now enables scholars to revisit and challenge long-held philosophical insights, particularly intuitions concerning paradigms or communities within specific contexts such as quantum gravity. As a final reflection, paraphrasing Clausewitz, ‘Computation is the continuation of philosophy by other means’. The accompanying slide, titled ‘Plural pursuit across communities and topics’, visually defines plural pursuit as a ‘one-to-one mapping between communities and topics’ and presents a ‘Community-topic correlation matrix’ with red and blue blocks, noting that ‘A block-diagonal matrix would imply that communities specialize into distinct domains.’",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#additional-visual-materials",
    "href": "chapter_ai-nepi_015.html#additional-visual-materials",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.16 Additional Visual Materials",
    "text": "13.16 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n\n\n\nSlide 10\n\n\n\nThis slide serves as a summary, titled ‘Summary’ in large green font at the top left. The main content area lists three key topics, likely representing the structure or main points of the presentation. The first point, ‘Quantum gravity and plural pursuit in science’, is displayed in a faded, light green colour. The second point, ‘Plural pursuit across scales: a bottom-up approach’, is prominently highlighted in a vibrant green, suggesting it is the current focus or a primary takeaway. The third point, ‘The physicists’ intuition: a top-down approach’, is also in a faded, light green colour. This visual emphasis indicates a progression through or a specific focus on the ‘bottom-up approach’ within the context of plural pursuit across scales. A persistent black banner at the top left of the slide reiterates these three topics, with ‘Plural pursuit across scales: a bottom-up approach’ also highlighted in white text, reinforcing its current relevance. The bottom of the slide features a black footer bar with the authors’ names, ‘L. Gautheron, Mike D. Schneider’, on the left. On the right, a green section of the footer displays the presentation title, ‘Plural pursuit across scales’, and the slide number ‘6 / 23’. Standard presentation navigation icons are also visible in the bottom centre.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThis chapter details a comparative study assessing the efficacy of Latent Dirichlet Allocation (LDA) and BERTopic models when applied to distinct textual levels: titles, abstracts, and full texts within a scientific corpus. The authors aimed to ascertain whether topic modelling on titles or abstracts suffices, or if full-text analysis remains indispensable, particularly given the substantial resources required for its processing.\nTheir methodology involved constituting a corpus of scientific articles, segmenting these into titles, abstracts, and full texts, and subsequently applying both LDA and BERTopic approaches. A comprehensive analysis, encompassing both qualitative and quantitative methods, facilitated the comparison of the resulting topic models. Key quantitative metrics included the Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.\nThe authors’ findings indicate that title-based models generally exhibit poor performance, whilst abstract models consistently demonstrate robust and meaningful topic extraction, often aligning well with full-text models. Full-text models, whilst offering comprehensive coverage, can present challenges such as loosely defined topics or class-size imbalances, particularly with BERTopic. Ultimately, the study recommends employing topic modelling on abstracts or full texts with either LDA or BERTopic, provided such approaches do not lead to misclassification of relevant documents.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "href": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Methodological Approach",
    "text": "14.1 Research Question and Methodological Approach\n\n\n\nSlide 02\n\n\nThis research addresses a critical inquiry: whether applying topic modelling solely to titles or abstracts suffices, or if full-text analysis remains an indispensable requirement. This question gains particular urgency given the substantial resources demanded for the acquisition, preprocessing, and subsequent analysis of extensive full-text corpora.\nTo investigate this, the authors meticulously constituted a corpus of scientific articles. They then systematically identified and segmented the title, abstract, and full-text sections from each article. Subsequently, they applied two prominent topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. The resulting topic models underwent rigorous analysis and comparison, employing both qualitative and quantitative methodologies.\nThe overall workflow involved segmenting the scientific corpus into titles, abstracts, and full texts. Each segment then served as input for both LDA and BERTopic models. The outputs from these models were then subjected to both qualitative and quantitative scrutiny. This comprehensive approach aimed to provide robust insights into the comparative performance of these models across different textual granularities.\nBeyond this specific investigation, topic modelling itself stands as a vital analytical instrument for processing vast scientific literature, especially within the history, philosophy, and sociology of science. Historically, researchers have deployed topic modelling for diverse tasks, including discerning research trends and paradigm shifts, identifying thematic substructures and interrelationships, and charting the evolution of scientific vocabulary. These prior applications have consistently involved various textual structures, ranging from titles and abstracts to complete full texts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Material: Astrobiology Corpus for Qualitative Comparison",
    "text": "14.2 Material: Astrobiology Corpus for Qualitative Comparison\n\n\n\nSlide 05\n\n\nThe authors grounded this study in an extensive topic analysis of an astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a thorough evaluation process, they selected a full-text LDA model comprising 25 distinct topics as the primary material for comparison.\nTheir analysis of these 25 topics involved a meticulous examination of their most representative words and documents, enabling the authors to assign a descriptive name to each topic based on its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric derived from the topics’ presence within the documents. A community detection algorithm then identified four thematic clusters, designated by letters A, B, C, and D, and visually distinguished by red, green, yellow, and blue colours, respectively.\nA graphical representation visually conveys these findings, illustrating the correlations amongst the 25 topics. This graph incorporates topic labels and the colour variations corresponding to their thematic clusters. Crucially, the thickness of the lines connecting topics denotes the strength of their correlation, whilst the size of each circle reflects the topic’s overall prevalence across all documents. This comprehensive analytical framework enables a robust qualitative comparison of the six distinct topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Methodology: Quantitative Analysis Metrics",
    "text": "14.3 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 06\n\n\nFor the quantitative dimension of this study, the authors employed four distinct metrics to compare the various topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between two document clusterings, with a correction applied for chance agreement. This metric precisely quantifies the extent to which documents cluster together, or diverge, across different models.\nSecondly, Topic Diversity assessed the proportion of distinct top words, thereby determining whether individual topics within a given model were characterised by unique vocabulary. Thirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words, evaluating how effectively these top words collectively represented the documents assigned to each topic. Finally, Coherence CV, calculated as the average cosine relative distance between top words within topics, provided an assessment of whether these top words formed a semantically meaningful grouping. Each of these metrics is underpinned by specific mathematical formulations, ensuring rigorous quantitative comparison.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Results: Adjusted Rand Index Between Topic Models",
    "text": "14.4 Results: Adjusted Rand Index Between Topic Models\n\n\n\nSlide 07\n\n\nThe Adjusted Rand Index (ARI) provides crucial insights into the similarities amongst the six topic models. A value of zero for this metric indicates a clustering equivalent to random assignment. Analysis of the heatmap reveals that the LDA model applied to titles stands out as the most distinct, consistently demonstrating poor similarity with all other models, as evidenced by ARI values below 0.20, depicted by yellow hues in the visualisation.\nConversely, the remaining models generally exhibit a superior overall match, with ARI values consistently exceeding 0.20. Notably, BERTopic models display a stronger internal correspondence, with their inter-model ARI values typically surpassing 0.35. The BERTopic abstract model emerges as particularly central within this network of similarities, demonstrating robust correspondence with every other model, apart from the outlier LDA title model, with values consistently above 0.30. The heatmap visually encapsulates these relationships, where warmer colours signify higher degrees of similarity between the compared topic models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: LDA Full-text Versus Abstracts and Titles",
    "text": "14.5 Results: LDA Full-text Versus Abstracts and Titles\n\n\n\nSlide 08\n\n\nA more granular analysis of the LDA models provides detailed insights into their interrelationships. Table A, which compares the LDA full-text model with the LDA abstract model, indicates a generally good overall fit. This strong correspondence is evident from the reddish diagonal in the table, signifying that each topic from one model typically aligns with a topic from the other, sharing a high proportion of common documents.\nHowever, this alignment is not without dynamic shifts. Three full-text LDA topics effectively disappear, represented by long horizontal dark grey lines. Conversely, three full-text topics fragment into multiple topics within the abstract model, visible as short horizontal dark grey lines. The abstract model also sees the emergence of three entirely new topics, marked by long vertical dark grey lines, whilst three topics arise from mergers, again indicated by short horizontal dark grey lines. Furthermore, one small class, comprising fewer than 50 documents, is discernible within the abstract topics.\nIn stark contrast, Table B, comparing the LDA full-text model with the LDA title model, reveals a poor overall fit. This disparity necessitates substantial reorganisation, manifested by a proliferation of vertical and horizontal dark lines across the table. This indicates that numerous full-text topics vanish, whilst a considerable number of new abstract topics emerge, highlighting a significant divergence in thematic representation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles",
    "text": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles\n\n\n\nSlide 09\n\n\nAnalysis of the BERTopic models, when compared against the LDA full-text baseline, reveals varied performance. Table C, which juxtaposes LDA full-text with BERTopic full-text, indicates an average overall fit. Within this comparison, eight LDA topics vanish along the horizontal axis, whilst six LDA topics fragment into the BERTopic model. Conversely, the vertical axis shows the emergence of five new BERTopic topics, with one topic resulting from mergers. A notable observation from the total document count is the presence of four small classes alongside one exceptionally large class.\nMoving to Table D, the comparison between LDA full-text and BERTopic abstract demonstrates a relatively good overall fit. Here, four LDA topics disappear, whilst six topics undergo splitting. The vertical axis reveals two new BERTopic topics appearing and four topics resulting from mergers. Crucially, this model maintains balanced class sizes.\nFinally, Table E, comparing LDA full-text with BERTopic title, again indicates an average overall fit. In this instance, seven LDA topics disappear, and one topic splits. The vertical axis shows seven new BERTopic topics emerging, with one topic resulting from a merger. The total document count for this model highlights three small classes and one large class. These heatmaps collectively illustrate the proportions of shared documents between topics across these diverse model comparisons.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: LDA - Comparing Top-words",
    "text": "14.7 Results: LDA - Comparing Top-words\n\n\n\nSlide 11\n\n\nAn examination of the top words within the LDA models revealed that topics were generally well-formed across all iterations. The authors observed several robust topics that maintained strong correspondence across the full-text, abstract, and title models. The topic “A-Radiation-spore” serves as a prime example of this consistency.\nConversely, certain topics from the full-text model fragmented into multiple, more granular topics within both the abstract and title models. For instance, the splitting of “A-Life-civilization” proved semantically coherent, yielding a broader topic encompassing research in astrobiology. However, the fragmentation of “B-Chemistry” presented a more ambiguous case, necessitating further analysis for clear interpretation.\nFurthermore, the study identified instances where topics from the full-text model coalesced into new, merged topics within the abstract and title models. A notable example is the merger of “B-Amino-acid” and “B-Protein-gene-rna” in the LDA abstract model. This particular consolidation formed a more general topic, which aligns logically with the underlying subject matter. The visual representation provides side-by-side tables illustrating the top words for selected topics across these LDA models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: BERTopic - Comparing Top-words",
    "text": "14.8 Results: BERTopic - Comparing Top-words\n\n\n\nSlide 12\n\n\nContinuing the assessment of top words, the three BERTopic models also yielded relatively well-formed topics. The robustness of “A-Radiation-spore” persisted across all models, including LDA Full-text, BERTopic Full-text, BERTopic Abstract, and BERTopic Title, underscoring its consistent thematic representation.\nWhilst “A-Life-civilization” generally maintained its stability across the BERTopic models, it exhibited some instances of splitting. This fragmentation led to the emergence of more narrowly defined topics specifically pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic also underwent splitting across the BERTopic models, resulting in a series of more focused thematic areas. The visual data provides comparative tables of top words from selected topics across these models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results of Quantitative Analysis: Coherence",
    "text": "14.9 Results of Quantitative Analysis: Coherence\n\n\n\nSlide 12\n\n\nThe coherence metric, specifically Coherence CV, provides a quantitative assessment of the semantic meaningfulness of the top words within each topic. Across a range of topics from 5 to 50, distinct patterns emerged. Models based on titles consistently exhibited the poorest coherence. Conversely, abstract models demonstrably outperformed full-text models in this regard.\nFurthermore, BERTopic models generally achieved superior coherence compared to LDA, particularly for abstract and title-based analyses. However, this performance differential tended to diminish as the number of topics increased, indicating a convergence in coherence scores at higher topic counts. Ultimately, the BERTopic abstract model unequivocally emerged as the leading performer in terms of topic coherence. A line graph visually represents these trends, plotting the Coherence CV for each of the six models against varying numbers of topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results of Quantitative Analysis: Diversity",
    "text": "14.10 Results of Quantitative Analysis: Diversity\n\n\n\nSlide 13\n\n\nRegarding the diversity of top words characterising the topics, a clear trend emerged: diversity generally diminishes as the number of topics increases. Within this context, models derived from titles consistently offered the highest diversity, surpassing their abstract or full-text counterparts.\nMoreover, BERTopic models demonstrated superior diversity compared to LDA across the board. The BERTopic title model ultimately emerged as the top performer in terms of diversity, with the BERTopic full-text model closely trailing. A line graph visually illustrates these diversity trends for each of the six models across varying topic counts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results of Quantitative Analysis: Joint Recall",
    "text": "14.11 Results of Quantitative Analysis: Joint Recall\n\n\n\nSlide 13\n\n\nThe Joint Recall metric assesses the efficacy with which the top words collectively represent every document assigned to a given topic. Analysis revealed that models based on titles consistently yielded the poorest recall. Conversely, full-text models demonstrated superior performance compared to their abstract and title counterparts.\nIn terms of algorithmic performance, LDA models generally exhibited better Joint Recall than BERTopic. The LDA full-text and BERTopic full-text models emerged as the leading performers in this category, with the BERTopic abstract model following very closely behind. A line graph visually depicts the micro Joint Recall for each of the six models across a range of topic numbers.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Summary of Model Performance",
    "text": "14.12 Summary of Model Performance\n\n\n\nSlide 14\n\n\nThe authors compiled the individual assessment results to offer a holistic perspective on the models’ performance. For each criterion—overall fit, top-words, coherence, diversity, and joint recall—a circular representation indicates performance: a black circle denotes the highest score, a white circle signifies a lesser score, and a half-black, half-white circle indicates intermediate performance. Crucially, the study underscores that no single model emerges as universally superior; rather, diverse research objectives inherently dictate varying needs and, consequently, different model choices.\nFor instance, if the primary objective involves the discovery of main topics without stringent requirements for precise document classification, then issues such as poor recall or large class sizes might be acceptable. In such scenarios, the BERTopic Full-text model performed commendably, albeit with some observed class imbalance. Similarly, whilst far from optimal, the BERTopic Title model did yield certain robust topics that were consistently identified across other models.\nConversely, if the aim is to achieve maximum document coverage across all topics, then neither BERTopic Full-text nor BERTopic Title is recommended, as both approaches lead to large document classes and, in the case of BERTopic Title, poor recall. Furthermore, the LDA Title model receives a general non-recommendation due to its consistently poor performance across nearly all assessments. In essence, the study advocates for conducting topic modelling on either abstracts or full texts, employing either LDA or BERTopic, provided that such applications do not result in the misclassification of documents pertinent to specific topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Discussion and Conclusion",
    "text": "14.13 Discussion and Conclusion\n\n\n\nSlide 16\n\n\nThis research yields several pivotal findings. Firstly, title models consistently exhibit poor performance, primarily attributable to the inherent lack of information within titles, which can consequently lead to erroneous document classification. Nevertheless, the BERTopic title model, surprisingly, generated numerous meaningful topics, suggesting that future efforts might focus on striking a balance between precisely defined topics and comprehensive document coverage.\nSecondly, full-text models occasionally encounter difficulties in processing vast quantities of information. With LDA, topics can become more broadly defined and encompass wider coverage, potentially including secondary or transverse themes such as methodologies. Conversely, BERTopic, when applied to full texts, can produce overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently demonstrate strong performance with summary information. Their results align remarkably well with the LDA full-text model, as well as with both LDA and BERTopic abstract models. This consistency underscores their utility in capturing core thematic content.\nFourthly, the study highlights the notable robustness of topics. Across the board, the authors identified highly similar topics, a finding that facilitates the application of meta-analytic methods to pinpoint the most enduring and robust themes. Moreover, this consistency suggests the potential for employing relative distance metrics across models to identify an optimal solution; in this study, the BERTopic abstract model emerged as such an optimum, performing exceptionally well across all other metrics.\nFinally, the findings prompt a consideration of new model development. It appears feasible and potentially beneficial to leverage the structural information inherent in documents—specifically, full text, abstract, and title—to extract more semantically rich sets of top words or topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "",
    "text": "Overview\nThe presentation introduces a novel architectural approach for Large Language Models (LLMs), termed the “Time Transformer”, designed to imbue them with explicit temporal awareness. This innovation directly addresses the inherent limitation of current LLMs, which derive only an implicit understanding of time from statistical patterns within their training data. The speaker highlights that whilst existing models demonstrate remarkable capabilities, their lack of explicit temporal conditioning can lead to inconsistencies when processing information that evolves over time, such as historical data. The proposed “Time Transformer” integrates a dedicated temporal dimension directly into the token embeddings, thereby enabling the model to learn and reproduce changing linguistic patterns as a function of time. The authors validated this concept using a small generative LLM trained on a highly constrained dataset of Met Office weather reports, demonstrating its ability to efficiently capture and reproduce time-dependent linguistic shifts. The presentation explores the theoretical underpinnings of this approach, details the model architecture and data preparation, and presents two experiments demonstrating its efficacy in learning synthetic temporal drifts. Furthermore, it outlines potential applications, including historical analysis and instruction-tuned models, whilst acknowledging challenges related to fine-tuning and data curation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#limitations-of-current-llms-and-the-need-for-temporal-awareness",
    "href": "chapter_ai-nepi_017.html#limitations-of-current-llms-and-the-need-for-temporal-awareness",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.1 Limitations of Current LLMs and the Need for Temporal Awareness",
    "text": "15.1 Limitations of Current LLMs and the Need for Temporal Awareness\n\n\n\nSlide 02\n\n\nCurrent Large Language Models (LLMs) inherently possess only an implicit understanding of time, which they derive statistically from the vast textual corpora used for their training. Whilst these models exhibit a profound grasp of temporal concepts, their comprehension stems from subtle cues embedded within the data rather than explicit temporal conditioning. Explicit time awareness, however, would demonstrably enhance their utility, particularly within historical analysis and, indeed, across a broader spectrum of applications.\nConsider, for instance, two sentences that differ solely in their temporal context, such as ‘The primary architecture for processing text through Neural Networks is LSTM’ and ‘The primary architecture for processing text through Neural Networks is Transformer.’ Without explicit temporal information, these statements, representing different states of affairs in 2017 and 2025 respectively, directly contradict one another within the LLM’s training data. The model then struggles to perfectly fulfil its objective, as it must arbitrarily favour one, inevitably making an error regarding the other. Furthermore, a discernible recency bias often influences LLM predictions, favouring more contemporary information. Current methods, such as prompt engineering, merely attempt to exploit the model’s implicit temporal understanding, akin to ‘fishing in the dark’ for desired outcomes.\nTo overcome these limitations, the authors propose integrating time directly into the token embeddings of Transformer-based LLMs. This architectural modification aims to render LLMs explicitly time-aware, enabling them to precisely learn and reproduce the evolving patterns within their training data as a direct function of time. A proof of concept, utilising a small generative LLM, has already been developed to validate this innovative approach.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-temporal-dependence-in-llms",
    "href": "chapter_ai-nepi_017.html#formalising-temporal-dependence-in-llms",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.2 Formalising Temporal Dependence in LLMs",
    "text": "15.2 Formalising Temporal Dependence in LLMs\n\n\n\nSlide 03\n\n\nFundamentally, Large Language Models operate by estimating the probability distribution over their vocabulary for the next token, conditioned on a given sequence of preceding tokens. This process is mathematically represented as p(x_n | x_1, …, x_{n-1}). However, in real-world scenarios, the likelihood of a token appearing within a specific context is not static; rather, it is intrinsically dependent on time, thus becoming p(x_n | x_1, …, x_{n-1}, t).\nExtending this principle, the joint probability for an entire sequence of tokens uttered at a particular time t is expressed as the product of conditional probabilities: p(x_1, …, x_n | t) = product p(x_k | x_1, …, x_{k-1}, t). Despite this inherent temporal variability, current LLM training processes frequently treat these probability distributions as static. Consequently, during inference, these models can only reflect temporal drift in the underlying token sequences through in-context learning, a mechanism that relies on the immediate context provided rather than an explicit, integrated understanding of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-temporal-drift-in-llm-training",
    "href": "chapter_ai-nepi_017.html#addressing-temporal-drift-in-llm-training",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.3 Addressing Temporal Drift in LLM Training",
    "text": "15.3 Addressing Temporal Drift in LLM Training\n\n\n\nSlide 04\n\n\nA significant challenge in current Large Language Model (LLM) training lies in their tendency to treat inherently time-dependent probability distributions as static. This simplification means that whilst the real-world likelihood of a token given its preceding context is a direct function of time—for instance, the probability of ‘transformer’ completing a sentence was effectively zero in 2017—LLMs primarily reflect such temporal drift only through in-context learning during inference.\nTo improve upon this, the authors must develop more effective methods for modelling these dynamic, time-dependent probability distributions. Existing strategies, such as ‘time slicing’—where distinct models are trained for specific temporal segments, assuming static distributions within those slices—prove remarkably data inefficient. A more streamlined and integrated approach is therefore imperative.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-time-transformer-concept-and-data-acquisition",
    "href": "chapter_ai-nepi_017.html#the-time-transformer-concept-and-data-acquisition",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.4 The Time Transformer Concept and Data Acquisition",
    "text": "15.4 The Time Transformer Concept and Data Acquisition\n\n\n\nSlide 05\n\n\nTo overcome the limitations of implicit temporal understanding, the authors propose an innovative solution termed the ‘Time Transformer’. This concept centres on a remarkably simple yet profound architectural adjustment: reserving a single dimension within the token embedding space specifically for time. This dedicated dimension explicitly conveys the utterance date for each token sequence, thereby providing direct temporal context.\nThe initial implementation employs a non-trainable, min-max normalised ‘day of the year’ as the time embedding. This particular choice strategically exploits natural seasonal variations inherent in the chosen dataset, such as the prevalence of snow in winter or heat in summer. However, the framework readily accommodates alternative time embeddings as required.\nFor the proof of concept, the research team selected Met Office weather reports as the primary dataset. This text corpus is characterised by its limited vocabulary and simple, repetitive language, making it an ideal candidate for initial validation. The UK’s national meteorological service issues these daily reports, and historical data remains accessible through their digital archive. Additionally, the ‘TinyStories’ dataset was identified as another potentially suitable resource for similar investigations.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#weather-report-dataset-processing",
    "href": "chapter_ai-nepi_017.html#weather-report-dataset-processing",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.5 Weather Report Dataset Processing",
    "text": "15.5 Weather Report Dataset Processing\n\n\n\nSlide 06\n\n\nThe research team systematically acquired the dataset by scraping daily weather reports from Met Office PDFs spanning the years 2018 to 2024. This process yielded approximately 2,500 reports, each comprising between 150 and 200 words. For text processing, the team employed tf.keras.layers.TextVectorization, standardising the input by converting text to lowercase and stripping punctuation. Crucially, the tokenisation process avoided sub-word segmentation and deliberately neglected case and interpunctuation, reflecting the inherently simple nature of the language. This straightforward approach resulted in a remarkably concise vocabulary of just 3,395 unique words across the entire seven-year corpus. An illustrative example, the Daily Weather Summary for Sunday 04 August 2019, details showery rain and mist, whilst the Daily Extremes table highlights a highest maximum temperature of 27.5°C recorded in Writtle, Essex, and a lowest maximum of 14.1°C in Fair Isle, Shetland.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#vanilla-transformer-model-architecture-and-training",
    "href": "chapter_ai-nepi_017.html#vanilla-transformer-model-architecture-and-training",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.6 Vanilla Transformer Model Architecture and Training",
    "text": "15.6 Vanilla Transformer Model Architecture and Training\n\n\n\nSlide 07\n\n\nThe authors constructed a modest-sized, decoder-only Transformer architecture, termed the ‘Vanilla model’, to establish a baseline for language pattern learning within the weather report dataset. This architecture processes input through an Embedding Layer with a d_model of 512, followed by Positional Encoding and a Dropout layer set at a 0.1 rate. Subsequently, the processed input traverses a stack of four Multi-Head Attention Decoder Blocks. Each decoder layer comprises a Multi-Head Attention mechanism with eight heads and a key_dim of 512, succeeded by an Add & Norm operation, then a Feed Forward Network (dff=2048), and another Add & Norm. The final output from these layers feeds into a Dense Layer, sized to the vocabulary, which ultimately produces the model’s output.\nThis compact model contains 39 million parameters, equating to approximately 150 MB, a stark contrast to models such as GPT-4, which commands 1.8 trillion parameters across 120 layers. Despite its modest scale, the model trains with remarkable efficiency on an HPC cluster in Munich, utilising two A100 GPUs, completing each epoch in merely 11 seconds—a speed attributable to both the small dataset and the model’s compact size. The associated code, available on GitHub, was primarily developed for foundational understanding rather than production use. During training, the model’s accuracy steadily improved, with training accuracy reaching approximately 0.47 and validation accuracy stabilising around 0.38, demonstrating its capacity to perfectly reproduce the language patterns observed in the weather reports.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#integrating-time-into-the-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#integrating-time-into-the-transformer-architecture",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.7 Integrating Time into the Transformer Architecture",
    "text": "15.7 Integrating Time into the Transformer Architecture\n\n\n\nSlide 08\n\n\nThe established ‘Vanilla model’ demonstrates a robust capacity to perfectly reproduce the language of weather reports. For instance, when provided with a seed sequence such as ‘During the night, a band…’, the model autoregressively generates coherent and contextually relevant text, closely mimicking actual weather forecasts.\nThe transition to a ‘Time Transformer’ involves a remarkably minimal architectural adjustment. Instead of solely embedding all information within a 512-dimensional latent semantic space, the authors propose reserving one dimension specifically for temporal data. This dedicated dimension explicitly informs each token about the precise date on which its sequence was uttered. The current implementation employs a non-trainable, min-max normalised ‘day of the year’ as this time embedding, a choice driven by the desire to leverage natural seasonal variations inherent in weather data, such as the distinct patterns of snow in winter and heat in summer. Crucially, this design allows for the integration of various other time embedding approaches as required by different datasets or research objectives.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-characteristics-and-tokenisation-for-time-aware-models",
    "href": "chapter_ai-nepi_017.html#dataset-characteristics-and-tokenisation-for-time-aware-models",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.8 Dataset Characteristics and Tokenisation for Time-Aware Models",
    "text": "15.8 Dataset Characteristics and Tokenisation for Time-Aware Models\n\n\n\nSlide 09\n\n\nTo rigorously test their proposed time-aware model, the authors sought a dataset characterised by restricted, repetitive language and a small vocabulary, thereby simplifying the task of pattern learning. The UK Met Office weather reports proved an ideal choice, readily accessible online from the national meteorological service. The ‘Tiny Stories’ dataset was also identified as a potential alternative for future investigations.\nThe data, originally presented as monthly PDFs containing daily reports, was systematically scraped for the period spanning 2018 to 2024. This yielded approximately 2,500 reports, each comprising between 150 and 200 words. The tokenisation process remained intentionally simple, eschewing sub-word segmentation and deliberately neglecting both case and interpunctuation. This streamlined approach underscored the inherent simplicity of the language, resulting in a remarkably compact vocabulary of just 3,400 words across the entire seven-year corpus.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#comparative-architecture-of-vanilla-and-time-transformers",
    "href": "chapter_ai-nepi_017.html#comparative-architecture-of-vanilla-and-time-transformers",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.9 Comparative Architecture of Vanilla and Time Transformers",
    "text": "15.9 Comparative Architecture of Vanilla and Time Transformers\n\n\n\nSlide 10\n\n\nThe ‘Time Transformer’ represents a minimal yet impactful architectural adjustment to the standard Transformer decoder. In a conventional Vanilla Transformer, the input directly flows into an Embedding Layer, typically with a d_model of 512, followed by Positional Encoding, a Dropout layer, and then a series of four Decoder Layers before culminating in a Final Dense Layer and the ultimate output.\nConversely, the ‘Time Transformer’ introduces two distinct inputs: textual data and temporal data. The textual input undergoes embedding into a d_model of 511, whilst the temporal data is embedded into a dedicated d_model of 1. These two embedded streams are then concatenated, maintaining the overall embedding dimension, before proceeding through the familiar sequence of Positional Encoding, Dropout, and the identical stack of four Decoder Layers. The time dimension itself is implemented as a non-trainable, min-max normalised ‘day of the year’, calculated using the formula (day of year - 1) / (365 - 1). This explicit integration directly addresses the fundamental challenge that whilst Large Language Models estimate token probabilities based on preceding sequences, real-world token likelihoods are inherently time-dependent, a dynamic often overlooked by training processes that treat distributions as static. Consequently, without this explicit temporal conditioning, current models can only reflect temporal drift through less efficient in-context learning during inference.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-1---efficient-learning-of-temporal-drift-synonymic-succession",
    "href": "chapter_ai-nepi_017.html#experiment-1---efficient-learning-of-temporal-drift-synonymic-succession",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.10 Experiment 1 - Efficient Learning of Temporal Drift (Synonymic Succession)",
    "text": "15.10 Experiment 1 - Efficient Learning of Temporal Drift (Synonymic Succession)\n\n\n\nSlide 11\n\n\nTo assess the ‘Time Transformer’s’ capacity for efficiently learning temporal drift, the authors conducted a ‘synonymic succession’ experiment. This involved injecting a synthetic, time-dependent drift directly into the training data: the word ‘rain’ was progressively replaced by ‘liquid sunshine’ throughout the year. The objective was to ascertain whether the model could reproduce this engineered temporal dependence within its predicted token sequences for each day of the year.\nThe probability of this replacement followed an S-shaped curve, commencing near zero in January and gradually ascending to approach 1.00 by the year’s end. Analysis of monthly occurrences in the generated sequences clearly demonstrated the model’s successful capture of this drift: ‘Rain’ occurrences predominated in the earlier months, whilst ‘Liquidsunshine’ became significantly more frequent and eventually dominant in the latter half of the year, particularly from August through December.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-2---changing-weather-patterns-and-collocation-fixation",
    "href": "chapter_ai-nepi_017.html#experiment-2---changing-weather-patterns-and-collocation-fixation",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.11 Experiment 2 - Changing Weather Patterns and Collocation Fixation",
    "text": "15.11 Experiment 2 - Changing Weather Patterns and Collocation Fixation\n\n\n\nSlide 13\n\n\nA second experiment, termed ‘changing a weather pattern’ or ‘fixation of a collocation’, further investigated the model’s ability to capture temporal shifts. This involved injecting a synthetic, time-dependent change in co-occurrence: the pattern ‘rain’ followed by any word other than ‘and’ was progressively altered to ‘rain and snow’. Linguistically, this simulates the ‘fixation of a collocation’, akin to the established phrase ‘bread and butter’.\nThe results, visualised through monthly comparisons, clearly demonstrated the injected temporal shift. ‘Rain Only’ occurrences were notably higher in the first half of the year, whilst ‘Rain and Snow’ occurrences became significantly more frequent in the latter half, peaking around October and November. Furthermore, an analysis of attention weights revealed that the token ‘snow’ consistently exhibited the highest attention on ‘rain’, followed by ‘heavy’ and ‘and’ (specifically in Head 5), indicating that the model successfully learned this evolving co-occurrence pattern. Example sentences from Day 1 and Day 363 illustrate this learned association, consistently featuring ‘heavy rain and snow’ as the year progresses.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-next-steps-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-next-steps-and-challenges",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.12 Proof of Concept, Applications, Next Steps, and Challenges",
    "text": "15.12 Proof of Concept, Applications, Next Steps, and Challenges\n\n\n\nSlide 14\n\n\nThe conducted research establishes a clear proof of concept: Transformer-based Large Language Models can be efficiently rendered time-aware through the integration of a dedicated temporal dimension within their token embeddings. This innovation opens several compelling applications. A foundational ‘Time Transformer’, for instance, could provide an exceptional basis for a myriad of downstream tasks involving historical data. Moreover, an instruction-tuned variant would empower users to ‘talk to a specific time’, potentially yielding superior results even in common usage scenarios where interaction with the present is desired. Beyond temporal dynamics, the methodology readily extends to modelling dependencies on other contextual or metadata dimensions, such as country or genre.\nFuture research endeavours include benchmarking this approach against explicit time-token methods and rigorously testing for potential increases in training efficiency. Nevertheless, challenges persist towards broader application. Uncertainty surrounds the feasibility and efficiency of fine-tuning due to the architectural modifications. Furthermore, the approach necessitates a departure from the simplicity of metadata-free self-supervised learning, plunging the authors into the intricate complexities of data curation. Fundamental questions also arise, such as determining the generation time of a given token sequence and exploring the utility of a more modest, targeted encoder model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#supplementary-resource",
    "href": "chapter_ai-nepi_017.html#supplementary-resource",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.13 Supplementary Resource",
    "text": "15.13 Supplementary Resource\n\n\n\nSlide 16\n\n\nA supplementary resource, specifically a ChatGPT conversation, is available for further exploration. This resource can be accessed directly via the provided URL: https://chatgpt.com/c/67b8237a-2a48-8012-9862-80af84830a17.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#additional-visual-materials",
    "href": "chapter_ai-nepi_017.html#additional-visual-materials",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.14 Additional Visual Materials",
    "text": "15.14 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n Failed to analyse slide",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers within the Cascade project, a Marie Curie doctoral network, meticulously explore the computational analysis of semantic change. PhD student Sophia Aguilar leads this investigation, focusing on modelling diverse contextual factors and their intricate interplay. Building upon previous work that modelled distinct context types in isolation, the current objective is to integrate these approaches, thereby illuminating their complex interactions.\nThe chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a pivotal pilot study. Linguists engaged in this endeavour examine how language adapts to real-world transformations, drawing upon register theory and principles of rational communication. The study aims to detect periods of significant linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately comprehend the linguistic mechanisms and communicative drivers underpinning these transformations. To this end, the authors propose a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics, positioning context as a central signal and seeking to overcome limitations of existing methods in capturing the interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot\nWithin the Cascade project, a Marie Curie doctoral network, researchers meticulously investigate the computational analysis of semantic change. PhD student Sophia Aguilar spearheads efforts to model context comprehensively, examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing upon the Royal Society Corpus (RSC). This historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors:\n\nSituational (where)\nTemporal (when)\nExperiential (what)\nInterpersonal (who)\nTextual (how)\nCausal (why)\n\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal higher divergence due to substantial linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time—for example, from 1725 to 1845—reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models\nBeyond temporal detection, the investigation delves into paradigmatic context and the dynamics of conceptual change, referencing work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand who spearheads and propagates these linguistic and conceptual shifts, Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, the research team examines shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, visualised through graphs plotting surprisal against year, supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community using that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. Aguilar’s proposed framework positions context as a central signal for modelling language dynamics, identifying Graph Convolutional Networks (GCNs) as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four stages:\n\nData Sampling: This stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: The process begins by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the authors refine network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research acknowledges several profound questions that delineate its current limitations and chart future directions. A primary concern involves the nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview\nThe research team investigates the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the investigators reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]