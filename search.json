[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html",
    "href": "ai-nepi_001_chapter.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview\nThis workshop convenes researchers to explore the burgeoning applications of Large Language Models (LLMs) within the history, philosophy, and sociology of science (HPSS). It represents a confluence of scholarly interests, aiming to foster discussion on novel AI-assisted methodologies and their potential to transform research in these disciplines. The event, held from April 2nd to 4th, 2025, at TU Berlin and online, brings together diverse perspectives to examine how computational tools can offer fresh insights into scientific discovery, conceptual evolution, and the intricate dynamics of knowledge production.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#overview",
    "href": "ai-nepi_001_chapter.html#overview",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Workshop title slide featuring a blueprint-style illustration, event details including dates, location (TU Berlin, Room H3005, + online), keynote speakers (Iryna Gurevych, Pierluigi Cassotti & Nina Tahmasebi), organisers (Gerd Graßhoff, Arno Simons, Adrian Wüthrich, Michael Zichert), and logos including nepi and ERC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#the-genesis-of-a-collaborative-endeavour",
    "href": "ai-nepi_001_chapter.html#the-genesis-of-a-collaborative-endeavour",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 The Genesis of a Collaborative Endeavour",
    "text": "2.1 The Genesis of a Collaborative Endeavour\nThe workshop emerged from two distinct yet complementary initiatives. One impetus originated within the “Network Epistemology in Practice” (NEPI) project. Arno Simons, a key member of this project, pioneered the training of one of the initial large language models specifically on physics texts, a domain of primary interest to the NEPI team. He proposed a broader discussion of such work, a suggestion that readily convinced Michael Zichert, also integral to the project, who had himself employed LLMs to analyse conceptual issues in physics.\nA second stream of inspiration came from Gerd Graßhoff, a cooperation partner of the NEPI project with a long-standing connection to its members. Professor Graßhoff has consistently championed the application of artificial intelligence in the history and philosophy of science, particularly for analysing processes of scientific discovery. He independently conceived of a workshop focused on novel AI-assisted methods for HPSS. Recognising the shared vision, these initiatives merged, culminating in the present workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#network-epistemology-in-practice-illuminating-knowledge-creation",
    "href": "ai-nepi_001_chapter.html#network-epistemology-in-practice-illuminating-knowledge-creation",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Network Epistemology in Practice: Illuminating Knowledge Creation",
    "text": "2.2 Network Epistemology in Practice: Illuminating Knowledge Creation\nFunding for this workshop stems from the European Research Council (ERC) grant supporting the “Network Epistemology in Practice” (NEPI) project. Within NEPI, researchers investigate the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The objective is to understand more deeply how one of the world’s largest and most prominent research collaborations collectively generates new knowledge.\nTo achieve this, the project employs a dual approach. Firstly, network analysis helps to delineate the communication structures within the collaboration. Secondly, semantic tools, crucially involving the use of LLMs, are utilised to trace the flow of ideas through these established network structures. This application of LLMs to understand idea propagation within complex scientific communities represents a significant area of current research interest.\n\n2.2.1 Broader Applications and Anticipation\nBeyond the specific focus of the NEPI project, this workshop aims to showcase a wide array of other applications for LLMs in HPSS. A rich programme of presentations promises to unveil diverse ideas and methodologies, fostering a vibrant exchange of knowledge and stimulating further innovation in the field.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#essential-contributions-and-support",
    "href": "ai-nepi_001_chapter.html#essential-contributions-and-support",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Essential Contributions and Support",
    "text": "2.3 Essential Contributions and Support\nThe realisation of this event owes much to the dedicated efforts of several individuals. Svenja Goetz, Lea Stengel, and Julia Kim provided invaluable assistance in conceptualising aspects of the workshop and managing numerous administrative and organisational tasks. Their contributions were vital to navigating the complexities of planning.\nFurthermore, the technical execution of the workshop, including the recording of keynotes and other sessions, and the provision of a seamless online experience via Zoom, benefits from the expertise of Oliver Ziegler and his Unicam team. Their support ensures that the proceedings are accessible and well-documented.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#navigating-the-workshop-participation-and-interaction",
    "href": "ai-nepi_001_chapter.html#navigating-the-workshop-participation-and-interaction",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Navigating the Workshop: Participation and Interaction",
    "text": "2.4 Navigating the Workshop: Participation and Interaction\nTo ensure a productive and inclusive environment for all attendees, several modalities for participation and interaction have been established.\n\n2.4.1 Recording and Consent\nParticipants should be aware that all sessions are being recorded. This information was provided during registration, at which point consent was requested. A camera is directed towards the speaker, complemented by four roving microphones and an iPhone serving as a backup audio recorder. Subject to presenter approval, videos of the talks, including the audio of the discussion segments (featuring video of the presenter only, not the audience), will be uploaded to the workshop’s YouTube channel. Attendees with concerns or requiring further information regarding the recording are encouraged to approach the organisers.\n\n\n2.4.2 Engagement and Discussion Protocols\nGiven the large number of participants and the relatively condensed schedule for presentations, a structured approach to questions and comments will facilitate efficient discussions. Following each presentation, approximately four questions or comments will be collected. The presenter will then have the opportunity to address these collectively. This method aims to maximise the exchange of ideas within the available time.\nTo supplement in-session discussions, an Etherpad (or Cryptpad) has been established. A QR code provides access, allowing attendees to post comments or questions after sessions. This platform offers presenters an additional avenue to review and respond to feedback. During sessions, both online participants and those present in person can utilise the Zoom chat function to post questions or comments at any time.\n\n\n2.4.3 Networking Opportunities\nThe workshop design incorporates ample opportunities for informal networking. Lunch breaks, coffee breaks, a modest reception, and a workshop dinner (for which participation was confirmed due to limited seating) are intended to facilitate interaction amongst researchers and fellows. Coffee breaks and refreshments will be available in the main workshop area. Lunch and the reception will take place in room H2051, located down the hall towards the far end of the building, then one floor down. Guidance will be available for navigating to this location.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "href": "ai-nepi_001_chapter.html#introducing-the-keynote-speakers",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Introducing the Keynote Speakers",
    "text": "2.5 Introducing the Keynote Speakers\nThe workshop features distinguished keynote speakers who bring exceptional expertise to the exploration of LLMs in HPSS.\n\n2.5.1 Pierluigi Cassotti and Nina Tahmasebi: Exploring Semantic Change\nThe first keynote address will be delivered by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi is the Principal Investigator of the “Change is Key!” research programme in Gothenburg, and Pierluigi Cassotti is a researcher within this project. Their collective work on semantic change detection is highly regarded, encompassing technical contributions such as benchmark creation and broader considerations regarding the application of data science methods to humanities research questions. This focus aligns perfectly with the workshop’s thematic concerns.\n\n\n\nSlide introducing Keynote 1: ‘Large-scale text analysis for the study of cultural and societal change’ by Pierluigi Cassotti and Nina Tahmasebi, with their portraits and the ‘Change is Key!’ tagline.\n\n\n\n\n2.5.2 Iryna Gurevych: Advancing Cross-Document NLP\nThe second keynote, scheduled for the late afternoon of the following day, will be presented by Iryna Gurevych. Professor Gurevych heads the Ubiquitous Knowledge Processing Lab at the Technical University Darmstadt. Her research focuses on information extraction, semantic text processing, and machine learning, alongside the application of Natural Language Processing (NLP) to the social sciences and humanities. Her expertise in elevating NLP to the cross-document level offers another compelling perspective for the workshop attendees.\n\n\n\nSlide introducing Keynote 2: ‘How to InterText? Elevating NLP to the cross-document level’ by Iryna Gurevych, with her photograph.\n\n\nWith these introductions complete, the stage is set for an engaging exploration of Large Language Models and their transformative potential for the History, Philosophy, and Sociology of Science.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html",
    "href": "ai-nepi_003_chapter.html",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "",
    "text": "Overview\nThis chapter navigates the evolving landscape of Large Language Models (LLMs), commencing with a foundational primer on their core architecture. It then explores their diverse applications within History and Philosophy of Science and Science Studies (HPSS), considering various adaptation strategies. Finally, the chapter offers critical reflections on the specific challenges and opportunities these powerful tools present for HPSS research, emphasising the need for methodological rigour and LLM literacy. The discussion aims to equip readers with a nuanced understanding of LLMs, fostering informed engagement with these transformative technologies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#overview",
    "href": "ai-nepi_003_chapter.html#overview",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "",
    "text": "Today’s Menu slide outlining the chapter’s structure: Primer on LLMs, Applications in HPSS, and Reflections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "href": "ai-nepi_003_chapter.html#a-primer-on-large-language-models",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.1 A Primer on Large Language Models",
    "text": "3.1 A Primer on Large Language Models\nThe journey into contemporary Large Language Models invariably begins with the Transformer architecture, a pivotal development that underpins nearly all modern LLMs. Researchers at Google Brain introduced this model in 2017, initially for machine translation tasks, such as converting German text to English (Vaswani2017?).\n\n\n\nFigure 1: The Transformer - model architecture, illustrating the encoder and decoder components side-by-side. The left diagram details BERT (Encoder) and the right details a generic Decoder architecture.\n\n\nThe Transformer comprises two primary interconnected streams: an encoder and a decoder. In its original translation application, the encoder processes the input sentence (e.g., in German), converting words into numerical representations. These numbers undergo several layers of processing—or ‘crunching’—where contextualised word embeddings become progressively more refined layer by layer. Subsequently, these numerical representations transfer to the decoder stream. The decoder then generates the output sentence (e.g., in English) word by word. Each generated word feeds back into the decoder, influencing the prediction of subsequent words until the complete translated sentence emerges.\nA crucial distinction exists between the operational modes of the encoder and decoder. The encoder reads the entire input sentence simultaneously, allowing each word to interact with every other word in the sentence. This mechanism enables the model to construct a comprehensive representation of the sentence’s complete meaning, capturing what is often termed “bidirectional full-context”. Conversely, the decoder operates sequentially; when generating an English word, it can only consider the words previously generated in that sentence. It cannot ‘look into the future’ because its fundamental task is to predict the next word based on the preceding context.\n\n3.1.1 Evolution into Pre-trained Language Models\nShortly after Vaswani and colleagues published their seminal paper (Vaswani2017?), researchers began re-engineering the encoder and decoder streams individually. This work led to the development of pre-trained language models (PLMs). These PLMs represent a shift away from translation per se, towards models possessing a profound general understanding or generative capacity for language. Such models can subsequently undergo minor additional training, or fine-tuning, to perform a wide array of specific Natural Language Processing (NLP) tasks.\n\n\n\nDiagram illustrating the general Transformer model architecture, showing inputs, embeddings, positional encoding, multi-head attention, and feed-forward layers within both encoder and decoder stacks.\n\n\n\n\n3.1.2 Encoder-based Models: The BERT Family\nThe encoder component of the Transformer architecture gave rise to models like BERT (Bidirectional Encoder Representations from Transformers), first introduced by Devlin and colleagues in 2018 (Devlin2018?). The BERT family of models remains highly influential.\n\n\n\nDiagram comparing BERT (Encoder) on the left with a generic Transformer block on the right, illustrating data flow and key components like multi-head attention and feed-forward networks.\n\n\nBERT’s defining characteristic, inherited from the encoder, is its bidirectionality. Each word in an input sequence can ‘attend’ to all other words in both directions (left and right). This allows BERT to build a deep, contextual understanding of the entire input at once. While the specifics of the acronym “Bidirectional Encoder-based Representations from Transformers” are less critical now, the core principle of full-context understanding remains paramount for these models.\n\n\n3.1.3 Decoder-based Models: The GPT Lineage\nOn the other side of the architectural spectrum, researchers developed models based on the Transformer’s decoder component. Prominent amongst these are the GPT (Generative Pre-trained Transformer) models, which power systems like ChatGPT (Radford2018?).\n\n\n\nDiagram comparing BERT (Encoder) and GPT (Decoder) architectures within the broader Transformer framework, highlighting bidirectional full-context for BERT and unidirectional generative for GPT.\n\n\nGPT models, due to their decoder-based structure, can only consider preceding tokens when generating new text. This unidirectional constraint, however, is precisely what enables them to generate novel text, a capability generally lacking in BERT-like models. Consequently, BERT and GPT models serve fundamentally different purposes: BERT excels at understanding language coherently, whilst GPT excels at producing language.\nBeyond these two primary families, a diverse ecosystem of models exists. Some models combine encoder and decoder functionalities. Others employ sophisticated techniques to make decoders behave more like encoders for specific tasks, such as the XLM and XLNet architectures. Understanding the core distinction between generative models (like GPT) that produce language and full-context models (like BERT) that comprehensively understand sentences provides a crucial foundation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "href": "ai-nepi_003_chapter.html#adapting-and-specialising-language-models",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.2 Adapting and Specialising Language Models",
    "text": "3.2 Adapting and Specialising Language Models\nThe proliferation of scientific language models, as surveyed by Ho and colleagues (Ho2024?), underscores the diverse efforts to tailor these technologies for specific research domains.\n\n\n\nA landscape diagram of Scientific Large Language Models, showing a timeline and categorisation of various models by type (Encoders, Decoders, Enc-Dec) and indicating open-source versus closed-source status.\n\n\nAdapting these powerful pre-trained models to specific scientific language or tasks involves several strategies.\n\n3.2.1 Strategies for Domain Adaptation\nInitial pre-training, where a model first encounters language, demands substantial computational resources and data. During this phase, models learn language by predicting the next token (as in GPT models) or by predicting randomly masked words within a sentence (as in BERT models). For many research groups, undertaking full pre-training from scratch is infeasible.\nA more accessible approach involves continued pre-training. Researchers can take an existing pre-trained model, such as a general BERT model, and continue its training on a specialised corpus, for instance, a collection of physics texts. This allows the model to adapt its parameters to the nuances of that specific domain.\nAlternatively, one can use pre-trained models as feature extractors. By adding a few extra layers on top of a pre-trained model, researchers can train these new layers for specific downstream tasks, such as sentiment classification or named entity recognition.\nContrastive learning offers another key method, particularly for generating sentence or document embeddings. While word embeddings capture semantic relationships between words, many applications require representations for entire sentences or documents. SentenceBERT, for example, employs contrastive learning to fine-tune BERT-like models to produce meaningful sentence embeddings, placing semantically similar sentences close together in the embedding space. This technique is vital for tasks requiring semantic similarity assessment at a level beyond individual words.\n\n\n3.2.2 Retrieval Augmented Generation (RAG)\nRetrieval Augmented Generation (RAG) has emerged as a significant technique for adapting LLMs to specific domains or tasks, often without requiring extensive model retraining. RAG systems typically involve multiple models acting in concert.\n\n\n\nDiagram illustrating the Retrieval Augmented Generation (RAG) process, showing a ‘Retrieval’ phase (querying documents, pooling) and a ‘Generation’ phase (using retrieved context with a generative model).\n\n\nIn a RAG pipeline, a user query (e.g., “What are LLMs?”) is first encoded, often by a BERT-like model, into a sentence embedding. This embedding is then used to search a database of relevant documents, retrieving passages most similar to the query. These retrieved passages provide specific context. The pipeline then integrates this retrieved information into the prompt supplied to a generative model (like GPT). The generative model uses this augmented context to produce a more informed and domain-specific answer. Many contemporary applications, including some functionalities within ChatGPT that involve searching the internet, utilise RAG principles. Reasoning models and the increasingly discussed ‘agents’ are also typically not single LLMs but rather complex systems of LLMs combined with various other tools and data sources.\n\n\n3.2.3 Key Distinctions to Remember\nTo navigate the LLM landscape effectively, several core distinctions warrant reiteration. These include the fundamental differences between encoder, decoder, and encoder-decoder architectures. Grasping various fine-tuning strategies is also essential. Furthermore, understanding the distinction between word embeddings and sentence (or document) embeddings is crucial, as they serve different analytical purposes. Finally, appreciating the different levels of abstraction at which these models operate—from token processing to document-level understanding—helps in selecting and applying them appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "href": "ai-nepi_003_chapter.html#applications-of-llms-in-hpss-research",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.3 Applications of LLMs in HPSS Research",
    "text": "3.3 Applications of LLMs in HPSS Research\nA nascent but growing body of work explores the utility of LLMs as tools within History and Philosophy of Science and Science Studies (HPSS) research. Preliminary surveys reveal several emerging categories of application.\n\n\n\nSlide listing applications of LLMs in HPSS, categorised into: Dealing with data and sources, Knowledge structures, Knowledge dynamics, and Knowledge practices.\n\n\n\n3.3.1 Categorising HPSS Applications\nResearchers are employing LLMs for a variety of tasks:\n\nDealing with data and sources: This includes parsing and extracting structured information from texts, such as publication types, acknowledgements, or citations. Interacting with sources through summarisation or RAG-type ‘chatting with your documents’ also falls into this category.\nAnalysing knowledge structures: LLMs assist in extracting entities like scientific instruments, celestial bodies, or chemical compounds. They also aid in mapping complex relationships, such as those between disciplines, interdisciplinary fields, or science-policy discourses.\nInvestigating knowledge dynamics: Conceptual histories of terms (e.g., “theory” in Digital Humanities, or “virtual” and “Planck” in physics) can be traced using LLM-derived embeddings. Identifying novelty, such as breakthrough papers or emerging technologies, represents another application.\nExamining knowledge practices: LLMs can support argument reconstruction by identifying premises and conclusions or causal relationships. Citation context analysis, an established HPSS method, can be enhanced to determine the purpose or sentiment of citations. Discourse analysis, focusing on elements like hedge sentences, jargon, or boundary work, also benefits from these tools.\n\n\n\n3.3.2 Observed Trends and Recurring Concerns\nThe application of LLMs in HPSS exhibits several notable trends and prompts recurring concerns amongst researchers.\n\n\n\nSlide summarising trends and concerns in LLM use for HPSS: accelerating interest, varying customisation, repeating concerns (resources, opaqueness, data, benchmarks, model trade-offs), and a trend toward accessibility.\n\n\nAn accelerating interest in LLMs is evident, with studies appearing not only in information science journals like Scientometrics and JASIST but also increasingly in journals traditionally less focused on computational methods. This suggests that the semantic capabilities of LLMs are attracting qualitative researchers and philosophers.\nThe degree of customisation varies widely. Some researchers develop new architectures or undertake custom pre-training, whilst others fine-tune existing models or use off-the-shelf tools like ChatGPT.\nSeveral concerns consistently surface. The substantial computational resources required for training and, in some cases, running large models pose a significant barrier. The ‘opaqueness’ or lack of interpretability of some models remains a challenge. A scarcity of suitable training data and domain-specific benchmarks for HPSS tasks is frequently noted. Researchers also grapple with trade-offs between different model types (e.g., BERT-like versus GPT-like). The potential for generative models to ‘hallucinate’ or produce plausible but incorrect information is another significant concern, although this issue is gradually improving with newer models and techniques like RAG.\nDespite these challenges, a trend towards greater accessibility is apparent. Tools like BERTopic, which simplifies topic modelling, are gaining popularity due to their ease of use and robust maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "href": "ai-nepi_003_chapter.html#critical-reflections-and-future-pathways",
    "title": "3  The Transformer’s Legacy: Understanding Large Language Models and their Application in HPSS Research",
    "section": "3.4 Critical Reflections and Future Pathways",
    "text": "3.4 Critical Reflections and Future Pathways\nEngaging with LLMs in HPSS necessitates careful consideration of specific disciplinary challenges, a commitment to building LLM literacy, and adherence to core HPSS methodologies.\n\n\n\nSlide outlining key reflections: Acknowledging HPSS-specific challenges, Building LLM literacy, and Staying true to HPSS methodologies.\n\n\n\n3.4.1 Acknowledging HPSS-Specific Challenges\nSeveral challenges are particular to HPSS contexts. The historical evolution of concepts and language is crucial; LLMs are typically trained on modern language, which may lead to biases or misinterpretations when applied to historical texts. HPSS often adopts a reconstructive, critical, and reflective perspective, seeking to read between the lines and understand texts within their situated socio-historical contexts, including subtle discursive strategies. Current LLMs are not inherently trained for this type of nuanced reading. Furthermore, HPSS research frequently contends with sparse data, multiple languages, archaic scripts, and incompletely digitised archives.\n\n\n3.4.2 The Imperative of LLM Literacy\nTo address these challenges effectively, HPSS researchers must cultivate LLM literacy. This involves familiarising themselves with the underlying principles of LLMs, NLP, and Deep Learning—encompassing both the tools and their theoretical underpinnings. It requires learning to identify the most appropriate model architectures and training regimes for specific HPSS research questions and data. The terminology itself is in flux; the term “LLM” may become less adequate as models become increasingly multimodal, incorporating images, sound, and other data types. The definition of “large” in “Large Language Model” also shifts rapidly with technological advancements. Developing shared datasets and benchmarks tailored to HPSS problems is another vital aspect of building collective literacy and capability. For tasks involving multilinguality, understanding which models are suitable or whether custom training is feasible given available resources becomes paramount.\n\n\n3.4.3 Upholding HPSS Methodological Integrity\nWhilst embracing new tools, it is essential to remain true to established HPSS methodologies. HPSS research problems must be thoughtfully translated into NLP tasks (e.g., classification, generation, summarisation) without allowing the technical task to overshadow or distort the original research question. Simultaneously, LLMs offer new opportunities for bridging qualitative and quantitative approaches, potentially fostering richer, mixed-methods research designs.\nLLMs may offer novel ways to address core HPSS questions. For instance, contextualised word embeddings can track the evolving meanings of concepts like “Planck” across different contexts (Max Planck the person, Planck institutes, the Planck satellite, Planck length), revealing shifts in scientific discourse over time. There is potential, though requiring careful exploration, to use LLMs to investigate complex phenomena such as paradigm shifts and incommensurability.\nFinally, HPSS can reflect on its own pre-history concerning some concepts now central to LLMs. For example, co-word analysis, developed in the 1980s by science studies scholars like Michel Callon and Arie Rip, shares intellectual roots with current embedding-based approaches to mapping knowledge landscapes.\n\n\n3.4.4 The Evolving Landscape: Agents and Beyond\nThe field of language modelling is developing at a rapid pace. The rise of ‘agents’—systems where LLMs interact with other tools and data sources to perform complex tasks autonomously—signals a further evolution. Interestingly, some of the language used by computer scientists to describe these emerging agentic systems echoes concepts from Actor-Network Theory (ANT) and other STS frameworks, suggesting that HPSS theories may offer valuable conceptual tools for understanding and critically engaging with these technological advancements. The journey requires continuous learning, critical assessment, and a commitment to harnessing these powerful models responsibly in the pursuit of insightful HPSS research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Transformer's Legacy: Understanding Large Language Models and their Application in HPSS Research</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html",
    "href": "ai-nepi_004_chapter.html",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "",
    "text": "Overview\nOpenAlex Mapper facilitates the exploration of scholarly literature by projecting search queries from the OpenAlex database onto a pre-computed base-map of scientific articles. The underlying workflow encompasses three principal stages. Initially, an embedding model, Specter 2 (Singh2022?), undergoes fine-tuning to enhance its ability to distinguish between academic disciplines. Subsequently, a base-map materialises from a random sample of 300,000 articles drawn from OpenAlex; their abstracts are embedded and then projected into a two-dimensional space using Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?). This process yields both the visual base-map and a trained UMAP model. Finally, for an individual user query, the system retrieves relevant records from OpenAlex, embeds their abstracts using the fine-tuned Specter 2 model, and projects these new embeddings onto the existing base-map via the trained UMAP model. The outcome is an interactive map, accessible online, which visually situates the user’s query results within the broader landscape of science.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#introduction-to-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.1 Introduction to OpenAlex Mapper",
    "text": "4.1 Introduction to OpenAlex Mapper\nResearchers at Utrecht University and the University of Vienna developed OpenAlex Mapper, a tool designed to navigate and analyse the vast expanse of scholarly communication. Maximilian Noichl, in collaboration with Andrea Loettgers and Taya Knuuttila, spearheaded this initiative, which received support from an ERC grant focused on ‘Possible Life’. The tool offers an interactive platform for investigating interdisciplinary connections and the topical distribution of research. Users can access the slides accompanying this work and interact with the tool itself via the developer’s website (maxnoichl.eu/talk), allowing for a direct engagement with its capabilities. This chapter elucidates the technical underpinnings of OpenAlex Mapper, demonstrates its practical application, and explores its potential contributions to research in the History, Philosophy, and Sociology of Science (HPSS).\n\n\n\nA visual representation of interconnected academic fields, illustrating the concept behind OpenAlex Mapper.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "href": "ai-nepi_004_chapter.html#technical-architecture-of-openalex-mapper",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.2 Technical Architecture of OpenAlex Mapper",
    "text": "4.2 Technical Architecture of OpenAlex Mapper\nThe operational framework of OpenAlex Mapper integrates several sophisticated computational techniques to generate its insightful visualisations. Its construction involved a multi-step process, beginning with model refinement and culminating in a system for dynamic query projection.\n\n\n\nDiagram illustrating the three-stage workflow of OpenAlex Mapper: Finetuning the embedding model, Base-map preparation, and Individual user-query processing.\n\n\n\n4.2.1 Fine-tuning the Embedding Model\nThe initial phase concentrated on refining a language model to better capture the nuances of disciplinary distinctions. Investigators selected Specter 2 (Singh2022?), a language model adept at generating embeddings for scientific documents. They fine-tuned this model using a dataset of articles from closely related disciplinary backgrounds. This procedure trained the model to distinguish more effectively between these proximate fields, thereby improving its sensitivity to disciplinary boundaries. Visualisations produced through UMAP dimensionality reduction during this training process confirmed the enhanced separation of disciplines. These adjustments to the language model were incremental, rather than a complete retraining, yet crucial for the subsequent mapping accuracy.\n\n\n4.2.2 Base-map Preparation\nFollowing the model refinement, attention turned towards constructing the foundational map of scientific literature. For this, researchers utilised the OpenAlex database, a comprehensive and openly accessible repository of scholarly material that surpasses Web of Science and Scopus in its inclusiveness and ease of batch querying. From OpenAlex, they randomly sampled 300,000 article abstracts. These abstracts were then processed using the fine-tuned Specter 2 model to generate high-dimensional embeddings.\nSubsequently, Uniform Manifold Approximation and Projection (UMAP) (McInnes2018?) served to reduce these embeddings from their high-dimensional space to a two-dimensional representation. This projection formed the ‘base-map’, a visual landscape of scientific research. Crucially, the UMAP model trained during this stage was preserved for later use.\n\n\n4.2.3 Processing Individual User Queries\nOpenAlex Mapper empowers users to explore this base-map with their own research questions. An individual can submit an arbitrary OpenAlex search query, typically as a URL. The tool then downloads the corresponding records from OpenAlex, often employing PyAlex for this task. It proceeds to embed the abstracts of these retrieved documents using the same fine-tuned Specter 2 model employed for the base-map.\nThe core of the dynamic mapping lies in the next step: the newly generated embeddings are projected into the two-dimensional space using the previously trained UMAP model. This ensures that the queried articles are positioned on the map as if they had been part of the original layout process. UMAP’s architecture facilitates this projection of new data onto an existing embedding. The final output is an interactive map, available online and for download via data-mapplot, where the user’s query results appear highlighted against the backdrop of the broader scientific landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "href": "ai-nepi_004_chapter.html#utilising-openalex-mapper-a-practical-guide",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.3 Utilising OpenAlex Mapper: A Practical Guide",
    "text": "4.3 Utilising OpenAlex Mapper: A Practical Guide\nEngaging with OpenAlex Mapper is a relatively straightforward process, designed to be accessible for researchers seeking to explore interdisciplinary connections. The tool is available online at https://m7n-openalex-mapper.hf.space.\n\n\n\nThe user interface of OpenAlex Mapper, showing input fields for OpenAlex search URLs and settings for sample size and plot customisation.\n\n\nThe primary workflow involves these steps:\n\nNavigate to the OpenAlex website (openalex.org).\nConduct a search for a topic, author, institution, or any other entity of interest, utilising the full search capabilities of OpenAlex. For instance, one might search for papers discussing “scale-free network models”, articles published by a specific university in a given year, or publications citing a particular seminal work.\nOnce the search results appear, copy the URL from the browser’s address bar. This URL encapsulates the precise query.\nReturn to the OpenAlex Mapper interface and paste this URL into the designated “OpenAlex-search URL” input field.\nAdjust sample settings if necessary. Given that embedding abstracts can be computationally intensive, particularly for large result sets, users can opt to reduce the sample size. Options include selecting the first ‘n’ samples or other sampling methods.\nConfigure plot settings, such as choosing to colour points by publication date or displaying the citation graph over the map.\nClick the “Run Query” button.\n\nBehind the scenes, OpenAlex Mapper then downloads the specified records from OpenAlex. It embeds the abstracts of these documents and projects them onto the base-map. After a processing period, the interactive visualisation appears, allowing users to explore where their query results cluster and how they relate to different regions of the scientific map.\n\n\n\nThe OpenAlex website displaying search results for “scale free networks”, from which a URL can be copied for use in OpenAlex Mapper.\n\n\nThe tool provides immediate visual feedback on the distribution of the queried literature. For example, a search for “coriander” might reveal its presence not only in expected fields like botany or food science but also in unexpected areas such as epidemiology or public health, prompting further investigation into these connections. Similarly, mapping publications on “scale-free network models” can illustrate their prevalence and application across diverse scientific domains.\n\n\n\nThe OpenAlex Mapper interface processing a query for “scale free networks”, with a progress bar indicating the embedding stage.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_004_chapter.html#applications-in-the-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.4 Applications in the History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper offers a valuable methodological addition to the toolkit of HPSS researchers. It aims to help bridge the gap between detailed, qualitative case studies and the broader, large-scale dynamics of contemporary science. Many insights in HPSS derive from close-up views of specific scientific episodes, often based on close reading of texts, interaction with scientists, or ethnographic methods. Whilst these studies provide rich understanding, generalising their findings or validating them in the context of global, rapidly evolving “big science” presents a considerable challenge.\nThe tool assists in addressing questions about the reach, influence, and contextual embedding of scientific ideas, models, and methods. For instance, one might ask: Where did the Hopfield Model, developed in a specific context, truly gain traction and find lasting application across the sciences? OpenAlex Mapper allows for such heuristic, qualitative investigations to be supported and guided by quantitative, large-scale data analysis, whilst always enabling a return to the underlying textual sources.\n\n4.4.1 Investigating Model Templates\nResearchers initially developed OpenAlex Mapper with the study of ‘model templates’ in mind. In the philosophy of science, model templates refer to abstract structural forms of models that recur across diverse scientific disciplines, potentially structuring scientific inquiry in ways orthogonal to traditional disciplinary boundaries. Using the tool, investigators can map the occurrence of specific model templates—such as percolation models, network models, or agent-based models—revealing their distinct, sometimes non-continuous, footprints across the scientific landscape. This visualisation can illuminate how similar formalisms are adopted and adapted in varied epistemic contexts.\n\n\n4.4.2 Mapping Conceptual Landscapes\nThe tool also proves useful for exploring the distribution and interrelation of scientific concepts. For example, one can map the concept of ‘phase transition’ and contrast its disciplinary spread with that of ‘emergence’. Whilst conceptual mapping is an established practice, OpenAlex Mapper extends this capability to broad interdisciplinary contexts, overcoming common difficulties associated with acquiring and harmonising diverse datasets. It allows researchers to visualise how concepts travel, transform, and are contested across different fields.\n\n\n4.4.3 Analysing Methodological Distributions\nA further application lies in examining the distribution of specific research methods. Consider the ongoing debate in philosophy of science regarding the role of machine learning techniques versus classical statistical methods in scientific discovery. OpenAlex Mapper can map the usage of a machine learning technique like the random forest model against a more traditional method such as logistic regression. Observing distinct patterns in their disciplinary uptake—for instance, why neuroscientists might favour random forests whilst psychiatric researchers predominantly use logistic regressions for thematically similar problems—can generate new philosophical questions about methodological choice, epistemic justification, and disciplinary cultures.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "href": "ai-nepi_004_chapter.html#methodological-considerations-and-limitations",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.5 Methodological Considerations and Limitations",
    "text": "4.5 Methodological Considerations and Limitations\nWhilst OpenAlex Mapper offers powerful analytical capabilities, several qualifications warrant attention.\n\n\n\nA slide summarising key qualifications and limitations of the OpenAlex Mapper tool and its underlying data.\n\n\nA list of these considerations includes:\n\nData Source Imperfections: The utility of the tool is intrinsically linked to the comprehensiveness and accuracy of the OpenAlex database. Although data quality is generally reasonable, and comparable to other major bibliographic databases, it is not flawless. Users must remain mindful of potential biases or gaps in the underlying data.\nLanguage Limitations: Currently, the fine-tuned Specter 2 model employed by OpenAlex Mapper processes only English-language texts. This inherently limits the scope of analysis, particularly for research published in other languages or for historical periods where English was less dominant in scientific communication. Future developments could incorporate multilingual models to address this, although high-quality, science-trained multilingual models are not yet widely available.\nDependency on Abstracts and Titles: The embedding process relies on the availability of abstracts or, at a minimum, informative titles. Sources lacking such textual information cannot be effectively processed or mapped, potentially excluding certain types of publications or older literature.\nUMAP Algorithm Characteristics: The UMAP algorithm, central to the dimensionality reduction and visualisation, possesses certain characteristics that influence the output.\n\nStochasticity: UMAP is a stochastic algorithm, meaning that each run can produce slightly different layouts. The generated map represents one realisation amongst many possibilities.\nDimensionality Trade-offs: Projecting high-dimensional data (such as the 768 dimensions of Specter embeddings) into a mere two dimensions inevitably involves trade-offs and potential distortions. The algorithm must prioritise certain relationships, which can lead to some misalignments or compressions of the true semantic distances between documents.\n\n\nA working paper, “Philosophy at Scale: Introducing OpenAlex Mapper,” provides a more detailed exposition of the technical aspects and further discusses these methodological points.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_004_chapter.html#conclusion",
    "href": "ai-nepi_004_chapter.html#conclusion",
    "title": "4  The Workflow and Utility of OpenAlex Mapper",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nOpenAlex Mapper represents a novel approach to navigating and understanding the complex, interconnected web of scientific knowledge. By combining advanced language models with powerful dimensionality reduction techniques, it provides researchers, particularly within HPSS, with an interactive means to explore the distribution of concepts, methods, and intellectual traditions across diverse disciplinary landscapes. Its capacity to ground large-scale visualisations in specific textual sources offers a promising avenue for integrating qualitative insights with quantitative analyses, thereby enriching our understanding of how science operates and evolves. Despite certain limitations inherent in its data sources and algorithms, the tool furnishes a dynamic platform for heuristic investigation and the generation of new research questions about the structure and dynamics of scholarly communication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Workflow and Utility of OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html",
    "href": "ai-nepi_005_chapter.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\n(inproceedings?){DanilovaAangenendt2025, author = {Danilova, Vera and Aangenendt, Gijs}, title = {Post-OCR Correction of Historical German Periodicals using LLMs}, booktitle = {Proceedings of the Third Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2025)}, publisher = {ACL}, year = {2025} }\n(inproceedings?){DanilovaSoderfeldt2025, author = {Danilova, Vera and Söderfeldt, Ylva}, title = {Classifying Textual Genre in Historical Magazines (1875-1990)}, booktitle = {Proceedings of the 9th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2025)}, publisher = {ACL}, year = {2025} }\n(book?){Petrenz2004, author = {Petrenz, P.}, year = {2004}, title = {A placeholder title for Petrenz 2004 on genre classification}, publisher = {Placeholder Publisher} }\n(book?){Kessler1997, author = {Kessler, B. and Nunberg, G. and Schütze, H.}, year = {1997}, title = {Automatic detection of text genre}, booktitle = {Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics}, pages = {32–38}, publisher = {ACL} }\n(book?){Broersma2010, author = {Broersma, M.}, year = {2010}, title = {A placeholder title for Broersma 2010 on communicative strategies}, publisher = {Placeholder Publisher} }\n(inproceedings?){Conneau2020, author = {Conneau, Alexis and Khandelwal, Kartik and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{’a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin}, title = {Unsupervised Cross-lingual Representation Learning at Scale}, booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, month = jul, year = {2020}, address = {Online}, publisher = {Association for Computational Linguistics}, url = {https://www.aclweb.org/anthology/2020.acl-main.747}, doi = {10.18653/v1/2020.acl-main.747}, pages = {8440–8451} }\n(inproceedings?){Devlin2019, author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding}, booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, month = jun, year = {2019}, address = {Minneapolis, Minnesota}, publisher = {Association for Computational Linguistics}, url = {https://www.aclweb.org/anthology/N19-1423}, doi = {10.18653/v1/N19-1423}, pages = {4171–4186} }\n(inproceedings?){Schweter2022, author = {Schweter, Stefan and H tarihi, Erion}, title = {hm{BERT}: Historical Multilingual Language Models for Named Entity Recognition}, booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference}, month = jun, year = {2022}, address = {Marseille, France}, publisher = {European Language Resources Association}, url = {https://aclanthology.org/2022.lrec-1.561}, pages = {5235–5242} }\n(inproceedings?){LepekhinSharoff2022, author = {Lepekhin, Nikita and Sharoff, Serge}, title = {Web Genre Classification with Deep Learning Models: A Comparative Study}, booktitle = {Proceedings of the 2nd Workshop on Resources for Computational Humanities and Social Sciences (ResHum 2022)}, month = jun, year = {2022}, address = {Marseille, France}, publisher = {European Language Resources Association}, url = {https://aclanthology.org/2022.reshum-1.1}, pages = {1–9} }\n(inproceedings?){KuzmanLjubesic2023, author = {Kuzman, Taja and Fi{}er, Darja and Ljube{}i{'c}, Nikola}, title = {Leveraging Pretrained Language Models for Web Genre Identification}, booktitle = {Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing}, month = sep, year = {2023}, address = {Varna, Bulgaria}, publisher = {INCOMA Ltd.}, url = {https://aclanthology.org/2023.ranlp-1.69}, pages = {620–629} }\n(inproceedings?){Laippala2023, author = {Laippala, Veronika and Luotolahti, Juhani and Ginter, Filip and Kanerva, Jenna and Salakoski, Tapio}, title = {Fin{GENRE}: A Finnish Multi-Genre Corpus with Genre Boundary Detection}, booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)}, month = may, year = {2023}, address = {Tórshavn, Faroe Islands}, publisher = {University of Tartu Library}, url = {https://aclanthology.org/2023.nodalida-1.15}, pages = {140–151} }\n(inproceedings?){Kuzman2023, author = {Kuzman, Taja and Ljube{}i{'c}, Nikola and Fi{}er, Darja}, title = {{X-GENRE}: A Cross-lingual Open-source Text Genre Classification Dataset and Models}, booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, month = may, year = {2023}, address = {Dubrovnik, Croatia}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2023.eacl-main.211}, pages = {2901–2913} }\nThis chapter delves into the ActDisease project, exploring its objectives, the dataset of historical medical periodicals it employs, and the inherent challenges encountered during dataset digitisation. Subsequently, it details a series of genre classification experiments. These experiments encompass the motivation behind genre classification, an examination of zero-shot and few-shot classification techniques, and specific trials involving few-shot prompting with the Llama-3.1 8b Instruct model. The chapter culminates in a conclusion that synthesises the findings and their implications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#about-actdisease",
    "href": "ai-nepi_005_chapter.html#about-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 About ActDisease",
    "text": "5.1 About ActDisease\nThe ActDisease project, an initiative funded by the European Research Council (ERC), investigates the histories of patient organisations across Europe. Central to this research are the periodicals published by these organisations in England, Germany, France, and Great Britain. These documents serve as the primary source material for understanding the evolution and impact of patient advocacy.\n\n\n\nTitle slide of the presentation ‘GENRE CLASSIFICATION FOR HISTORICAL MEDICAL PERIODICALS’, ActDisease Project, listing authors and affiliation.\n\n\n\n5.1.1 About the Project\nActDisease, an acronym for ‘Acting out Disease – How Patient Organizations Shaped Modern Medicine’, is an ERC-funded research endeavour. Its core purpose is to study how patient organisations in 20th-century Europe contributed to shaping disease concepts, illness experiences, and medical practices. The project focuses on ten European patient organisations from Sweden, Germany, France, and Great Britain, covering a period from approximately 1890 to 1990. The principal source materials are the periodicals, mostly magazines, produced by these patient organisations.\n\n\n\nSlide describing the ActDisease project: its funding, purpose, focus, and main source material, with an image of Heligoland, Germany.\n\n\n\n\n5.1.2 Dataset Description\nThe ActDisease dataset comprises a private, recently digitised collection of patient organisation magazines. This collection encompasses materials from Germany, Sweden, France, and the United Kingdom, covering diseases such as allergy/asthma, diabetes, multiple sclerosis, lung diseases, and rheumatism/paralysis. The accompanying image displays a table that summarises the magazines by country, disease, total page count, and year coverage, amounting to 96,186 pages in total. Initial explorations reveal a diverse array of text types within these materials, with notable similarities in content across all magazines.\n\n\n\nSlide detailing the ActDisease Dataset with a table summarising magazines by country, disease, size, and year coverage, alongside example magazine covers.\n\n\n\n\n5.1.3 Dataset Digitisation Challenges\nThe digitisation process for the ActDisease dataset primarily involved Optical Character Recognition (OCR) using ABBYY FineReader Server 14. Whilst this software performed well on most common layouts and fonts, several challenges persist. Complex layouts, slanted text, rare fonts, and varying scan or photograph quality continue to pose difficulties for OCR accuracy. Consequently, remaining issues include OCR errors, particularly in German and French texts, and disrupted reading order. Researchers conducted experiments on post-OCR correction of German texts using instruction-tuned generative models to address some of these problems (DanilovaAangenendt2025?).\nFurthermore, OCR errors appear frequently in creative texts, such as advertisements, humour pages, and poems. A significant challenge arises from the co-occurrence of different text types within a single page—for instance, an administrative report might appear alongside an advertisement and a humour section. This heterogeneity means that conventional topic models and term counts, which do not account for such juxtapositions, are likely biased towards the most frequent text type on a page.\n\n\n\nSlide outlining digitization challenges, including OCR issues with complex layouts and creative texts, and showing examples of historical periodical pages.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "href": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 Genre Classification Experiments",
    "text": "5.2 Genre Classification Experiments\nThe inherent diversity of texts within the historical medical periodicals necessitates a robust method for distinguishing between them. Genre classification emerges as a pivotal approach to address this need.\n\n5.2.1 Motivation for Genre Classification\nAn examination of the ActDisease materials reveals a wide variety of text types, which, interestingly, exhibit similarities across all magazines. Different text types, such as administrative reports, advertisements, and humour sections, often appear side-by-side on the same page. This textual diversity poses a challenge for analytical methods like yearly and decade-based topic models or term counts, as these methods typically do not account for such internal heterogeneity.\n\n\n\nSlide highlighting challenges in analysing diverse text types within historical magazines.\n\n\nGenre, therefore, presents itself as a useful concept for differentiating kinds of text. In Language Technology, genre is often defined as a class of documents sharing a communicative purpose (Petrenz2004?; Kessler1997?)—a definition that proves highly applicable here. The ability to classify genre is crucial for exploring the data from multiple perspectives to construct historical arguments. Specifically, genre classification enables the comparative study of communicative strategies across different countries, diseases, and publications over time (Broersma2010?). It also facilitates a more fine-grained analysis of term distributions and topic models within distinct genre groups.\n\n\n\nSlide explaining why genre is a useful concept for classification and its benefits for historical analysis.\n\n\nThe ActDisease data showcases a rich tapestry of genres. Examples include poetry, academic reports (such as studies on the pancreas), legal documents (like deeds of covenant), and advertisements (for instance, for chocolate aimed at diabetics). Instructive messages, including recipes or medical advice, feature prominently, alongside patient organisation reports detailing meetings and activities. Narratives about patients’ lives also constitute a significant portion of the content.\n\n\n\nSlide illustrating the variety of genres found in the ActDisease dataset, such as patient experiences, advertisements, and instructive texts.\n\n\n\n\n5.2.2 Zero-Shot and Few-Shot Classification\nGiven the scarcity of annotated data within the ActDisease project, researchers explored both zero-shot and few-shot learning approaches for genre classification (DanilovaSoderfeldt2025?). For zero-shot learning, key research questions focused on whether genre labels from publicly available datasets could be efficiently mapped to the project’s custom labels and how performance would vary across different datasets and models. For few-shot learning, the investigation centred on how performance changes with varying training set sizes across models and whether prior fine-tuning on the full dataset could substantially enhance performance.\n\n\n\nSlide outlining research questions for zero-shot and few-shot learning due to limited annotated data.\n\n\n\n5.2.2.1 Genre Definition and Annotation\nThe project team, under the supervision of the main historian, defined the genre labels. The aim was to create labels that are useful for separating content within the ActDisease materials and sufficiently general for potential application to similar datasets. The defined genres include:\n\nAcademic: Research-based reports or explanations of scientific ideas (e.g., research article, report).\nAdministrative: Documents on organisational activities (e.g., meeting minutes, reports, announcements).\nAdvertisement: Promotes products or services for commercial purposes.\nGuide: Provides step-by-step instructions (e.g., health tips, legal advice, recipes).\nFiction: Entertains and emotionally engages (e.g., stories, poems, humour, myths).\nLegal: Explains legal terms and conditions (e.g., contracts, rules, amendments).\nNews: Reports recent events and developments.\nNonfiction Prose: Narrates real events or describes cultural/historical topics (e.g., memoir, essay, documentary).\nQA (Question & Answer): Structured as questions with expert answers, typically from periodical sections.\n\n\n\n\nSlide presenting a table of genres and their definitions used in the ActDisease project.\n\n\nResearchers selected the paragraph as the annotation unit, merging paragraphs from the ABBYY FineReader output based on font patterns (type, size, bold, italic) within a page. Annotators sampled from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal”, specifically focusing on the first and mid-year issues for each year. A team of four historians and two computational linguists, all either native or proficient in Swedish and German, performed the annotation. Each paragraph received two annotations, achieving an average inter-annotator agreement of 0.95 Krippendorff’s alpha, indicating a high level of consistency. Annotators used a structured file format, assigning hard genre labels to each paragraph.\n \n\n\n5.2.2.2 Data Splits and Distribution\nFor the experiments, researchers first split the annotated data into training and held-out sets, with the held-out set comprising approximately 30% of the data. For few-shot experiments, they further divided the held-out set equally and balanced it by label. Researchers excluded the ‘Legal’ and ‘News’ genres from these few-shot experiments due to insufficient training data. Researchers utilised the entire test set for zero-shot experiments. The distribution of genres across languages (German and Swedish) in the training and held-out samples reveals some imbalances. Notably, there is a strong imbalance in ‘Advertisement’ and ‘Nonfiction Prose’ across the two languages.\n\n\n\nSlide presenting bar charts of genre distribution in the ActDisease training and held-out samples for German and Swedish languages.\n\n\n\n\n5.2.2.3 External Datasets and Genre Mapping for Zero-Shot Learning\nTo facilitate zero-shot experiments, researchers incorporated external datasets. These included modern datasets from previous work on automatic web genre classification: the Corpus of Online Registers of English (CORE) and the Functional Text Dimensions (FTD) dataset, both annotated at the document level. Additionally, they used a sample from Universal Dependencies (UDM) Treebanks, which contains sentence-level annotations in multiple languages.\nTwo annotators independently performed the genre mapping from these external datasets to the ActDisease categories. For the final mapping, researchers selected only assignments with full agreement. This process revealed that for some ActDisease genres, no directly suitable labels existed in the available external datasets. The pipeline for creating training data involved this mapping, followed by preprocessing, chunking, and sampling in several configurations based on language family and label levels (ActDisease original vs. external dataset original).\n\n\n\nSlide showing a table mapping ActDisease genre categories to those in CORE, UDM, and FTD datasets.\n\n\n\n\n5.2.2.4 Models Employed\nResearchers selected multilingual encoders for these experiments, models that have demonstrated success in previous automatic genre classification tasks. The chosen models were:\n\nXLM-RoBERTa (Conneau2020?)\nmBERT (multilingual BERT) (Devlin2019?)\nhistorical mBERT (hmBERT) (Schweter2022?)\n\nBERT-like models have seen extensive use in prior work on web register and genre classification (LepekhinSharoff2022?; KuzmanLjubesic2023?; Laippala2023?). XLM-RoBERTa is recognised as a state-of-the-art web genre classifier (Kuzman2023?). The inclusion of hmBERT was particularly pertinent as it is pretrained on a large corpus of multilingual historical newspapers, encompassing the languages in the ActDisease dataset. mBERT was included for comparison with hmBERT, as direct comparison with XLM-RoBERTa is not straightforward. Fine-tuning these models on all configurations of the training data (derived from FTD, CORE, UDM, and a merged set) yielded a total of 48 fine-tuned models. Researchers typically average subsequent metrics across these configurations.\n \n\n\n5.2.2.5 Zero-Shot Learning Evaluation\nIn evaluating zero-shot learning, the imperfect overlap between label sets necessitated an analysis of individual genres and confusion matrices to avoid potential biases. The state-of-the-art web genre classifier, X-GENRE, served as a baseline, considering only the most similar labels.\n\n\n\nIntroduction slide for Zero-Shot Learning Evaluation.\n\n\nOverall, models fine-tuned on the Functional Text Dimensions (FTD) dataset, using the established mapping, performed better. In most FTD configurations, researchers observed no systematic bias, and per-genre metrics were quite good. An interesting observation emerged: on certain datasets, some models handled specific genres much more effectively than others on average. For instance, XLM-RoBERTa demonstrated superior prediction of ‘QA’ (Question & Answer) texts compared to other models when fine-tuned on UDM. Conversely, hmBERT, when fine-tuned on UDM, showed a 16% average increase in correct ‘Administrative’ predictions over XLM-RoBERTa and mBERT. Models based on the CORE dataset proved adept at predicting the ‘Legal’ genre. However, researchers noted class-specific biases in other datasets: UDM fine-tuning tended towards ‘News’ (as the ‘News’ training data had the highest number of Germanic instances, mostly German), whilst CORE fine-tuning leaned towards ‘Guide’ (as only ‘Guide’ training data in CORE was multilingual).\n\n\n\nSlide summarising key results from zero-shot learning, highlighting performance with FTD and specific model-dataset-genre strengths.\n\n\nConfusion matrices for specific configurations illustrate this behaviour. For example, hmBERT fine-tuned on UDM (hmbert_UDM_True_True) shows strong performance for ‘Administrative’. XLM-RoBERTa fine-tuned on CORE (xlmr_CORE_True_False) effectively identifies ‘Legal’ and ‘Academic’ texts. XLM-RoBERTa fine-tuned on UDM (xlmr_UDM_False_False) excels with ‘QA’. Finally, XLM-RoBERTa fine-tuned on FTD (xlmr_FTD_False_False) accurately classifies ‘Legal’ texts.\n\n\n\nSlide displaying four confusion matrices for different model configurations in zero-shot learning, highlighting specific genre prediction strengths.\n\n\nThe table below presents detailed average F1 scores per category, averaged across data configurations. Highlighted values in the original presentation (not reproduced here as bold text) indicate performance that is not a result of systematic biases towards those categories. Notably, models fine-tuned on FTD and CORE show strong F1 scores for the ‘Legal’ genre. hmBERT (UDM) performs well for ‘Administrative’, and XLM-RoBERTa (UDM) for ‘QA’.\n\n\n\nTable of zero-shot per-category F1 scores averaged across data configurations for different models and datasets.\n\n\nAnalysis of average performance across different training configurations (balancing strategies, language family inclusion) for each external dataset (FTD, CORE, UDM) reveals nuances. For FTD, balancing by original labels alongside ActDisease labels ([B2]) or including only Germanic languages ([G+]) decreased performance compared to balancing by ActDisease labels alone ([B1]) or including all language families ([G-]). For CORE, the small number of Finnish and French instances (in the ‘Guide’ genre) slightly decreased performance. For UDM, the presence of other language families and balancing generally improved performance in terms of macro F1.\n\n\n\nSlide showing average F1 scores for different training configurations within FTD, CORE, and UDM datasets.\n\n\n\n\n5.2.2.6 Few-Shot Learning Evaluation\nThe investigation then turned to few-shot learning scenarios.\n\n\n\nIntroduction slide for Few-Shot Learning Evaluation.\n\n\nExperiments demonstrated how models performed with varying training data sizes, both with and without prior Masked Language Model (MLM) fine-tuning on the entire ActDisease dataset. This prior MLM fine-tuning (+MLM) proved clearly advantageous. F1 scores generally increased with the number of training instances, although they remained below 0.8 even with 1182 instances. Notably, hmBERT-MLM (the historical model with prior fine-tuning) outperformed other models, particularly at larger dataset sizes, boosting its performance significantly and even surpassing other models by a small margin.\n\n\n\nLine graph showing few-shot learning performance (F1 score vs. dataset size) for different models with and without MLM fine-tuning.\n\n\nA detailed examination of scores revealed that hmBERT-MLM’s superior performance is largely attributable to its sustained ability to differentiate between ‘Fiction’ and ‘Nonfiction Prose’ as dataset size increases. In contrast, other models, especially XLM-RoBERTa-MLM, exhibited a drastic drop in performance for ‘Fiction’ when using the full-sized training dataset (1182 instances), often over-predicting ‘Nonfiction Prose’ for ‘Fiction’ instances. Both these genres in the ActDisease data frequently contain narratives about patient experiences, particularly concerning diabetes. It is plausible that with a larger data size, the linguistic features of these two genres become more similar, especially as they are confined to the specific domain of patient organisation magazines focused on diabetes and often share themes and narrative structures. This suggests that more data, or perhaps more nuanced features, might be necessary to improve discrimination between these closely related genres.\n \n\n\n\n5.2.3 Few-Shot Prompting Llama-3.1 8b Instruct\nRecognising the limitations of available data for extensive instruction tuning, researchers also explored few-shot prompting with Llama-3.1 8b Instruct, a prominent multilingual generative model with open weights. The prompt structure incorporated genre definitions and two to three carefully selected examples for each genre. The instruction guided the model to label input text with one of the defined genres based on its perceived purpose and content.\n\n\n\nSlide illustrating the prompt structure used for few-shot prompting of Llama-3.1 8b Instruct, including genre definitions and example placeholders.\n\n\nThe results from few-shot prompting Llama-3.1 8b Instruct on the zero-shot test set (the entire held-out set) indicate that the model handles certain labels reasonably well. For instance, ‘Legal’ texts achieved an F1-score of 0.84, and ‘Academic’ and ‘Advertisement’ texts scored 0.72 and 0.73, respectively. However, the provision of only two or three examples proved insufficient for the model to adequately represent and distinguish more nuanced genres such as ‘Nonfiction Prose’ (F1-score 0.49), ‘Administrative’ (F1-score 0.60), and ‘News’ (F1-score 0.08). The overall macro average F1-score was 0.59. The confusion matrix reveals particular difficulties in distinguishing ‘Nonfiction Prose’ from ‘Fiction’ and ‘Administrative’ texts, and ‘Advertisement’ from ‘Administrative’ texts.\n\n\n\nSlide presenting results (F1 scores and confusion matrix) for few-shot prompting of Llama-3.1 8b Instruct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#conclusion",
    "href": "ai-nepi_005_chapter.html#conclusion",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nHistorical periodicals, particularly popular magazines, represent a promising yet challenging source for research into the history of science and medicine. Their genre-rich nature reflects diverse communicative strategies employed over time. Accurately accounting for these genres is crucial for the detailed interpretation of text mining results.\nThis exploration demonstrates that genre classification can significantly enhance the accessibility of such complex historical sources for computational analysis. When faced with no training data, researchers can successfully leverage available modern datasets, provided the genre categories are sufficiently general-purpose. Alternatively, few-shot prompting of capable open generative models, like Llama-3.1 8b Instruct, can achieve decent quality for some genres, although performance may be limited for categories requiring more nuanced understanding with minimal examples.\nHowever, if some annotated data is available, even in limited quantities, few-shot learning with multilingual encoders—such as XLM-RoBERTa or, notably, historical multilingual BERT (hmBERT)—especially when combined with prior Masked Language Model (MLM) fine-tuning on the target domain data, emerges as a superior strategy. For the ActDisease project, this approach yielded the most promising results, with hmBERT-MLM showing considerable gains in performance.\n\n\n\nSlide summarising the main conclusions regarding genre richness in popular magazines and effective strategies for genre classification.\n\n\nOngoing and future efforts aim to further refine these methodologies and apply them to specific historical hypotheses. This includes developing a new annotation scheme with more fine-grained genres, an annotation project financed by Swe-CLARIN, exploring synthetic data generation techniques, and implementing active learning strategies to improve classifier quality efficiently. These endeavours seek to enhance the utility of these methods for both the ActDisease project and the broader digital humanities community.\n\n\n\nSlide outlining future and present work, including working with historical hypotheses, new annotation schemes, and advanced machine learning techniques.\n\n\n\n\nAcknowledgements\nThe project team extends its gratitude to the annotators: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. We also thank Dr Maria Skeppstedt and the anonymous reviewers for their valuable feedback. This research received funding from the European Research Council (ERC-2021-STG, 101040999). The Centre for Digital Humanities and Social Sciences at Uppsala University provided essential support in the form of GPUs and data storage.\n\n\n\nSlide listing acknowledgements to the project team, reviewers, European Research Council, and Centre for Digital Humanities and Social Sciences.\n\n\nFor further information, please visit the project website.\n\n\n\nThank you slide with a QR code.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html",
    "href": "ai-nepi_006_chapter.html",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "",
    "text": "Overview\nThe VERITRACE web application, currently in its ‘alpha’ stage of development, represents an ambitious step towards new research methodologies. This preliminary version is not yet publicly accessible, requiring substantial further work; it serves more as a promise of future capabilities. Central to its current iteration, researchers are testing a BERT-based Large Language Model (LLM), specifically LaBSE (Language-agnostic BERT Sentence Embedding), to generate vector embeddings. These embeddings aim to represent every passage within the project’s extensive textual corpus. However, initial assessments suggest this model may not ultimately prove sufficient for the complex demands of the research. The screenshots presented herein offer a glimpse into the application’s design and potential, though they remain a very poor substitute for direct interaction with the evolving platform.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "href": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence",
    "text": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence\nThe VERITRACE project, a five-year ERC Starting Grant initiative, embarks on an ambitious journey to trace the intellectual currents flowing from the early modern ‘ancient wisdom’ tradition into the burgeoning field of natural philosophy and science of that era. This tradition manifests in a diverse collection of works, including notable texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and perhaps most famously for historians of chemistry, the Corpus Hermeticum. These 140 core texts form a ‘close reading corpus’, providing a focused lens on this influential body of thought.\n\n\n\nPresentation Title Slide illustrating the VERITRACE project’s scope and context.\n\n\nHistorical records confirm the impact of these ancient wisdom texts; for instance, Newton engaged with the Sibylline Oracles, and Kepler possessed familiarity with the Corpus Hermeticum. Nevertheless, the project seeks to delve deeper, aiming to uncover a far broader network of texts and intellectual connections that interacted with this tradition. Many of these works, often penned by lesser-known authors, constitute what one scholar has termed ‘the great unread’, frequently overlooked by historians due to their sheer volume and obscurity. Consequently, VERITRACE focuses on bringing these neglected sources to light.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)",
    "text": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)\nTo address its core research questions, the VERITRACE project pioneers large-scale, multilingual exploration within the domain of computational History, Philosophy, and Sociology of Science (HPSS). The team develops tools not only for conventional keyword searching but also for the sophisticated identification of textual reuse. This encompasses both direct, lexical quotation—instances where authors use verbatim material from other works, perhaps without explicit citation—and more subtle, indirect influences. Such indirect reuse might involve paraphrase or allusions that, whilst not direct copies, would have been recognisable to contemporary readers as originating from sources like the Corpus Hermeticum.\n\n\n\nThe VERITRACE project team and its mission statement.\n\n\nEffectively, the project endeavours to construct an ‘early modern plagiarism detector’ capable of navigating a vast, multilingual corpus. Beyond identifying direct and indirect textual linkages, a primary objective is to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this comprehensive analytical approach, researchers anticipate the emergence of new patterns and insights into the intellectual history and philosophy of science.\n\n\n\nKey objectives of the VERITRACE project in computational HPSS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "href": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.3 Navigating a Vast Multilingual Corpus",
    "text": "6.3 Navigating a Vast Multilingual Corpus\nThe foundation of this investigation rests upon a large, diverse, and multilingual dataset, focusing exclusively on printed books and texts, thereby excluding handwritten materials from its current scope. This corpus draws from three primary data sources and encompasses works in at least six different languages, published over approximately two centuries. The chronological parameters span from 1540, chosen for specific historical reasons, to 1728, shortly after Newton’s death.\nKey data repositories include:\n\nEarly English Books Online (EEBO)\nGallica, the digital library of the French National Library\nThe Bavarian State Library, which constitutes the largest single source\n\nCollectively, these sources contribute to a corpus of roughly 430,000 books. State-of-the-art digital techniques are employed to analyse this extensive collection of early modern texts.\n\n\n\nOverview of the large, diverse, and multilingual dataset used by VERITRACE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "href": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.4 Core Challenges and the Role of Large Language Models",
    "text": "6.4 Core Challenges and the Role of Large Language Models\nSeveral core challenges are inherent in a project of this scale and complexity. Variable Optical Character Recognition (OCR) quality presents a significant hurdle. The textual data, supplied directly by libraries in raw formats such as XML, HOCR, or even HTML files, often lacks ground truth page images. This variability in OCR accuracy inevitably affects all downstream processing and analytical tasks. Managing early modern typography and semantics across at least six languages introduces further complexities. Furthermore, the sheer volume of data—hundreds of thousands of texts printed across Europe over nearly 200 years—demands robust computational strategies.\nLarge Language Models (LLMs) play a crucial role in addressing these challenges. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, acting as ‘judges’ in this process. Whilst this application holds considerable interest, the current focus shifts towards the encoder side. Here, BERT-based LLMs generate embeddings to encode the semantic meaning of sentences and short passages (groups of sentences) within the textual corpus. This encoding is fundamental to the project’s semantic matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "href": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.5 Employing LLMs as Judges for Metadata Enrichment",
    "text": "6.5 Employing LLMs as Judges for Metadata Enrichment\nOne specific, albeit challenging, application of LLMs within VERITRACE involves their use as ‘judges’ to enrich metadata. The basic motivation stems from the desire to map records from high-quality external sources, such as the Universal Short Title Catalogue (USTC), onto the project’s own records. Successful mapping creates enriched metadata, less likely to require extensive manual cleaning.\n\n\n\nCase study overview of using LLMs as judges to enrich VERITRACE metadata.\n\n\nWhilst some mapping can be automated using external identifiers, many records lack such straightforward connections. Compounding this, much of the project’s internal data has not yet undergone cleaning, making matching a non-trivial task. The manual comparison of bibliographic metadata—assessing pairs of records to determine if they represent the same underlying printed text—is exceedingly tedious. Team members faced the prospect of reviewing tens of thousands of such pairs, highlighting the need for an automated solution.\n\n6.5.1 Initial Attempts and Emerging Hurdles\nTo address this, researchers are exploring a panel, or ‘bench’, of LLMs. Extensive prompt guidelines direct these models to evaluate potential matches, which are initially generated via fuzzy matching algorithms. The LLMs provide yes/no decisions along with reasoning for why a pair of records may or may not represent the same underlying text.\n\n\n\nExample of metadata comparison for LLM judging.\n\n\nThis endeavour remains a work in progress. A major current challenge is the prevalence of hallucinations in the output from the open-source models (e.g., Llama-based) currently under evaluation. Attempts to mitigate this by requesting more structured output, paradoxically, often lead to more generic and less helpful responses, particularly in the reasoning provided by the models. Achieving the right balance in prompting to elicit accurate and insightful judgments is an ongoing refinement process. Despite these initial difficulties, the potential for LLMs to save considerable time in metadata enrichment remains significant, and further investigation is warranted.\n\n\n\nExample of prompt guidelines and LLM output for metadata matching.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "href": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.6 Introducing the VERITRACE Web Application",
    "text": "6.6 Introducing the VERITRACE Web Application\nThe VERITRACE web application serves as the primary interface for exploring the project’s data and analytical tools. This platform is exceptionally new; indeed, its introduction here marks its first public discussion, preceding even internal team dissemination. As an ‘alpha’ version, it is not yet publicly available and remains under active development on a local machine, with screenshots offering a preliminary view. It functions more as a demonstration of the project’s aspirations than a finalised product.\n\n\n\nOverview of the VERITRACE Web Application’s alpha version.\n\n\nCurrently, testing involves a BERT-based LLM (LaBSE) to generate vector embeddings for every passage in the corpus. However, early indications suggest this model may not possess the requisite sophistication for the project’s ultimate goals, particularly for nuanced semantic matching. The application’s development continues, with these initial explorations informing future refinements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "href": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.7 The Data Processing Backbone",
    "text": "6.7 The Data Processing Backbone\nTransforming raw textual data from library sources into a queryable format within an Elasticsearch database—the backend of the web application—involves an intricate data processing pipeline. This multi-stage process is far from a simple button-push operation. Numerous steps are essential to prepare the data for analysis.\n\n\n\nDiagram of the VERITRACE data processing pipeline dashboard and stages.\n\n\nThese steps include:\n\nExtracting text into manageable files.\nGenerating mappings of all character positions.\nSegmenting texts into meaningful units.\nAssessing OCR quality.\n\nEach of these fifteen stages requires careful optimisation. The generation of embeddings, crucial for semantic analysis, occurs near the end of this complex pipeline. Significant background work underpins the functionality accessible through the web interface.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "href": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata",
    "text": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata\nThe VERITRACE web application offers several modules for interacting with the corpus. The ‘Explore’ section, for instance, provides users with comprehensive statistics about the dataset, drawn directly from a MongoDB database. At present, this encompasses 427,305 metadata records describing the books within the collection. This area allows researchers to gain an overview of the corpus’s composition, including language distributions, data sources, publication decades, and prominent publication places.\n\n\n\nThe VERITRACE ‘Explore’ interface showing corpus statistics.\n\n\nBeyond aggregate statistics, a ‘Metadata Explorer’ enables users to browse and inspect the rich metadata associated with each text. A key feature here is detailed language information. Language identification algorithms operate on every text, down to segments of approximately 50 characters. This granularity is vital because many early modern texts are multilingual, often containing sections in Greek or other languages alongside the primary Latin, for example. The system identifies these languages and their proportions within each document—such as a text being 15% Greek and 85% Latin—classifying them as ‘substantively multilingual’.\nFurthermore, the system attempts to assess OCR quality on a page-by-page basis. This is a challenging task without access to ground truth page images, relying instead on analysis of the raw text. Nevertheless, providing page-level quality assessments, rather than a single score for an entire book, offers more nuanced information for researchers. The efficacy of this OCR assessment method continues to be evaluated.\n\n\n\nThe VERITRACE ‘Metadata Explorer’ interface displaying detailed record information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "href": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry",
    "text": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry\nFor many scholars, the ‘Search’ function will likely be the initial point of engagement. The web application supports standard keyword searches across the corpus. Even with a prototype dataset of only 132 files (rather than the full 430,000), the Elasticsearch index already occupies 15 gigabytes, hinting at the terabytes of data the full system will manage. A simple search for “Hermes” in this prototype, for example, might yield 22 documents with 332 total matches.\n\n\n\nThe VERITRACE ‘Search’ interface showing basic and fielded query examples.\n\n\nLeveraging the power of Elasticsearch, users can execute far more complex queries. Fielded queries allow searching within specific metadata, such as finding all books by Kepler that also contain the keyword “Hermes”. Advanced capabilities include Boolean operators (AND, OR), nested queries, and proximity searches—for instance, locating texts where “Hermes” and “Plato” appear within ten words of each other.\nAn ‘Analyse’ section is planned, though not yet implemented. This module will incorporate tools for:\n\nTopic modelling\nLatent Semantic Analysis (LSA)\nDiachronic analysis, to explore linguistic and conceptual shifts over time.\n\nInsights from the wider research community inform the development of these analytical features.\n\n\n\nThe VERITRACE ‘Analyse’ interface showing planned analysis tools.\n\n\nRecognising the importance of accessing original source materials, a ‘Read’ section integrates a Mirador viewer. This allows scholars to view PDF facsimiles of every text in the corpus, alongside its metadata, mirroring the experience of browsing a physical library’s digital collection.\n\n\n\nThe VERITRACE ‘Read’ interface with an integrated Mirador viewer for text facsimiles.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "href": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.10 Unveiling Textual Reuse: The Match Functionality",
    "text": "6.10 Unveiling Textual Reuse: The Match Functionality\nA cornerstone of the VERITRACE web application is its ‘Match’ section, designed to identify textual reuse between different works. This tool allows users to specify query texts and comparison texts. Comparisons can be performed between single documents, across multiple selected documents (e.g., comparing Newton’s Latin Opticks to all of Kepler’s works in the database), or, ambitiously, between one text and the entire corpus. The latter presents considerable computational challenges regarding processing time and user experience, but remains a developmental goal.\n\n\n\nThe VERITRACE ‘Match’ interface for configuring textual similarity comparisons.\n\n\n\n6.10.1 Lexical and Semantic Matching Approaches\nThe system offers two fundamental types of matching:\n\nLexical matching: This approach uses keyword-based techniques to find passages with similar vocabulary. It is effective for identifying direct textual parallels but is language-dependent.\nSemantic matching: Employing vector embeddings, this method seeks conceptually similar passages, even if they share little or no common vocabulary. This is crucial for a multilingual corpus where translations or paraphrases might obscure lexical links.\n\nHybrid approaches, combining lexical and semantic methods with adjustable weighting, are also available.\n\n\n6.10.2 Customisable Parameters for Nuanced Analysis\nRecognising that text matching is not a one-size-fits-all process, the interface exposes numerous parameters for users to tweak. Whilst default settings provide a balanced starting point, users can adjust elements such as minimum similarity scores to refine search results according to their specific research needs. Different matching modes—‘Standard’, ‘Comprehensive’ (for maximum recall, albeit slower), and ‘Faster’ (for higher precision with potentially fewer results)—offer further control over the comparison process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "href": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.11 Validating the Approach: Sanity Checks and Case Studies",
    "text": "6.11 Validating the Approach: Sanity Checks and Case Studies\nTo evaluate the efficacy of the matching tools, researchers conduct several ‘sanity checks’ using known textual relationships. One such check involves comparing Newton’s Latin version of his Opticks (1719 edition) with the English edition from 1718. These texts, being translations of each other, provide a useful test case.\n\n6.11.1 Sanity Check 1: Lexical Matching Across Languages\nWhen a lexical match is performed between the Latin and English editions of Opticks, the expectation is that no significant matches will be found, given their different languages. Using the ‘Standard’ matching mode, this holds true—no matches are reported. Interestingly, the ‘Comprehensive’ mode does identify three matches, revealing small sections of English text, likely from prefatory material, within the predominantly Latin edition. This demonstrates the sensitivity of different modes and confirms the general principle that lexical matching is language-specific.\n\n\n\nResults of a lexical match attempt between Latin and English versions of Newton’s Opticks, showing no significant matches.\n\n\n\n\n6.11.2 Sanity Check 2: Lexical Self-Matching\nAs another baseline, lexically matching a text against itself should, ideally, yield near-perfect results. When Newton’s English Opticks is compared to itself, the system reports a high degree of similarity, with extensive coverage and quality scores. The interface provides detailed statistics, including the number of passages compared and the distribution of similarity scores, offering transparency into the matching process. Automatic highlighting displays the query passage on the left and the comparison passage on the right, along with their similarity score.\n\n\n\nResults of lexically matching Newton’s Opticks to itself, showing high similarity.\n\n\n\n\n6.11.3 Sanity Check 3: Semantic Matching of Translations\nThe real test for the LLM-powered tools comes with semantic matching across languages. When comparing the Latin and English Opticks using semantic matching, the system should identify conceptual similarities despite the linguistic differences.\n\n\n\nConfiguration for a semantic match between Newton’s Latin Optice and its English translation.\n\n\nInitial results from such semantic comparisons appear reasonable. Passages discussing similar concepts, such as colours, are identified as matches, even with underlying OCR imperfections. This suggests that the vector embeddings are capturing some level of conceptual correspondence across translations.\n\n\n\nExamples of semantic matches found between Latin and English versions of Newton’s Opticks.\n\n\n\n\n6.11.4 Preliminary Findings and Model Adequacy\nHowever, the semantic matching performance is not without its issues. Whilst the quality score for identified matches can be high, indicating strong similarity for the pairs found, the coverage score—representing how much of the documents are involved in matches—can sometimes be lower than expected. This discrepancy might, in part, reflect genuine differences between editions; for instance, the Latin edition of Opticks is considerably longer than the English one.\n\n\n\nSummary statistics for the semantic match between Latin and English Opticks, highlighting areas for investigation.\n\n\nNevertheless, further queries using the current LaBSE embedding model suggest it may not be entirely adequate for the nuanced demands of historical textual analysis. The potential for ‘out-of-domain model collapse’—where a model trained on general modern text performs poorly on specialised historical corpora—is a concern.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "href": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.12 Future Directions and Outstanding Challenges",
    "text": "6.12 Future Directions and Outstanding Challenges\nAs the VERITRACE project progresses, several critical issues and areas for development lie on the horizon. The choice of vector embedding model is paramount. LaBSE, selected partly for its efficiency in terms of storage and processing speed, may prove insufficient. Alternative models, such as XLM-Roberta, intfloat multilingual-e5-large, or specialised historical mBERT variants, present other trade-offs between accuracy, storage requirements, and inference time. A fundamental question is whether to persist with pre-trained models or to invest in fine-tuning a base model specifically on the VERITRACE historical corpus.\n\n\n\nA summary of key issues and future challenges for the VERITRACE project.\n\n\nFurther challenges include:\n\nSemantic drift: The meaning of words and concepts changes over time. How effectively current LLMs handle such diachronic semantic shifts across centuries and languages within the same vector space remains an open question.\nOCR quality: Poor OCR accuracy profoundly impacts downstream tasks, from basic sentence segmentation to complex semantic analysis. Re-OCRing the entire corpus is not feasible. Strategies might involve selectively re-OCRing the poorest quality texts or investing effort in locating existing higher-quality digital versions.\nScaling and performance: The current prototype, operating on only 132 texts, already shows query times of around 15 seconds for complex operations. Scaling these capabilities to the full corpus of 430,000 texts will undoubtedly present significant performance engineering challenges.\n\nAddressing these multifaceted issues will be crucial for realising the full potential of VERITRACE to illuminate the complex intellectual heritage of early modern science. Continued research, methodological refinement, and community engagement will guide these future endeavours.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html",
    "href": "ai-nepi_007_chapter.html",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThe field of explainable Artificial Intelligence (AI) garners increasing attention, yet a universally accepted definition of what constitutes an ‘explanation’, particularly within the machine learning community, remains an evolving concept. This chapter delves into the nuances of interpretability for Large Language Models (LLMs), exploring methods that offer transparency, practical applications, and the potential for novel scientific insights, especially within the humanities. We begin by establishing a foundational understanding of explanations in machine learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#overview",
    "href": "ai-nepi_007_chapter.html#overview",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "",
    "text": "Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities. Speaker: Oliver Eberle, Senior Researcher, Berlin Institute for Learning and Data (BIFOLD), TU Berlin. Logos of BIFOLD and TU Berlin.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#understanding-explainable-ai-xai",
    "href": "ai-nepi_007_chapter.html#understanding-explainable-ai-xai",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.1 Understanding Explainable AI (XAI)",
    "text": "7.1 Understanding Explainable AI (XAI)\nHistorically, a significant portion of machine learning development centred on visual data, primarily images. Only in the last decade or so has the field intensified its focus on language, although foundational work in this area extends further into the past. The major shifts in language-focused AI, however, are relatively recent phenomena.\n\n7.1.1 XAI 1.0: Feature Attributions\nTo comprehend the internal workings of ‘black box’ machine learning models, researchers initially concentrated on classification tasks. Typically, an input, such as an image containing a specific object, would be fed into a model, which would then, ideally, produce a correct prediction. Nevertheless, the user often remained unaware of the basis for this classification.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’. Dark blue background with white text.\n\n\nConsequently, the domain of explainable AI dedicated approximately a decade of research to understanding and tracing the origins of these predictions. A common output from such investigations was a heatmap, visually indicating which pixels were most influential in the model’s decision-making process. For instance, a heatmap might clearly show why a model recognised a rooster in an image.\n\n\n\nSlide titled ‘Explainable AI (XAI) 1.0’ with subtitle ‘Feature attributions’. Dark blue background with white text.\n\n\n\n\n7.1.2 The Rationale for Explainability\nThe pursuit of explainability addresses several critical needs. Primarily, it serves to verify predictions, ensuring that a model operates on a reasonable basis. Furthermore, explainability aids in correcting errors and understanding how models make mistakes. It can also illuminate the learning process itself, as models occasionally discover surprising or unconventional solutions to problems. Increasingly, explainability is vital for ensuring compliance with regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-advent-of-generative-ai-expanding-capabilities-and-challenges",
    "href": "ai-nepi_007_chapter.html#the-advent-of-generative-ai-expanding-capabilities-and-challenges",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.2 The Advent of Generative AI: Expanding Capabilities and Challenges",
    "text": "7.2 The Advent of Generative AI: Expanding Capabilities and Challenges\nThe landscape of AI, once dominated by classification models, has undergone a significant transformation with the rise of Generative AI (GenAI) over approximately the last five years. This paradigm shift marks a departure from models designed for specific tasks towards those with multifaceted capabilities.\n\n\n\nDiagram illustrating a black box AI system: input image of a rooster leads to ‘Black Box AI System’, which outputs ‘Rooster’. Citation: Samek et al. (2017).\n\n\nUnlike their predecessors, contemporary GenAI models can classify, identify similar images, generate entirely new images, and respond to queries on a vast array of topics. This versatility, however, introduces considerable complexity in grounding a model’s prediction or an LLM’s answer to specific input features. The following discussion explores avenues beyond simple heatmap representations, considering feature interactions and more mechanistic perspectives to understand these advanced systems. Today’s foundation models function not only as multi-task systems but also as models of the world, capturing insights about society and the evolution of textual features over time, which underpins much of the current interest in them.\n\n\n\nDiagram contrasting classification models with generative AI. Generative AI can produce various outputs like ‘rooster’, ‘similar images’, ‘generate examples’, ‘Q&A’. Mentions ‘beyond heatmaps’, ‘feature interactions’, ‘mechanistic view’. Citation: Samek et al. (2017).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#the-imperative-of-recognising-model-errors",
    "href": "ai-nepi_007_chapter.html#the-imperative-of-recognising-model-errors",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.3 The Imperative of Recognising Model Errors",
    "text": "7.3 The Imperative of Recognising Model Errors\nIt is crucial to acknowledge that even advanced models can, and do, make surprising mistakes. Two well-documented examples illustrate this point. In one instance, a standard object classifier incorrectly based its identification of a boat on the surrounding water rather than the boat itself. The water, being a correlated feature and texturally simpler to detect, became the misleading focal point for the model (Lapuschkin2019?).\nAnother, more recent, example involves multi-step planning errors in LLMs. When tasked with the Tower of Hanoi puzzle—moving disks from a starting peg to a destination peg according to specific rules—an LLM might incorrectly attempt to move the largest, inaccessible disk directly to the target. This demonstrates a failure to comprehend the fundamental physical constraints of the problem (MondalWebb2024?). While more recent reasoning models may exhibit improved performance, such errors have been observed in fairly standard models like Llama 3.something.\n\n\n\nSlide titled ‘Models can make mistakes’. Left: Object detection example (boat identified by water, Lapuschkin et al., Nat Commun ’19). Right: Multi-step planning example (Tower of Hanoi error, Mondal & Webb et al., arxiv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#xai-2.0-towards-structured-interpretability",
    "href": "ai-nepi_007_chapter.html#xai-2.0-towards-structured-interpretability",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.4 XAI 2.0: Towards Structured Interpretability",
    "text": "7.4 XAI 2.0: Towards Structured Interpretability\nTo move beyond the limitations of heatmap-based explanations, the concept of structured interpretability offers a more nuanced approach to understanding model behaviour. This progression is sometimes referred to as XAI 2.0.\n\n\n\nSlide titled ‘XAI 2.0’ with subtitle ‘Structured Interpretability’. Dark blue background with white text.\n\n\n\n7.4.1 First-Order Explanations: Identifying Key Features\nFirst-order explanations, as previously touched upon, are particularly useful for elucidating the decisions of classifiers. They allow for the generation of heatmaps that highlight influential features. For instance, in a project involving a table classifier for historical data, the goal was to distinguish subgroups within these tables. To ensure the classifier operated meaningfully, heatmaps verified that its predictions were based on relevant features. Indeed, these visualisations confirmed that the model correctly focused on numerical content to identify numerical tables—a sensible proxy.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ diagram with x₁ highlighted, and ‘classifier predictions’ with images of historical tables.\n\n\n\n\n7.4.2 Second-Order Explanations: Uncovering Pairwise Relationships\nInvestigations then extended to second-order features, where pairwise relationships between features became significant. This was particularly evident when examining similarity. For example, when calculating a similarity score (e.g., a dot product) between the embeddings of two images or, in this context, two tables, explaining this prediction reveals the importance of feature interactions. Such explanations can highlight interactions between specific digits, confirming, for instance, that two tables are identical and that the model functions as intended.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Left side shows ‘first-order’ diagram and ‘classifier predictions’. Right side is mostly empty. This slide focuses on the first-order aspect before evolving.\n\n\n\n\n7.4.3 Higher-Order Interactions: Revealing Complex Structures\nMore recent work delves into graph structures, where higher-order interactions prove more meaningful. Consider a citation network, or a network of books or other entities, upon which a classification task is trained. In such scenarios, subgraphs or feature walks—sets of features that become relevant collectively—can be identified. This approach yields more complex insights into model behaviour and moves towards a circuit-level understanding of their operations. These explorations represent ongoing efforts in the field of interpretability.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Three columns: ‘first-order’ (x₁ highlighted), ‘second-order’ (x₁-x₄ interaction), ‘higher-order’ (x₁, x₂, x₄ triangle). Below are examples: classifier predictions, pairwise relationships between tables, and graph structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "href": "ai-nepi_007_chapter.html#applications-in-language-and-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.5 Applications in Language and the Humanities",
    "text": "7.5 Applications in Language and the Humanities\nThe principles of structured interpretability find compelling applications when analysing language models and deriving insights within humanities research.\n\n\n\nSlide titled ‘Identifying relevant features and their interactions for interpretability’. Three columns: ‘first-order’, ‘second-order’, ‘higher-order’ with corresponding diagrams and examples. This slide serves as a visual summary before diving into specific examples.\n\n\n\n7.5.1 First-Order Attributions in LLMs: Sentiment Prediction and Bias Detection\nA standard application for first-order attributions involves sentiment prediction, often using movie reviews—a common dataset in the language processing community. By training a model on such reviews and subsequently examining the basis of its predictions, researchers can gain valuable insights. Heatmaps, generated using methods tailored for transformers, can rank sentences and highlight influential tokens.\n\n\n\nSlide titled ‘First-Order Attributions in LLMs’. Dark blue background with white text.\n\n\nSuch analyses have revealed that certain features disproportionately affect sentiment scores. For example, male Western names (e.g., Lee, Barry, Raphael, or references to the Cohen brothers) tend to correlate with positive reviews. Conversely, names perceived as foreign (e.g., Saddam, Castro, Chan) are more likely to be associated with negative scores. These findings underscore the presence of biases within models, a well-recognised issue in the AI community. Explainable AI techniques prove highly effective in detecting these fine-grained biases.\n\n\n7.5.2 First-Order Attributions for Long-Range Dependencies in LLMs\nAnother area of investigation concerns long-range dependencies in LLMs. In a typical scenario, an LLM processes a long context window—perhaps up to 8,000 tokens from Wikipedia articles—and is then prompted to generate a summary. The model begins to produce free text, and the objective is to determine the origin of this generated information within the provided context. Specifically, researchers explore whether models can effectively utilise information from distant parts of the input.\nFindings indicate that models predominantly focus on the latter portions of the context, prioritising information presented more recently. While they can access and pull information from earlier in the context, doing so is significantly less probable (note that analyses often use a log scale for counts). This tendency is important to bear in mind when using LLMs for summarisation; the output may not be a balanced representation of the entire text but rather skewed towards content nearer to the prompt (Jafari2024?).\n\n\n\nSlide titled ‘Ex 1b: First-Order Attributions for Long-Range Dependencies in LLMs’. Setup: generating summaries for long inputs. Results: example text with highlighted tokens indicating source of generated summary. Citation: Jafari et al., MambaLRP (NeurIPS ’24).\n\n\n\n\n7.5.3 Second and Higher-Order Interactions in Text\nMoving to second and higher-order interactions, consider a standard scenario involving sentence embeddings. Given a pair of sentences, such as “A cat I really like” and “It is a great cat,” a model (e.g., BERT or Sentence-BERT) produces an embedding and a similarity score. However, the reasons behind this specific similarity value often remain opaque.\n\n\n\nSlide titled ‘Second & Higher-Order Interactions in Text’. Dark blue background with white text.\n\n\nSecond-order explanations can illuminate these reasons by providing interaction scores between tokens. These scores reveal why the model considered the sentences highly similar. Even in toy examples, common patterns emerge, such as noun-matching strategies (synonyms or identical noun tokens), noun-verb interactions, and connections involving separator tokens. The model’s strategy often resembles a ‘bag of token types’. This suggests that, despite their complexity, models are forced to compress vast amounts of information and, in doing so, may rely on relatively simplistic strategies. This observation, perhaps not immediately intuitive, is relevant for anyone embedding data and subsequently ranking items based on similarity.\n\n\n7.5.4 Graph Neural Networks for Structured Predictions in Language\nGraph Neural Networks (GNNs) offer another avenue for exploring structured information, yielding attributions in terms of ‘walks’ or feature interactions. Intriguingly, GNNs, which inherently encode structural information, can be framed as LLMs. This is because the attention mechanism within transformers essentially dictates how tokens can ‘message pass’ or influence one another. This conceptual link allows for the application of GNN-based interpretability methods to language.\n\n\n\nSlide titled ‘Graph Neural Networks for Structured Predictions’. Shows input graph, interaction, and prediction stages.\n\n\nFor example, standard first-order explanations (akin to a Bag-of-Words approach) may fail to capture the complexity of language, such as negation. A sentence like “First, I didn’t like the boring pictures” might receive a high positive score simply due to the presence of “like,” overlooking the negation. In contrast, more sophisticated higher-order explanation methods can correctly identify that the initial negative phrase contributes negatively, whilst the subsequent positive part of a sentence (“but it is certainly one of the best movies I have ever seen”) is appropriately recognised, respecting the hierarchical structure of the language. This demonstrates how higher-order interactions can lead to a more accurate understanding of complex linguistic structures. Natural language’s hierarchical nature is well-suited to graph representations, and training a GNN (or an LLM framed as such) on tasks like movie review sentiment analysis allows for the extraction of these insightful walks (Schnake2022?).\n\n\n\nSlide titled ‘Ex 3: Interaction of nodes learns complex language structure’. Setup: GNN/LLM on movie review sentiment. Example sentence with standard (BoW) vs. high-order interaction explanations. Citation: Schnake et al., Higher-Order Explanations of Graph Neural Networks via Relevant Walks. (TPAMI ’22).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#ai-driven-scientific-insights-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#ai-driven-scientific-insights-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.6 AI-driven Scientific Insights in the Humanities",
    "text": "7.6 AI-driven Scientific Insights in the Humanities\nThe application of AI, particularly explainable AI (XAI), extends into the humanities, offering novel methodologies for research and discovery. These techniques can help to analyse complex historical and cultural data at scale.\n\n\n\nSlide with diagonal split: left dark blue, right beige with text ‘B. AI-based Scientific Insights in the Humanities’.\n\n\n\n7.6.1 Extracting Visual Definitions from Corpora\nInitial explorations using heatmap-based approaches have proven fruitful in projects involving historical artefacts. For instance, when working with a corpus of mathematical instruments, a classifier was developed to categorise items (e.g., as a ‘machine’ or a ‘mathematical instrument’). In collaboration with historians (Matteo Valleriani, Jochen Büttner, and others), these visual definitions generated by the AI were scrutinised to determine if they could provide more objective classification criteria. This process underscores the necessity of close collaboration with domain experts to ensure the meaningfulness of AI-derived definitions. One finding from this work was that fine-grained scales on mathematical instruments are highly relevant features for the model’s decision-making process (ElHajjEberle2023?).\n\n\n\nSlide titled ‘Ex 4: Extracting visual definitions from corpora’. Left: Engraving of a machine and descriptions of historical instruments. Right: Class-specific heatmap explanations for ‘math. instrument’, ‘machine’, ‘scientific illustration’. Citation: El-Hajj & Eberle+, Explainability and transparency in the realm of DH. (Int J Digit Humanities ’23).\n\n\n\n\n7.6.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA more extensive collaborative project focused on numerical tables from early modern texts, specifically the Sphaera corpus (1472-1650) (Valleriani2019?). Historians approached the Berlin Institute for Learning and Data (BIFOLD) with this data, seeking an automated method to match tables with similar semantics—a task previously unfeasible at scale.\n\n\n\nSlide titled ‘Ex 5: Corpus-level analysis of early modern astronomical tables’. Images of various early modern astronomical tables. Corpora: Sphaera Corpus (Valleriani+’19), Sacrobosco Table Corpus (Eberle+’24).\n\n\n\n7.6.2.1 The XAI-Historian: Aiding Historical Research\nTogether, a workflow was developed to assist historians in gaining insights from this large-scale data. This collaboration gave rise to the concept of the ‘XAI-Historian’—an historian equipped with AI and XAI tools to discover case studies and engage in more data-driven hypothesis generation. Instead of feeding entire tables into a large foundation model (which proved ineffective due to the out-of-domain nature of the data), a smaller, custom model was trained to detect numerical bigrams (pairs of adjacent numbers). XAI methods then verified that this model functioned as intended; for example, by confirming that identical bigrams (e.g., ‘38’ and ‘38’) in two different input tables were correctly identified and matched. This validation engendered trust in the model’s decisions, allowing its use for broader analysis (Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Ex 5: Historical insights at scale: xAI-Historian’. Setup: Historical tables, Sacrobosco Corpus. Diagram of workflow: data collections -&gt; atomization-recomposition (input table, bigram map, histograms) -&gt; corpus-level analysis (embedding, data similarity). Citation: Eberle et al., Historical insights at scale. (Sci Adv ’24).\n\n\n\n\n7.6.2.2 Verifying Modelling and Features using XAI and Historians\nHistorical tables serve as carriers of scientific knowledge processes, such as the mathematisation of science. Representing these tables using a ‘bag of bigrams’ (e.g., ‘01’ or ‘21’) became a key strategy, especially given limited annotations. This involved a learned feature extractor combined with hard-coded structural information. The bigram model’s performance, when verified by expert ground truth, showed strong histogram correlations and superior cluster purity compared to unigram or standard VGG-16 approaches, confirming its efficacy (Eberle2022TPAMI?; Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Verifying modeling and features using XAI and Historians’. Setup: tables as bag of bigrams. Results: scatter plot, bigram model matching, expert ground truth (histogram correlation table, cluster classification bar chart). Citations: Eberle et al. (TPAMI ’22), Eberle et al. (Sci Adv ’24).\n\n\n\n\n7.6.2.3 Cluster Entropy Analysis for Innovation Insights\nWith a reliable model for table representation, case studies commenced. Cluster entropy was employed to investigate the spread of innovation across Europe during the early modern period. The publishing output of various cities was analysed; each city produced a ‘programme’ of printed works, some more diverse than others. Some locations focused on reprinting existing materials, whilst others fostered greater novelty. Quantifying this diversity at scale was previously challenging.\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Setup: cluster entropy. Results: clustering diagram (Lisbon, Venice, Wittenberg), map of Sphaera publication cities. Citation: Eberle et al. (Sci Adv ’24).\n\n\nThe approach involved using the model-derived representations to perform distance-based clustering of tables. The entropy of these clusters for each city then served as a measure of its print programme’s diversity. A low entropy score indicated repetitive content, whereas a higher entropy score signified a more varied output. This analysis identified Frankfurt and Wittenberg as cities with particularly low entropy. Frankfurt am Main was already known as a centre for reprinting editions. More strikingly, Wittenberg’s low diversity was linked to political control by Protestant reformers, who actively limited the print programme and dictated the curriculum to be published. This AI-driven finding corroborated existing historical intuition and uncovered a quantifiable aspect of this historical anomaly (Eberle2024SciAdv?).\n\n\n\nSlide titled ‘Cluster entropy analysis to investigate innovation’. Setup: cluster entropy. Results: table clusters diagram, bar chart of H(p) - H(pmax) for cities, highlighting Frankfurt/Main and Wittenberg with explanations. Citation: Eberle et al. (Sci Adv ’24).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_007_chapter.html#conclusion-the-evolving-role-of-ai-based-methods-in-the-humanities",
    "href": "ai-nepi_007_chapter.html#conclusion-the-evolving-role-of-ai-based-methods-in-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities",
    "section": "7.7 Conclusion: The Evolving Role of AI-based Methods in the Humanities",
    "text": "7.7 Conclusion: The Evolving Role of AI-based Methods in the Humanities\nThe integration of AI-based methods into humanities research presents both significant opportunities and distinct challenges.\n\n\n\nSlide titled ‘Conclusion - AI-based methods for the Humanities’. Bullet points summarising challenges and opportunities.\n\n\nHumanities and Digital Humanities (DH) researchers have historically focused extensively on the digitisation of source material. However, the automated analysis of these corpora is far from trivial, often complicated by data heterogeneity and a scarcity of labels. Multimodality further compounds these complexities.\nNevertheless, Machine Learning (ML) and Explainable AI (XAI) offer the potential to scale humanities research and foster novel research directions. Foundation Models, including LLMs, alongside prompting techniques, can assist with intermediate tasks such as labelling, data curation, and error correction. For more complex research questions, however, their utility currently remains limited.\nA primary roadblock is the challenge posed by low-resource data, which impacts the scaling laws fundamental to many ML models. Furthermore, out-of-domain transfer, especially for historical and small-scale datasets, requires thorough evaluation. Current LLMs are predominantly trained and aligned for natural language tasks and code generation, and their applicability to specialised humanities data cannot be assumed without careful validation. Continued interdisciplinary collaboration will be crucial in navigating these challenges and harnessing the full potential of AI in enriching our understanding of the human past and its cultural artefacts.\n\nNote: A bibliography.bib file with the following (or similar) entries would be required for the citations to render correctly: (article?){Samek2017, title={Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models}, author={Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert}, journal={ITU Journal: ICT Discoveries}, volume={1}, number={1}, pages={39–57}, year={2017} } (article?){Lapuschkin2019, author = {Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert}, title = {Unmasking Clever Hans predictors and assessing what machines really learn}, journal = {Nature Communications}, volume = {10}, number = {1}, pages = {1096}, year = {2019}, doi = {10.1038/s41467-019-08987-4} } (misc?){MondalWebb2024, author = {Mondal, ShB and Webb, N and others}, title = {Multi-step planning mistakes of LLMs}, year = {2024}, eprint = {arXiv:xxxx.xxxxx}, archiveprefix = {arXiv} } (inproceedings?){Jafari2024, title={MambaLRP: Long-Range Positive Explanations for Mamba Models}, author={Jafari, Ali and others}, booktitle={Advances in Neural Information Processing Systems (NeurIPS)}, year={2024} } (article?){Schnake2022, author = {Schnake, Thomas and Eberle, Oliver and Sch{\"u}tt, Kristof T. and M{\"u}ller, Klaus-Robert and Samek, Wojciech}, title = {Higher-Order Explanations of Graph Neural Networks via Relevant Walks}, journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, volume = {45}, number = {5}, pages = {5995–6007}, year = {2022}, doi = {10.1109/TPAMI.2022.3202015} } (book?){Valleriani2019, editor = {Valleriani, Matteo}, title = {The Sphaera Corpus: A Collection of Texts Written in the Context of the Medieval Study of the Sphere}, publisher = {Max Planck Institute for the History of Science}, year = {2019}, series = {Edition Open Access} } (article?){ElHajjEberle2023, author = {El-Hajj, Hiba and Eberle, Oliver and others}, title = {Explainability and transparency in the realm of DH}, journal = {International Journal of Digital Humanities}, year = {2023}, doi = {10.1007/s42803-023-00069-0} } (article?){Eberle2024Sacrobosco, author = {Eberle, Oliver and others}, title = {The Sacrobosco Table Corpus (1472-1650)}, year = {2024}, note = {Details to be confirmed} } (article?){Eberle2024SciAdv, author = {Eberle, Oliver and Valleriani, Matteo and B{\"u}ttner, Jochen and others}, title = {Historical insights at scale: Automated analysis of early modern astronomical tables with explainable AI}, journal = {Science Advances}, year = {2024}, doi = {10.1126/sciadv.adk1000} } (article?){Eberle2022TPAMI, author = {Eberle, Oliver and others}, title = {Learning Semantic Similarity for Numerical Tables}, journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, year = {2022}, doi = {10.1109/TPAMI.2022.xxxxxxx} }",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html",
    "href": "ai-nepi_008_chapter.html",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThe application of Large Language Models (LLMs) to the History, Philosophy, and Sociology of Science (HPSS) presents intriguing possibilities. Nevertheless, current LLM technologies exhibit significant limitations that curtail their utility for rigorous scholarly inquiry. These models frequently require an adversarial mechanism to counteract hallucinations, and a fundamental misunderstanding persists: embedding vectors do not equate to the meanings of expressions. For LLMs to serve as genuine tools for knowledge discovery, they must transcend the mere formulation of plausible yet false statements and avoid the uncritical repetition of information disseminated across internet media. Instead, the pursuit of well-justified conclusions and the capacity to formulate plans for scientific investigation remain critical, yet largely unaddressed, challenges. Presently, no existing model adequately performs these tasks, nor does current technological development offer immediate prospects for achieving these essential goals.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#overview",
    "href": "ai-nepi_008_chapter.html#overview",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Title slide: Modeling Science - LLM for HPSS.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#addressing-current-llm-deficiencies",
    "href": "ai-nepi_008_chapter.html#addressing-current-llm-deficiencies",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Addressing Current LLM Deficiencies",
    "text": "8.1 Addressing Current LLM Deficiencies\nA critical examination reveals several areas where contemporary LLMs fall short of the requirements for robust academic use in HPSS. Fundamentally, an opponent mechanism is needed to actively counter the generation of erroneous or misleading information, commonly termed hallucinations. Furthermore, it is crucial to recognise that embedding vectors, whilst useful for certain computational tasks, do not inherently capture the semantic meaning of expressions.\n\n\n\nKey deficiencies in current LLM capabilities.\n\n\nLLMs intended for scholarly applications should not merely generate text that sounds convincing but may be factually incorrect. Their function must extend beyond echoing content readily available on the internet, which often lacks rigorous verification. Instead, these systems ought to prioritise the generation of information that is best justified by available evidence. Moreover, the ability to develop and propose coherent plans for scientific inquiry represents a sophisticated cognitive function that current LLMs do not possess, even in nascent forms. Addressing these deficiencies is paramount for the responsible and effective integration of LLMs into scientific and historical research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#the-centrality-of-validation-and-computational-epistemology",
    "href": "ai-nepi_008_chapter.html#the-centrality-of-validation-and-computational-epistemology",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 The Centrality of Validation and Computational Epistemology",
    "text": "8.2 The Centrality of Validation and Computational Epistemology\nTo bridge these identified gaps, the concept of validation emerges as an indispensable component. Validation, in this context, encompasses the processes that provide reasons, arguments, and evidence both for and against the truth of a given proposition. It also extends to furnishing justifications for or against the pursuit of particular actions or lines of inquiry.\n\n\n\nThe concept of validation and epistemic agency.\n\n\nAddressing the methodological challenges of validation necessitates a new disciplinary approach, here termed Computational Epistemology. This proposed field would systematically develop methods and methodologies to instil validation capabilities within computational systems. Central to this endeavour is the cultivation of epistemic agency. This involves enabling systems to identify propositions that extend beyond mere sentential forms, to discern and analyse argumentation within texts and historical sources, and to recognise the intentions, plans, and actions of historical persons as documented or inferred from their extant traces.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#ai-assisted-historical-inquiry-in-practice",
    "href": "ai-nepi_008_chapter.html#ai-assisted-historical-inquiry-in-practice",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 AI-Assisted Historical Inquiry in Practice",
    "text": "8.3 AI-Assisted Historical Inquiry in Practice\nA practical implementation of these principles can be observed in a specialised working environment designed for AI-assisted historical investigation. Consider, for instance, a historical inquiry into the construction of the Sanssouci palace, a project involving the distinguished 18th-century mathematician Leonhard Euler. A persistent question in the history of science concerns the extent of Euler’s involvement and whether any failures in the construction were attributable to him or to others; this episode remains one of the significant construction failures of its era, prompting longstanding debates amongst historians.\n\n\n\nA working environment for AI-assisted historical inquiry.\n\n\nWithin this digital workbench, researchers can pose specific queries, such as, “Reconstruct which persons carried out which work on the water fountain.” The objective is to obtain a validated, qualified answer that relies on verifiable evidence, rather than conjecture. The system, employing an AI agent (in this instance, named ‘Bernoulli’), can then analyse historical sources to produce a detailed list of individuals, their specific contributions, timelines, remuneration, and outcomes. This interface typically includes an inquiry window and tools for interacting with the AI agent. However, a primary difficulty arises: effective inquiry demands more than processing a single PDF document. It requires the capacity to search across all available sources, a task that simple indexing and token-based concentration cannot adequately address.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#essential-foundations-for-scholarly-ai-systems",
    "href": "ai-nepi_008_chapter.html#essential-foundations-for-scholarly-ai-systems",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Essential Foundations for Scholarly AI Systems",
    "text": "8.4 Essential Foundations for Scholarly AI Systems\nDeveloping such sophisticated AI-assisted inquiry systems necessitates several core components. Firstly, a scholarly curated editorial board, which has meticulously worked on the primary sources, provides an indispensable foundation. An exemplar of such foundational work is the Opera Omnia of Euler, comprising 86 volumes compiled over approximately 120 years by numerous scholars, a project that concluded only recently with the editing of all his publications and correspondence. This corpus is further complemented by the work of other scholars.\n\n\n\nInterface displaying source material and AI-generated analysis.\n\n\nThis curated collection of content items effectively serves as a substitute for reliance on opaque embeddings. It forms a detailed database encompassing chronologies of actions, communicated expressions (such as letters or publications), the evolution of terminology and language used by historical figures, and records of tools and materials employed. Each item within this inventory is validated by source material, offering a rich, historically grounded record of activities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#leveraging-the-scholarium-with-advanced-multimodal-models",
    "href": "ai-nepi_008_chapter.html#leveraging-the-scholarium-with-advanced-multimodal-models",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Leveraging the Scholarium with Advanced Multimodal Models",
    "text": "8.5 Leveraging the Scholarium with Advanced Multimodal Models\nOnce these meticulously documented records are established within a framework—perhaps termed a ‘Scholarium’—they can be interrogated using advanced, accessible multimodal AI models. Current findings suggest that multimodal models, such as the latest iterations of Google’s Gemini, are particularly well-suited to meet the complex requirements of these tasks. These models possess the capability to synthesise information from diverse sources, including both textual and visual data, thereby enriching the analytical process.\n\n\n\nThe Scholarium concept with examples of curated scholarly editions.\n\n\nThe Scholarium would draw upon significant scholarly undertakings, such as the Opera Bernoulli Euler, Kepler’s Gesammelte Werke, and Brahe’s Opera Omnia. These comprehensive editions represent decades, even centuries, of dedicated scholarship, providing the high-quality, verified data essential for meaningful AI-driven analysis in HPSS.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#implementing-registries-and-standardised-protocols",
    "href": "ai-nepi_008_chapter.html#implementing-registries-and-standardised-protocols",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Implementing Registries and Standardised Protocols",
    "text": "8.6 Implementing Registries and Standardised Protocols\nTo manage and utilise this curated scholarly information effectively, a shift from reliance on simple embeddings towards structured registries is necessary. These registries would catalogue various types of information, including:\n\nPersonal actions: Communication acts such as letters, publications, and reports.\nStatements: Logical implications, arguments, inquiries, and the use of specific language, terminology, concepts, models, methods, tools, data, and evidence.\n\n\n\n\nThe Scholarium’s registry approach and the Model Context Protocol.\n\n\nThe accessibility and interoperability of these registries depend on robust Application Programming Interfaces (APIs). Initiatives like the Model Context Protocol (MCP), advanced by organisations such as Anthropic, offer pathways towards standardising how AI models interact with such rich datasets, ensuring that contextual information is preserved and appropriately utilised.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "ai-nepi_008_chapter.html#ensuring-sustainability-through-fair-infrastructure-and-open-collaboration",
    "href": "ai-nepi_008_chapter.html#ensuring-sustainability-through-fair-infrastructure-and-open-collaboration",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.7 Ensuring Sustainability through FAIR Infrastructure and Open Collaboration",
    "text": "8.7 Ensuring Sustainability through FAIR Infrastructure and Open Collaboration\nThe long-term viability of such scholarly resources hinges upon a robust and Findable, Accessible, Interoperable, and Reusable (FAIR) data infrastructure. Platforms like Zenodo, hosted by CERN in Geneva, offer a reliable solution for storing and publishing these curated datasets, ensuring their availability for many years to come.\n\n\n\nThe Zenodo platform for FAIR data infrastructure.\n\n\nTechnical support for maintaining and developing this infrastructure is also crucial. A startup, OpenScienceTechnology, provides services for running the necessary systems, including an MCP API server. This server facilitates worldwide access to the curated data, enabling artificial intelligence models to conduct inquiries via a standardised API.\n\n\n\nPrinciples of technical support through OpenScienceTechnology.\n\n\nThis entire endeavour thrives on a spirit of open collaboration. The principles guiding this work include commitments to:\n\nOpen Source software\nOpen Access to publications and resources\nOpen Data, supported by mechanisms like the MCP API Server\nOpen Collaboration amongst researchers, institutions, and technical providers\n\n\n\n\nCore tenets of OpenScienceTechnology’s support model.\n\n\nThrough these combined efforts—rigorous scholarship, advanced AI, robust infrastructure, and open practices—we can aspire to create powerful new tools for exploring the history, philosophy, and sociology of science.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "",
    "text": "Overview\nThis study critically examines the capacity of Large Language Models (LLMs) to assess biases within publications classified under the Sustainable Development Goals (SDGs) across three major bibliometric databases: Web of Science, Scopus, and OpenAlex. A primary objective involved employing LLMs, specifically a fine-tuned DistilGPT2 model, not only to detect these biases but also to demonstrate the feasibility of automating information extraction for informing research policy and decision-making.\nThe research team selected five SDGs pertaining to socioeconomic inequalities—SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure). They processed a common corpus of over 15 million publications, indexed across all three databases between January 2015 and July 2023. By fine-tuning separate DistilGPT2 instances for each SDG and database combination, utilising publication titles and abstracts, the team developed a robust method to benchmark LLM-generated content against official SDG targets. This involved crafting specific prompts derived from these targets and meticulously analysing the LLM responses across dimensions such as locations, actors, data/metrics, and thematic focuses.\nKey findings revealed a significant, systematic omission in the database classifications concerning disadvantaged individuals, the poorest nations, and numerous underrepresented topics explicitly mentioned in SDG targets. Conversely, the classifications demonstrated a marked emphasis on economic superpowers and highly developed countries. The study also highlighted divergent methodologies amongst the databases, with Web of Science exhibiting a more theoretical orientation compared to the empirical leanings of Scopus and OpenAlex. These outcomes underscore the profound influence of ostensibly objective bibliometric classification practices on the perceived landscape of SDG-related research, potentially impacting resource allocation and policy formulation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-and-motivation-the-performative-nature-of-bibliometric-databases",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.1 Background and Motivation: The Performative Nature of Bibliometric Databases",
    "text": "9.1 Background and Motivation: The Performative Nature of Bibliometric Databases\n\n\n\nSlide 01\n\n\nMatteo Ottaviani and Stephan Stahlschmidt initiated an investigation into the application of Large Language Models (LLMs) for assessing biases within scientific publications, as classified by major bibliometric databases. This work acknowledges the critical role that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, fulfil within the sociology of science. Indeed, these platforms significantly influence the behaviours and decisions of academics, researchers, funding bodies, and policymakers alike.\nThese databases, however, are far from neutral entities; they respond to diverse political and commercial interests, inherently possessing a performative nature. This performativity shapes how the science system is understood and how value is attributed within it—a concept explored by scholars such as Whitley (2000) and Winkler (1988). The current study specifically considers Web of Science, Scopus, and OpenAlex.\nBuilding upon prior research examining the labelling of Sustainable Development Goals (SDGs) and the construction of search queries, this study addresses a persistent challenge. Armitage et al. (2020), for instance, observed that SDG labelling by various providers yielded disparate results with minimal overlap. Such classification discrepancies can foster divergent perceptions of research priorities. Consequently, these disparities may profoundly impact resource allocation and policy decisions, frequently intertwined with underlying political and commercial interests. This investigation, therefore, scrutinises the aggregate effects arising from how bibliometric databases process metadata and how this subsequently influences diverse stakeholders.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "href": "chapter_ai-nepi_009.html#case-study-llms-for-sdg-research-analysis",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.2 Case Study: LLMs for SDG Research Analysis",
    "text": "9.2 Case Study: LLMs for SDG Research Analysis\n\n\n\nSlide 04\n\n\nThis investigation centres on a case study analysing the representation of United Nations Sustainable Development Goals (SDGs) within bibliometric data, as detailed by Ottaviani & Stahlschmidt (2024). A primary motivation for this research stems from a desire to comprehend the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly given the prospective integration of LLM-based analytical tools. To this end, the investigators employed relatively small pre-trained Large Language Models, selecting DistilGPT2 for its specific characteristics.\nThe core methodological approach involved fine-tuning these LLMs. Researchers trained separate models on distinct subsets of publication abstracts, with each subset corresponding to a particular SDG classification provided by one of the bibliometric databases under review. This strategy enabled the LLM technology to fulfil a dual role: firstly, as an instrument for detecting inherent data biases; and secondly, as a demonstration of concept. This latter role explored the feasibility of LLMs in automating information extraction processes, thereby informing decision-making within the research domain. Ultimately, the project aimed to conduct a broadly applicable exercise, assessing these aggregate effects and gauging their potential impact on research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "href": "chapter_ai-nepi_009.html#methodological-design-dependencies-actors-data-and-initial-classification-comparisons",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons",
    "text": "9.3 Methodological Design: Dependencies, Actors, Data, and Initial Classification Comparisons\n\n\n\nSlide 03\n\n\nThe researchers conceptualised a chain of dependencies to frame their analysis. This chain posits that SDG classification practices define “Research on SDGs,” which in turn informs decision-making aimed at aligning with these goals, ultimately impacting socioeconomic inequalities. Various actors—including researchers, small and medium-sized enterprises (SMEs), and governments—process this “Research on SDGs.” The introduction of an LLM as a bias detector, it is posited, influences the adoption of LLMs in research policy, which itself can affect socioeconomic inequalities.\nThe study focused on three principal bibliometric databases: the proprietary Web of Science (Clarivate, US) and Scopus (Elsevier, UK), alongside the open-access OpenAlex (formerly Microsoft, US). Researchers selected five SDGs directly relating to socioeconomic inequalities. These comprised SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduced Inequalities) for the socio-equity dimension, complemented by SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure) for the economic development dimension.\nA substantial dataset formed the basis of the analysis: a jointly indexed subset of 15,471,336 publications. These publications, shared across all three databases and identified via exact DOI matching, spanned from January 2015 to July 2023. Investigators then applied the distinct SDG classification standards of Web of Science, Scopus, and OpenAlex to this common corpus for the five chosen SDGs. This process yielded three unique subsets of publications for each SDG, one corresponding to each database’s classification.\nInitial comparisons of these SDG-classified papers revealed a strikingly low overlap amongst the databases, a finding consistent with earlier work by Armitage (2020). For instance, concerning SDG4 (Quality Education), only 7.2% of the relevant publications in the shared corpus were classified as such by all three databases. Similarly low intersection rates were observed for SDG5 (Gender Equality) at 4.8%, SDG10 (Reduced Inequalities) at 2.9%, SDG8 (Decent Work) at 2.5%, and SDG9 (Industry/Innovation) at a mere 2.0%. An interesting anomaly noted was that Web of Science classified approximately 10% of its SDG5-related publications as originating from the field of mathematics, including topics like geometrical differential equations, indicating potential classification idiosyncrasies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "href": "chapter_ai-nepi_009.html#llm-implementation-distilgpt2-selection-and-fine-tuning",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning",
    "text": "9.4 LLM Implementation: DistilGPT2 Selection and Fine-Tuning\n\n\n\nSlide 07\n\n\nInvestigators initially conceived of building bespoke Large Language Models, each trained exclusively on publications classified under a specific SDG by a particular bibliometric database. However, developing LLMs entirely from scratch proved a prohibitively resource-intensive endeavour. Consequently, the team adopted a pragmatic compromise: fine-tuning an existing, pre-trained LLM known for having limited prior knowledge, using publication abstracts as the training material.\nThe choice fell upon DistilGPT2. This selection was deliberate, as prominent commercial and large open-source LLMs were deemed ineligible. Such models often possess pre-existing knowledge about SDGs and strong semantic associations derived from their extensive training datasets, which can include sources like Wikipedia and Reddit discussions. DistilGPT2, in contrast, is a lightweight, English-speaking variant of the open-source GPT2 model that utilises a technique called “distillation,” as described by Sanh (2019). With 82 million parameters—significantly fewer than models like GPT-4—DistilGPT2 offered feasibility for working with proprietary datasets; importantly, it was assessed to have no significant prior semantic understanding of the specific publication domain or the prompts to be used.\nThe fine-tuning procedure involved creating 15 distinct LLM instances: one for each of the five selected SDGs, replicated across the three bibliometric databases. For this fine-tuning, researchers utilised the titles and abstracts of the classified publications. The task was structured such that the LLM, when given a new title as a prompt, would generate a new abstract, with the training aimed at maximising the similarity of this output to the characteristics of the source corpus.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "href": "chapter_ai-nepi_009.html#benchmarking-sdg-targets-and-prompt-engineering",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.5 Benchmarking: SDG Targets and Prompt Engineering",
    "text": "9.5 Benchmarking: SDG Targets and Prompt Engineering\n\n\n\nSlide 10\n\n\nTo evaluate the fine-tuned LLMs, researchers developed a benchmarking methodology rooted in the official structure of the UN Sustainable Development Goals. Each SDG is defined by a series of specific targets; for the SDGs under analysis, this typically ranged from eight to twelve targets per goal. For instance, SDG4 (Quality Education) includes targets such as ensuring all children complete primary and secondary education (Target 4.1), providing access to early childhood development, guaranteeing equal access to vocational and tertiary education for all, enhancing youth and adult skills for employment, eliminating gender disparities in education, and ensuring literacy and numeracy for all learners by 2030, as outlined in the UN’s 2030 Agenda for SDGs.\nBased on this structure, the team implemented a systematic prompt generation strategy. For every individual target within each of the five selected SDGs, ten distinct questions, or prompts, were carefully crafted. Each of these prompts was designed to probe different aspects and nuances of its corresponding target. This meticulous process yielded a specific set of 80 to 120 prompts for each SDG.\nThese target-derived prompts formed the cornerstone of the benchmarking standard. Their primary purpose was to establish a ground truth against which the LLM responses could be measured, thereby defining compliance with the stated objectives of the SDGs. Furthermore, this approach facilitated the identification of “biases” or significant informational omissions. The underlying rationale is straightforward: if an LLM, fine-tuned on a corpus of literature purportedly related to a specific SDG, cannot generate relevant responses to prompts directly addressing that SDG’s official targets, it indicates that information crucial to those targets is either missing or substantially underrepresented within the dataset upon which the LLM was trained. This method provides a systematic way to assess both the completeness of the information captured by the database classifications and the potential biases therein.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "href": "chapter_ai-nepi_009.html#research-design-and-analytical-workflow",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.6 Research Design and Analytical Workflow",
    "text": "9.6 Research Design and Analytical Workflow\n\n\n\nSlide 11\n\n\nThe comprehensive research design involved several distinct stages, beginning with the input data: sets of publication abstracts classified under a specific Sustainable Development Goal (SDG#) by one of the three bibliometric databases (DB#). Each of these curated sets of abstracts then served as the training material to fine-tune an instance of the DistilGPT-2 model. This procedure resulted in a collection of specialised LLMs, each uniquely adapted to the content associated with a particular SDG as represented by a specific database (denoted as Fine-tuned DistilGPT-2 SDG# DB#).\nSubsequently, the prompting process commenced. Researchers utilised the previously developed sets of prompts, each tailored to a specific SDG#. These prompts were systematically inputted into the corresponding fine-tuned DistilGPT-2 model. To explore the variability in LLM output and ensure robustness, the team applied three distinct decoding strategies for generating responses: top-k sampling, nucleus (or top-p) sampling, and contrastive search. This approach yielded three distinct sets of responses for every SDG and database combination, reflecting the nuances of each decoding method.\nFor the initial analysis of these generated responses, researchers applied a filter based on the words used in the original prompts. Following this, noun phrases were extracted from the filtered responses, creating a structured dataset of key terms (Noun phrases SDG# DB#). However, the analytical scrutiny extended beyond this. To ensure a thorough and nuanced comparison, investigators also conducted direct searches within the full text of the LLM-generated responses, complementing the insights derived from noun phrase analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "href": "chapter_ai-nepi_009.html#llm-response-analysis-uncovering-biases",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.7 LLM Response Analysis: Uncovering Biases",
    "text": "9.7 LLM Response Analysis: Uncovering Biases\n\n\n\nSlide 12\n\n\nResearchers analysed the LLM-generated responses by matching the extracted noun phrases against the official targets of each Sustainable Development Goal. This analysis was structured around four key data dimensions: Locations, Actors, Data/Metrics, and Focuses. For every SDG under review, the team assessed the degree of compliance with its targets and identified any discernible biases. Importantly, this process also highlighted differences in how the three bibliometric databases represented SDG-related research.\nAn illustrative example using SDG4 (Quality Education) revealed significant omissions. The LLM responses, reflecting the underlying database classifications, inadequately addressed numerous geographical areas, including most African countries (with the exception of South Africa), other developing nations, Least Developed Countries (LDCs), and Small Island Developing States (SIDS). Similarly, critical groups of actors were systematically overlooked, such as vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Many crucial thematic focuses pertinent to SDG4 were also underrepresented or entirely missing from the LLM outputs. These included vocational training, scholarships, the creation of safe and inclusive learning environments, education for sustainable lifestyles, human rights education, the promotion of peace and non-violence, global citizenship, the appreciation of cultural diversity, and even fundamental aspects like free primary and secondary education and tertiary education.\nExtending these observations across all five selected SDGs, several patterns emerged. Regarding locations, LDCs received scant attention, with Sub-Saharan Africa being mentioned primarily in the context of SDG8. The United States held an “undoubted monopoly” in terms of mentions, followed by South Africa and China, and then the UK and Australia. In the realm of metrics and data, the LLMs frequently recalled specific surveys like the Demographic and Health Surveys (DHS) and World Values Survey (WVS) as data sources. Various indicators, benchmarks, and research methodologies—spanning theoretical, empirical, and thematic analyses, as well as market dynamics and macroeconomics—were also identified, with semantic networks formed after fine-tuning indicating recurrent survey data.\nA consistent and concerning finding related to actors: discriminated and vulnerable categories were systematically overlooked across all analysed SDGs. Even when prompts specifically targeted these groups for different SDGs, the LLMs failed to generate substantial, macro-level responses. In terms of thematic focuses, many SDG-specific sensitive topics, such as human trafficking, human exploitation, and migration, were notably absent. Furthermore, the analysis discerned database-specific tendencies. Across three different SDGs, Web of Science’s classified literature leaned towards a more theoretical approach. Conversely, both Scopus and OpenAlex appeared to favour and represent more empirical research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "href": "chapter_ai-nepi_009.html#synthesis-and-limitations",
    "title": "9  Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases",
    "section": "9.8 Synthesis and Limitations",
    "text": "9.8 Synthesis and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s findings highlight a critical issue: employing Large Language Models as an analytical instrument, mediating between the SDG classifications provided by bibliometric databases and their interpretation by policymakers, uncovers a systematic deficiency in the underlying data. Specifically, scientific publications, as classified under various SDGs, frequently overlook the most disadvantaged categories of individuals, the poorest nations, and numerous underrepresented topics that are, in fact, explicit focuses of the SDG targets themselves. In stark contrast, the classified literature demonstrates considerable attention towards economic superpowers and rapidly developing countries. These results unequivocally show how an ostensibly objective, science-informed practice such as the bibliometric classification of SDGs can wield a decisive influence on perceived research landscapes and priorities.\nResearchers also acknowledged several inherent methodological limitations. Large Language Models exhibit high sensitivity to a range of factors. These include the specific model architecture chosen, although DistilGPT2 was selected for its suitability to the task. The nature of the training data is also paramount; this was partly addressed by utilising three distinct databases, which provided varied training corpora for the LLMs. Furthermore, hyper-parameters, general model parameters, and the chosen decoding strategy all significantly influence LLM behaviour. The impact of decoding strategy was partially mitigated by employing three different recognised methods (top-k, nucleus sampling, and contrastive search). Finally, whilst the study employed a general framework, the use of more developed or specialised LLM architectures could potentially reveal different or more nuanced outcomes. Despite efforts to account for variations in training data and decoding strategies, these elements remain influential variables in LLM performance and output.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Leveraging Large Language Models to Assess Biases in Sustainable Development Goal Classifications Across Major Bibliometric Databases</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "",
    "text": "Overview\nResearchers investigate the complexities of parsing footnotes within law and humanities scholarship, a domain poorly served by conventional bibliometric databases. Their work addresses the critical need for accurate citation data to construct citation graphs. These graphs prove invaluable for tracing patterns in knowledge production, reconstructing influences, and measuring the reception of ideas in intellectual history. The project identifies significant deficiencies in existing data sources such as Web of Science, Scopus, and even OpenAlex, particularly concerning non-English, pre-digital, and non-“A-journal” publications. A primary challenge stems from the intricate nature of humanities footnotes—often termed “footnotes from hell”—which contain extensive commentary and messy, embedded references that traditional machine learning tools, such as those based on conditional random forests, struggle to process effectively.\nTo overcome these limitations, the research explores the potential of Large Language Models (LLMs) and Vision Language Models (VLMs) for reference extraction. Recognising the paramount importance of result trustworthiness, the team embarked on creating a high-quality gold standard dataset. This dataset comprises over 1,100 footnotes from 25 articles spanning various languages (French, German, Spanish, Italian, Portuguese) and a significant historical period (1958-2018), meticulously encoded in TEI XML. This standard facilitates interoperability, allows for contextual markup beyond simple reference management, and enables comparison with existing tools like Grobid.\nFurthermore, the researchers developed Llamore, a lightweight Python package designed for reference extraction from text or PDFs using LLMs/VLMs. Llamore supports both open and closed models and provides an evaluation framework based on the F1 score, incorporating a sophisticated alignment process for comparing extracted references against gold standard data. Initial results demonstrate Llamore, using Gemini 2.0 Flash, significantly outperforms Grobid on their specialised humanities dataset. Nevertheless, Grobid remains competitive on datasets it was trained for, such as the PLOS 1000 biomedical dataset. Future work aims to expand the training data, enhance evaluation metrics, and add support for more nuanced citation analysis, including citation context and resolution of abbreviations like op cit.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "href": "chapter_ai-nepi_010.html#the-imperative-of-citation-graphs-addressing-bibliometric-gaps-in-social-sciences-and-humanities",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities",
    "text": "10.1 The Imperative of Citation Graphs: Addressing Bibliometric Gaps in Social Sciences and Humanities\n\n\n\nSlide 01\n\n\nResearchers embark upon the challenge of parsing footnotes within law and humanities scholarship, a task with which current Large Language Models (LLMs) and other algorithms often struggle. Their primary objective involves generating the specific data required to construct comprehensive citation graphs. Such graphs offer powerful tools for intellectual historians, enabling the discovery of patterns and intricate relationships within the production of knowledge. Moreover, they facilitate the reconstruction of scholarly influences and allow for the measurement of how published ideas are received over time. An illustrative application involves tracking shifts in the most-cited authors, exemplified by an interactive web application analysing the Journal of Law and Society between 1994 and 2003.\nAn extremely poor coverage of historical Social Sciences and Humanities (SSH) material by established bibliometric datasources significantly impedes this research. Prominent databases like Web of Science, Scopus, and even the more accessible OpenAlex, prove largely inadequate for this domain, as they simply do not contain the requisite data. Compounding this issue, Web of Science and Scopus prove prohibitively expensive and operate under highly restrictive licences, creating dependencies undesirable for open scholarly inquiry. Whilst OpenAlex offers an open-access alternative, its coverage for the specialised content needed—particularly non-“A-journals,” pre-digital publications, and non-English language works—remains insufficient. For instance, data for the Zeitschrift für Rechtssoziologie, a German journal for law and society, reveals a stark lack of citation information prior to the 2000s in both Dimensions and OpenAlex.\nSeveral factors contribute to this poor coverage. Primarily, commercial interest in humanities scholarship pales in comparison to that for STEM fields, medicine, and economics, which dominate these large bibliometric databases. Furthermore, these platforms typically prioritise the “impact factor” as a metric for science evaluation, a concern quite distinct from the nuanced inquiries of intellectual history.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "href": "chapter_ai-nepi_010.html#the-intricacies-of-humanities-footnotes-limitations-of-conventional-tools",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools",
    "text": "10.2 The Intricacies of Humanities Footnotes: Limitations of Conventional Tools\n\n\n\nSlide 04\n\n\nBeyond database limitations, researchers identify the inherent complexity of humanities footnotes—aptly termed “footnotes from hell”—as a core challenge. These footnotes frequently feature extensive commentary and disordered data, all embedded within a significant amount of textual “noise,” as examples of German and English academic texts illustrate. Consequently, creating accurate training data for these intricate structures becomes an arduous task. Traditional annotation methods demand a laborious process of manually identifying and tagging various bibliographic elements, such as author, title, and publication date, often within specialised software interfaces.\nFurthermore, existing tools, predominantly reliant on Conditional Random Forests and similar machine learning approaches, prove incapable of effectively handling such complex footnotes. Their performance significantly degrades when confronted with this type of data. For instance, performance metrics for the ExCite tool, detailed by Boulanger and Iurshina (2022), demonstrate variable extraction and segmentation accuracy across different training datasets, highlighting the difficulties with footnoted material. The challenges are multifaceted, encompassing varying citation styles, the complexities of multilingual terminology, and the pervasive use of ellipses, abbreviations (like idem or derselbe), and cross-references. Ambiguities, such as discerning whether an initial numeral signifies a volume or a page number, can perplex even human readers. Misleading capitalisation and the appearance of personal names within titles, which are then erroneously identified as authors, further complicate automated extraction. Language models may also struggle with specialised terminology with which they are unacquainted.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "href": "chapter_ai-nepi_010.html#large-language-models-for-reference-extraction-the-imperative-of-rigorous-evaluation",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation",
    "text": "10.3 Large Language Models for Reference Extraction: The Imperative of Rigorous Evaluation\n\n\n\nSlide 10\n\n\nScientists now explore Large Language Models (LLMs) as a promising avenue for tackling reference extraction. Early experiments conducted in 2022 with models like text-davinci-003 already indicated the considerable power of LLMs to extract references from disordered textual data. Newer models, including Vision Language Models (VLMs) capable of directly processing PDF documents, hold the promise of even greater efficacy. Researchers investigate various methods, such as prompt engineering, Retrieval Augmented Generation (RAG), and finetuning, to harness these capabilities.\nNevertheless, a critical question looms: can one trust the results generated by these models? The potential for error, exemplified by a widely reported incident of a lawyer misusing ChatGPT in federal court, underscores this concern. A guiding principle for the research, therefore, necessitates avoiding attempts to solve problems for which no validation data exists. This requires developing a robust testing and evaluation solution. Such a solution must rest upon three pillars:\n\na high-quality Gold Standard dataset\na flexible framework that can readily adapt to the fast-moving landscape of AI technology\nsolid testing and evaluation algorithms capable of producing comparable and reliable metrics",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "href": "chapter_ai-nepi_010.html#crafting-a-high-quality-gold-standard-a-tei-xml-approach",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.4 Crafting a High-Quality Gold Standard: A TEI XML Approach",
    "text": "10.4 Crafting a High-Quality Gold Standard: A TEI XML Approach\n\n\n\nSlide 10\n\n\nAndreas Wagner detailed the team’s efforts to compile a high-quality dataset suitable for both training and evaluation, opting for TEI XML encoding. This choice, whilst perhaps less common in contemporary machine learning circles, stands as the preeminent standard within text-based humanities and digital editorics. Several compelling reasons underpin this decision. TEI XML provides a well-established, comprehensively specified standard for text interchange, surpassing the capabilities of purely bibliographical standards like CSL or BibTeX by covering a broader range of textual phenomena. Crucially, it extends beyond mere reference management to include citations, cross-references, and other forms of contextual markup, which can prove invaluable for tasks such as classifying citation intention. Furthermore, adopting TEI allows researchers to tap into a wealth of existing text collections and corpora from digital editorics projects, many of which publish their source data in this format, sometimes including detailed reference encodings.\nAnother significant advantage of TEI XML lies in the extensive tooling available. Grobid, a prominent tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. Utilising the same data format enables direct performance comparisons with Grobid, facilitates the sharing of training data with the Grobid team and others, and allows the project to leverage Grobid’s existing training resources.\nThe dataset currently under development draws from open-access journals. It involves the meticulous encoding of over 1,100 footnotes extracted from 25 articles, encompassing a diverse range of languages—French, German, Spanish, Italian, and Portuguese—and spanning a considerable period from 1958 to 2018. This collection anticipates yielding over 1,600 individual references; importantly, multiple references to the same work are encoded separately to capture the context of each occurrence. This endeavour remains a work in progress, having adapted its strategy midway to focus on Open Access journals and to incorporate PDFs alongside text, reference strings, and parsed TEI structures. Despite its strengths, TEI XML is no panacea; conceptual challenges, such as distinguishing pointers from references, and technical complexities, like handling constrained elements versus elliptic material, persist. These considerations lead to a fundamental question: how precisely should “performance” be defined and measured in this context?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-llm-driven-reference-extraction-and-assessment",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment",
    "text": "10.5 Llamore: A Python Package for LLM-Driven Reference Extraction and Assessment\n\n\n\nSlide 14\n\n\nDavid Carreto Fidalgo introduced Llamore, an acronym for Large Language Models for Reference Extraction. Engineers developed this Python package to perform two primary functions: firstly, to extract citation data from raw input text or PDF documents utilising (multimodal) LLMs, and secondly, to evaluate the performance of this extraction process. Llamore processes textual or PDF inputs and outputs references formatted as TEI XML. When provided with gold standard references, it generates an F1 score as an evaluation metric.\nTwo principal objectives guided Llamore’s creation. It needed to be lightweight, containing fewer than 2000 lines of code and functioning as an interface to a user’s chosen model rather than embedding models itself. Concurrently, compatibility with both open and closed-source LLMs and VLMs formed a key design consideration. Users can install Llamore via pip. For extraction, one defines an extractor based on the desired model (e.g., GeminiExtractor, OpenAIExtractor). Notably, the OpenAIExtractor ensures broad compatibility with open model serving frameworks like Ollama and VLLM, which typically offer OpenAI-compatible API endpoints. The chosen extractor then processes a PDF or a raw text string, returning references that can be exported to an XML file in TEI biblStruct format. For evaluation, the F1 class is imported and used to compute a macro-average F1 score by comparing the extracted references against gold standard references; users can specify parameters like Levenshtein distance for matching.\nThe evaluation hinges on the F1 score, a well-established metric for structured data comparison, deriving from precision (matches divided by predicted elements) and recall (matches divided by gold elements). An F1 score of 1 signifies perfect extraction, whilst 0 indicates no matches. A crucial aspect of evaluation involves aligning the set of extracted references with the set of gold references. Llamore tackles this by formulating it as an Unbalanced Assignment Problem, employing a solver from the SciPy library. This process involves calculating F1 scores for every possible pairing of extracted and gold references, constructing a matrix of these scores, and then identifying the assignment that maximises the total F1 score under the constraint of unique pairings. This sophisticated alignment ensures accurate macro-averaging, with missing or hallucinated references appropriately penalised with an F1 score of zero. This alignment methodology strongly resembles recent work by Baka and colleagues.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "href": "chapter_ai-nepi_010.html#comparative-performance-key-insights-and-future-directions",
    "title": "10  Large Language Models for Footnote Parsing in Law and Humanities Scholarship",
    "section": "10.6 Comparative Performance, Key Insights, and Future Directions",
    "text": "10.6 Comparative Performance, Key Insights, and Future Directions\n\n\n\nSlide 20\n\n\nTo assess Llamore’s efficacy, researchers conducted comparative performance evaluations. On the PLOS 1000 dataset, comprising 1000 PDFs from the biomedical domain, Llamore (utilising Gemini 2.0 Flash) achieved an F1 score (macro average, exact match) of 0.62, performing on par with Grobid’s score of 0.61. This result is notable given that Grobid was trained on portions of this type of biomedical literature. However, a stark contrast emerged during evaluation on the team’s custom humanities dataset. Here, Grobid’s F1 score plummeted to 0.14, indicating significant difficulty in extracting references. In contrast, Llamore achieved an F1 score of 0.45, demonstrating substantially better, indeed threefold improved, performance on this challenging, footnoted material.\nThese findings lead to several key takeaways. Grobid remains a preferable option for literature similar to its training data, primarily because it operates much faster and consumes fewer resources. Nevertheless, for the complex, footnoted literature characteristic of the humanities, experiments with Llamore paired with Gemini models reveal a significant performance advantage. One must note that these current performance metrics pertain to pure reference extraction and do not yet encompass more nuanced analyses such as citation context or cross-referencing.\nLooking ahead, the team plans to expand their efforts by producing more training data and further refining test metrics. A significant focus will augment Llamore’s capabilities to support more sophisticated analyses. This includes identifying citations in their context (e.g., determining if a citation is approving or critical), resolving abbreviations like op cit., extracting specific pages cited, and accurately counting multiple citations to the same work.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Large Language Models for Footnote Parsing in Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science Dynamics and AI",
    "section": "",
    "text": "Overview\nResearchers at DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, a prominent research institute, have pioneered an innovative AI-driven solution. This system directly addresses the pervasive challenge of information overload within scientific research. Their primary objective centred on crafting a system that enables users to ‘chat with papers’ from a carefully curated collection, specifically focusing on articles from the method-data-analysis (mda) journal.\nThis ambitious endeavour harnesses sophisticated data processing pipelines, collectively termed EverythingData. These pipelines encompass meticulous term extraction, precise embedding generation, and the efficient utilisation of a Qdrant vector store. Central to this architecture is the Ghostwriter interface, which elegantly facilitates natural language interaction with the processed documents. The underlying methodology draws heavily upon Retrieval Augmented Generation (RAG), seamlessly integrating vector spaces with knowledge graphs, notably Wikidata, to significantly enhance contextual understanding and factual accuracy. This approach, inspired by concepts such as GraphRAG, employs a local, one-billion-parameter Large Language Model (LLM) as both a reasoning engine and an intuitive interface; Gemma3 specifically handles translation tasks.\nThe system’s architecture meticulously splits papers into identifiable blocks, links entities to knowledge graphs for robust grounding, and inherently supports multilingual queries. Initial tests, conducted on a collection of 100 mda articles, compellingly demonstrate the system’s capacity to deliver factual, referenced answers. Crucially, it explicitly states when information is absent, thereby preventing hallucinations, and readily permits iterative query refinement. Moreover, this work proposes a novel methodology for creating LLM benchmarks by decoupling knowledge into Wikidata identifiers, thus paving the way for more sustainable and verifiable AI in scientific knowledge production.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "href": "chapter_ai-nepi_011.html#context-science-dynamics-information-overload-and-ai",
    "title": "11  Science Dynamics and AI",
    "section": "11.1 Context: Science Dynamics, Information Overload, and AI",
    "text": "11.1 Context: Science Dynamics, Information Overload, and AI\n\n\n\nSlide 01\n\n\nThis exploration into AI’s pivotal role in science dynamics stems from a collaborative endeavour between DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, a research-active archive. Whilst DANS primarily focuses on data archiving, GESIS also conducts extensive research. The initiative originated from the profound experimentation of Slava Tikhonov, a senior research engineer at DANS, who has meticulously developed intricate data processing pipelines—complex systems aptly characterised by Arno Simons as a ‘tangle of interwoven components’.\nModern sciences evolve with remarkable rapidity and increasing differentiation. This trajectory, however, poses a significant challenge: how can researchers effectively review, evaluate, and select pertinent information from an ever-expanding corpus? Consequently, scholars confront a veritable ‘flood of information’. The ability to locate and comprehend existing knowledge forms a fundamental precondition for creating new insights, whether by individuals or broader academic communities. A central question thus emerges: can machines, particularly the latest AI technologies that have paradoxically contributed to this information proliferation, also assist in the knowledge production process itself? This chapter investigates this crucial question through the lens of Information Retrieval, aiming to elucidate complex AI solutions comprehensively via a practical use case.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "href": "chapter_ai-nepi_011.html#research-objectives-and-system-introduction-ghostwriter-and-everythingdata",
    "title": "11  Science Dynamics and AI",
    "section": "11.2 Research Objectives and System Introduction: Ghostwriter and EverythingData",
    "text": "11.2 Research Objectives and System Introduction: Ghostwriter and EverythingData\n\n\n\nSlide 02\n\n\nInvestigators pursued a specific research question: could they construct an AI solution enabling users to interact conversationally with a selected collection of academic papers? To address this, they developed a system comprising two primary components, internally designated Ghostwriter and EverythingData. Ghostwriter functions as the user-facing interface, whilst EverythingData encompasses the intricate array of backend processes.\nThis chapter introduces foundational concepts such as Information Retrieval, human-machine interaction, and the Retrieval-Augmented Generation (RAG) technique in generative AI. Subsequently, it details the compelling use case involving the method-data-analysis (mda) journal. A core segment elucidates the workflow underpinning this ‘local’ or ‘tailored’ AI solution, providing illustrations of both front-end and back-end operations, before concluding with a summary and outlook.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-approach",
    "title": "11  Science Dynamics and AI",
    "section": "11.3 The Ghostwriter Interface: A Novel Information Retrieval Approach",
    "text": "11.3 The Ghostwriter Interface: A Novel Information Retrieval Approach\n\n\n\nSlide 03\n\n\nResearchers conceptualise the Ghostwriter system as a novel interface for Information Retrieval. Its design philosophy draws inspiration from Slava Tikhonov’s insightful metaphors, which distinguish between two modes of interaction: ‘talking to the librarian’ and ‘talking to the expert’. The ‘librarian’ symbolises engagement with structured data, knowledge organisation systems, and pre-existing classifications. Conversely, the ‘expert’ represents interaction through natural language.\nCrucially, the Ghostwriter interface enables users to converse with both these symbolic entities simultaneously. This capability rests upon a local Large Language Model (LLM) operating on a target data collection, which is further embedded within a network of supplementary data interpretation sources accessible via APIs. This innovative approach seeks to overcome the classic Information Retrieval challenge, where users often require prior knowledge of a database’s schema and typical values to formulate effective queries and obtain optimal results.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "href": "chapter_ai-nepi_011.html#theoretical-framework-retrieval-augmented-generation-rag-with-graphrag",
    "title": "11  Science Dynamics and AI",
    "section": "11.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG",
    "text": "11.4 Theoretical Framework: Retrieval Augmented Generation (RAG) with GraphRAG\n\n\n\nSlide 04\n\n\nThe system’s development firmly situates itself within the broader scientific discourse of Retrieval Augmented Generation (RAG). For a comprehensive introduction to this topic, particularly the integration of knowledge graphs, readers are highly encouraged to consult Philip Rattliff’s seminal paper, ‘GenAI Knowledge Graph The GraphRAG Manifesto: Adding Knowledge to GenAI’ (Neo4j, 11 July 2024). Arno Simons also merits acknowledgement for his significant contributions in this domain, particularly concerning the RAG ‘tool box’.\nThis RAG approach fundamentally comprises three main ingredients. Firstly, researchers construct a vector space from the content of data files, encoding them as embeddings that meticulously capture properties and their attributes; various Machine Learning algorithms and diverse LLMs compute these embeddings. Secondly, a graph forms a metadata layer, seamlessly integrating with diverse ontologies and controlled vocabularies, including those pertinent to responsible AI, and is expressed using the Croissant ML standard. The overarching vision, termed GraphRAG, aims to unify these graph and vector components within a single model. Developers plan to implement this ‘locally’, conceptualising it as a form of Distributed AI where the LLM serves as both an ‘interface’ between human and AI and a ‘reasoning engine’. In practice, the LLM connects to a ‘RAG library’ (representing the graph), navigates datasets, and consumes embeddings (the vectors) to provide essential context for its operations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "href": "chapter_ai-nepi_011.html#system-workflow-from-document-ingestion-to-query-response",
    "title": "11  Science Dynamics and AI",
    "section": "11.5 System Workflow: From Document Ingestion to Query Response",
    "text": "11.5 System Workflow: From Document Ingestion to Query Response\n\n\n\nSlide 05\n\n\nThe system processes information through a clearly defined workflow, commencing with a collection of input documents. For demonstration purposes, researchers utilised articles from the mda journal, scraping a small number, though any document collection can serve as input. These documents first enter the EverythingData backend, where a series of sophisticated operations transform them. Initially, the system ingests information into a vector store, employing Qdrant for this purpose. Subsequent operations include meticulous term extraction, the construction of precise embeddings, and various enrichments. Notably, selected terms become structured data within a graph, further enriched by linking to external resources such as Wikidata. This coupling with knowledge graphs proves crucial, as it significantly enhances the value of words, phrases, and embeddings by adding layers of contextualisation.\nAll processed data then populates a ‘vector space RAG-Graph’, which forms the core reasoning substrate. When a user poses a question in natural language via the Ghostwriter interface, this query directly interacts with the vector space RAG-Graph. In response, the system delivers not only a list of relevant documents, typical of conventional information retrieval systems, but also a generated summary or explanatory text that the underlying ‘machinery’ deems pertinent to the user’s question. An accompanying diagram from TheAidedge.io compellingly illustrates the comparative roles of vector and graph databases within RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "href": "chapter_ai-nepi_011.html#implementation-and-use-case-interacting-with-mda-journal-articles",
    "title": "11  Science Dynamics and AI",
    "section": "11.6 Implementation and Use Case: Interacting with MDA Journal Articles",
    "text": "11.6 Implementation and Use Case: Interacting with MDA Journal Articles\n\n\n\nSlide 07\n\n\nSlava Tikhonov, a senior research engineer at DANS, meticulously detailed the system’s implementation, drawing upon his early engagement with Large Language Models (LLMs) since testing GPT-2 in 2020. His methodology involves deconstructing the LLM training process into smaller, adaptable components. This approach yields a versatile system applicable not only to academic papers but also to diverse web content; for instance, it can interact with spreadsheets, enabling users to query specific values and receive factual, non-hallucinated responses derived exclusively from the spreadsheet’s data. For complex queries, the system employs a one-billion-parameter LLM, significantly enhanced by integrated knowledge graphs.\nThe primary use case centres on the ‘mda methods, data, analyses’ journal, a GESIS publication. Engineers ingested papers from this journal into the Ghostwriter tool, thereby creating a distinct collection. A core design principle dictates that the system must not rely on any general knowledge pre-loaded into the LLM; instead, it must answer questions using only factual information present within the specified papers. If the requested information is absent, the system transparently states ‘I don’t know’, thereby avoiding speculation. For testing, developers utilised a collection of 100 articles scraped from the mda website. The Ghostwriter instance for these mda papers remains accessible at https://gesis.now.museum. The GESIS ‘Ask Questions’ interface, presented as part of the broader ecosystem, allows users to add new content collections via various means, including single webpages, website crawlers, or RSS feeds.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "href": "chapter_ai-nepi_011.html#core-functionality-factual-chat-referencing-and-iterative-refinement",
    "title": "11  Science Dynamics and AI",
    "section": "11.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement",
    "text": "11.7 Core Functionality: Factual Chat, Referencing, and Iterative Refinement\n\n\n\nSlide 10\n\n\nThe Ghostwriter system compellingly demonstrates its core functionalities through practical examples. When a user queries, for instance, ‘explain male breadwinner model to me’, the system furnishes a detailed explanation and, crucially, provides precise references to the source documents. This implementation actively prevents hallucination by meticulously locating information within the ingested texts. Engineers achieve this accuracy by splitting each paper into small, identifiable blocks, each assigned a unique identifier. LLM-based techniques then intelligently connect and retrieve these blocks, whilst weights and other methods further refine the process. Furthermore, knowledge graphs assist in predicting which specific text segments are most likely to provide a relevant answer to a given question.\nShould a query be refined—for instance, to ‘explain how data was collected on male breadwinner model’—and the information proves absent from the current corpus, the system responds transparently: ‘According to the provided text, there is no direct information about how data was collected on the male breadwinner model.’ This honesty represents a key feature. The system also supports iterative improvement; an ‘add paper’ button enables users to integrate new documents they discover externally. Subsequently, if the same question is posed, the system can readily utilise this newly incorporated information. For all queries, the interface displays source papers, including their titles, direct links (e.g., to mda.gesis.org), and relevance scores, although not all listed documents may contain the exact query terms in their full text.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#advanced-backend-mechanisms-entity-management-and-multilingual-capabilities",
    "title": "11  Science Dynamics and AI",
    "section": "11.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities",
    "text": "11.8 Advanced Backend Mechanisms: Entity Management and Multilingual Capabilities\n\n\n\nSlide 11\n\n\nSeveral advanced backend mechanisms empower the Ghostwriter system, particularly in entity management and multilingual support. An entity extraction pipeline annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby effectively bridging vector spaces and knowledge graphs. These entities then link to richer knowledge graph representations, with Wikidata serving as a prime example; this linkage proves vital for establishing ground truth. The system also offers immediate multilinguality, enabling seamless interaction even when query and document languages differ. Finally, the LLM processes the retrieved, relevant text pieces to generate a coherent summary or ‘explanatory text’.\nDelving deeper into fact extraction, a user’s query undergoes mapping to a graph representation, and its constituent strings are meticulously annotated with ‘facts’. For instance, terms such as ‘gender roles’ or ‘male breadwinner model’ connect to concepts like ‘societal expectations’ or ‘economic systems’. This process relies on a Knowledge Organisation System (KOS) that can be iteratively applied to reveal progressively deeper semantic layers beneath a term. Instead of relying on free-text strings, the system links entities to Wikidata, thereby obtaining unique identifiers. These identifiers intrinsically connect to multilingual translations and a wealth of properties, allowing, for example, the term ‘male’ to resolve to its specific Wikidata entry (e.g., Q12308941), with LLM embeddings providing similarity scores for disambiguation.\nMultilingual capability is robustly implemented. The system identifies the core concept of a query, such as ‘male breadwinner model’ (represented as ‘bread winner mo’). An LLM, specifically Gemma3, then generates translations of this core concept into a multitude of languages, including Czech, Danish, German, and Japanese, amongst others. These translations effectively broaden the search parameters for the primary LLM.\nThis sophisticated approach underpins a broader vision for Knowledge Organisation Systems. By converting concepts into Wikidata identifiers, knowledge becomes decoupled from the specifics of individual questions or papers. Such abstracted knowledge can be stored independently of any single LLM, thereby fostering model agnosticism. This decoupling also facilitates a novel benchmarking methodology: different LLMs, even those yet to be developed, can be evaluated by their ability to return the same set of identifiers for identical conceptual queries. Any deviations would signal potential issues with a model’s suitability for certain tasks. Collaborations with industry leaders like Google and Meta aim to establish this KOS-centric methodology as a sustainable and foundational element for future scientific endeavours, positioning KOS as a cornerstone of future knowledge management.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science Dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "",
    "text": "Overview\nPhilosophical inquiry, a discipline demanding exceptional linguistic and semantic precision, increasingly explores the application of Retrieval Augmented Generation (RAG) systems. Standard Large Language Models (LLMs), however, pose significant limitations for rigorous philosophical research. These include restricted access to complete textual sources, a propensity for hallucination, an inability to learn texts verbatim, and constrained context windows.\nRAG architecture directly addresses these issues. By integrating a curated data corpus, a sophisticated retrieval mechanism, and prompt augmentation with retrieved text, RAG enables direct text access, manages extensive corpora, and crucially, facilitates robust source attribution. Its potential applications in philosophy are broad, encompassing both didactic and research uses. In didactics, RAG systems could allow students to interactively explore complex texts, such as Locke’s Essay concerning Human Understanding. For research, applications range from efficient fact retrieval from handbooks and the exploration of previously unexamined corpora, to the identification of specific passages for close reading, and ultimately, the potential to answer nuanced research questions.\nTo investigate these promising applications, researchers constructed an example RAG system, utilising the Stanford Encyclopedia of Philosophy (SEP) as its primary data source. They meticulously scraped the SEP’s content, converting it into markdown format. Initial development, however, revealed surprisingly poor performance from basic RAG configurations, prompting a qualitative study into optimal system architectures. This comprehensive study necessitated extensive refinement of various components. Researchers meticulously tweaked model choices, including generative LLMs like gpt-4o-mini and embedding models, alongside numerous hyperparameters such as top-k, token limits, temperature, chunk size, and overlap. They also explored algorithmic enhancements, notably reranking mechanisms. Evaluating the unstructured text answers, which frequently articulated complex philosophical propositions, proved particularly challenging, underscoring the critical need for robust evaluation standards.\nCrucial findings emerged regarding chunking strategies: these profoundly impact system performance. For the highly systematised SEP, treating entire main sections—averaging 3,000 words—as individual retrievable documents yielded demonstrably superior results. This outcome proved counter-intuitive, as these sections considerably exceeded the embedding model’s typical input length of approximately 500 words. Furthermore, reranking retrieved documents using a generative LLM to assess relevance substantially enhanced performance. This process, which scores documents based on informativeness and the length of relevant passages, significantly improved the quality of generated answers, albeit at an increased computational cost. Whilst RAG systems adeptly integrate verbatim corpora, mitigate hallucinations, and provide citations, they demand meticulous, corpus-specific tuning. A significant challenge, however, became apparent: RAG systems may underperform on broad overview questions. Their inherent focus on locally retrieved information can inadvertently obscure the larger conceptual landscape. This observation points towards a compelling need for more flexible, perhaps agentic, RAG systems, capable of discerning and adapting to diverse question types.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "href": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry-with-retrieval-augmented-generation",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation",
    "text": "12.1 Addressing LLM Limitations in Philosophical Inquiry with Retrieval Augmented Generation\n\n\n\nSlide 01\n\n\nPhilosophical inquiry frequently grapples with intricate questions. Consider, for instance, elucidating Aristotle’s theory of matter within the Physics, or tracing the evolution of Einstein’s concept of locality from his early works on relativity to his 1948 paper addressing quantum mechanics and ‘Wirklichkeit’. Whilst standard Large Language Models (LLMs) like ChatGPT can generate superficially plausible and differentiated responses to such queries, they exhibit significant limitations when applied to rigorous philosophical research.\nA primary constraint involves access to textual sources. LLMs typically lack dynamic access to the full text of scholarly works, even if those texts formed part of their training data. Consequently, requests for specific quotations from chapters or papers may lead to hallucinations or an admission of inability. Even when online search capabilities are activated, copyright restrictions can prevent the reproduction of material.\nFurthermore, the fundamental training mechanisms of LLMs are engineered to avoid mere parroting of texts. Instead, they learn generalisable statistical patterns of language production, with explicit mechanisms preventing verbatim memorisation. This contrasts sharply with the needs of philosophical research, which hinges on meticulous engagement with original source materials and their precise, fine-grained formulations. The limited context window of current LLMs—for instance, ChatGPT-4o’s 128,000 tokens—also poses a significant hurdle when dealing with extensive philosophical corpora.\nTo surmount these challenges, researchers propose Retrieval Augmented Generation (RAG) systems. A RAG system’s architecture typically involves a curated data source, such as the complete corpus of Aristotle’s or Einstein’s writings. From this source, documents are retrieved using methods like semantic search, hybrid approaches, or traditional keyword search. These retrieved documents, or relevant chunks thereof, then augment the user’s original prompt before processing by the LLM. This setup directly tackles the problem of text access, provides a mechanism for managing large context sizes by focusing on relevant segments, and crucially, facilitates attribution by enabling the system to cite the sources for its generated claims, much like the numbered citations seen in tools such as Perplexity AI.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "href": "chapter_ai-nepi_012.html#applications-of-rag-systems-in-philosophical-scholarship",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.2 Applications of RAG Systems in Philosophical Scholarship",
    "text": "12.2 Applications of RAG Systems in Philosophical Scholarship\n\n\n\nSlide 09\n\n\nRAG systems offer a promising avenue for interacting with philosophical corpora, distinguished by their capacity to integrate detailed domain knowledge and rely on verbatim textual evidence. This capability opens several valuable applications within philosophical scholarship, broadly categorised into didactic and research uses.\nIn the realm of didactics, RAG systems can transform how students engage with challenging philosophical texts. For instance, students approaching Locke’s Essay concerning Human Understanding can benefit immensely from the ability to pose repeated questions. This iterative process proves highly instructive, allowing them to start with broad inquiries, such as “What is Locke’s general idea?”, and progressively delve into more specific aspects, like “What is his idea in epistemology?” or “What is his theory of matter?”. Such interactions foster a deeper, more nuanced understanding of the material.\nBeyond educational settings, RAG systems hold significant potential for research. They can streamline the process of looking up facts in handbooks, a task that traditionally involved manually searching physical volumes for information, perhaps for a footnote. RAG offers a more efficient method, potentially with greater reliability than relying on the unverified outputs of standard LLMs. Furthermore, researchers can employ RAG systems to explore unexamined corpora; once digitised, collections of unpublished manuscripts or less-studied texts can be “chatted with” to gain an overview and deeper insights into their content. Another key research application is the identification of specific passages relevant to a particular research question, thereby facilitating focused close reading. Ultimately, the aspiration is that RAG systems might, at some point, become capable of providing detailed answers to at least certain components of complex philosophical research questions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "href": "chapter_ai-nepi_012.html#developing-and-refining-a-rag-system-for-philosophical-texts-the-sep-rag-project",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project",
    "text": "12.3 Developing and Refining a RAG System for Philosophical Texts: The SEP RAG Project\n\n\n\nSlide 09\n\n\nResearchers embarked on a project to construct an example RAG system, selecting the Stanford Encyclopedia of Philosophy (SEP)—a widely respected online handbook—as the primary data source. The initial step involved scraping the SEP’s content and converting it into markdown format. Originally, the ambition was to develop a directly useful tool for the philosophical community. However, early attempts to implement a standard textbook RAG system, comprising distinct retrieval and generation components, produced answers of surprisingly poor quality; indeed, these initial outputs were often inferior to those obtainable from a standalone LLM like ChatGPT.\nThis experience prompted a significant shift in focus. The project evolved into a qualitative study aimed at determining the optimal configuration for RAG systems tailored to the specific demands of philosophical texts. Achieving improved performance necessitated a meticulous process of refining numerous aspects of the system. This involved careful selection of both the generative LLM and the embedding model responsible for understanding text semantics. Extensive hyperparameter tuning became essential, covering parameters such as top-k (the number of documents retrieved), maximum input and output token lengths, the temperature or top-p settings influencing the creativity of the generated text, and the strategies for chunk size and overlap in document segmentation. Beyond parameter adjustments, researchers explored more complex algorithmic solutions, such as implementing reranking mechanisms to mitigate problems like semantic mismatch between the query and retrieved documents.\nThe methodology for enhancing the system was predominantly one of trial and error, guided by theoretical insights into how RAG components interact. A significant hurdle encountered throughout this process was the evaluation of the system’s output. Philosophical RAG systems generate answers in free, unstructured text, often articulating complex propositions rather than simple atomic facts (unlike, for example, a historical query seeking Wittgenstein’s last place of living, which expects a city name). Consequently, robust evaluation standards are paramount to assess whether these generated propositions accurately convey the intended philosophical concepts and facts, a non-trivial task.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-interface-and-functionality-details",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.4 SEP RAG System: Interface and Functionality Details",
    "text": "12.4 SEP RAG System: Interface and Functionality Details\n\n\n\nSlide 09\n\n\nThe developed Stanford Encyclopedia of Philosophy RAG (SEP RAG) system features a user-facing frontend and a backend constructed with Python, amounting to a few thousand lines of code. The frontend interface provides users with considerable control over the generation process. Within its input section, users can configure several key parameters: they can select the generative model (with gpt-4o-mini shown as an example), view the chosen model’s maximum prompt token limit (e.g., 128,000 tokens), and set a specific prompt token limit for the RAG system’s input (e.g., 15,000 tokens). Additionally, users can define a persona for the LLM—for instance, “You are an expert philosopher. You answer meticulously and precisely”—and specify the number of texts to retrieve for context (e.g., 15). A dedicated field allows for the input of a philosophical question, such as “What is priority monism?”, followed by a “Generate answer” button to initiate the process.\nUpon generation, the system presents its output in a structured manner. Notably, it offers a comparative view, displaying the answer from a standalone LLM (serving as a benchmark) alongside the answer produced by the SEP RAG system. This side-by-side presentation facilitates a more effective assessment of the RAG system’s contribution. Furthermore, the output includes a detailed list of the texts retrieved from the SEP. This list specifies the names of the articles and the particular section headings that the system identified as relevant. Crucially, it also indicates which of these retrieved texts were ultimately included in the augmented prompt passed to the LLM and which, if any, were truncated due to the imposed prompt length limitations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-document-chunking-for-philosophical-corpora",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.5 Optimising Document Chunking for Philosophical Corpora",
    "text": "12.5 Optimising Document Chunking for Philosophical Corpora\n\n\n\nSlide 15\n\n\nResearchers dedicated particular attention to optimising the hyperparameter of chunk size, which dictates how documents are segmented for retrieval and processing. They explored several distinct options for this segmentation. One approach involved a fixed number of words or tokens, for example, 500, a method often favoured in computer science for its straightforward implementation. Another considered using natural paragraph breaks as delimiters. A third strategy focused on segmenting the source material by its inherent sections, potentially at various levels of the document hierarchy.\nThrough experimentation with the Stanford Encyclopedia of Philosophy, a clear finding emerged: the most effective results were achieved when entire main sections of SEP articles were treated as the individual “documents” for retrieval. This outcome was somewhat surprising because the average length of these main sections—around 3,000 words—considerably surpassed the input limit of the embedding model, which could process only a little over 500 words at a time.\nThe proposed explanation for this counterintuitive success rests on the specific nature of the SEP. It is a highly systematised and meticulously structured encyclopedic work. Within such a well-organised corpus, the initial portion of a main section (the first 500 words or so that the embedding model can ingest) likely contains enough concentrated information to represent the semantic core of the entire section adequately. However, it is important to note a caveat: this successful strategy of using large, section-level chunks may not be universally applicable. Its efficacy is probably tied to the SEP’s unique characteristics and might not translate effectively to more heterogeneous textual collections or corpora that lack such a high degree of internal systematisation and clear structural demarcation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "href": "chapter_ai-nepi_012.html#implementing-reranking-to-enhance-retrieval-relevance",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.6 Implementing Reranking to Enhance Retrieval Relevance",
    "text": "12.6 Implementing Reranking to Enhance Retrieval Relevance\n\n\n\nSlide 18\n\n\nResearchers identified that an initial retrieval process, even one based on semantic similarity, can sometimes include documents that are not genuinely relevant to the user’s specific question—these are known as false positives. To address this limitation, they incorporated an additional step: reranking. The primary aim of reranking is to re-evaluate and reorder the initially retrieved set of documents, arranging them according to their true relevance to the posed query.\nThe implemented solution involves leveraging a generative Large Language Model (gLLM) to perform this relevance assessment. This choice stems from the understanding that gLLMs exhibit more sophisticated semantic differentiation capabilities than embedding models can offer on their own. Consequently, a gLLM can provide a more nuanced and accurate judgement of how well each retrieved text pertains to the question. During the reranking process, the gLLM scores each candidate document based on specific categories, notably its informativeness concerning the query and the length of the relevant passage contained within it. These individual scores are then aggregated into a “Total Score,” which quantifies the overall relevance of each document.\nThe introduction of this reranking stage proved highly effective. Evaluations demonstrated that it leads to very good results, significantly enhancing the quality and relevance of the documents ultimately used to generate the answer. However, this improvement comes at a cost: the reranking step, by invoking a powerful gLLM for each retrieved document, substantially multiplies the computational resources required, which in turn can increase the monetary expense of operating the RAG system.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "href": "chapter_ai-nepi_012.html#overall-assessment-advantages-caveats-and-future-directions-for-rag-in-philosophy",
    "title": "12  Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges",
    "section": "12.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy",
    "text": "12.7 Overall Assessment: Advantages, Caveats, and Future Directions for RAG in Philosophy\n\n\n\nSlide 18\n\n\nRetrieval Augmented Generation systems present several distinct advantages for philosophical scholarship. They can seamlessly integrate verbatim corpora, ensuring that answers are grounded in authentic textual evidence, and can effectively incorporate domain-specific and specialised knowledge. These capabilities lead to the generation of more detailed answers and, crucially, a dramatic reduction in the incidence of hallucinations. Furthermore, the ability of RAG systems to cite the relevant documents underpinning their responses directly supports scientific rigour and verifiability, making them, in principle, well-suited for assisting in scholarly tasks.\nNevertheless, several points of caution warrant consideration. RAG systems are not “plug-and-play” solutions; they inherently demand extensive and continuous tweaking to achieve optimal performance. The ideal settings for hyperparameters and model choices are not universal but are instead highly contingent upon the specific characteristics of the corpus in use and the nature of the questions typically posed to the system. Rigorous evaluation of RAG outputs is paramount. This requires establishing a representative set of test questions along with clearly defined expected or ideal answers. Such evaluation processes become particularly challenging when working with unexplored or novel corpora, and the active involvement of domain experts—in this case, philosophers—is indispensable for any meaningful assessment of quality and accuracy.\nResearchers also identified specific challenges and limitations. A significant issue arises if the retrieval mechanism fails to locate any relevant documents; in such instances, the quality of the generated answer tends to decrease substantially, often necessitating adjustments to the user’s prompt. An intriguing, somewhat counterintuitive finding was that RAG systems can sometimes produce worse results for broad, widely discussed overview questions, such as “What are the central arguments against scientific realism?”. The hypothesised reason for this phenomenon is that the RAG system’s operational prompt directs it to focus intently on the local information contained within the retrieved texts. This localised focus, whilst beneficial for specific queries, can inadvertently distract from or fail to adequately synthesise the broader, more encompassing perspective required to answer overview questions comprehensively.\nLooking ahead, these observations underscore the need for more flexible RAG systems. Future developments may involve systems capable of discerning between different types of questions and adapting their strategies accordingly, potentially moving in the direction of more sophisticated “agentic” RAG architectures.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Retrieval Augmented Generation (RAG) in Philosophical Research: Applications and Methodological Challenges</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "",
    "text": "Overview\nThis chapter presents ongoing research, undertaken jointly with Mike Schneider from the University of Missouri, exploring how computational methods and social network analysis can address fundamental questions in the philosophy of science. The investigation centres on the field of quantum gravity, examining whether its research landscape exemplifies “plural pursuit”—distinct, concurrent instances of normal science aimed at a common problem-solving goal.\nResearchers initiated a bottom-up reconstruction of the quantum gravity research landscape. This involved a linguistic analysis of 228,748 abstracts and titles from Inspire HEP, employing the Bertopic pipeline for embedding and unsupervised clustering into 611 fine-grained topics. Simultaneously, a social network analysis of a co-authorship graph, encompassing 30,000 physicists, identified 819 communities through community detection methods.\nTo address the inherent scale-dependency of topics and communities, the researchers implemented hierarchical clustering for both. They applied Ward agglomerative clustering for topics and hierarchical stochastic block modelling for communities. An adaptive topic coarse-graining strategy, based on the Minimum Description Length criterion, further refined the initial 611 topics to a more manageable 50, thereby balancing model fit with complexity to illuminate the field’s social structure.\nThis bottom-up reconstruction was then rigorously confronted with a top-down approach, derived from physicists’ own intuitions. These insights were gathered via a survey of founding members of the International Society for Quantum Gravity. A Support Vector Machine (SVM) classifier, trained on hand-coded labels and text embeddings (all-MiniLM-L6-v2), subsequently predicted paper categorisation according to physicists’ identified approaches, such as string theory, supergravity, and holography.\nFindings suggest that whilst some bottom-up derived topics align well with physicists’ categorisations, others—notably supergravity and string theory—appear merged within the socio-epistemic structure. This reflects an evolution in the field not always captured by historical or purely conceptual distinctions. Ultimately, this work demonstrates that computational methods can effectively revisit and challenge long-standing philosophical intuitions concerning scientific structures like paradigms and communities.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.1 Conceptualising Plural Pursuit in Quantum Gravity",
    "text": "13.1 Conceptualising Plural Pursuit in Quantum Gravity\n\n\n\nSlide 01\n\n\nResearchers initiated an inquiry into the structure of scientific fields, specifically focusing on a long-standing challenge in fundamental physics: the formulation of a quantum theory of gravity. This ambitious endeavour seeks to reconcile our understanding of phenomena at very small scales with those at very large scales. The field of quantum gravity currently presents a multitude of attempted solutions, with string theory standing as the most prominent amongst others, including supergravity, loop quantum gravity (encompassing spin foams), causal set theory, and asymptotic safety.\nTo characterise this situation of multiple, simultaneous research efforts, the investigators, in collaboration with philosopher Mike Schneider, introduce the notion of “plural pursuit”. They define plural pursuit as the existence of distinct yet concurrent instances of normal science, all dedicated to a common problem-solving goal—in this case, the unification of quantum mechanics and general relativity. Furthermore, each such instance of normal science articulates through a specific social community intertwined with an intellectual disciplinary matrix. This conceptualisation draws upon established frameworks in the philosophy of science, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’s research programmes.\nConsequently, a central empirical question arises: does quantum gravity research indeed constitute an instance of plural pursuit? Answering this involves determining whether the field comprises independent communities, each pursuing different paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-linguistic-and-social-networks",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.2 Bottom-Up Reconstruction: Linguistic and Social Networks",
    "text": "13.2 Bottom-Up Reconstruction: Linguistic and Social Networks\n\n\n\nSlide 03\n\n\nTo empirically investigate the structure of quantum gravity research, scientists embarked on a bottom-up reconstruction of its landscape. This reconstruction aimed to delineate not only the intellectual and linguistic fabric of the field but also its underlying social structure. For this purpose, they gathered a substantial corpus comprising 228,748 abstracts and titles from theoretical physics publications listed on the Inspire HEP database.\nThe analytical process proceeded in two main stages. Firstly, a linguistic analysis sought to map the intellectual structure. Researchers employed the Bertopic pipeline, initially spatialising the documents into an embedding space (L.1). Subsequently, they performed unsupervised clustering on this space (L.2), yielding a highly fine-grained partition of 611 distinct topics. This level of detail was deemed necessary to capture niche research approaches within quantum gravity, some of which might encompass only around one hundred papers. Based on this classification, researchers then assigned each physicist a specialty (σ), defined as the most prevalent topic appearing across their individual publications (L.3).\nIn parallel, a social network analysis scrutinised the co-authorship graph of the field. In this graph, physicists represent nodes, and co-authorship relationships form the edges. Applying community detection methods to this network, which included approximately 30,000 physicists, researchers identified around 819 distinct communities (S.1). This dual analysis thus provided two distinct ways of partitioning the scientists: one based on their topical specialisations and another based on their collaborative communities.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#addressing-scale-in-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.3 Addressing Scale in Plural Pursuit",
    "text": "13.3 Addressing Scale in Plural Pursuit\n\n\n\nSlide 06\n\n\nInvestigators conceptualise plural pursuit, in terms of their computational constructs, as an intuitive one-to-one mapping between social communities and intellectual topics. If such a mapping existed perfectly, a correlation matrix plotting communities against topics would appear block-diagonal. This would signify that each community specialises exclusively in a distinct topical domain, thereby indicating a clear division of intellectual labour.\nHowever, when researchers directly correlated their initial fine-grained partitions of 819 communities and 611 topics, the resulting matrix—visualised using a measure related to normalised pointwise mutual information, npmic,k—proved exceedingly complex and difficult to interpret. Several factors contribute to this intricacy. Firstly, the level of fine-graining in the topic partition is somewhat arbitrary; a broad research programme like string theory, for instance, might be fragmented across numerous fine-grained topics. Secondly, multiple, distinct communities can simultaneously pursue large research programmes, influenced by various micro-social dynamics.\nMore fundamentally, both computational notions of “topic” and “community” are scale-dependent. This implies that literature and social networks can be partitioned at different levels of granularity. This technical issue mirrors a conceptual reality: research programmes themselves are often nested. For instance, one can hierarchically categorise string theory into families like Superstring Theory (further branching into Type II and Heterotic theories, with sub-branches such as Type IIA, Type IIB, Heterotic SO(32), and Heterotic E8 × E8), Bosonic String Theory, and Type I string theory. Consequently, to robustly identify instances of plural pursuit, one must confront and resolve the ambiguity introduced by these multiple, interacting scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "href": "chapter_ai-nepi_015.html#hierarchical-analysis-and-adaptive-topic-coarse-graining",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining",
    "text": "13.4 Hierarchical Analysis and Adaptive Topic Coarse-Graining\n\n\n\nSlide 09\n\n\nTo navigate the complexities of scale, researchers proposed a hierarchical reconstruction of the quantum gravity landscape. For topics, they implemented Ward agglomerative clustering, starting with the initial 611 fine-grained topics and iteratively merging them based on an objective function, visualised as a dendrogram. For communities, they employed hierarchical stochastic block modelling (drawing on Peixoto, 2014), which inherently learns a multi-level partition of the co-authorship network into increasingly coarse-grained communities. These hierarchical structures permit observation of the system—comprising physicists, their specialties, and their community affiliations—at various scales. However, the choice of which specific scale to analyse for topics and communities remains initially arbitrary, with different choices leading to different correlation patterns and interpretations.\nAn adaptive topic coarse-graining strategy was developed to address this challenge. The underlying idea is to simplify the detailed topic structure by merging topics, provided such merging does not discard information crucial for understanding the social organisation of the field. Many subtle linguistic distinctions, whilst conceptually valid, might not influence how scientists collaborate. This strategy relies on the Minimum Description Length (MDL) criterion, which seeks a partition σ that minimises the sum of two terms: one representing the negative log-likelihood of the social graph G given the partition σ (model fit), and another representing the negative log-probability of the partition σ itself (model complexity). This balances the explanatory power of the topic partition regarding social structure against the desire for a less complex, more parsimonious partition. Applying this, the initial ~600 topics were refined to just 50. Notably, this process preserved some small-scale linguistic topics deemed important for social structure, whilst amalgamating others into broader categories.\nWith these 50 adaptively coarse-grained topics, investigators re-examined the correlation matrix, attempting to match each topic to community structures across different hierarchical levels. Some very broad topics, such as one encompassing general quantum field theory and quantum gravity aspects, appeared ubiquitous and not tied to any specific community. In contrast, the string theory topic demonstrated a strong correspondence with a community structure at the third level of the community hierarchy. Other research programmes, like loop quantum gravity, seemed to align with communities at much finer-grained levels. These findings suggest a complex interplay rather than a simple, clear-cut instance of plural pursuit; nested structures were evident, such as a smaller community focused on holography existing within the larger string theory community. This indicates an entanglement of different scales and a lack of straightforward division of intellectual labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-with-physicists-intuitions",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.5 Confronting Bottom-Up with Physicists’ Intuitions",
    "text": "13.5 Confronting Bottom-Up with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nResearchers then sought to confront their bottom-up reconstruction of the quantum gravity landscape with the intuitions of physicists working within the field. They conducted a survey targeting the founding members of the International Society for Quantum Gravity, asking them to list the quantum gravity approaches they perceived as structuring the overall research landscape. Although consensus was not universal, this exercise yielded a consolidated list of approaches, including asymptotic safety, causal sets, string theory, supergravity, and holography, amongst many others. For subsequent detailed analysis, the investigators focused on string theory, supergravity, and holography, particularly because some physicists expressed uncertainty about whether these should be considered entirely separate. Indeed, some argued that supergravity and aspects of holography are fundamentally rooted in string theory, despite historical and certain conceptual distinctions.\nTo operationalise this top-down perspective, they trained a Support Vector Machine (SVM) classifier. The classifier’s task was to predict the categorisation of papers into these physicist-defined approaches, using text embeddings derived from titles and abstracts (via the all-MiniLM-L6-v2 model) and trained on a set of hand-coded labels. The resulting supervised, top-down classification was then compared against the 50 coarse-grained bottom-up topics using a correlation heatmap.\nThis comparison revealed that certain physicist-defined approaches aligned well with the emergent bottom-up topics, especially those that were well-defined and conceptually autonomous. Conversely, approaches that were more phenomenological or lacked a fully developed conceptual framework showed poorer correspondence. A significant observation concerned a large “string theory” cluster identified through the bottom-up, scale-aware analysis; this cluster appeared to encompass both supergravity and string theory. This finding resonated with feedback from the survey, exemplified by one physicist who noted the substantial overlap in personnel working on supergravity and string theory, questioning whether the communities could be meaningfully separated. This suggests that the bottom-up methodology, by filtering out linguistic nuances that lack significant correlates in the social structure, can reveal how fields evolve socio-epistemically, sometimes merging entities that were once more distinct. The initial, finer-grained linguistic clustering, however, does correctly capture these underlying conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "title": "13  Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation",
    "section": "13.6 Conclusions and Philosophical Implications",
    "text": "13.6 Conclusions and Philosophical Implications\n\n\n\nSlide 18\n\n\nThis investigation yields several important conclusions regarding the structure and analysis of scientific fields. Firstly, socio-epistemic systems, such as research fields, can and should be observed at multiple scales. Consequently, fundamental concepts like “communities” and “disciplinary matrices” are inherently scale-dependent. Secondly, the task of identifying configurations of plural pursuit—which ideally manifest as a one-to-one mapping between distinct communities and their unique intellectual substrates—requires a careful matching of these social and intellectual structures across relevant scales.\nSpecifically for the domain of quantum gravity, the bottom-up reconstruction of its research landscape offers a valuable tool. It can serve to either confirm or prompt a re-assessment of certain intuitions that physicists themselves hold about the organisation and evolution of their field. More broadly, this work underscores a significant methodological point: the advent of increasingly powerful computational methods provides new avenues to rigorously revisit, challenge, or refine philosophical insights that have, for extended periods, relied primarily on intuition. This applies particularly to understanding complex structures like scientific paradigms and research communities.\nIn essence, the researchers propose a powerful synergy, suggesting that, to paraphrase Clausewitz, computation can be viewed as the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit: A Multi-Scale Investigation</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "",
    "text": "Overview\nResearchers Francis Lareau, from the Université de Sherbrooke and the Université du Québec à Montréal, alongside Christophe Malaterre, also from the Université du Québec à Montréal, conducted a comprehensive comparative study. Their investigation sought to determine whether applying topic modelling to titles or abstracts suffices for scientific literature, or if full-text analysis remains indispensable, particularly within the history, philosophy, and sociology of science. This inquiry directly addresses the substantial resources full-text processing demands.\nThe study meticulously constituted a corpus of scientific articles, subsequently identifying their title, abstract, and full-text sections. Researchers then applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these three textual levels. Following this, they rigorously analysed and compared the six resulting topic models.\nThis comparison employed both qualitative methods, drawing upon a pre-existing detailed analysis of an astrobiology corpus, and quantitative measures. These quantitative metrics included the Adjusted Rand Index, topic diversity, joint recall, and Coherence CV. The findings aim to guide researchers in selecting appropriate text levels for topic modelling, aligning their choices with specific objectives and resource constraints by highlighting performance variations across different models and textual structures.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "href": "chapter_ai-nepi_016.html#research-rationale-topic-modelling-efficacy-across-textual-levels",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels",
    "text": "14.1 Research Rationale: Topic Modelling Efficacy Across Textual Levels\n\n\n\nSlide 01\n\n\nInvestigators initiated an inquiry to ascertain the most effective textual level—titles, abstracts, or full-texts—for applying topic modelling techniques to scientific literature. This area holds particular relevance for the history, philosophy, and sociology of science. Topic modelling has, indeed, emerged as a crucial instrument for dissecting substantial volumes of scholarly publications.\nThis powerful technique enables diverse analytical tasks, such as identifying research trends and paradigm shifts, uncovering thematic substructures and their interrelations, and tracing the evolution of scientific terminology. Observations from existing literature, however, reveal a varied application of topic modelling across these different textual components.\nThis background prompts a central research question: can analyses restricted to titles or abstracts yield sufficient insights, or does comprehensive full-text analysis remain essential? The considerable resources demanded for obtaining, preprocessing, and analysing complete full-text corpora lend urgency to this question.\nTo address this, investigators first assembled a corpus of scientific articles. Subsequently, they meticulously identified the title, abstract, and full-text sections for each document. Two distinct topic modelling methodologies, Latent Dirichlet Allocation (LDA) and BERTopic, were then applied to each of these three textual levels. Finally, the six generated topic models underwent rigorous qualitative and quantitative comparison to evaluate their respective performances.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#methodology-topic-modelling-approaches-lda-and-bertopic",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.2 Methodology: Topic Modelling Approaches – LDA and BERTopic",
    "text": "14.2 Methodology: Topic Modelling Approaches – LDA and BERTopic\n\n\n\nSlide 05\n\n\nInvestigators employed two distinct topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches operate on the premise that documents can be translated into numerical vectors. This transformation allows topics to be identified through the analysis of repetitions that highlight linguistic regularities. Machine learning techniques then automate the detection of these underlying patterns.\nLatent Dirichlet Allocation, a well-established statistical method, constructs simple vector representations by counting word occurrences within documents. Within this framework, topics manifest as latent variables governed by Dirichlet’s probability distribution. A key advantage of LDA is its capacity to handle extensive texts, rendering it applicable to titles, abstracts, and full-text documents alike.\nConversely, BERTopic offers a more recent, modular alternative. This approach leverages vector representations derived from Large Language Models, with BERT (Bidirectional Encoder Representations from Transformers) serving as its foundational model. In BERTopic, topics emerge as clusters of similar documents.\nWhilst earlier iterations of BERTopic faced limitations with long texts, this study incorporated new embedding techniques. These advancements enable the processing of substantial textual inputs, up to approximately 131,000 tokens, thereby extending BERTopic’s utility to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "href": "chapter_ai-nepi_016.html#methodology-material-for-qualitative-comparison-an-astrobiology-corpus",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus",
    "text": "14.3 Methodology: Material for Qualitative Comparison – An Astrobiology Corpus\n\n\n\nSlide 07\n\n\nFor the qualitative comparison, researchers utilised material from a prior in-depth topic analysis of an astrobiology corpus, detailed in Malaterre & Lareau (2023). Following a thorough evaluation process, they selected an existing Latent Dirichlet Allocation (LDA) full-text model, which featured 25 distinct topics, as a reference. Each of these 25 topics had undergone meticulous analysis, examining its most representative words and associated documents, leading to the generation of a descriptive label for each topic using pertinent keywords.\nSubsequently, the interrelations between these topics were quantified. Researchers calculated the mutual correlation based on how topics appeared together within documents. A community detection algorithm then processed these correlations, successfully identifying four overarching thematic clusters. These clusters received designations using letters (A, B, C, D) and distinct colours (red, green, yellow, and blue) for clarity.\nThe study presented these findings visually, employing a graph that illustrated the correlations between the 25 topics, complete with their assigned labels and colour-coded cluster memberships. In this graphical representation, the thickness of the lines signified the strength of the correlation between connected topics, whilst the size of the circles indicated the overall prevalence of each topic throughout the entire document collection. In essence, this pre-existing, detailed analysis provided a robust qualitative foundation against which the six topic models generated in the current investigation could be systematically compared.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.4 Methodology: Quantitative Analysis Metrics",
    "text": "14.4 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nResearchers employed four distinct metrics for the quantitative analysis to compare the topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings produced by the models, with a crucial correction for agreements that might occur by chance. An ARI value of zero typically signifies a random clustering.\nSecondly, topic diversity was assessed. This metric quantifies the proportion of distinct top words that characterise the topics within a given topic model, indicating whether different topics are described by unique sets of terms. Thirdly, joint recall provided a measure of how well the top words collectively represent the documents classified under each topic. Specifically, it evaluates the average document-topic recall, considering the relationship between a topic’s top words and its associated documents.\nFinally, coherence, specifically Coherence CV, was measured. This metric aims to determine if the top words constituting a topic are semantically related and form a meaningful group. Its calculation involves averaging the cosine relative distance between the top words within each topic.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis-of-model-similarity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.5 Results: Adjusted Rand Index Analysis of Model Similarity",
    "text": "14.5 Results: Adjusted Rand Index Analysis of Model Similarity\n\n\n\nSlide 09\n\n\nThe application of the Adjusted Rand Index (ARI) across all six topic models revealed varying degrees of similarity between them. As a reminder, an ARI score of zero signifies that the agreement between two clusterings is no better than random. The results, often visualised as a heatmap, indicated that the Latent Dirichlet Allocation (LDA) model applied to titles (LDA Title) was the most distinct. It showed the lowest similarity to the other models, with ARI values generally falling below 0.2.\nIn contrast, all other models demonstrated a better overall match with one another, achieving ARI values consistently above 0.2. Notably, the BERTopic models exhibited a stronger internal coherence; they tended to align more closely with each other, yielding ARI values that surpassed 0.35. Within this group, the BERTopic model applied to abstracts (BERTopic Abstract) emerged as a somewhat central figure, as it corresponded well with nearly every other model, the only significant exception being the divergent LDA Title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-of-lda-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.6 Results: Qualitative Comparison of LDA Models",
    "text": "14.6 Results: Qualitative Comparison of LDA Models\n\n\n\nSlide 09\n\n\nA more detailed qualitative analysis focused on the Latent Dirichlet Allocation (LDA) models. When comparing the LDA full-text model with the LDA abstract model (Table A), researchers observed a good overall fit. This conclusion arose because topics from one model generally found a corresponding match in the other, evidenced by a high proportion of shared documents, which formed a noticeable reddish diagonal in the suitably organised heatmap.\nNevertheless, some transformations occurred: three topics present in the full-text LDA model disappeared in the abstract model, whilst another three full-text topics split into multiple, more granular topics within the abstract representation. Conversely, three entirely new topics emerged in the LDA abstract model, and three other abstract topics appeared to be the result of mergers from the full-text model. An additional observation was the presence of one small class, or topic, in the LDA abstract model, encompassing fewer than 50 documents.\nThe comparison between the LDA full-text model and the LDA title model (Table B) revealed a starkly different picture. Here, the fit was poor, indicating substantial reorganisation of thematic structures. Numerous topics from the full-text model disappeared when moving to the title-based model, and concurrently, many new topics emerged that were specific to the LDA title analysis. The heatmap for this comparison displayed a profusion of dark vertical and horizontal lines, visually underscoring the extensive restructuring of topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "href": "chapter_ai-nepi_016.html#results-qualitative-comparison-involving-bertopic-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.7 Results: Qualitative Comparison Involving BERTopic Models",
    "text": "14.7 Results: Qualitative Comparison Involving BERTopic Models\n\n\n\nSlide 11\n\n\nInvestigators then examined the BERTopic models in comparison. When contrasting the BERTopic full-text model with the original LDA full-text model (Table C), they found an average overall fit. From the perspective of the LDA full-text topics, eight disappeared in the BERTopic full-text representation, and six were split into more granular topics. Conversely, five new topics emerged within the BERTopic full-text model, and one topic appeared to be the result of a merger. This model, however, presented class size issues: specifically, four small classes and one extremely large class.\nNext, comparing the BERTopic abstract model against the LDA abstract model (Table D), researchers noted a relatively good overall fit. In this transition, four topics from the LDA abstract model disappeared, whilst six were split. The BERTopic abstract model introduced two new topics and featured four topics that resulted from mergers. Importantly, the class sizes in this BERTopic abstract model were generally balanced.\nFinally, the comparison between the BERTopic title model and the LDA title model (Table E) indicated an average fit. Seven topics from the LDA title model were absent in the BERTopic title model, and one LDA title topic was split. The BERTopic title model, in turn, presented seven new topics and one topic formed by a merger. This model also exhibited class size concerns, with three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-lda-top-words-qualitative-analysis",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.8 Results: LDA Top-Words Qualitative Analysis",
    "text": "14.8 Results: LDA Top-Words Qualitative Analysis\n\n\n\nSlide 13\n\n\nA qualitative assessment of the top words defining topics within the Latent Dirichlet Allocation (LDA) models revealed that, generally, the topics were relatively well-formed across the full-text, abstract, and title variations. Investigators observed instances of robust topics that maintained a strong correspondence across all three LDA models. A notable example was the topic labelled “A-Radiation spore” in the LDA full-text model, which aligned closely with semantically similar topics in both the LDA abstract model (characterised by top words such as “radiation,” “spore,” and “space”) and the LDA title model (with top words including “space,” “simulated,” and “spore”).\nFurthermore, some topics identified in the full-text model underwent splitting, fragmenting into several distinct topics within the abstract and title models. For instance, the “A-Life civilization” topic from the full-text analysis split, and one of its resultant components in the abstract model cohered into a general theme concerning research and astrobiology; this particular split was deemed logical. Another full-text topic, “B-Chemistry,” also fragmented, though its resulting divisions proved more challenging to interpret readily without deeper investigation.\nConversely, the analysis also identified instances of topic merging. Certain topics from the full-text model consolidated into new, more encompassing topics in the other LDA models. For example, the distinct full-text topics “B-Amino-acid” and “B-Protein-gene-rna” merged within the LDA abstract model. This fusion created a broader, more generalised topic, a development considered to be a sensible thematic consolidation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-words-qualitative-analysis",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.9 Results: BERTopic Top-Words Qualitative Analysis",
    "text": "14.9 Results: BERTopic Top-Words Qualitative Analysis\n\n\n\nSlide 14\n\n\nContinuing the top-words assessment with the three BERTopic models (full-text, abstract, and title), researchers again found that the topics were, on the whole, relatively well-formed when compared against the LDA full-text baseline. The previously identified robust topic, “A-Radiation spore,” demonstrated its stability by maintaining good correspondence across all BERTopic model variations as well.\nThe topic “A-Life-civilization,” also from the LDA full-text model, showed relative stability when analysed with BERTopic across the different text levels. However, it did undergo some degree of splitting here and there. These divisions typically resulted in the formation of narrower, more specific topics pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic from the LDA full-text model, when subjected to BERTopic analysis across the full-text, abstract, and title inputs, also tended to split. This fragmentation consistently led to the emergence of more narrowly focused chemical themes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-coherence-cv",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.10 Results: Quantitative Analysis – Coherence (CV)",
    "text": "14.10 Results: Quantitative Analysis – Coherence (CV)\n\n\n\nSlide 15\n\n\nResearchers then presented the performance metrics for all six models, considering a range of topic numbers from 5 to 50. Beginning with coherence (specifically Coherence CV), which assesses the semantic relatedness of a topic’s top words, several patterns emerged. The analysis revealed that models based on titles consistently yielded the poorest coherence scores.\nComparing text levels, abstract-based models generally demonstrated superior coherence to their full-text counterparts. When contrasting the modelling techniques, BERTopic typically outperformed Latent Dirichlet Allocation (LDA) in terms of coherence for both abstract and title inputs. However, this advantage for BERTopic tended to lessen as the specified number of topics for the models rose. Across all configurations, the BERTopic Abstract model clearly emerged as the top performer for this particular metric.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-topic-diversity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.11 Results: Quantitative Analysis – Topic Diversity",
    "text": "14.11 Results: Quantitative Analysis – Topic Diversity\n\n\n\nSlide 16\n\n\nThe analysis of topic diversity, which measures the extent to which topics are described by distinct sets of words, showed a general trend: diversity tended to decrease as the number of topics in the models increased. Models constructed from titles offered better diversity scores when compared to their abstract or full-text equivalents.\nRegarding the modelling techniques, BERTopic consistently achieved higher diversity scores than Latent Dirichlet Allocation (LDA). The BERTopic Title model emerged as the winner for this metric, although the BERTopic Full-text model followed very closely in performance.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-quantitative-analysis-joint-recall",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.12 Results: Quantitative Analysis – Joint Recall",
    "text": "14.12 Results: Quantitative Analysis – Joint Recall\n\n\n\nSlide 17\n\n\nJoint recall, a metric that evaluates how effectively the top words of a topic collectively represent all documents classified within that topic, yielded further insights. Models based on titles demonstrated the poorest performance in terms of joint recall. Conversely, full-text models generally outperformed their abstract and title-based counterparts on this measure.\nWhen comparing the two primary modelling techniques, Latent Dirichlet Allocation (LDA) tended to achieve better joint recall than BERTopic. The LDA Full-text model and the BERTopic Full-text model emerged as the top performers for joint recall, with the BERTopic Abstract model also demonstrating strong results, following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#results-summary-of-model-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.13 Results: Summary of Model Performance",
    "text": "14.13 Results: Summary of Model Performance\n\n\n\nSlide 17\n\n\nTo provide a consolidated view, researchers assembled the various results into a summary table. This table depicted the performance of each of the six models—LDA Full-text, LDA Abstract, LDA Title, BERTopic Full-text, BERTopic Abstract, and BERTopic Title—across several assessment categories: overall fit, top-words quality, coherence, diversity, and joint recall. Performance levels were visually represented using circles, where a fully black circle indicated the highest score and a white circle denoted the lowest.\nIt is crucial to recognise that these results do not point to a single, universally superior model. The optimal choice invariably depends on the specific research objectives and needs of the investigator. For instance, if the primary aim involves discovering major thematic trends, and the precise classification of every single document is not paramount, then metrics like poor recall or the presence of a large class of unassigned documents might not present critical drawbacks.\nConversely, if the objective demands that all identified topics comprehensively cover the maximum number of relevant documents, then certain models become less suitable. Specifically, researchers do not recommend BERTopic Full-text and BERTopic Title for such tasks, as they both tended to produce large groups of unclassified documents; BERTopic Title also suffered from poor recall. The LDA Title model is likewise not advised for this scenario, given its generally weak performance across almost all assessment criteria.\nIn light of these findings, the researchers generally recommend performing topic modelling on either abstracts or full-texts, using either LDA or BERTopic. This recommendation comes with the important proviso that the chosen combination does not lead to significant misclassification of documents pertinent to the topics of interest.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion-implications-and-future-directions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.14 Discussion and Conclusion: Implications and Future Directions",
    "text": "14.14 Discussion and Conclusion: Implications and Future Directions\n\n\n\nSlide 17\n\n\nThe research culminated in several key findings and pointed towards future avenues of exploration. Firstly, title-based models generally exhibited poor performance. A plausible explanation for this lies in the inherent lack of informational content within titles alone, which can consequently lead to the misclassification of documents. Nevertheless, it is noteworthy that even the BERTopic Title model managed to identify a number of meaningful topics, suggesting that the utility of titles is not entirely negligible. This highlights a potential need to strike a balance between achieving well-defined topics and ensuring adequate document coverage for each topic.\nSecondly, full-text models presented their own set of challenges. With Latent Dirichlet Allocation (LDA) applied to full-texts, topics sometimes appeared more loosely defined and broader in their thematic scope. Furthermore, such models occasionally identified transverse topics—for instance, those related to methodology—which might be secondary to the primary research themes of interest. BERTopic, when applied to full-texts, sometimes produced topics that were overly narrow. This specificity could lead to poor document coverage and contribute to problems with class size, such as the emergence of extremely large, undifferentiated clusters of documents.\nThirdly, abstract-based models demonstrated commendable performance. The results derived from abstracts showed consistency between both LDA and BERTopic approaches. Moreover, these abstract models aligned well with the LDA full-text model, indicating that abstracts often provide a balanced and effective summary of information suitable for topic modelling.\nA fourth significant observation concerned the robustness of topics. Overall, the study found that very similar thematic structures emerged across the diverse range of models and text levels analysed. This consistency opens possibilities for employing meta-analytic techniques to pinpoint the most robust and consistently identified topics. Furthermore, the relative distances or similarities between models (such as those measured by the Adjusted Rand Index) could potentially be used to identify an optimal or most central model. In this particular study, the BERTopic Abstract model appeared to fulfil such a role, performing strongly across various metrics.\nLastly, the findings prompt consideration of new modelling approaches. Researchers hypothesise that it might be feasible to develop novel models, or refine existing ones, by explicitly leveraging the structural information inherent in scientific articles—that is, the distinct characteristics of full-texts, abstracts, and titles. Such an approach could potentially lead to the extraction of an even more meaningful and nuanced set of topics or defining top-words.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of *LDA* and *BERTopic* Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models",
    "section": "",
    "text": "Overview\nResearchers at the Max Planck Institute of Geoanthropology propose a novel architecture, termed the “Time Transformer”, designed to imbue Large Language Models (LLMs) with explicit time-awareness. This work addresses a critical limitation: current LLMs possess only an implicit, statistically derived understanding of time, hindering their capacity to accurately process temporally evolving information, particularly for historical analysis. The core innovation involves augmenting standard Transformer models by incorporating a dedicated temporal dimension into token embeddings, thereby explicitly encoding the utterance time for each token.\nA proof-of-concept study utilised a small generative LLM, trained on a corpus of Met Office weather reports spanning 2018 to 2024. For this experiment, the temporal dimension represented the min-max normalised day of the year. The Time Transformer demonstrated its capability to learn and reproduce both synthetically introduced temporal drifts in language patterns—such as synonym replacement and alterations in word co-occurrence—and naturally occurring seasonal variations within the weather data. Key components of this investigation comprised the baseline vanilla Transformer model, its Time Transformer modification, and the curated dataset of weather reports. Development and training relied on standard LLM frameworks and an HPC cluster equipped with NVIDIA A100 GPUs. Potential applications extend beyond historical analysis to include the creation of foundation models for diverse time-sensitive tasks, enabling interactions with specific temporal contexts, and possibly modelling other contextual metadata dimensions like geography or genre. Nevertheless, this approach necessitates training models from scratch, presents challenges in curating temporal metadata, and raises questions regarding the feasibility of fine-tuning existing models.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "href": "chapter_ai-nepi_017.html#the-temporal-deficit-in-current-language-models",
    "title": "15  Time-Aware Language Models",
    "section": "15.1 The Temporal Deficit in Current Language Models",
    "text": "15.1 The Temporal Deficit in Current Language Models\n\n\n\nSlide 01\n\n\nJochen Büttner, from the Max Planck Institute of Geoanthropology, introduced a foundational concept aimed at enhancing language models. His presentation formalised an idea with potential applications in historical analysis (HPSS), though the speaker acknowledged its basic nature and solicited information regarding any pre-existing similar work.\nResearchers argued that current Large Language Models operate with merely an implicit comprehension of time, a comprehension statistically distilled from the vast quantities of text encountered during training. Whilst these models demonstrate a considerable, albeit indirect, grasp of temporal concepts, explicit time-awareness promises significant benefits, particularly for historical analysis and potentially broader applications. Consider, for instance, two statements: “The primary architectures for processing text through NNs are LSTMs,” accurate around 2017, and “The primary architectures for processing text through NNs are Transformers,” pertinent circa 2025. Humans effortlessly resolve the apparent contradiction by understanding the different temporal contexts. However, within an LLM’s training data, which lacks explicit temporal markers, these statements directly compete, compelling the model towards an unavoidable error in at least one instance.\nConsequently, during inference, an LLM prompted with “The primary architectures for processing text through NNs are” will likely predict “Transformers,” influenced by an inherent recency bias from its training. Eliciting an older truth, such as “LSTMs,” often necessitates careful prompt engineering—perhaps by adding “In 2017” or altering verb tenses—a process researchers describe as somewhat haphazard. The central objective, therefore, involves engineering explicitly time-aware LLMs, empowering them to learn and reproduce evolving patterns within training data as a direct function of time.\nFormally, standard LLMs estimate the probability of a subsequent token given a sequence of preceding tokens, denoted p(xn | x1, …, xn-1). In reality, this probability remains non-static; it dynamically changes with time, correctly represented as p(xn | x1, …, xn-1, t). For instance, the likelihood of “Transformers” completing the aforementioned sentence in 2017 was effectively zero. One can express the probability of an entire token sequence uttered at a specific time t as the product of these conditional probabilities: p(x1, x2, …, xn | t) = Πk=1 to n p(xk | x1, …, xk-1, t). Current models can only mirror temporal shifts in these underlying distributions through in-context learning during inference, a less direct mechanism.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#architecting-time-awareness-the-time-transformer",
    "title": "15  Time-Aware Language Models",
    "section": "15.2 Architecting Time-Awareness: The Time Transformer",
    "text": "15.2 Architecting Time-Awareness: The Time Transformer\n\n\n\nSlide 13\n\n\nAddressing the challenge of modelling the time-dependent probability distribution p(xn | x1, …, xn-1, t) necessitated a novel approach. One existing strategy, time slicing, involves training distinct models for separate temporal segments, assuming distributions remain relatively static within each slice. However, this technique proves exceptionally data-inefficient.\nConsequently, researchers conceived the “Time Transformer”, an architecture distinguished by its elegant simplicity. Standard Natural Language Processing tasks commence by transforming words or tokens into vectorial representations—embeddings—which models refine during training. The Time Transformer innovates by appending an additional dimension to these latent semantic token features, specifically encoding the token’s origin time. Thus, every token in a sequence, uttered at a particular time, receives this explicit temporal information. For instance, the representation for “cat” would subtly differ in this dimension depending on whether it was uttered recently or several years prior.\nOne can formalise this time-aware embedding as E(x, t) = {e1(x), e2(x), …, ed-1(x), φ(t)}, where φ(t) represents the encoded time. The Transformer model then processes a sequence of these augmented embeddings, [E(x1, t), E(x2, t), …, E(xn-1, t)], to predict the time-conditioned probability pθ(xn | x1, …, xn-1, t). The training objective remains the minimisation of the negative log likelihood across the dataset: minθ - Σi=1 to N Σk=1 to n(i) log pθ(xk(i) | x1(i), …, xk-1(i), t(i)). Through this mechanism, temporal information directly ‘injects’ into every token’s representation, enabling the model to learn precisely how significantly the time dimension influences each individual token.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "href": "chapter_ai-nepi_017.html#empirical-validation-experimental-design-and-implementation",
    "title": "15  Time-Aware Language Models",
    "section": "15.3 Empirical Validation: Experimental Design and Implementation",
    "text": "15.3 Empirical Validation: Experimental Design and Implementation\n\n\n\nSlide 10\n\n\nTo validate the Time Transformer concept, researchers required a dataset characterised by a limited vocabulary and simple, repetitive language, thereby facilitating the training of a small generative LLM. Met Office weather reports from the UK’s National Meteorological Service, accessible via their digital archive (https://digital.nmla.metoffice.gov.uk/), fulfilled these criteria admirably. Researchers scraped data spanning 2018 to 2024, yielding approximately 2,500 reports, each comprising around 150-200 words. They also noted an alternative dataset, TinyStories. Preprocessing involved extracting daily reports from monthly PDFs and applying a straightforward tokenisation strategy: no sub-word units, and a disregard for case and interpunctuation. This yielded a modest vocabulary of 3,395 unique words.\nResearchers first constructed a baseline ‘vanilla’ Transformer model. This decoder-only architecture comprised an embedding layer, positional encoding, and dropout, followed by four decoder blocks—each containing multi-head attention (with eight heads), residual connections with layer normalisation, and a feed-forward network—culminating in a final dense layer for output probability distribution. This relatively small model, with 39 million parameters (150MB), contrasts sharply with models such as GPT-4 (1.8 trillion parameters). Training occurred on an HPC cluster in Munich, utilising two NVIDIA A100 GPUs, achieving a rapid 11 seconds per epoch owing to the dataset’s and model’s compactness. The associated code is available on GitHub (j-buettner/time_transformer), though primarily serving as a learning tool. This vanilla model demonstrated proficiency in replicating the language of the weather reports.\nTransitioning to the Time Transformer involved a minimal architectural adjustment. Instead of a standard embedding, researchers incorporated time data by reserving one dimension within the, for example, 512-dimensional latent semantic space for a temporal signal. They concatenated this time value with the token’s semantic embedding before positional encoding. Specifically, a non-trainable, min-max normalised day of the year (calculated as (day of year - 1) / 364) served as the time embedding, a choice made to exploit natural seasonal variations in weather patterns. Researchers acknowledged that alternative methods for encoding time could also be employed.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "href": "chapter_ai-nepi_017.html#learning-temporal-dynamics-experimental-outcomes",
    "title": "15  Time-Aware Language Models",
    "section": "15.4 Learning Temporal Dynamics: Experimental Outcomes",
    "text": "15.4 Learning Temporal Dynamics: Experimental Outcomes\n\n\n\nSlide 16\n\n\nThe primary inquiry guiding these experiments sought to determine whether the Time Transformer could efficiently learn temporal drift within the underlying data distribution. A first experiment, termed “synonymic succession,” involved injecting a synthetic temporal drift. Researchers implemented a time-dependent replacement of the word “rain” with “liquid sunshine,” where the probability of replacement followed a sigmoid function across the days of the year—commencing at zero and culminating at one by year’s end. By generating a weather prediction for each day and analysing the monthly frequencies of these terms, they found the model accurately reproduced this injected pattern: “rain” predominated early in the year, whilst “liquid sunshine” emerged towards the end, with a clear mid-year transition, all subject to expected statistical fluctuations.\nBeyond synthetic changes, the model also captured naturally occurring seasonal patterns, such as the increased frequency of terms like “snow” and “sleet” in winter months, and “hot” or “warm” in summer. However, researchers viewed these as simpler instances of temporal influence, primarily affecting word frequencies. To explore a more complex scenario, a second experiment focused on altering a co-occurrence pattern, which they described as the “fixation of a collocation.” Here, they synthetically replaced instances of “rain” not immediately followed by “and” with “rain and snow” in a time-dependent manner. This aimed to render “rain and snow” an obligatory pairing by the year’s end, akin to how “bread and butter” functions as a fixed phrase. Again, analysis of daily predictions across the year confirmed the model’s success: towards the year’s end, predictions almost exclusively featured “rain and snow,” whilst earlier in the year, “rain” could appear alone—though “rain and snow” also occurred, reflecting genuine meteorological conditions for periods like January.\nInvestigations into the model’s internal workings, specifically its attention mechanisms (using techniques alluded to as ‘excite’), revealed that certain attention heads had specialised in capturing these temporal dependencies. For instance, the attention paid from “snow” back to “rain” (when generating “rain and snow”) varied appropriately with the time of year. Furthermore, early-year co-occurrences of “rain and snow” often correctly conditioned on contextual cues like “cold system,” underscoring the model’s ability to learn nuanced patterns. These findings collectively provided a proof of concept: Transformer-based LLMs can indeed be rendered efficiently time-aware through the simple addition of a temporal dimension to their token embeddings.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "href": "chapter_ai-nepi_017.html#broader-implications-and-future-trajectories",
    "title": "15  Time-Aware Language Models",
    "section": "15.5 Broader Implications and Future Trajectories",
    "text": "15.5 Broader Implications and Future Trajectories\n\n\n\nSlide 21\n\n\nThe successful proof of concept for the Time Transformer opens several avenues for application and further research. A foundational Time Transformer could provide a robust basis for numerous downstream tasks reliant on historical data. Furthermore, an instruction-tuned version might enable users to interact with information as it existed at a specific point in time, potentially even enhancing present-focused interactions by providing a richer temporal context. This architectural principle could, moreover, extend to model dependencies on other metadata dimensions, such as geographical origin or textual genre.\nRegarding future work, researchers identified several promising directions. Benchmarking the Time Transformer against approaches that treat time as an explicit token within the input sequence would prove valuable. Another important investigation involves testing whether the inclusion of an explicit temporal dimension enhances training efficiency; the hypothesis posits that it could aid the model in more readily deciphering complex temporal patterns that are otherwise only implicitly cued.\nNevertheless, translating this concept into widespread practical application faces notable challenges. The architectural modification—the addition of a temporal dimension to embeddings—raises questions about the feasibility and efficiency of fine-tuning existing pre-trained models; indeed, it may necessitate training new models from scratch. This, in turn, implies significant computational costs for any application beyond the small-scale demonstration. A crucial shift from current practices involves the loss of metadata-free self-supervised learning; the Time Transformer requires meticulous data curation to assign a temporal marker to every token sequence. For historians, accurately determining the ‘generation date’ of textual material can prove complex, involving considerations of original utterance, reprints, and publication lags.\nAs a concluding reflection, the presenter suggested that a more modest, targeted encoder model, akin to BERT, built upon the same time-aware principle, might offer a pragmatic path for specific tasks that do not require full generative capabilities. Such a model could focus on learning relevant temporal patterns without the overhead of modelling all linguistic intricacies. Collaboration on exploring these targeted applications is welcomed.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#overview",
    "href": "chapter_ai-nepi_018.html#overview",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Researchers presented a two-part investigation into leveraging Large Language Models (LLMs) for enhancing metadata and conducting diachronic analyses of chemical knowledge within historical scientific texts. The first part detailed the application of LLMs to improve metadata for a diachronic corpus, specifically focusing on categorising articles by scientific discipline, assigning semantic tags (topics), and generating abstractive summaries. The second part presented a case study analysing the evolution of the chemical space over time across different disciplines, aiming to identify periods of heightened interdisciplinarity and knowledge transfer. This work utilised the Philosophical Transactions of the Royal Society of London, a corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and almost 300 million tokens. For metadata enrichment, the team employed Llama 3, particularly the Hermes-2-Pro-Llama-3-8B model, fine-tuned for structured output. For the diachronic analysis of chemical terms, ChemDataExtractor, a Python module, identified chemical substances, and Kullback-Leibler Divergence (KLD) measured changes in their usage over time, both within and between disciplines such as chemistry, biology, and physics. Key findings include the successful application of LLMs for article categorisation, the identification of distinct evolutionary patterns in chemical terminology across disciplines, and the detection of knowledge transfer instances between scientific fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "href": "chapter_ai-nepi_018.html#project-objectives-and-structure",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Project Objectives and Structure",
    "text": "16.1 Project Objectives and Structure\n\n\n\nSlide 01\n\n\n    Researchers outlined a project, presented by Diego Alves and Sergey and developed in collaboration with LLM expert Badr Abdullah, titled \"Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.\" The investigation divides into two distinct parts. Part one explores the application of Large Language Models (LLMs) to enhance the metadata associated with historical texts, particularly within a diachronic corpus. This enhancement focuses on categorising articles by scientific discipline, assigning relevant semantic tags or topics, and creating abstractive summaries. Subsequently, part two delves into a case study analysing the evolution of the chemical space across different disciplines over time. This analysis specifically seeks to identify periods marked by significant interdisciplinarity and instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "href": "chapter_ai-nepi_018.html#data-source-the-philosophical-transactions-of-the-royal-society",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 Data Source: The Philosophical Transactions of the Royal Society",
    "text": "16.2 Data Source: The Philosophical Transactions of the Royal Society\n\n\n\nSlide 01\n\n\n    The project investigates how scientific English evolved over time to become an optimised medium for expert-to-expert communication, concurrently analysing phenomena such as knowledge transfer and the identification of influential papers and authors. For this purpose, researchers utilised the Philosophical Transactions of the Royal Society of London. First published in 1665, this journal holds the distinction of being the oldest scientific journal in continuous publication. It played a pivotal role in the development of scientific communication, notably establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge, and it continues to be a highly respected publication today.\n\n    Throughout its history, the journal has featured numerous influential contributions. For instance, in the 17th century, Isaac Newton published his \"New Theory about Light and Colours\" (1672). The 18th century saw Benjamin Franklin's account of \"The 'Philadelphia Experiment'\" concerning the electrical kite. Later, in the 19th century, James Clerk Maxwell detailed his \"Dynamical Theory of the Electromagnetic Field\" (1865). Beyond these seminal works, the corpus also contains more curious papers, such as Monsieur Autour's speculations on the inhabitants of the Earth and Moon. However, the current research refrains from fact-checking or assessing the scientific validity of these historical papers, focusing instead on other analytical dimensions.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "href": "chapter_ai-nepi_018.html#the-royal-society-corpus-rsc-6.0-and-existing-metadata",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata",
    "text": "16.3 The Royal Society Corpus (RSC) 6.0 and Existing Metadata\n\n\n\nSlide 08\n\n\n    Researchers employed the Royal Society Corpus (RSC) version 6.0 full for their analysis. This extensive collection covers over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 individual texts and amounting to almost 300 million tokens. The corpus already possesses some encoded metadata, including author names, century, year, and volume information.\n\n    A previous study attempted to define research disciplines and classify papers within this corpus using Latent Dirichlet Allocation (LDA) topic modelling. The output from this LDA analysis, however, revealed a mixture of disciplines, sub-disciplines, and even types of texts, such as \"Observation\" and \"Reporting,\" rather than purely thematic categories. A visual representation of this earlier classification showed a hierarchical structure with categories like \"LifeScience1,\" \"LifeScience2,\" alongside \"Chemistry\" and \"Physics,\" highlighting the need for a more refined categorisation approach.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#part-i-llms-for-metadata-enrichment-and-knowledge-organisation",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation",
    "text": "16.4 Part I: LLMs for Metadata Enrichment and Knowledge Organisation\n\n\n\nSlide 10\n\n\n    The first part of the project focused on employing Large Language Models (LLMs) to enhance existing metadata and generate new metadata types for the historical texts. Researchers aimed to leverage LLMs for several information management and knowledge organisation tasks. These included text clean-up, summarisation, improved information extraction, categorisation, and enhanced access and retrieval capabilities, with the potential to feed structured information into knowledge graphs.\n\n    To illustrate the process, an example article titled \"A Spot in one of the Belts of Jupiter\" was considered. Such historical texts often present syntactic complexities characteristic of older writing styles. For this article, the LLM was tasked with providing a hierarchical categorisation (e.g., Discipline: Astronomy, Sub-discipline: Planetary Science), a list of relevant index terms (e.g., Astronomy, Jupiter, Telescopes), and a concise \"Too Long; Didn't Read\" (TL;DR) summary. An example summary provided was: \"The author reports observing a spot in one of Jupiter's belts using a 12-foot telescope. The spot was found to move from east to west within two hours, indicating movement on the planet's surface.\"",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "href": "chapter_ai-nepi_018.html#llm-selection-and-prompt-engineering-for-metadata-generation",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 LLM Selection and Prompt Engineering for Metadata Generation",
    "text": "16.5 LLM Selection and Prompt Engineering for Metadata Generation\n\n\n\nSlide 13\n\n\n    Researchers selected Llama 3 for their metadata enrichment tasks, a new release in the Llama LLM family offering 8 billion (8B) and 70 billion (70B) parameter versions, with a 400B parameter model currently in training. This model, accessible via Hugging Face, reportedly offers significant improvements over predecessors like Mistral and Llama 2. The team specifically utilised instruction-tuned versions, deemed suitable for their objectives, and employed Hermes-2-Pro-Llama-3-8B, a variant fine-tuned for generating structured output, particularly JSON and YAML.\n\n    A detailed system prompt guided the LLM. The prompt first established a role: \"Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.\" It then defined the objective: to read, analyse, and organise the corpus to create a structured database facilitating research. The input was described as OCR-extracted text snippets with existing metadata.\n\n    Four main tasks were specified. Task A involved reading and analysing the article to suggest an alternative, more content-reflective title. Task B required writing a concise 3-4 sentence TL;DR summary in simple language, suitable for a high school student. Task C mandated the identification of exactly five main topics, conceptualised as Wikipedia-style keywords for scientific sub-fields. Finally, Task D involved identifying a primary scientific discipline from a predefined list of nine (Physics, Chemistry, Environmental  Earth Sciences, Astronomy, Biology  Life Sciences, Medicine  Health Sciences, Mathematics  Statistics, Engineering  Technology, Social Sciences  Humanities) and a corresponding second-level sub-discipline, which the LLM could freely define but could not be one of the primary disciplines.\n\n    An example input provided to the LLM included metadata for Isaac Newton's 1672 letter, which has a very long original title, along with a text snippet. The desired output was specified in YAML format, exemplified by a revised title (\"A New Theory of Light and Colours\"), five topics (e.g., \"Optics,\" \"Refraction\"), a TL;DR summary, and the categorisation (\"Physics,\" \"Optics  Light\"). The prompt concluded by strictly requiring valid YAML output.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "href": "chapter_ai-nepi_018.html#llm-output-validation-and-discipline-distribution-analysis",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 LLM Output Validation and Discipline Distribution Analysis",
    "text": "16.6 LLM Output Validation and Discipline Distribution Analysis\n\n\n\nSlide 15\n\n\n    Initial sanity checks on the LLM's output revealed promising results. An impressive 99.81% of the generated outputs (17,486 out of 17,520) conformed to a valid YAML structure, with only a minor 0.19% failing this validation. Regarding the prediction of scientific disciplines, 94% fell within the predefined set of nine categories. However, some hallucinations and errors occurred; for instance, the LLM occasionally produced minor variations like \"Earth Sciences\" instead of the specified \"Environmental  Earth Sciences,\" or invented entirely novel categories such as \"Music.\" In other cases, it mistakenly included the numerical index of a discipline as part of its name or classified sub-disciplines like \"Neurology\" or \"Zoology\" as primary disciplines. Despite these anomalies, the researchers concluded that the majority of papers received correct discipline assignments.\n\n    An analysis of the distribution of scientific disciplines over time, based on the LLM's categorisation, revealed distinct trends. Up until the end of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. A notable peak in chemical articles emerged in the late 18th century, coinciding with the chemical revolution, after which chemistry established itself as a principal pillar of the Royal Society's publications. Progressing into the 19th and 20th centuries, three main pillars—Biology, Physics, and Chemistry—became dominant.\n\n    Furthermore, researchers visualised the TL;DR summaries using t-SNE projections. This technique demonstrated how different disciplines clustered in the semantic space. Chemistry appeared centrally, with significant overlap with Physics and Biology. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters. This static visualisation hinted at the potential for diachronic analysis, enabling the observation of shifts and evolving overlaps between disciplines over time. These LLM-derived discipline categorisations subsequently informed the diachronic analysis of the chemical space.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "href": "chapter_ai-nepi_018.html#part-ii-diachronic-analysis-of-the-chemical-space-methodology",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Part II: Diachronic Analysis of the Chemical Space Methodology",
    "text": "16.7 Part II: Diachronic Analysis of the Chemical Space Methodology\n\n\n\nSlide 13\n\n\n    The second part of the research concentrated on a diachronic analysis of the chemical space, focusing on three disciplines most prevalent in the corpus: Chemistry, Biology, and Physics. To achieve this, researchers first needed to extract chemical terms from the texts. They employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initially, applying ChemDataExtractor to the full text of articles produced a significant amount of noise. Consequently, a refined two-pass approach was adopted: the tool was first run on the texts, and then re-applied to the list of substances generated in the initial pass, which effectively reduced the noisy output.\n\n    For analysing the evolution of this chemical space, the team utilised Kullback-Leibler Divergence (KLD). KLD, or relative entropy, serves to detect changes across different situational contexts. It quantifies the number of additional bits required to encode a given dataset (A) when using a non-optimal model based on a different dataset (B). Higher KLD values indicate greater dissimilarity between the datasets, whilst lower values suggest similarity.\n\n    Researchers applied KLD in two distinct ways. Firstly, they conducted an intra-discipline diachronic analysis by comparing a future period (dataset A) with a past period (dataset B) within each of the three disciplines (Chemistry, Physics, and Biology) independently. This involved a sliding window technique: for a given central year, chemical term frequencies from a 20-year window preceding it were compared against those from a 20-year window succeeding it. The central year was then advanced by five years, and the comparison repeated, allowing them to trace the evolution of the chemical space within each discipline over the entire timeline. Secondly, they performed an inter-discipline comparative analysis, making pairwise comparisons of the chemical space in Chemistry versus Physics, and Chemistry versus Biology, based on 50-year segments of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-intra-disciplinary-evolution",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 KLD Analysis Results: Intra-Disciplinary Evolution",
    "text": "16.8 KLD Analysis Results: Intra-Disciplinary Evolution\n\n\n\nSlide 21\n\n\n    The Kullback-Leibler Divergence (KLD) analysis per discipline revealed that the evolutionary trend of the chemical space was quite similar across Chemistry, Biology, and Physics. Peaks and troughs in KLD values, indicating periods of significant change or stability respectively, occurred at roughly the same times for all three fields. Notably, towards the end of the analysed timeline, the KLD plots tended to flatten, and the overall KLD values decreased, suggesting less variation in chemical terminology between successive future and past periods.\n\n    Researchers then focused on a prominent KLD peak observed in the late 18th century (approximately 1740-1810). The KLD methodology permitted a closer examination of the specific chemical substances contributing most to this divergence. During the sub-period of 1776-1816, in both Biology and Physics, one or two particular elements exhibited extremely high KLD values, indicating they were primary drivers of change in the chemical lexicon of those fields. Despite these drivers, the analysis showed that largely the same set of chemical elements featured prominently across Chemistry, Biology, and Physics during this era.\n\n    This pattern contrasted significantly with observations from a later period, specifically the second half of the 19th century (approximately 1850-1900). Here, the KLD graphs for Biology and Physics were much more populated with influential chemical substances, and the individual contributions of these elements to the overall KLD were more uniform. A differentiation in the types of substances also became apparent: Biology's chemical lexicon began to evolve distinctly towards terms associated with biochemistry. Simultaneously, Chemistry and Physics showed an increasing focus on noble gases and radioactive elements, many of which were discovered towards the end of the 19th century.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "href": "chapter_ai-nepi_018.html#kld-analysis-results-inter-disciplinary-comparison-and-knowledge-transfer",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer",
    "text": "16.9 KLD Analysis Results: Inter-Disciplinary Comparison and Knowledge Transfer\n\n\n\nSlide 24\n\n\n    Pairwise comparisons using Kullback-Leibler Divergence (KLD) further illuminated the distinct chemical focuses of the disciplines, particularly evident in word clouds representing the second half of the 20th century. When comparing Chemistry and Biology, the word cloud for Biology featured a greater prominence of substances related to biochemical processes within living organisms. In contrast, Chemistry's word cloud highlighted substances associated with organic chemistry, such as hydrocarbons and benzene. A comparison between Chemistry and Physics revealed that Physics's chemical lexicon was characterized by a higher frequency of metals—including rare earth metals, semi-metals, and radioactive metals—alongside noble gases. These interdisciplinary comparisons effectively identified thematic divergences in chemical terminology.\n\n    Crucially, this pairwise KLD analysis also enabled the detection of instances termed \"knowledge transfer.\" This phenomenon describes situations where a chemical element, initially ranked as highly distinctive of Chemistry in an earlier period, subsequently becomes more characteristic of either Biology or Physics in a later period. For example, when comparing Chemistry and Physics, the element tin was found to be distinctive of Chemistry during the first half of the 18th century but shifted to become more distinctive of Physics in the second half of that century. Similar shifts for other elements were observed in the early 20th century. A comparable pattern emerged in the Chemistry versus Biology comparison during the 20th century, where elements becoming distinctive of Biology were, once again, frequently related to biochemical processes.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "title": "16  Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.10 Conclusion and Future Work",
    "text": "16.10 Conclusion and Future Work\n\n\n\nSlide 21\n\n\n    In conclusion, the researchers successfully employed a Large Language Model (LLM) to improve article categorisation and topic modelling for texts within their historical corpus. Building upon these LLM-generated classifications, they conducted a diachronic analysis of the chemical space across three key disciplines—Chemistry, Biology, and Physics—and performed interdisciplinary comparisons of this chemical space.\n\n    Looking ahead, several avenues for future work present themselves. For the LLM-focused component (Part I), plans include testing alternative LLMs and undertaking a formal evaluation of the results obtained from the Llama 3 model. Regarding the analysis of the chemical space (Part II), researchers intend to pursue a more fine-grained interdisciplinary analysis, potentially by experimenting with different sizes for diachronic sliding windows and varying comparison timeframes. Furthermore, they aim to expand the analysis to include additional disciplines, such as comparing Chemistry with Medicine, and to explore tracing the evolution of the chemical space using the concept of surprisal.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leverage Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers are exploring the computational analysis of semantic change, focusing on the intricate modelling of diverse contextual factors and their dynamic interplay. This investigation forms an integral part of the Cascade project, a prestigious Marie Curie doctoral network, with significant contributions from PhD student Sofía Aguilar. Previous work meticulously modelled different context types in isolation; the current objective, however, seeks to synthesise these approaches to illuminate their complex interactions.\nThe chemical revolution, specifically the profound conceptual shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a compelling pilot study. Linguists involved in this endeavour examine how language adapts to real-world transformations, drawing upon established register theory and principles of rational communication. The study aims to detect periods of linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately uncover the linguistic mechanisms and communicative drivers underpinning these transformations. A novel framework, employing Graph Convolutional Networks (GCNs), is proposed to model language dynamics by treating context as a central signal, thereby aiming to overcome limitations of existing methods in capturing the nuanced interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-contextual-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Contextual Frameworks and the Chemical Revolution Pilot\n\n\n\nSlide 01\n\n\nWithin the Cascade project, a Marie Curie doctoral network, researchers are rigorously investigating the computational analysis of semantic change. PhD student Sofía Aguilar spearheads efforts to model context comprehensively, meticulously examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more holistic understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing extensively upon the Royal Society Corpus (RSC). This pivotal historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation richly documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the iconic painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors: situational (where), temporal (when), experiential (what), interpersonal (who), textual (how), and causal (why).\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers inherent variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-through-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts through Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts through Kullback-Leibler Divergence\n\n\n\nSlide 04\n\n\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal substantially higher divergence due to profound linguistic evolution.\nResearchers Degaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. This technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time (e.g., from 1725 to 1845) reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 precisely coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can meticulously track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "href": "chapter_ai-nepi_019.html#paradigmatic-change-and-scientific-influence-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Paradigmatic Change and Scientific Influence: Cascade Models",
    "text": "17.3 Paradigmatic Change and Scientific Influence: Cascade Models\n\n\n\nSlide 08\n\n\nBeyond temporal detection, this investigation delves into paradigmatic context and the dynamics of conceptual change, referencing seminal work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand precisely who spearheads and propagates these linguistic and conceptual shifts, researchers Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that elegantly illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "href": "chapter_ai-nepi_019.html#linguistic-realisation-and-communicative-drivers-the-role-of-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal",
    "text": "17.4 Linguistic Realisation and Communicative Drivers: The Role of Surprisal\n\n\n\nSlide 10\n\n\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with valuable contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is directly proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, researchers meticulously examine shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, vividly visualised through graphs plotting surprisal against year, robustly supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community utilising that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-novel-framework-graph-convolutional-networks-for-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics",
    "text": "17.5 A Novel Framework: Graph Convolutional Networks for Contextual Dynamics\n\n\n\nSlide 12\n\n\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the profound understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. The proposed framework positions context as a central signal for modelling language dynamics, with Graph Convolutional Networks (GCNs) identified as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four distinct stages:\n\nData Sampling: This initial stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: Researchers begin by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, network size is refined using community detection algorithms, such as that proposed by Riolo and Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering profound insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Directions",
    "text": "17.6 Reflections: Limitations and Future Directions\n\n\n\nSlide 16\n\n\nThis research acknowledges several profound questions that delineate its current limitations and chart compelling future directions. A primary concern involves the very nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context truly integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "",
    "text": "Overview\nResearchers meticulously investigate the processes and impacts of science funding, moving beyond conventional bibliometric analyses of publications and grants. This inquiry focuses upon the National Human Genome Research Institute (NHGRI), renowned for its pioneering contributions to biology, particularly through the Human Genome Project. An interdisciplinary team, encompassing historians, physicists, ethicists, computer scientists, and former NHGRI leadership, analyses an extensive internal archive. This repository contains over two million pages of documents from the NHGRI, including meeting notes, handwritten correspondence, presentations, spreadsheets, and emails. Growing by 5% annually through continuous digitisation, this archive offers unparalleled insight into the internal workings of a major funding agency.\nMethodologically, the project employs advanced computational techniques to process and analyse these born-physical and born-digital artefacts. These techniques include developing a custom-built handwriting removal model using a U-Net architecture. This process enhances the accuracy of Optical Character Recognition (OCR) for printed text whilst enabling a distinct processing pipeline solely for handwriting recognition. Multimodal models, harnessing visual, textual, and structural modalities, facilitate sophisticated tasks such as entity extraction and synthetic data generation to train classifiers. Robust systems for entity and Personally Identifiable Information (PII) recognition meticulously mask and disambiguate sensitive data, including names, organisations, email addresses, locations, and identification numbers, achieving high F1 scores with relatively small fine-tuned datasets.\nA pivotal aspect of the research involved reconstructing and analysing the NHGRI’s internal and external email correspondence network. By extracting entities from thousands of scanned paper emails and linking them, researchers successfully recreated an intricate web of communication from the Human Genome Project era. This reconstructed network encompasses 62,511 distinct email conversations originating from 5,414 individual scanned emails. Network analysis, including stochastic block modelling and brokerage role analysis, uncovered leadership structures, such as an informal ‘Kitchen Cabinet’ within the International HapMap Project. It also revealed patterns of information flow, suggesting NHGRI leadership frequently adopted a consultative, rather than gatekeeping, posture.\nFurthermore, the research explored funding decisions by computationally modelling the factors influencing organism selection for genome sequencing post-Human Genome Project. This model integrated biological features (e.g., genome size, distance to model organisms), project characteristics (e.g., team size, submission history, gender equity, internal proposal origin), applicant reputational metrics (e.g., H-index, community size, network centrality), and linguistic characteristics of proposals (e.g., argumentation, repetitiveness). The findings suggest that all these feature categories collectively predicted funding success, with reputational factors like H-index and community size demonstrating a Matthew Effect.\nThe project’s broader objective centres on rendering such born-physical archives Findable, Accessible, Interoperable, and Reusable (FAIR). This involves applying these methodologies to other extensive archives, including federal court records and seismological data. Crucially, this endeavour underscores the imperative of preserving these rich historical data sources and developing sophisticated tools to extract knowledge, inform policy, enhance data accessibility, and address pressing scientific questions. The consortium actively seeks testers, partners, and users for its ‘Born Physical, Studied Digitally’ initiative, which receives support from the NIH, NVIDIA, and the NSF.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "href": "chapter_ai-nepi_020.html#limitations-of-current-science-funding-analysis",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.1 Limitations of Current Science Funding Analysis",
    "text": "18.1 Limitations of Current Science Funding Analysis\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds support research endeavours, expecting them to yield societal benefits such as informed policy, clinical advancements, and new technologies. Scholars in the science of science predominantly study this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Indeed, analyses of these sources, including bibliometrics, have offered valuable insights into diverse aspects of science, including its long-term impact, the evolution of team sizes, the emergence of interdisciplinary fields, and the career trajectories of scientists.\nNevertheless, relying solely on the scientific article presents an incomplete, even skewed, representation of the complex scientific process. To assume that bibliometrics fully encapsulates the essence of science constitutes an oversimplification. A deeper understanding necessitates investigating the processes that precede publication, moving beyond the flawed picture painted by articles alone. Such an approach could illuminate critical questions: Does scientific inquiry shape funding priorities, or do funding agendas dictate the direction of science? Within the innovation pipeline, from initial ideation to eventual long-term impact, where do innovations flourish, diffuse across domains, or ultimately falter?\nNotably, the focus on published articles means that failed projects, which could offer significant learning opportunities, often remain unexamined. Beyond direct financial support, funders may also contribute through public data provision, community engagement initiatives, technology development, and cooperative agreements. All these factors influence both the creation of knowledge and the scholars involved. A dynamic interplay also exists between grant funding and technological development.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.2 The Human Genome Project: A Paradigm of ‘Big Science’",
    "text": "18.2 The Human Genome Project: A Paradigm of ‘Big Science’\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a landmark example of ‘big science’ in biology, analogous to large-scale projects in fields like particle physics. This colossal undertaking brought together tens of countries and thousands of researchers with the shared goal of sequencing the entire human genome. Its significance extends across multiple dimensions. Firstly, the HGP captured public interest to an extent previously unseen for a biological research programme, shifting focus from laboratory-based studies of organisms like Drosophila and C. elegans to a grand human-centric endeavour. Secondly, its impact resonates profoundly today; a vast majority of modern biological research, particularly omics methodologies, would be unfeasible without the reference genome it produced. Indeed, the HGP effectively established genomics as a distinct scientific discipline.\nFurthermore, the project pioneered new data-sharing practices, now widely adopted, and forged a powerful synergy between computational science and biology. Two principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, crucially for this study, the National Human Genome Research Institute (NHGRI), which served as the HGP division within the US National Institutes of Health (NIH). Dr Francis Collins, then Director of the NHGRI and later Director of the NIH, played a pivotal leadership role.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "href": "chapter_ai-nepi_020.html#nhgri-an-innovative-funding-agency",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.3 NHGRI: An Innovative Funding Agency",
    "text": "18.3 NHGRI: An Innovative Funding Agency\n\n\n\nSlide 04\n\n\nAnalysis reveals the National Human Genome Research Institute (NHGRI) as a particularly innovative funding body within the National Institutes of Health (NIH). Several bibliometric indicators support this assessment when comparing NHGRI to other NIH institutes. For instance, NHGRI-funded research accounts for a larger share of manuscripts in the top 5% most cited publications. Moreover, its output demonstrates substantial long-term citation impact (measured after ten years) and generates significant citations from patents, indicating translation into clinical applications. The research funded by NHGRI also exhibits high ‘disruption’ scores, suggesting it often pioneers new directions.\nWhilst these metrics establish NHGRI’s innovative capacity, the underlying reasons for this success remain less understood. Consequently, an interdisciplinary research team has assembled to investigate the processes and practices that enable NHGRI to lead innovation. This team comprises experts from diverse fields, including history, physics, ethics, and computer science, and notably includes Dr Francis Collins, a former Director of both NHGRI and NIH. Their collective aim is to unravel the factors contributing to the rise of genomics, identify potential failure points and innovation spillovers, and understand the collaborative mechanisms between funding agencies and academic scientists that foster scientific advancement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-and-complex-source",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.4 The NHGRI Archive: A Rich and Complex Source",
    "text": "18.4 The NHGRI Archive: A Rich and Complex Source\n\n\n\nSlide 07\n\n\nOwing to the recognised historical importance of the Human Genome Project, the NHGRI meticulously preserved a substantial collection of its internal documentation, spanning from the 1980s and 1990s into subsequent years. This internal archive constitutes a rich repository, containing diverse materials such as the daily meeting notes of scientists coordinating the genome project, handwritten annotations from correspondence, conference agendas, formal presentations, detailed spreadsheets, and newspaper clippings chronicling the period. Additionally, it includes various internal forms, research proposals, and extensive email communications.\nThe sheer volume of this collection, currently exceeding two million pages and expanding by approximately 5% each year due to continuous digitisation, presents a formidable challenge. Effectively analysing such a vast and complex born-physical and born-digital artefact at scale necessitates innovative approaches, forming the core of the research team’s methodological development.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "href": "chapter_ai-nepi_020.html#internal-archives-versus-public-data",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.5 Internal Archives Versus Public Data",
    "text": "18.5 Internal Archives Versus Public Data\n\n\n\nSlide 09\n\n\nThe internal documents housed within the NHGRI archive possess characteristics and content fundamentally distinct from publicly accessible information, such as Requests for Applications (RFAs) and peer-reviewed publications. Whilst scholars can readily access RFAs and publications through databases like PubMed or NIH RePORTER, the internal records offer a different perspective. Visualisations, such as t-SNE plots, demonstrate that these internal materials form distinct clusters, separate from the clusters representing public RFAs and publications.\nThese internal records provide detailed accounts of numerous large-scale genomic projects initiated and funded by NHGRI. Examples include the Ethical, Legal, and Social Implications (ELSI) Program’s LSAC, modENCODE, eMERGE, ENCODE, the foundational Human Genome Project itself, PAGE, the International HapMap Project, H3Africa, and the NHGRI-EBI GWAS Catalog. Many of these initiatives represented substantial investments, often amounting to tens or hundreds of millions of dollars, and mobilised thousands of researchers globally. Their collective purpose was to develop crucial resources for the genomics community, thereby catalysing the advancement of the entire field.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-processing",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.6 Computational Methodologies for Archive Processing",
    "text": "18.6 Computational Methodologies for Archive Processing\n\n\n\nSlide 10\n\n\nResearchers developed specialised computational methods to manage the born-physical archive, a significant portion of which contains handwritten material. The use of AI for handwriting analysis presents not only technical hurdles but also ethical considerations regarding the unknown nature of handwritten content. To address this, the team trained a custom-built handwriting model, employing a U-Net architecture, specifically to identify and remove handwritten portions from documents. This process offers a dual benefit: it enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text and simultaneously enables the creation of a distinct processing pipeline dedicated solely to handwriting recognition.\nBeyond handwriting, the project leverages advances in multimodal models, drawing from the document intelligence research community. These models ingeniously combine visual information (the document image), textual content, and layout structure. Such an integrated approach supports diverse tasks, including sophisticated entity extraction. Furthermore, it facilitates the generation of synthetic documents, which serve as valuable training data for developing and refining new classification algorithms, thereby improving the system’s analytical capabilities over time.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "href": "chapter_ai-nepi_020.html#entity-and-pii-recognition",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.7 Entity and PII Recognition",
    "text": "18.7 Entity and PII Recognition\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous handling of sensitive information. The documents contain genuine Personally Identifiable Information (PII), including details of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles today. Consequently, the researchers implemented robust methods for entity and PII recognition. Their system effectively identifies, masks, and disambiguates such sensitive data throughout the vast collection.\nThe performance of these recognition models proves strong, as evidenced by F1 scores that improve with increased fine-tuning data for various entity types. These include persons (PERSON), organisations (ORG), email addresses (EMAIL), locations (LOC), and identification numbers (IDNUM). This careful approach ensures privacy and ethical compliance whilst enabling scholarly analysis of the archive’s content.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "href": "chapter_ai-nepi_020.html#reconstructing-the-nhgri-correspondence-network",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.8 Reconstructing the NHGRI Correspondence Network",
    "text": "18.8 Reconstructing the NHGRI Correspondence Network\n\n\n\nSlide 13\n\n\nResearchers conducted a detailed case study involving the reconstruction of email networks from the NHGRI archive. By extracting entities from thousands of scanned paper copies of emails and linking them, they successfully recreated the intricate web of correspondence that occurred during the Human Genome Project era. This reconstructed network encompasses 62,511 distinct email conversations originating from 5,414 individual scanned emails. Each node in the visualised network typically represents an individual involved in these communications, with affiliations to NIH (National Institutes of Health) or external entities (such as other funding agencies, companies, or universities) clearly delineated.\nApplying network analysis techniques, such as stochastic block modelling for community detection, yielded significant insights, particularly concerning the coordination of large-scale initiatives like the International HapMap Project. The HapMap Project, a major genomics endeavour following the HGP, focused on human genetic variation and laid the groundwork for genome-wide association studies (GWAS). Managing such a complex project, involving numerous universities and agencies, relied on formal structures like a steering committee. However, the computational analysis, performed in an unsupervised manner, revealed a hitherto undocumented informal leadership circle, dubbed the ‘Kitchen Cabinet’. This group, referencing a term from the Nixonian political era, apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "href": "chapter_ai-nepi_020.html#information-brokerage-and-leadership-dynamics",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.9 Information Brokerage and Leadership Dynamics",
    "text": "18.9 Information Brokerage and Leadership Dynamics\n\n\n\nSlide 16\n\n\nFurther investigation into the NHGRI’s operational dynamics focused on comparing the communication patterns of the informal ‘Kitchen Cabinet’ with those of the formal Steering Committee. Utilising brokerage role analysis, which assesses how individuals or groups mediate information flow within a network, researchers identified distinct behavioural patterns. This analysis categorises nodes based on their interaction styles, such as ‘consultant’ (receiving information and disseminating it back within their own group) or ‘gatekeeper’ (receiving information but not sharing it back with the originating group).\nThe findings indicate that the ‘Kitchen Cabinet’ primarily functioned in a consultant capacity, a pattern that distinguished it from other formal leadership structures active at the time. Notably, individuals like Francis Collins appeared to play significant consultant roles within this informal group. This suggests a leadership style at NHGRI that favoured consultation and open information exchange rather than restrictive gatekeeping, potentially contributing to the agency’s collaborative successes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-genome-sequencing",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.10 Modelling Funding Decisions for Genome Sequencing",
    "text": "18.10 Modelling Funding Decisions for Genome Sequencing\n\n\n\nSlide 13\n\n\nThe research extended to analysing the funding agency’s decision-making processes through portfolio analysis, specifically modelling the choices made when selecting non-human organisms for genome sequencing following the completion of the Human Genome Project. Funding agencies like NHGRI faced complex decisions in allocating resources amongst numerous proposals from different organismal research communities, each advocating for their chosen species (e.g., various primates). To understand these decisions, scientists developed a machine learning model designed to recapitulate the actual funding outcomes.\nThis computational model incorporated a diverse array of features. Biological characteristics, such as the proposed organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (Area Under Curve, AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, measures of gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors submitting the proposal, the overall size of the relevant research community, the proposers’ centrality within the NHGRI network, and the breadth of community support, showed strong predictive power (AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, also contributed to the model’s accuracy (AUC: 0.85 ± 0.04).\nCrucially, when all these feature sets were combined, the model achieved a high level of performance in predicting funding decisions (AUC: 0.94 ± 0.03). This indicates that a multifaceted approach, considering biological, project-related, reputational, and linguistic factors, is necessary to understand the complexities of such funding allocations.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "href": "chapter_ai-nepi_020.html#interpreting-funding-decisions-the-matthew-effect",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.11 Interpreting Funding Decisions: The Matthew Effect",
    "text": "18.11 Interpreting Funding Decisions: The Matthew Effect\n\n\n\nSlide 15\n\n\nTo delve deeper into the factors driving funding decisions, researchers employed feature interpretability techniques. These methods help to elucidate how individual features within the computational model influence the predicted outcome, thereby identifying characteristics associated with a higher or lower probability of receiving funding. One significant observation from this analysis is the presence of a ‘Matthew Effect’, a phenomenon where initial advantages tend to accumulate further advantages.\nSpecifically, the analysis revealed that proposals submitted by authors with a higher maximum H-index were more likely to secure funding. Similarly, organisms supported by a larger, more established research community also had a greater chance of being selected for sequencing. This pattern aligns with the strategic objectives of funding agencies, which often prioritise projects perceived as having a higher potential for significant downstream impact and eventual clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "href": "chapter_ai-nepi_020.html#broader-applications-from-archives-to-knowledge",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.12 Broader Applications: From Archives to Knowledge",
    "text": "18.12 Broader Applications: From Archives to Knowledge\n\n\n\nSlide 16\n\n\nThe methodologies and insights gained from the NHGRI archive project extend to a broader vision of leveraging born-physical archives through computational analysis. This specific study serves as a compelling example of how such historical data, when processed with advanced tools, can yield valuable knowledge. The research consortium collaborates with various partners and utilises diverse data sources beyond the NHGRI, including federal court records from the United States and seismological data (seismograms) from the EarthScope Consortium.\nA generalised workflow underpins these efforts. It begins with the acquisition of data and metadata from these archives. Subsequently, a sophisticated knowledge creation pipeline applies a series of computational processes. These include:\n\npage stream segmentation to delineate document structures\nhandwriting extraction\nentity disambiguation to resolve identities\nlayout modelling to understand document formats\ndocument categorisation\nfurther entity recognition\nredaction of personal information for privacy\nmodelling of decisions or processes captured in the records\n\nThe ultimate aim of this comprehensive approach is to transform raw archival data into actionable insights that can address pressing scientific questions, inform policy-making, and significantly enhance the accessibility of these rich historical resources. This transformation relies on robust algorithms and a well-developed cyberinfrastructure.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "href": "chapter_ai-nepi_020.html#preservation-collaboration-and-nhgris-significance",
    "title": "18  Beyond Traditional Views of Science Funding",
    "section": "18.13 Preservation, Collaboration, and NHGRI’s Significance",
    "text": "18.13 Preservation, Collaboration, and NHGRI’s Significance\n\n\n\nSlide 17\n\n\nA significant challenge remains in the preservation of born-physical data, much of which currently resides in vulnerable conditions, such as shipping containers susceptible to damage and neglect. The imperative to safeguard these invaluable historical records for future scholarly and scientific inquiry cannot be overstated. Recognising this, the ‘Born Physical, Studied Digitally’ consortium actively seeks collaboration, inviting testers, partners, and users to engage with their tools and methodologies. This initiative receives support from prominent organisations including the National Institutes of Health (specifically NHGRI), NVIDIA, and the National Science Foundation (NSF).\nThe speaker also highlighted a pertinent contemporary issue: recent proposals within the United States to dissolve the NHGRI. This underscores the critical need to appreciate the agency’s historical contributions. Evidence suggests NHGRI stands as one of the most innovative funding agencies in the annals of science. Consequently, the rich data contained within its archives promise to unlock answers to numerous significant scientific questions, reinforcing the importance of its continued study and preservation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Beyond Traditional Views of Science Funding</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nResearchers Malte Vogl, Raphael Schlattmann, and Alex Kaye confront the formidable challenge of extracting structured knowledge from historically significant, yet computationally intractable, unstructured biographical sources. Their pioneering work introduces a novel method to transform such materials—typically printed books or dictionaries—into readily queryable knowledge graphs.\nThis innovative approach transcends the conventional reliance on pre-structured datasets, prevalent within History and Philosophy of Science (HPSS). Instead, it strategically employs Large Language Models (LLMs) not as infallible oracles, but as integral components within a meticulously engineered pipeline. The primary objective involves imposing rigorous structure upon unstructured data, such as Polish and German biographical dictionaries, thereby facilitating complex historical inquiries. These investigations might, for instance, explore the formation of intellectual networks or trace the evolution of professional connections over time.\nThe team expertly utilises tools like Neo4j for graph representation, having developed a sophisticated multi-stage pipeline that incorporates crucial human oversight to ensure both accuracy and relevance. Ultimately, this methodology enables the construction of controllable knowledge graphs, where entities—individuals, locations, organisations, and publications—become distinct nodes, and their interconnections, meticulously derived from textual evidence, form the relational edges.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "href": "chapter_ai-nepi_021.html#transforming-unstructured-biographical-data-into-queryable-knowledge-graphs",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs",
    "text": "19.1 Transforming Unstructured Biographical Data into Queryable Knowledge Graphs\n\n\n\nSlide 01\n\n\nResearchers confront a significant challenge in historical studies: many invaluable biographical sources persist in unstructured textual formats, such as printed dictionaries and compendia. Whilst existing History and Philosophy of Science (HPSS) datasets often comprise structured information—for instance, publication databases or email archives—these unstructured materials, despite their rich detail, resist computational analysis. Consequently, this project pioneers a methodology to systematically impose structure upon such data, specifically targeting biographical entries. The core idea leverages Large Language Models (LLMs), not as standalone solutions, but as crucial components within a larger, controllable pipeline engineered to construct knowledge graphs.\nHistorically, tools like Get Grass facilitated access to printed materials for digitisation. The current approach, however, aims for a deeper level of structuration. It conceptualises biographical information as a knowledge graph: a network where entities—individuals, geographical locations, countries, published works, or organisations—constitute the nodes. The relationships between these entities, as described in the source texts, form the connecting edges. Such a graph, once constructed, permits complex, structured queries. For instance, one might investigate how professional networks evolved within a specific discipline during a particular era, or trace the contacts an individual established over their career.\nThe team employs LLMs selectively, focusing on their utility for specific tasks within a broader information processing chain, rather than pursuing an elusive ‘perfect’ model. Source materials include Polish biographical collections and German-language resources such as the handbook Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. An entry from the latter, for Alexander Abusch, might detail his role as Minister für Kultur alongside birth and death dates. An extraction pipeline processes these text-based entries, often derived from scanned documents, transforming them into a visual and queryable graph, potentially managed in a system like Neo4j. This process thus converts isolated, albeit information-dense, textual accounts into a connected, structured dataset ripe for scholarly exploration.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "href": "chapter_ai-nepi_021.html#illustrative-extraction-structuring-polish-biographical-entries",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Illustrative Extraction: Structuring Polish Biographical Entries",
    "text": "19.2 Illustrative Extraction: Structuring Polish Biographical Entries\n\n\n\nSlide 06\n\n\nTo illustrate the extraction methodology, consider a Polish biographical entry for Bartsch Henryk, an evangelical priest (ks. ewang.). The text records his birth on 12th December 1832 in Wladyslawowo, situated within the Konin district. Furthermore, it details his extensive travels to Italy (Włochy), Greece (Grecja), the Holy Land (Ziemia Święta), and Egypt (Egipt). His scholarly contributions encompass several publications: Wspomnienia z podróży do Jerozolimy i Kairo (Warsaw, 1873), Listy z podróży po Grecji i Sycylji (Warsaw, 1874), and Z teki podróżniczej, szkice dawne i nowe oryginalne i tłumaczone (Warsaw, 1883). The entry itself cites Bystron’s Wielka Encyklopedja as a source.\nFrom this concise textual snippet, the system extracts key entities and their relationships. ‘Bartsch Henryk’ is identified as a PERSON, his ROLE as ‘ks. ewang.’, his birthdate as ‘12. XII. 1832’ (a DATE), and ‘Wladyslawowo’ along with the countries he visited as LOCATIONs. Relationships such as ‘born on’, ‘born in’, ‘located in’, and ‘travelled to’ link these entities. Consequently, this process yields a set of structured triples: (Bartsch Henryk, is a, ks. ewang.), (Bartsch Henryk, travelled to, Włochy), and so forth, effectively translating narrative information into a machine-readable format suitable for knowledge graph construction.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-architecture-and-guiding-principles-for-knowledge-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction",
    "text": "19.3 Pipeline Architecture and Guiding Principles for Knowledge Extraction\n\n\n\nSlide 08\n\n\nEngineers have designed a sophisticated two-stage pipeline to transform raw textual data into structured knowledge graphs. This architecture systematically processes information, integrating both automated techniques and human expertise. The first stage, ‘Ontology-agnostic Open Information Extraction’ (OIE), focuses on identifying factual statements. It commences by loading and chunking the source data, then proceeds to OIE extraction, validates these extractions, and standardises them against benchmarks. A critical quality control checkpoint determines the sufficiency of the OIE output; insufficient output triggers a manual correction loop for a sample of triples.\nSubsequently, the second stage, ‘Ontology-driven Knowledge Graph (KG) building’, refines and structures this information. This stage commences by formulating Competency Questions (CQs) that define the KG’s desired analytical capabilities. Based on these CQs, developers create an ontology and define SHACL (Shapes Constraint Language) shapes for validation. The pipeline then maps the extracted triples to this ontology, disambiguates entities (potentially linking them to Wikidata IDs), represents the data using RDF-star, and subsequently performs SHACL validation. Similar to Stage 1, a quality control loop allows for manual correction of CQs, the ontology, or SHACL shapes if necessary. Both stages integrate layers for LLM interaction and crucial ‘Human in the Loop’ interventions, ensuring robust and reliable outcomes.\nSeveral core principles underpin this pipeline’s design. It is fundamentally research-driven and data-oriented; thus, ontology development directly addresses specific research questions and aligns with the data’s realistic provisions. The human-in-the-loop paradigm strategically combines LLM automation for efficiency with expert oversight at all critical junctures, balancing scale with quality. Transparency is paramount; researchers design each step for verifiability. Furthermore, task decomposition breaks the complex process into manageable, sequential units—notably performing OIE before aligning with an ontology. Finally, modularity ensures the system can adapt, allowing for the integration of improved models or components as technology evolves.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-1-workflow-open-information-extraction-in-detail",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Stage 1 Workflow: Open Information Extraction in Detail",
    "text": "19.4 Stage 1 Workflow: Open Information Extraction in Detail\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, Open Information Extraction (OIE), meticulously processes raw biographical texts to identify factual assertions. It commences by loading and chunking data: the system ingests pre-processed files containing biographical narratives and segments them into manageable units. This step yields semi-structured data, often organised tabularly, with each row corresponding to a text chunk and including identifiers such as name, role, the biographical snippet itself, and a unique chunk ID.\nFollowing this preparation, OIE extraction commences. For each chunk—comprising, for example, a person’s name such as ‘Havemann, Robert’ and a segment of their biography like ‘… 1935 Prom. mit … an der Univ. Berlin…’—the system attempts to extract factual statements. This process generates raw Subject-Predicate-Object (SPO) triples, enriched with pertinent metadata such as an associated timeframe and a confidence score indicating the extraction’s reliability.\nSubsequently, OIE validation scrutinises these raw triples. The original text chunk and its corresponding extracted triples serve as input. Human experts, or potentially other specialised LLMs, then assess the accuracy and relevance of each triple against the source text. This critical review produces a set of validated SPO triples. Finally, the OIE Standard step evaluates the overall quality of this extraction phase. Researchers compare the validated triples against a ‘Gold Standard’—a reference set of triples meticulously created or verified by domain experts. This comparison yields key performance indicators such as F1-score, Precision, and Recall, offering a quantitative measure of the OIE process’s success.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "href": "chapter_ai-nepi_021.html#stage-2-workflow-ontology-driven-knowledge-graph-construction-in-detail",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail",
    "text": "19.5 Stage 2 Workflow: Ontology-Driven Knowledge Graph Construction in Detail\n\n\n\nSlide 12\n\n\nThe second stage of the pipeline focuses on building the knowledge graph in an ontology-driven manner, ensuring the final structure aligns precisely with research objectives. This stage commences by formulating Competency Questions (CQs). Drawing upon a sample of validated triples from Stage 1, experts define the specific analytical questions the knowledge graph must be capable of answering. This process yields a set of manually refined CQs that guide subsequent development.\nNext, developers create the ontology. Using the CQs and the sample of validated triples as inputs, they design a formal ontology. This ontology specifies the classes of entities (e.g., Person, Organisation, Event), the properties these entities can possess, and the types of relationships that can exist between them, all meticulously tailored to address the CQs. This process yields a comprehensive Ontology Definition.\nWith the ontology established, ontology mapping commences. The pipeline processes the validated triples from Stage 1, mapping their constituent subjects, predicates, and objects to the corresponding classes and properties within the defined ontology. This step transforms the relatively raw triples into conceptual RDF (Resource Description Framework) statements. Finally, disambiguation and Wikidata ID linking refine the graph. This crucial step involves resolving ambiguities in entity references—for instance, ensuring that different individuals sharing the same name are correctly distinguished. The system links entities to external identifiers, such as Wikidata IDs, where feasible. This phase also incorporates RDF-star statement generation, allowing annotations or contextual information to attach directly to individual triples, thereby enriching the graph’s expressive power. This process yields a set of disambiguated triples and RDF-star statements, ready for SHACL validation and subsequent querying.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "href": "chapter_ai-nepi_021.html#application-case-study-analysing-zielińskis-polish-biographical-compilations",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations",
    "text": "19.6 Application Case Study: Analysing Zieliński’s Polish Biographical Compilations\n\n\n\nSlide 15\n\n\nResearchers applied the knowledge graph extraction pipeline to a set of significant Polish historical sources: three complementary compilations by Stanisław Zieliński. These encompass Mały słownik pionierów polskich kolonjalnych i morskich (1933), a biographical dictionary of colonial and maritime pioneers; Bibljografja czasopism polskich zagranicą, 1830-1934 (1935), a bibliography of Polish periodicals published abroad; and Wybitne czyny Polaków na obczyźnie (1935), a record of notable Polish achievements internationally.\nThe structured data extracted from these volumes enables the exploration of several nuanced research questions. For instance, the knowledge graph facilitates the identification of individuals or communities whose pivotal roles in developing ideas and practices might have been obscured by dominant historical narratives. It allows for an analysis of shifts in migration patterns, such as those occurring before and after the January Uprising of 1863. Furthermore, investigators can examine the function of specific journals, determining if they served as ‘boundary objects’ linking diverse intellectual or professional circles, or which publications proved most central to the communities of practice amongst Polish migrants.\nInitial analysis of the Zieliński data yielded a substantial social network graph containing 3,598 nodes and 5,443 edges. Visualisations of this network distinguish editors (coloured green) from other individuals (coloured pink), offering a preliminary glimpse into the relational structures embedded within these historical texts.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "href": "chapter_ai-nepi_021.html#application-case-study-exploring-east-german-biographies-from-wer-war-wer-in-der-ddr",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”",
    "text": "19.7 Application Case Study: Exploring East German Biographies from “Wer war wer in der DDR?”\n\n\n\nSlide 17\n\n\nAnother significant application of the extraction methodology involves the German biographical lexicon, Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. This extensive work, first published in the 1980s by the Bundesstiftung zur Aufarbeitung der SED-Diktatur and subsequently digitised in the 2000s, profiles approximately 4,000 prominent East German figures from diverse fields including politics, science, culture, and sports. Containing entries for individuals such as Gustav Hertz and Robert Havemann, it serves as an indispensable resource for researchers and journalists seeking to understand the complex legacy of the German Democratic Republic.\nThe structured data derived from this lexicon enables quantitative analyses of historical patterns. One such analysis, visually presented as a scatter plot, investigates the relationship between state award recipients, high-ranking positions, and affiliation with the Socialist Unity Party (SED). The plot maps individuals based on their award status (e.g., Karl-Marx-Orden, Nationalpreis der DDR) against their rate of holding high positions and their SED affiliation rate. Comparative statistics reveal striking correlations: for instance, 95.0% of the 38 Karl-Marx-Orden recipients were SED members, compared to only 38.5% of the 1,056 individuals in the sample without this award. Recipients of the Karl-Marx-Orden also held a significantly higher share of high positions (65.8%) compared to those with no awards (28.0%), and an average birth year of 1905.9, substantially earlier than the 1923.0 average for non-recipients. Further detailed breakdowns show that 100% of Karl-Marx-Orden recipients who held Politbüro positions were members, highlighting the award’s strong ties to the highest echelons of power.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#achievements-current-challenges-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.8 Achievements, Current Challenges, and Future Trajectories",
    "text": "19.8 Achievements, Current Challenges, and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project has successfully demonstrated a significant advancement: the transformation of isolated biographical entries into a resource capable of supporting complex structural queries. This marks a crucial step towards unlocking the rich, latent information within historical texts. Nevertheless, researchers identify ongoing challenges, primarily in refining entity disambiguation techniques to ensure greater accuracy and in enhancing benchmarking methodologies to rigorously assess pipeline performance.\nLooking ahead, researchers will immediately complete and finalise the current pipeline’s development. Subsequently, a systematic comparison of its outputs against alternative pipelines and existing software packages will provide valuable performance context. A key objective involves scaling the entire process to analyse full datasets, thereby enabling more comprehensive historical investigations.\nBeyond these immediate goals, future ambitions include fine-tuning the pipeline for highly specific research use cases. The team also plans to explore the potential of GraphRAG (Graph Retrieval Augmented Generation), which could allow users to query the knowledge graphs using natural language. Furthermore, the team expresses interest in constructing multilayered networks, possibly employing frameworks such as ModelSEN (Multilayer Social-Epistemic Networks), to facilitate even deeper structural analyses of the complex relationships within the historical data. Interested parties may contact Raphael Schlattmann, Alex Kaye, or Malte Vogl via their provided email addresses.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "20  References",
    "section": "",
    "text": "21 References\n{.unlisted}",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>References</span>"
    ]
  }
]