[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "ai-nepi_001_chapter.html",
    "href": "ai-nepi_001_chapter.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html",
    "href": "ai-nepi_005_chapter.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThis chapter delves into the ActDisease project, exploring its objectives, the dataset of historical medical periodicals it employs, and the inherent challenges encountered during dataset digitisation. Subsequently, it details a series of genre classification experiments. These experiments encompass the motivation behind genre classification, an examination of zero-shot and few-shot classification techniques, and specific trials involving few-shot prompting with the Llama-3.1 8b Instruct model. The chapter culminates in a conclusion that synthesises the findings and their implications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#about-actdisease",
    "href": "ai-nepi_005_chapter.html#about-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 About ActDisease",
    "text": "5.1 About ActDisease\nThe ActDisease project, an initiative funded by the European Research Council (ERC), investigates the histories of patient organisations across Europe. Central to this research are the periodicals published by these organisations in England, Germany, France, and Great Britain. These documents serve as the primary source material for understanding the evolution and impact of patient advocacy.\n\n\n\nTitle slide of the presentation ‘GENRE CLASSIFICATION FOR HISTORICAL MEDICAL PERIODICALS’, ActDisease Project, listing authors and affiliation.\n\n\n\n5.1.1 About the Project\nActDisease, an acronym for ‘Acting out Disease – How Patient Organizations Shaped Modern Medicine’, is an ERC-funded research endeavour. Its core purpose is to study how patient organisations in 20th-century Europe contributed to shaping disease concepts, illness experiences, and medical practices. The project focuses on ten European patient organisations from Sweden, Germany, France, and Great Britain, covering a period from approximately 1890 to 1990. The principal source materials are the periodicals, mostly magazines, produced by these patient organisations.\n\n\n\nSlide describing the ActDisease project: its funding, purpose, focus, and main source material, with an image of Heligoland, Germany.\n\n\n\n\n5.1.2 Dataset Description\nThe ActDisease dataset comprises a private, recently digitised collection of patient organisation magazines. This collection encompasses materials from Germany, Sweden, France, and the United Kingdom, covering diseases such as allergy/asthma, diabetes, multiple sclerosis, lung diseases, and rheumatism/paralysis. The accompanying image displays a table that summarises the magazines by country, disease, total page count, and year coverage, amounting to 96,186 pages in total. Initial explorations reveal a diverse array of text types within these materials, with notable similarities in content across all magazines.\n\n\n\nSlide detailing the ActDisease Dataset with a table summarising magazines by country, disease, size, and year coverage, alongside example magazine covers.\n\n\n\n\n5.1.3 Dataset Digitisation Challenges\nThe digitisation process for the ActDisease dataset primarily involved Optical Character Recognition (OCR) using ABBYY FineReader Server 14. Whilst this software performed well on most common layouts and fonts, several challenges persist. Complex layouts, slanted text, rare fonts, and varying scan or photograph quality continue to pose difficulties for OCR accuracy. Consequently, remaining issues include OCR errors, particularly in German and French texts, and disrupted reading order. Researchers conducted experiments on post-OCR correction of German texts using instruction-tuned generative models to address some of these problems .\nFurthermore, OCR errors appear frequently in creative texts, such as advertisements, humour pages, and poems. A significant challenge arises from the co-occurrence of different text types within a single page—for instance, an administrative report might appear alongside an advertisement and a humour section. This heterogeneity means that conventional topic models and term counts, which do not account for such juxtapositions, are likely biased towards the most frequent text type on a page.\n\n\n\nSlide outlining digitization challenges, including OCR issues with complex layouts and creative texts, and showing examples of historical periodical pages.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "href": "ai-nepi_005_chapter.html#genre-classification-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 Genre Classification Experiments",
    "text": "5.2 Genre Classification Experiments\nThe inherent diversity of texts within the historical medical periodicals necessitates a robust method for distinguishing between them. Genre classification emerges as a pivotal approach to address this need.\n\n5.2.1 Motivation for Genre Classification\nAn examination of the ActDisease materials reveals a wide variety of text types, which, interestingly, exhibit similarities across all magazines. Different text types, such as administrative reports, advertisements, and humour sections, often appear side-by-side on the same page. This textual diversity poses a challenge for analytical methods like yearly and decade-based topic models or term counts, as these methods typically do not account for such internal heterogeneity.\n\n\n\nSlide highlighting challenges in analysing diverse text types within historical magazines.\n\n\nGenre, therefore, presents itself as a useful concept for differentiating kinds of text. In Language Technology, genre is often defined as a class of documents sharing a communicative purpose —a definition that proves highly applicable here. The ability to classify genre is crucial for exploring the data from multiple perspectives to construct historical arguments. Specifically, genre classification enables the comparative study of communicative strategies across different countries, diseases, and publications over time . It also facilitates a more fine-grained analysis of term distributions and topic models within distinct genre groups.\n\n\n\nSlide explaining why genre is a useful concept for classification and its benefits for historical analysis.\n\n\nThe ActDisease data showcases a rich tapestry of genres. Examples include poetry, academic reports (such as studies on the pancreas), legal documents (like deeds of covenant), and advertisements (for instance, for chocolate aimed at diabetics). Instructive messages, including recipes or medical advice, feature prominently, alongside patient organisation reports detailing meetings and activities. Narratives about patients’ lives also constitute a significant portion of the content.\n\n\n\nSlide illustrating the variety of genres found in the ActDisease dataset, such as patient experiences, advertisements, and instructive texts.\n\n\n\n\n5.2.2 Zero-Shot and Few-Shot Classification\nGiven the scarcity of annotated data within the ActDisease project, researchers explored both zero-shot and few-shot learning approaches for genre classification . For zero-shot learning, key research questions focused on whether genre labels from publicly available datasets could be efficiently mapped to the project’s custom labels and how performance would vary across different datasets and models. For few-shot learning, the investigation centred on how performance changes with varying training set sizes across models and whether prior fine-tuning on the full dataset could substantially enhance performance.\n\n\n\nSlide outlining research questions for zero-shot and few-shot learning due to limited annotated data.\n\n\n\n5.2.2.1 Genre Definition and Annotation\nThe project team, under the supervision of the main historian, defined the genre labels. The aim was to create labels that are useful for separating content within the ActDisease materials and sufficiently general for potential application to similar datasets. The defined genres include:\n\nAcademic: Research-based reports or explanations of scientific ideas (e.g., research article, report).\nAdministrative: Documents on organisational activities (e.g., meeting minutes, reports, announcements).\nAdvertisement: Promotes products or services for commercial purposes.\nGuide: Provides step-by-step instructions (e.g., health tips, legal advice, recipes).\nFiction: Entertains and emotionally engages (e.g., stories, poems, humour, myths).\nLegal: Explains legal terms and conditions (e.g., contracts, rules, amendments).\nNews: Reports recent events and developments.\nNonfiction Prose: Narrates real events or describes cultural/historical topics (e.g., memoir, essay, documentary).\nQA (Question & Answer): Structured as questions with expert answers, typically from periodical sections.\n\n\n\n\nSlide presenting a table of genres and their definitions used in the ActDisease project.\n\n\nResearchers selected the paragraph as the annotation unit, merging paragraphs from the ABBYY FineReader output based on font patterns (type, size, bold, italic) within a page. Annotators sampled from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal”, specifically focusing on the first and mid-year issues for each year. A team of four historians and two computational linguists, all either native or proficient in Swedish and German, performed the annotation. Each paragraph received two annotations, achieving an average inter-annotator agreement of 0.95 Krippendorff’s alpha, indicating a high level of consistency. Annotators used a structured file format, assigning hard genre labels to each paragraph.\n \n\n\n5.2.2.2 Data Splits and Distribution\nFor the experiments, researchers first split the annotated data into training and held-out sets, with the held-out set comprising approximately 30% of the data. For few-shot experiments, they further divided the held-out set equally and balanced it by label. Researchers excluded the ‘Legal’ and ‘News’ genres from these few-shot experiments due to insufficient training data. Researchers utilised the entire test set for zero-shot experiments. The distribution of genres across languages (German and Swedish) in the training and held-out samples reveals some imbalances. Notably, there is a strong imbalance in ‘Advertisement’ and ‘Nonfiction Prose’ across the two languages.\n\n\n\nSlide presenting bar charts of genre distribution in the ActDisease training and held-out samples for German and Swedish languages.\n\n\n\n\n5.2.2.3 External Datasets and Genre Mapping for Zero-Shot Learning\nTo facilitate zero-shot experiments, researchers incorporated external datasets. These included modern datasets from previous work on automatic web genre classification: the Corpus of Online Registers of English (CORE) and the Functional Text Dimensions (FTD) dataset, both annotated at the document level. Additionally, they used a sample from Universal Dependencies (UDM) Treebanks, which contains sentence-level annotations in multiple languages.\nTwo annotators independently performed the genre mapping from these external datasets to the ActDisease categories. For the final mapping, researchers selected only assignments with full agreement. This process revealed that for some ActDisease genres, no directly suitable labels existed in the available external datasets. The pipeline for creating training data involved this mapping, followed by preprocessing, chunking, and sampling in several configurations based on language family and label levels (ActDisease original vs. external dataset original).\n\n\n\nSlide showing a table mapping ActDisease genre categories to those in CORE, UDM, and FTD datasets.\n\n\n\n\n5.2.2.4 Models Employed\nResearchers selected multilingual encoders for these experiments, models that have demonstrated success in previous automatic genre classification tasks. The chosen models were:\n\nXLM-RoBERTa\nmBERT (multilingual BERT)\nhistorical mBERT (hmBERT)\n\nBERT-like models have seen extensive use in prior work on web register and genre classification . XLM-RoBERTa is recognised as a state-of-the-art web genre classifier . The inclusion of hmBERT was particularly pertinent as it is pretrained on a large corpus of multilingual historical newspapers, encompassing the languages in the ActDisease dataset. mBERT was included for comparison with hmBERT, as direct comparison with XLM-RoBERTa is not straightforward. Fine-tuning these models on all configurations of the training data (derived from FTD, CORE, UDM, and a merged set) yielded a total of 48 fine-tuned models. Researchers typically average subsequent metrics across these configurations.\n \n\n\n5.2.2.5 Zero-Shot Learning Evaluation\nIn evaluating zero-shot learning, the imperfect overlap between label sets necessitated an analysis of individual genres and confusion matrices to avoid potential biases. The state-of-the-art web genre classifier, X-GENRE, served as a baseline, considering only the most similar labels.\n\n\n\nIntroduction slide for Zero-Shot Learning Evaluation.\n\n\nOverall, models fine-tuned on the Functional Text Dimensions (FTD) dataset, using the established mapping, performed better. In most FTD configurations, researchers observed no systematic bias, and per-genre metrics were quite good. An interesting observation emerged: on certain datasets, some models handled specific genres much more effectively than others on average. For instance, XLM-RoBERTa demonstrated superior prediction of ‘QA’ (Question & Answer) texts compared to other models when fine-tuned on UDM. Conversely, hmBERT, when fine-tuned on UDM, showed a 16% average increase in correct ‘Administrative’ predictions over XLM-RoBERTa and mBERT. Models based on the CORE dataset proved adept at predicting the ‘Legal’ genre. However, researchers noted class-specific biases in other datasets: UDM fine-tuning tended towards ‘News’ (as the ‘News’ training data had the highest number of Germanic instances, mostly German), whilst CORE fine-tuning leaned towards ‘Guide’ (as only ‘Guide’ training data in CORE was multilingual).\n\n\n\nSlide summarising key results from zero-shot learning, highlighting performance with FTD and specific model-dataset-genre strengths.\n\n\nConfusion matrices for specific configurations illustrate this behaviour. For example, hmBERT fine-tuned on UDM (hmbert_UDM_True_True) shows strong performance for ‘Administrative’. XLM-RoBERTa fine-tuned on CORE (xlmr_CORE_True_False) effectively identifies ‘Legal’ and ‘Academic’ texts. XLM-RoBERTa fine-tuned on UDM (xlmr_UDM_False_False) excels with ‘QA’. Finally, XLM-RoBERTa fine-tuned on FTD (xlmr_FTD_False_False) accurately classifies ‘Legal’ texts.\n\n\n\nSlide displaying four confusion matrices for different model configurations in zero-shot learning, highlighting specific genre prediction strengths.\n\n\nThe table below presents detailed average F1 scores per category, averaged across data configurations. Highlighted values in the original presentation (not reproduced here as bold text) indicate performance that is not a result of systematic biases towards those categories. Notably, models fine-tuned on FTD and CORE show strong F1 scores for the ‘Legal’ genre. hmBERT (UDM) performs well for ‘Administrative’, and XLM-RoBERTa (UDM) for ‘QA’.\n\n\n\nTable of zero-shot per-category F1 scores averaged across data configurations for different models and datasets.\n\n\nAnalysis of average performance across different training configurations (balancing strategies, language family inclusion) for each external dataset (FTD, CORE, UDM) reveals nuances. For FTD, balancing by original labels alongside ActDisease labels ([B2]) or including only Germanic languages ([G+]) decreased performance compared to balancing by ActDisease labels alone ([B1]) or including all language families ([G-]). For CORE, the small number of Finnish and French instances (in the ‘Guide’ genre) slightly decreased performance. For UDM, the presence of other language families and balancing generally improved performance in terms of macro F1.\n\n\n\nSlide showing average F1 scores for different training configurations within FTD, CORE, and UDM datasets.\n\n\n\n\n5.2.2.6 Few-Shot Learning Evaluation\nThe investigation then turned to few-shot learning scenarios.\n\n\n\nIntroduction slide for Few-Shot Learning Evaluation.\n\n\nExperiments demonstrated how models performed with varying training data sizes, both with and without prior Masked Language Model (MLM) fine-tuning on the entire ActDisease dataset. This prior MLM fine-tuning (+MLM) proved clearly advantageous. F1 scores generally increased with the number of training instances, although they remained below 0.8 even with 1182 instances. Notably, hmBERT-MLM (the historical model with prior fine-tuning) outperformed other models, particularly at larger dataset sizes, boosting its performance significantly and even surpassing other models by a small margin.\n\n\n\nLine graph showing few-shot learning performance (F1 score vs. dataset size) for different models with and without MLM fine-tuning.\n\n\nA detailed examination of scores revealed that hmBERT-MLM’s superior performance is largely attributable to its sustained ability to differentiate between ‘Fiction’ and ‘Nonfiction Prose’ as dataset size increases. In contrast, other models, especially XLM-RoBERTa-MLM, exhibited a drastic drop in performance for ‘Fiction’ when using the full-sized training dataset (1182 instances), often over-predicting ‘Nonfiction Prose’ for ‘Fiction’ instances. Both these genres in the ActDisease data frequently contain narratives about patient experiences, particularly concerning diabetes. It is plausible that with a larger data size, the linguistic features of these two genres become more similar, especially as they are confined to the specific domain of patient organisation magazines focused on diabetes and often share themes and narrative structures. This suggests that more data, or perhaps more nuanced features, might be necessary to improve discrimination between these closely related genres.\n \n\n\n\n5.2.3 Few-Shot Prompting Llama-3.1 8b Instruct\nRecognising the limitations of available data for extensive instruction tuning, researchers also explored few-shot prompting with Llama-3.1 8b Instruct, a prominent multilingual generative model with open weights. The prompt structure incorporated genre definitions and two to three carefully selected examples for each genre. The instruction guided the model to label input text with one of the defined genres based on its perceived purpose and content.\n\n\n\nSlide illustrating the prompt structure used for few-shot prompting of Llama-3.1 8b Instruct, including genre definitions and example placeholders.\n\n\nThe results from few-shot prompting Llama-3.1 8b Instruct on the zero-shot test set (the entire held-out set) indicate that the model handles certain labels reasonably well. For instance, ‘Legal’ texts achieved an F1-score of 0.84, and ‘Academic’ and ‘Advertisement’ texts scored 0.72 and 0.73, respectively. However, the provision of only two or three examples proved insufficient for the model to adequately represent and distinguish more nuanced genres such as ‘Nonfiction Prose’ (F1-score 0.49), ‘Administrative’ (F1-score 0.60), and ‘News’ (F1-score 0.08). The overall macro average F1-score was 0.59. The confusion matrix reveals particular difficulties in distinguishing ‘Nonfiction Prose’ from ‘Fiction’ and ‘Administrative’ texts, and ‘Advertisement’ from ‘Administrative’ texts.\n\n\n\nSlide presenting results (F1 scores and confusion matrix) for few-shot prompting of Llama-3.1 8b Instruct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_005_chapter.html#conclusion",
    "href": "ai-nepi_005_chapter.html#conclusion",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nHistorical periodicals, particularly popular magazines, represent a promising yet challenging source for research into the history of science and medicine. Their genre-rich nature reflects diverse communicative strategies employed over time. Accurately accounting for these genres is crucial for the detailed interpretation of text mining results.\nThis exploration demonstrates that genre classification can significantly enhance the accessibility of such complex historical sources for computational analysis. When faced with no training data, researchers can successfully leverage available modern datasets, provided the genre categories are sufficiently general-purpose. Alternatively, few-shot prompting of capable open generative models, like Llama-3.1 8b Instruct, can achieve decent quality for some genres, although performance may be limited for categories requiring more nuanced understanding with minimal examples.\nHowever, if some annotated data is available, even in limited quantities, few-shot learning with multilingual encoders—such as XLM-RoBERTa or, notably, historical multilingual BERT (hmBERT)—especially when combined with prior Masked Language Model (MLM) fine-tuning on the target domain data, emerges as a superior strategy. For the ActDisease project, this approach yielded the most promising results, with hmBERT-MLM showing considerable gains in performance.\n\n\n\nSlide summarising the main conclusions regarding genre richness in popular magazines and effective strategies for genre classification.\n\n\nOngoing and future efforts aim to further refine these methodologies and apply them to specific historical hypotheses. This includes developing a new annotation scheme with more fine-grained genres, an annotation project financed by Swe-CLARIN, exploring synthetic data generation techniques, and implementing active learning strategies to improve classifier quality efficiently. These endeavours seek to enhance the utility of these methods for both the ActDisease project and the broader digital humanities community.\n\n\n\nSlide outlining future and present work, including working with historical hypotheses, new annotation schemes, and advanced machine learning techniques.\n\n\n\nAcknowledgements\nThe project team extends its gratitude to the annotators: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. We also thank Dr Maria Skeppstedt and the anonymous reviewers for their valuable feedback. This research received funding from the European Research Council (ERC-2021-STG, 101040999). The Centre for Digital Humanities and Social Sciences at Uppsala University provided essential support in the form of GPUs and data storage.\n\n\n\nSlide listing acknowledgements to the project team, reviewers, European Research Council, and Centre for Digital Humanities and Social Sciences.\n\n\nFor further information, please visit the project website.\n\n\n\nThank you slide with a QR code.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html",
    "href": "ai-nepi_006_chapter.html",
    "title": "6  —",
    "section": "",
    "text": "Overview\nThe VERITRACE web application, currently in its ‘alpha’ stage of development, represents an ambitious step towards new research methodologies. This preliminary version is not yet publicly accessible, requiring substantial further work; it serves more as a promise of future capabilities. Central to its current iteration, researchers are testing a BERT-based Large Language Model (LLM), specifically LaBSE (Language-agnostic BERT Sentence Embedding), to generate vector embeddings. These embeddings aim to represent every passage within the project’s extensive textual corpus. However, initial assessments suggest this model may not ultimately prove sufficient for the complex demands of the research. The screenshots presented herein offer a glimpse into the application’s design and potential, though they remain a very poor substitute for direct interaction with the evolving platform.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "href": "ai-nepi_006_chapter.html#the-veritrace-project-uncovering-ancient-wisdoms-influence",
    "title": "6  —",
    "section": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence",
    "text": "6.1 The VERITRACE Project: Uncovering Ancient Wisdom’s Influence\nThe VERITRACE project, a five-year ERC Starting Grant initiative, embarks on an ambitious journey to trace the intellectual currents flowing from the early modern ‘ancient wisdom’ tradition into the burgeoning field of natural philosophy and science of that era. This tradition manifests in a diverse collection of works, including notable texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and perhaps most famously for historians of chemistry, the Corpus Hermeticum. These 140 core texts form a ‘close reading corpus’, providing a focused lens on this influential body of thought.\n\n\n\nPresentation Title Slide illustrating the VERITRACE project’s scope and context.\n\n\nHistorical records confirm the impact of these ancient wisdom texts; for instance, Newton engaged with the Sibylline Oracles, and Kepler possessed familiarity with the Corpus Hermeticum. Nevertheless, the project seeks to delve deeper, aiming to uncover a far broader network of texts and intellectual connections that interacted with this tradition. Many of these works, often penned by lesser-known authors, constitute what one scholar has termed ‘the great unread’, frequently overlooked by historians due to their sheer volume and obscurity. Consequently, VERITRACE focuses on bringing these neglected sources to light.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "href": "ai-nepi_006_chapter.html#advancing-computational-history-philosophy-and-sociology-of-science-hpss",
    "title": "6  —",
    "section": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)",
    "text": "6.2 Advancing Computational History, Philosophy, and Sociology of Science (HPSS)\nTo address its core research questions, the VERITRACE project pioneers large-scale, multilingual exploration within the domain of computational History, Philosophy, and Sociology of Science (HPSS). The team develops tools not only for conventional keyword searching but also for the sophisticated identification of textual reuse. This encompasses both direct, lexical quotation—instances where authors use verbatim material from other works, perhaps without explicit citation—and more subtle, indirect influences. Such indirect reuse might involve paraphrase or allusions that, whilst not direct copies, would have been recognisable to contemporary readers as originating from sources like the Corpus Hermeticum.\n\n\n\nThe VERITRACE project team and its mission statement.\n\n\nEffectively, the project endeavours to construct an ‘early modern plagiarism detector’ capable of navigating a vast, multilingual corpus. Beyond identifying direct and indirect textual linkages, a primary objective is to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this comprehensive analytical approach, researchers anticipate the emergence of new patterns and insights into the intellectual history and philosophy of science.\n\n\n\nKey objectives of the VERITRACE project in computational HPSS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "href": "ai-nepi_006_chapter.html#navigating-a-vast-multilingual-corpus",
    "title": "6  —",
    "section": "6.3 Navigating a Vast Multilingual Corpus",
    "text": "6.3 Navigating a Vast Multilingual Corpus\nThe foundation of this investigation rests upon a large, diverse, and multilingual dataset, focusing exclusively on printed books and texts, thereby excluding handwritten materials from its current scope. This corpus draws from three primary data sources and encompasses works in at least six different languages, published over approximately two centuries. The chronological parameters span from 1540, chosen for specific historical reasons, to 1728, shortly after Newton’s death.\nKey data repositories include:\n\nEarly English Books Online (EEBO)\nGallica, the digital library of the French National Library\nThe Bavarian State Library, which constitutes the largest single source\n\nCollectively, these sources contribute to a corpus of roughly 430,000 books. State-of-the-art digital techniques are employed to analyse this extensive collection of early modern texts.\n\n\n\nOverview of the large, diverse, and multilingual dataset used by VERITRACE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "href": "ai-nepi_006_chapter.html#core-challenges-and-the-role-of-large-language-models",
    "title": "6  —",
    "section": "6.4 Core Challenges and the Role of Large Language Models",
    "text": "6.4 Core Challenges and the Role of Large Language Models\nSeveral core challenges are inherent in a project of this scale and complexity. Variable Optical Character Recognition (OCR) quality presents a significant hurdle. The textual data, supplied directly by libraries in raw formats such as XML, HOCR, or even HTML files, often lacks ground truth page images. This variability in OCR accuracy inevitably affects all downstream processing and analytical tasks. Managing early modern typography and semantics across at least six languages introduces further complexities. Furthermore, the sheer volume of data—hundreds of thousands of texts printed across Europe over nearly 200 years—demands robust computational strategies.\nLarge Language Models (LLMs) play a crucial role in addressing these challenges. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, acting as ‘judges’ in this process. Whilst this application holds considerable interest, the current focus shifts towards the encoder side. Here, BERT-based LLMs generate embeddings to encode the semantic meaning of sentences and short passages (groups of sentences) within the textual corpus. This encoding is fundamental to the project’s semantic matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "href": "ai-nepi_006_chapter.html#employing-llms-as-judges-for-metadata-enrichment",
    "title": "6  —",
    "section": "6.5 Employing LLMs as Judges for Metadata Enrichment",
    "text": "6.5 Employing LLMs as Judges for Metadata Enrichment\nOne specific, albeit challenging, application of LLMs within VERITRACE involves their use as ‘judges’ to enrich metadata. The basic motivation stems from the desire to map records from high-quality external sources, such as the Universal Short Title Catalogue (USTC), onto the project’s own records. Successful mapping creates enriched metadata, less likely to require extensive manual cleaning.\n\n\n\nCase study overview of using LLMs as judges to enrich VERITRACE metadata.\n\n\nWhilst some mapping can be automated using external identifiers, many records lack such straightforward connections. Compounding this, much of the project’s internal data has not yet undergone cleaning, making matching a non-trivial task. The manual comparison of bibliographic metadata—assessing pairs of records to determine if they represent the same underlying printed text—is exceedingly tedious. Team members faced the prospect of reviewing tens of thousands of such pairs, highlighting the need for an automated solution.\n\n6.5.1 Initial Attempts and Emerging Hurdles\nTo address this, researchers are exploring a panel, or ‘bench’, of LLMs. Extensive prompt guidelines direct these models to evaluate potential matches, which are initially generated via fuzzy matching algorithms. The LLMs provide yes/no decisions along with reasoning for why a pair of records may or may not represent the same underlying text.\n\n\n\nExample of metadata comparison for LLM judging.\n\n\nThis endeavour remains a work in progress. A major current challenge is the prevalence of hallucinations in the output from the open-source models (e.g., Llama-based) currently under evaluation. Attempts to mitigate this by requesting more structured output, paradoxically, often lead to more generic and less helpful responses, particularly in the reasoning provided by the models. Achieving the right balance in prompting to elicit accurate and insightful judgments is an ongoing refinement process. Despite these initial difficulties, the potential for LLMs to save considerable time in metadata enrichment remains significant, and further investigation is warranted.\n\n\n\nExample of prompt guidelines and LLM output for metadata matching.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "href": "ai-nepi_006_chapter.html#introducing-the-veritrace-web-application",
    "title": "6  —",
    "section": "6.6 Introducing the VERITRACE Web Application",
    "text": "6.6 Introducing the VERITRACE Web Application\nThe VERITRACE web application serves as the primary interface for exploring the project’s data and analytical tools. This platform is exceptionally new; indeed, its introduction here marks its first public discussion, preceding even internal team dissemination. As an ‘alpha’ version, it is not yet publicly available and remains under active development on a local machine, with screenshots offering a preliminary view. It functions more as a demonstration of the project’s aspirations than a finalised product.\n\n\n\nOverview of the VERITRACE Web Application’s alpha version.\n\n\nCurrently, testing involves a BERT-based LLM (LaBSE) to generate vector embeddings for every passage in the corpus. However, early indications suggest this model may not possess the requisite sophistication for the project’s ultimate goals, particularly for nuanced semantic matching. The application’s development continues, with these initial explorations informing future refinements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "href": "ai-nepi_006_chapter.html#the-data-processing-backbone",
    "title": "6  —",
    "section": "6.7 The Data Processing Backbone",
    "text": "6.7 The Data Processing Backbone\nTransforming raw textual data from library sources into a queryable format within an Elasticsearch database—the backend of the web application—involves an intricate data processing pipeline. This multi-stage process is far from a simple button-push operation. Numerous steps are essential to prepare the data for analysis.\n\n\n\nDiagram of the VERITRACE data processing pipeline dashboard and stages.\n\n\nThese steps include:\n\nExtracting text into manageable files.\nGenerating mappings of all character positions.\nSegmenting texts into meaningful units.\nAssessing OCR quality.\n\nEach of these fifteen stages requires careful optimisation. The generation of embeddings, crucial for semantic analysis, occurs near the end of this complex pipeline. Significant background work underpins the functionality accessible through the web interface.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "href": "ai-nepi_006_chapter.html#exploring-the-veritrace-corpus-statistics-and-metadata",
    "title": "6  —",
    "section": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata",
    "text": "6.8 Exploring the VERITRACE Corpus: Statistics and Metadata\nThe VERITRACE web application offers several modules for interacting with the corpus. The ‘Explore’ section, for instance, provides users with comprehensive statistics about the dataset, drawn directly from a MongoDB database. At present, this encompasses 427,305 metadata records describing the books within the collection. This area allows researchers to gain an overview of the corpus’s composition, including language distributions, data sources, publication decades, and prominent publication places.\n\n\n\nThe VERITRACE ‘Explore’ interface showing corpus statistics.\n\n\nBeyond aggregate statistics, a ‘Metadata Explorer’ enables users to browse and inspect the rich metadata associated with each text. A key feature here is detailed language information. Language identification algorithms operate on every text, down to segments of approximately 50 characters. This granularity is vital because many early modern texts are multilingual, often containing sections in Greek or other languages alongside the primary Latin, for example. The system identifies these languages and their proportions within each document—such as a text being 15% Greek and 85% Latin—classifying them as ‘substantively multilingual’.\nFurthermore, the system attempts to assess OCR quality on a page-by-page basis. This is a challenging task without access to ground truth page images, relying instead on analysis of the raw text. Nevertheless, providing page-level quality assessments, rather than a single score for an entire book, offers more nuanced information for researchers. The efficacy of this OCR assessment method continues to be evaluated.\n\n\n\nThe VERITRACE ‘Metadata Explorer’ interface displaying detailed record information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "href": "ai-nepi_006_chapter.html#search-analysis-and-reading-tools-for-scholarly-inquiry",
    "title": "6  —",
    "section": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry",
    "text": "6.9 Search, Analysis, and Reading: Tools for Scholarly Inquiry\nFor many scholars, the ‘Search’ function will likely be the initial point of engagement. The web application supports standard keyword searches across the corpus. Even with a prototype dataset of only 132 files (rather than the full 430,000), the Elasticsearch index already occupies 15 gigabytes, hinting at the terabytes of data the full system will manage. A simple search for “Hermes” in this prototype, for example, might yield 22 documents with 332 total matches.\n\n\n\nThe VERITRACE ‘Search’ interface showing basic and fielded query examples.\n\n\nLeveraging the power of Elasticsearch, users can execute far more complex queries. Fielded queries allow searching within specific metadata, such as finding all books by Kepler that also contain the keyword “Hermes”. Advanced capabilities include Boolean operators (AND, OR), nested queries, and proximity searches—for instance, locating texts where “Hermes” and “Plato” appear within ten words of each other.\nAn ‘Analyse’ section is planned, though not yet implemented. This module will incorporate tools for:\n\nTopic modelling\nLatent Semantic Analysis (LSA)\nDiachronic analysis, to explore linguistic and conceptual shifts over time.\n\nInsights from the wider research community inform the development of these analytical features.\n\n\n\nThe VERITRACE ‘Analyse’ interface showing planned analysis tools.\n\n\nRecognising the importance of accessing original source materials, a ‘Read’ section integrates a Mirador viewer. This allows scholars to view PDF facsimiles of every text in the corpus, alongside its metadata, mirroring the experience of browsing a physical library’s digital collection.\n\n\n\nThe VERITRACE ‘Read’ interface with an integrated Mirador viewer for text facsimiles.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "href": "ai-nepi_006_chapter.html#unveiling-textual-reuse-the-match-functionality",
    "title": "6  —",
    "section": "6.10 Unveiling Textual Reuse: The Match Functionality",
    "text": "6.10 Unveiling Textual Reuse: The Match Functionality\nA cornerstone of the VERITRACE web application is its ‘Match’ section, designed to identify textual reuse between different works. This tool allows users to specify query texts and comparison texts. Comparisons can be performed between single documents, across multiple selected documents (e.g., comparing Newton’s Latin Opticks to all of Kepler’s works in the database), or, ambitiously, between one text and the entire corpus. The latter presents considerable computational challenges regarding processing time and user experience, but remains a developmental goal.\n\n\n\nThe VERITRACE ‘Match’ interface for configuring textual similarity comparisons.\n\n\n\n6.10.1 Lexical and Semantic Matching Approaches\nThe system offers two fundamental types of matching:\n\nLexical matching: This approach uses keyword-based techniques to find passages with similar vocabulary. It is effective for identifying direct textual parallels but is language-dependent.\nSemantic matching: Employing vector embeddings, this method seeks conceptually similar passages, even if they share little or no common vocabulary. This is crucial for a multilingual corpus where translations or paraphrases might obscure lexical links.\n\nHybrid approaches, combining lexical and semantic methods with adjustable weighting, are also available.\n\n\n6.10.2 Customisable Parameters for Nuanced Analysis\nRecognising that text matching is not a one-size-fits-all process, the interface exposes numerous parameters for users to tweak. Whilst default settings provide a balanced starting point, users can adjust elements such as minimum similarity scores to refine search results according to their specific research needs. Different matching modes—‘Standard’, ‘Comprehensive’ (for maximum recall, albeit slower), and ‘Faster’ (for higher precision with potentially fewer results)—offer further control over the comparison process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "href": "ai-nepi_006_chapter.html#validating-the-approach-sanity-checks-and-case-studies",
    "title": "6  —",
    "section": "6.11 Validating the Approach: Sanity Checks and Case Studies",
    "text": "6.11 Validating the Approach: Sanity Checks and Case Studies\nTo evaluate the efficacy of the matching tools, researchers conduct several ‘sanity checks’ using known textual relationships. One such check involves comparing Newton’s Latin version of his Opticks (1719 edition) with the English edition from 1718. These texts, being translations of each other, provide a useful test case.\n\n6.11.1 Sanity Check 1: Lexical Matching Across Languages\nWhen a lexical match is performed between the Latin and English editions of Opticks, the expectation is that no significant matches will be found, given their different languages. Using the ‘Standard’ matching mode, this holds true—no matches are reported. Interestingly, the ‘Comprehensive’ mode does identify three matches, revealing small sections of English text, likely from prefatory material, within the predominantly Latin edition. This demonstrates the sensitivity of different modes and confirms the general principle that lexical matching is language-specific.\n\n\n\nResults of a lexical match attempt between Latin and English versions of Newton’s Opticks, showing no significant matches.\n\n\n\n\n6.11.2 Sanity Check 2: Lexical Self-Matching\nAs another baseline, lexically matching a text against itself should, ideally, yield near-perfect results. When Newton’s English Opticks is compared to itself, the system reports a high degree of similarity, with extensive coverage and quality scores. The interface provides detailed statistics, including the number of passages compared and the distribution of similarity scores, offering transparency into the matching process. Automatic highlighting displays the query passage on the left and the comparison passage on the right, along with their similarity score.\n\n\n\nResults of lexically matching Newton’s Opticks to itself, showing high similarity.\n\n\n\n\n6.11.3 Sanity Check 3: Semantic Matching of Translations\nThe real test for the LLM-powered tools comes with semantic matching across languages. When comparing the Latin and English Opticks using semantic matching, the system should identify conceptual similarities despite the linguistic differences.\n\n\n\nConfiguration for a semantic match between Newton’s Latin Optice and its English translation.\n\n\nInitial results from such semantic comparisons appear reasonable. Passages discussing similar concepts, such as colours, are identified as matches, even with underlying OCR imperfections. This suggests that the vector embeddings are capturing some level of conceptual correspondence across translations.\n\n\n\nExamples of semantic matches found between Latin and English versions of Newton’s Opticks.\n\n\n\n\n6.11.4 Preliminary Findings and Model Adequacy\nHowever, the semantic matching performance is not without its issues. Whilst the quality score for identified matches can be high, indicating strong similarity for the pairs found, the coverage score—representing how much of the documents are involved in matches—can sometimes be lower than expected. This discrepancy might, in part, reflect genuine differences between editions; for instance, the Latin edition of Opticks is considerably longer than the English one.\n\n\n\nSummary statistics for the semantic match between Latin and English Opticks, highlighting areas for investigation.\n\n\nNevertheless, further queries using the current LaBSE embedding model suggest it may not be entirely adequate for the nuanced demands of historical textual analysis. The potential for ‘out-of-domain model collapse’—where a model trained on general modern text performs poorly on specialised historical corpora—is a concern.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "href": "ai-nepi_006_chapter.html#future-directions-and-outstanding-challenges",
    "title": "6  —",
    "section": "6.12 Future Directions and Outstanding Challenges",
    "text": "6.12 Future Directions and Outstanding Challenges\nAs the VERITRACE project progresses, several critical issues and areas for development lie on the horizon. The choice of vector embedding model is paramount. LaBSE, selected partly for its efficiency in terms of storage and processing speed, may prove insufficient. Alternative models, such as XLM-Roberta, intfloat multilingual-e5-large, or specialised historical mBERT variants, present other trade-offs between accuracy, storage requirements, and inference time. A fundamental question is whether to persist with pre-trained models or to invest in fine-tuning a base model specifically on the VERITRACE historical corpus.\n\n\n\nA summary of key issues and future challenges for the VERITRACE project.\n\n\nFurther challenges include:\n\nSemantic drift: The meaning of words and concepts changes over time. How effectively current LLMs handle such diachronic semantic shifts across centuries and languages within the same vector space remains an open question.\nOCR quality: Poor OCR accuracy profoundly impacts downstream tasks, from basic sentence segmentation to complex semantic analysis. Re-OCRing the entire corpus is not feasible. Strategies might involve selectively re-OCRing the poorest quality texts or investing effort in locating existing higher-quality digital versions.\nScaling and performance: The current prototype, operating on only 132 texts, already shows query times of around 15 seconds for complex operations. Scaling these capabilities to the full corpus of 430,000 texts will undoubtedly present significant performance engineering challenges.\n\nAddressing these multifaceted issues will be crucial for realising the full potential of VERITRACE to illuminate the complex intellectual heritage of early modern science. Continued research, methodological refinement, and community engagement will guide these future endeavours.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>---</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "10 The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#overview",
    "href": "chapter_ai-nepi_009.html#overview",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nThe project investigates the representation of research related to the UN Sustainable Development Goals (SDGs) within major bibliometric databases, specifically Web of Science, Scopus, and OpenAlex. The core objective is to use Large Language Models (LLMs) as a tool to detect potential biases embedded in the SDG classification standards applied by these databases. The research considers the performative nature of bibliometric databases and their influence on research priorities, resource allocation, and policy decisions.\nA case study focuses on five SDGs related to socioeconomic inequalities (SDG4, SDG5, SDG10, SDG8, SDG9). The methodology involves collecting a jointly indexed subset of publications from the three databases (15,471,336 publications from January 2015 to July 2023), classified according to each database’s standard for the selected SDGs.\nA key technical decision involves selecting a “light” pre-trained LLM, DistilGPT2 (82M parameters), to avoid embedding existing knowledge biases present in larger models trained on extensive web data. This model is fine-tuned separately on subsets of publication abstracts corresponding to each database’s classification for each of the five SDGs, resulting in 15 fine-tuned LLMs (DistilGPT2 {bibDB, SDG}).\nThe fine-tuned LLMs are then used to analyze the content, specifically by extracting noun phrases from responses to prompts related to SDG targets. The analysis reveals a systematic overlook in the data (classified publications) of disadvantaged categories of individuals, poorest countries, and underrepresented topics explicitly mentioned in SDG targets. Conversely, significant attention is paid to economic superpowers and highly developing countries.\nThe findings highlight how bibliometric classification, despite appearing objective, decisively shapes the representation of SDG-related research. Limitations include the high sensitivity of results to model architecture, training data, hyperparameters, and decoding strategy, as well as the general framework employed. The study demonstrates the potential of LLMs as detectors of biases in research data infrastructures and serves as a proof-of-concept for their introduction in automating information extraction for research policy decision-making.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases-background-and-implications",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases-background-and-implications",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.2 SDG Classification in Bibliometric Databases: Background and Implications",
    "text": "10.2 SDG Classification in Bibliometric Databases: Background and Implications\n\n\n\nSlide 01\n\n\nBibliometric databases operate as critical digital infrastructures within the sociology of science, facilitating bibliometric analyses and impact assessments for the scientific community. These databases are not neutral but possess a performative nature, based on specific understandings of the science system and inherent value attributions, as discussed by Whitley (2000) and Winkler (1988).\nMajor platforms such as Web of Science, Scopus, and OpenAlex have introduced bibliometric classifications designed to align published research with the United Nations Sustainable Development Goals (SDGs). Prior investigations, including research by Armitage et al. (2020), have demonstrated that SDG labeling performed by different providers (such as Elsevier, Bergen, and Aurora) yields disparate results with notably little overlap in the sets of classified publications.\nThese discrepancies in classification standards result in varying perceptions regarding research priorities, which in turn can potentially influence decisions related to resource allocation and policy. The design and operation of these databases are also influenced by political and commercial interests.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-motivation-and-llm-application",
    "href": "chapter_ai-nepi_009.html#case-study-motivation-and-llm-application",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.3 Case Study Motivation and LLM Application",
    "text": "10.3 Case Study Motivation and LLM Application\n\n\n\nSlide 02\n\n\nThe project undertakes a case study examining the representation of UN Sustainable Development Goals within bibliometric data, documented in Ottaviani & Stahlschmidt (2024). The primary motivation for this study is to assess the aggregated effects on the representation of SDG-related research within bibliometric databases that could arise from the introduction of LLM-based tools.\nThe method employed involves the use of (Little) Pre-trained Large Language Models, specifically DistilGPT2. These LLMs are separately trained, or fine-tuned, on distinct subsets of publication abstracts. These subsets are derived from publications classified according to the SDG standards of diverse bibliometric databases.\nThe LLM technology is utilized in two primary capacities: firstly, as a detector of biases present within the data; and secondly, as a proof-of-concept exercise to demonstrate the potential for introducing such models to automate information extraction processes intended to inform decision-making in research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#partial-chain-of-dependencies-and-llm-impact",
    "href": "chapter_ai-nepi_009.html#partial-chain-of-dependencies-and-llm-impact",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.4 Partial Chain of Dependencies and LLM Impact",
    "text": "10.4 Partial Chain of Dependencies and LLM Impact\n\n\n\nSlide 03\n\n\nA partial chain of dependencies is considered within the study’s framework. The SDG classification standards applied by databases are understood to define what constitutes “Research on SDGs”. Various actors, including researchers, Small and Medium-sized Enterprises (SMEs), governments, and intermediate figures, process this defined “Research on SDGs”. This research then serves to inform “Decision-making to align with SDGs”. Subsequently, “Decision-making to align with SDGs” is depicted as impacting “Socioeconomic inequalities”.\nThe introduction of LLMs into Research Policy is positioned as a mechanism for detecting “biases” present within the body of “Research on SDGs”. This “Introduction of LLM in Research Policy” is also shown to impact “Socioeconomic inequalities”. Within this chain, LLMs are considered to potentially alter the metadata associated with “Research on SDGs”, and these changes in metadata can influence the advices, choices, indicators, and measures derived from the research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#actors-and-selected-sdgs",
    "href": "chapter_ai-nepi_009.html#actors-and-selected-sdgs",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.5 Actors and Selected SDGs",
    "text": "10.5 Actors and Selected SDGs\n\n\n\nSlide 04\n\n\nThe study considers three primary bibliometric databases: Web of Science, managed by Clarivate (US); Scopus, managed by Elsevier (UK); and OpenAlex, which was formerly associated with Microsoft (US) but is now open source.\nTo investigate socioeconomic inequalities, five specific SDGs are selected: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduce inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure). These five SDGs are further categorized into two dimensions for analysis: the Equity or socio dimension, encompassing SDG4, SDG5, and SDG10; and the Economic and technological development or economic dimension, comprising SDG8 and SDG9.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data-and-benchmark",
    "href": "chapter_ai-nepi_009.html#processed-data-and-benchmark",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.6 Processed Data and Benchmark",
    "text": "10.6 Processed Data and Benchmark\n\n\n\nSlide 05\n\n\nThe study processes a jointly indexed subset of publications, totaling 15,471,336 publications. This subset consists of publications shared across all three bibliometric databases—Web of Science, Scopus, and OpenAlex—identified through exact DOI matching. The data collection spans the period from January 2015 to July 2023.\nThe analysis focuses on the application and performance of the three distinct classification standards used by these databases for the five selected SDGs. Crucially, this analysis is performed on the shared corpora, the jointly indexed subset, rather than the entirety of each database. This approach establishes a common benchmark for comparing the different classification outcomes. Consequently, for each specific SDG, three distinct subsets of publications are generated, each representing the set of publications classified under that SDG by one of the three bibliometric databases.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparing-sdg-classifications-and-identifying-dimensions-of-bias",
    "href": "chapter_ai-nepi_009.html#comparing-sdg-classifications-and-identifying-dimensions-of-bias",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.7 Comparing SDG Classifications and Identifying Dimensions of Bias",
    "text": "10.7 Comparing SDG Classifications and Identifying Dimensions of Bias\n\n\n\nSlide 05\n\n\nThe study compares the sets of SDG-classified papers among the three bibliometric databases. This comparison is visually represented using Venn diagrams, which show the overlap of publications classified under specific SDGs, such as SDG4, SDG5, and SDG10 for the socio dimension, and SDG8 and SDG9 for the economic dimension.\nThe analysis involves determining which specific SDG targets are addressed within the classified publications and which are not. This process helps identify potential biases, including those that might be indirectly considered through the targets. Four main dimensions where biases are observed are identified:\n\nLocations, which are mentioned in targets across all SDGs\nActors\nData and metrics, which primarily emerge as part of the LLM responses\nFocuses, which are specific to each SDG",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "href": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning-strategy",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.8 LLM Selection and Fine-tuning Strategy",
    "text": "10.8 LLM Selection and Fine-tuning Strategy\n\n\n\nSlide 08\n\n\nThe study involves a specific choice of LLM technology and the subsequent fine-tuning of 15 separate LLMs. Leading commercial and open-source pre-trained LLMs are considered ineligible for this work. This is because their extensive pre-training datasets, which include sources like Wikipedia and Reddit conversations, embed existing knowledge about SDGs and strong semantic associations that could introduce bias.\nA “fair compromise” is selected: DistilGPT2. This model is described as a “very light” pre-trained English-speaking variant of the open-source GPT2, utilizing a technique called “distillation” as described by Sanh (2019). DistilGPT2 has 82 Million parameters, significantly fewer than models like GPT4, which has 1.76 Trillion parameters (MxMxdM).\nThe choice of DistilGPT2 offers feasibility when working with proprietary data and its “little instructed” nature means its behavior is more closely aligned with the fine-tuning data. Fifteen distinct LLMs are fine-tuned, denoted as DistilGPT2 {bibDB, SDG}, corresponding to each combination of the three bibliometric databases and the five selected SDGs.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-overlook-by-the-llm",
    "href": "chapter_ai-nepi_009.html#systematic-overlook-by-the-llm",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.9 Systematic Overlook by the LLM",
    "text": "10.9 Systematic Overlook by the LLM\n\n\n\nSlide 14\n\n\nDespite being stimulated by specific prompts related to SDG targets, the fine-tuned LLM consistently demonstrates a systematic failure to address certain categories of locations, actors, and focuses. Locations that are systematically overlooked include African countries (with the exception of South Africa), developing countries (with a question mark noted regarding China), least developed countries, and Small Island Developing States.\nSimilarly, specific categories of actors are systematically overlooked, such as vulnerable people, persons with disabilities, indigenous peoples, and children in vulnerable situations. For SDG4, as an illustrative example, specific focuses that are systematically missed include vocational training, scholarships, the concept of safe, non-violent, inclusive, and effective environments, sustainable lifestyles, human rights, the promotion of a culture of peace and non-violence, global citizenship, the appreciation of cultural diversity, free primary and secondary education, and tertiary education. This pattern of systematic overlook is identified as a recurrent result observed across all five of the SDGs examined in the study.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#considerations-across-the-5-sdgs",
    "href": "chapter_ai-nepi_009.html#considerations-across-the-5-sdgs",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.10 Considerations Across the 5 SDGs",
    "text": "10.10 Considerations Across the 5 SDGs\nAcross the five SDGs studied, several consistent patterns and considerations emerge.\nRegarding Locations, least developed countries are barely addressed, with South-Saharan Africa noted specifically in relation to SDG8. Beyond the undoubted monopoly of the United States in mentions, South Africa and China are the most frequently quoted locations, followed by the UK and Australia.\nConcerning Actors, discriminated and vulnerable categories are systematically overlooked across the different SDGs, with no macro response observed for these groups.\nIn terms of Metrics, the analysis reveals that many different surveys are recalled as datasets, such as DHS (Demographic and Health Surveys) and WVS (World Values Survey). This indicates the presence of recurrent data from surveys within the semantic multi-layer networks formed after fine-tuning the LLMs. Various Research methodologies are also recalled, including theoretical, empirical, thematic analysis, market dynamics, and macroeconomics.\nThe Focuses identified are SDG-specific, but the most sensitive ones, such as Human Trafficking, human exploitation, and migration, are often missing. Furthermore, notable methodological differences are observed between the databases: Web of Science tends to show a very theoretical approach for certain SDGs, while Scopus and OpenAlex exhibit a significantly more empirical approach for the same goals.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-round-up-findings-and-limitations",
    "href": "chapter_ai-nepi_009.html#case-study-round-up-findings-and-limitations",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "10.11 Case Study Round-up: Findings and Limitations",
    "text": "10.11 Case Study Round-up: Findings and Limitations\nThe case study provides a round-up of key findings and acknowledged limitations. A primary finding is that introducing LLMs as an analytical AI tool positioned between the SDG classification process and the policymaker reveals a systematic overlook within the data, specifically the scientific publications classified by SDGs.\nThis systematic overlook pertains to the most disadvantaged categories of individuals, the poorest countries, and underrepresented topics that are explicitly focused on by SDG targets. Conversely, the data demonstrates full attention directed towards economic superpowers and highly developing countries. The results clearly underscore the decisive influence of the bibliometric classification of SDGs, highlighting that this practice, while appearing objective and science-informed, significantly shapes the representation of research.\nThe study acknowledges several limitations. High sensitivity is observed with respect to the model architecture used, the training data, the hyperparameters, and the decoding strategy. While the use of three different databases partially accounts for the sensitivity to training data, and the application of three different decoding strategies based on literature partially accounts for this sensitivity, these factors remain limitations. The framework employed is general, and the potential for exploring more developed model architectures is noted.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "",
    "text": "11 Extracting Citation Data from Law and Humanities Scholarship",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#overview",
    "href": "chapter_ai-nepi_010.html#overview",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.1 Overview",
    "text": "11.1 Overview\nThe project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (Web of Science, Scopus, OpenAlex) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses.\nTraditional machine learning tools like ExCite perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., ExCite accuracy around 0.22-0.26 for extraction). Footnotes in SSH are often complex, containing commentary, abbreviations, and multiple references (“footnotes from hell”). Creating training data for traditional methods is laborious using annotation tools.\nLarge Language Models (LLMs) and Vision Language Models (VLMs) show promise for handling messy textual data and PDFs, but the primary challenge is trusting the results due to potential hallucinations (e.g., inventing non-existent citations, as seen in legal cases).\nA robust testing and evaluation solution is required, necessitating a high-quality gold standard dataset, a flexible evaluation framework adaptable to fast-moving technology, and solid testing algorithms for comparable metrics.\nThe project develops a specialized gold standard dataset encoded in TEI XML, chosen for its well-established standard, detailed specification covering phenomena beyond mere reference management (including context for citation intention), and compatibility with existing digital humanities corpora and tools like Grobid. The dataset creation involves stages: screenshot of the PDF, segmentation of reference strings from non-reference text within footnotes, and parsed structured data.\nThe dataset currently includes over 1,500 references from open access journals to enable full publication from PDF to parsed data structures.\nA Python package named Llamore (Large Language Models for Reference Extraction) is developed. Llamore is lightweight, acting as an interface to various LLMs/VLMs (compatible with OpenAI API, covering Ollama, VLLM, etc.). It takes text or PDFs as input and outputs references in TEI XML format.\nIt also provides an evaluation function using the F1 score metric to compare extracted references against gold standard references. The F1 score calculation involves counting exact matches of bibliographic elements (analytic title, monographic title, surname, publication date, etc.) and dividing by the number of predicted and gold elements.\nThe problem of aligning extracted references to gold references is solved using an unbalanced assignment problem solver (from SciPy), maximizing the total F1 score with unique assignments and penalizing missing or hallucinated references with an F1 score of zero.\nEvaluation results show that Llamore performs comparably to Grobid on biomedical datasets (PLOS 1000) but significantly outperforms Grobid on the specialized humanities dataset, where Grobid struggles due to being out of distribution. While LLMs require significantly more compute than Grobid, their performance on complex SSH footnotes is superior.\nError analysis suggests issues include difficulty distinguishing volume/page numbers, misclassifying names in titles as authors, and misinterpreting terminology like “idem” as authors. The required F1 score for “good enough” data depends on the analysis goal (tendencies vs. accurate data). The current focus is on achieving reliable results before scaling up to avoid interpreting hallucinated data.\nFuture work could involve retrieval augmented generation (RAG) using disciplinary databases for validation and potentially human-in-the-loop workflows for error correction and iterative model improvement. The project aims for both specific research results (e.g., citation trends in a specific journal) and generic applicability to other projects.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#project-scope-and-problem",
    "href": "chapter_ai-nepi_010.html#project-scope-and-problem",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.2 Project Scope and Problem",
    "text": "11.2 Project Scope and Problem\n\n\n\nSlide 01\n\n\nThe project focuses on extracting citation data specifically from Law and Humanities scholarship, a domain characterized by extensive and complex footnotes. The primary challenge involves parsing these footnotes using Large Language Models (LLMs) or other algorithmic approaches.\nThe extracted data is intended for generating citation graphs, which are valuable tools in intellectual history and the history of science. Citation graphs enable the discovery of patterns and relationships within knowledge production, facilitate the reconstruction of intellectual influences, and allow for the measurement of the reception of specific ideas. An example application involves tracking the change in most-cited authors over time, such as an analysis conducted for the Journal of Law and Society between 1994 and 2003.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#problem-bibliometric-database-coverage",
    "href": "chapter_ai-nepi_010.html#problem-bibliometric-database-coverage",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.3 Problem: Bibliometric Database Coverage",
    "text": "11.3 Problem: Bibliometric Database Coverage\n\n\n\nSlide 01\n\n\nA significant problem is the extremely poor coverage of historical Social Sciences and Humanities (SSH) scholarship in existing bibliometric databases. The primary databases in this space include Web of Science, Scopus, and OpenAlex. Web of Science and Scopus are characterized by high costs and very restrictive licenses, making dependence on them undesirable.\nWhile OpenAlex is preferable due to its open access nature, it also lacks sufficient coverage for the specific content required for this research.\nCommon coverage gaps across these databases for SSH literature include a lack of inclusion for journals not classified as “A-journals,” insufficient data for publications from the pre-digital age, and a general lack of coverage for non-English language content.\nAn illustrative example is the Zeitschrift für Rechtssoziologie, a German journal established in 1980. Analysis of available citation data by decade shows very low coverage across Dimensions, OpenAlex, and Web of Science for the decades before the 2000s. Although coverage improves somewhat after 2000, it remains incomplete, particularly for Web of Science.\nSeveral factors contribute to this poor coverage in SSH. Firstly, there is a perceived lack of financial incentive compared to STEM, medicine, and economics, which are typically well-represented.\nSecondly, these databases often focus on metrics like the “impact factor,” which aligns with science evaluation goals but not necessarily with the objectives of intellectual history research. Finally, the literature itself presents a technical challenge due to the extensive use of complex footnotes, which are difficult for automated systems to process accurately.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#complex-footnotes-and-training-data",
    "href": "chapter_ai-nepi_010.html#complex-footnotes-and-training-data",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.4 Complex Footnotes and Training Data",
    "text": "11.4 Complex Footnotes and Training Data\n\n\n\nSlide 04\n\n\nProblem 2 concerns the nature of the footnotes themselves, often referred to as “footnotes from hell.” These are typical of humanities scholarship and contain complex structures, including commentary, messy formatting, abbreviations, parenthetical information, and multiple distinct references embedded within surrounding non-citation text. An example image displays a scanned page with a footnote exhibiting these characteristics, featuring content in both German and English, various abbreviations, and multiple citations within a single numbered entry.\nProblem 3 highlights the difficulty in creating training data for citation extraction. The traditional approach involves a laborious manual annotation process. A web-based annotation tool is utilized for this purpose, allowing users to highlight segments of text within footnotes and assign specific labels corresponding to bibliographic elements such as:\n\nAuthor\nTitle\nDate\nJournal\nVolume\nPages\nDOI\nISBN\nURL\nBackRef\nSignal\nIgnore\n\nand indicators for whether a segment is a reference or not. This manual annotation requires a significant investment of time and effort.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-existing-tools",
    "href": "chapter_ai-nepi_010.html#limitations-of-existing-tools",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.5 Limitations of Existing Tools",
    "text": "11.5 Limitations of Existing Tools\n\n\n\nSlide 05\n\n\nProblem 4 identifies a critical limitation: existing tools designed for citation extraction are unable to handle the complexity of humanities footnotes effectively. These traditional tools typically rely on machine learning methods such as Conditional Random Forests. However, their performance on the specific type of complex footnote data encountered in this domain is poor.\nAn example illustrating this limitation is the performance of the tool ExCite. Its performance is evaluated using metrics including Extraction Accuracy and Segmentation Accuracy across different training data configurations: Default, Footnoted, and Combined. The results show consistently low accuracy scores.\nWith Default training data, Extraction Accuracy is 0.24 and Segmentation Accuracy is 0.37. Using Footnoted training data yields Extraction Accuracy of 0.26 and Segmentation Accuracy of 0.37. The Combined training data results in Extraction Accuracy of 0.22 and Segmentation Accuracy of 0.47. These figures, sourced from Boulanger/Iurshina (2022), Table 1, demonstrate that ExCite, and by extension other similar traditional tools, struggle significantly with this task, regardless of the training data used.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llms-as-solution-the-trust-problem",
    "href": "chapter_ai-nepi_010.html#llms-as-solution-the-trust-problem",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.6 LLMs as Solution: The Trust Problem",
    "text": "11.6 LLMs as Solution: The Trust Problem\n\n\n\nSlide 05\n\n\nThe question arises whether Large Language Models (LLMs) can offer a solution to the challenges of extracting citation data from complex footnotes. Early experiments conducted in 2022 using models such as text-davinci-003 demonstrated the potential power of LLMs to extract references even from messy textual data. Newer models promise even better performance.\nFurthermore, Vision Language Models (VLMs) possess the capability to process PDFs directly, which is advantageous given that source materials are frequently available in this format.\nPotential methods for leveraging LLMs include prompt engineering, Retrieval Augmented Generation (RAG), and finetuning. However, a primary concern is the trustworthiness of the results produced by these models. A significant problem is the potential for hallucination, where LLMs invent information that does not exist, including fabricating citations.\nA notable example illustrating this issue involved a lawyer who used ChatGPT for a federal court filing and cited non-existent cases invented by the model, resulting in severe consequences. This highlights the critical principle that analysis should not be undertaken unless the results are known to be correct and sufficient validation data is available to verify accuracy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#solution-requirements",
    "href": "chapter_ai-nepi_010.html#solution-requirements",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.7 Solution Requirements",
    "text": "11.7 Solution Requirements\n\n\n\nSlide 06\n\n\nTo address the trust issue and enable reliable citation extraction, a robust testing and evaluation solution is required. This solution must meet several key requirements.\n\nFirstly, it necessitates a high-quality Gold Standard dataset against which extracted results can be compared.\nSecondly, it requires a flexible framework capable of adapting easily to the rapidly evolving technology landscape of LLMs and VLMs.\nFinally, the solution must incorporate solid testing and evaluation algorithms designed to produce comparable metrics, allowing for objective assessment of different approaches and models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#gold-standard-dataset",
    "href": "chapter_ai-nepi_010.html#gold-standard-dataset",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.8 Gold Standard Dataset",
    "text": "11.8 Gold Standard Dataset\n\n\n\nSlide 07\n\n\nThe project involves compiling a high-quality gold standard dataset intended for both training and evaluation purposes. The chosen encoding standard for this dataset is TEI XML. This standard was selected because it is well-established and precisely specified within the humanities and digital editorics fields.\nTEI XML is more comprehensive than simpler bibliographical standards like CSL or BibTeX, covering a wider range of phenomena and extending beyond basic reference management. It allows for encoding contextual information, which can be valuable for tasks such as classifying citation intention.\nFurthermore, adopting TEI XML enables the project to potentially utilize existing text collections and corpora from other digital editorics projects that publish their source data, including detailed reference encodings, in this format. Such existing corpora can also serve as resources for testing the generalization and robustness of the developed mechanisms.\nThe dataset establishment process is currently underway. The encoding involves several stages, illustrated by an example of footnote number four. These stages include capturing a screenshot of the PDF source, segmenting the text to distinguish the actual reference string from surrounding non-reference text within the footnote (such as introductory phrases), and finally creating a parsed structured data representation of the reference.\nMidway through the project, the strategy for building the dataset was adjusted. Initially, the focus was on data directly relevant to the primary research question. More recently, the decision was made to include the source PDFs and structure the dataset to enable the use of Vision Language Models (VLMs). The goal is to be able to publish the entire dataset pipeline, from the source PDFs through to the parsed data structures.\nTo facilitate this, the selection of source journals was changed to focus on open access publications. The dataset currently includes the encoding of over 1,500 references. It is noted that this count refers to occurrences, meaning the same work referenced multiple times is encoded separately to capture the specific context of each mention.\nA significant benefit of using the interoperable TEI XML standard is the availability of numerous tools. A particularly relevant tool for this project is Grobid, a popular system for reference and information extraction. Grobid utilizes TEI XML for its own training and evaluation processes. By using the same data format, the project can directly compare its performance against Grobid, potentially use Grobid’s existing training data, and contribute the newly created dataset to the Grobid development team.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-extraction-and-evaluation-tool",
    "href": "chapter_ai-nepi_010.html#llamore-extraction-and-evaluation-tool",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.9 Llamore: Extraction and Evaluation Tool",
    "text": "11.9 Llamore: Extraction and Evaluation Tool\n\n\n\nSlide 14\n\n\nThe project developed a tool named Llamore, which stands for Large Language Models for Reference Extraction. Llamore is implemented as a small Python package. Its core capabilities include taking text or PDF documents as input, extracting references from them, and exporting these extracted references as TEI formatted XML files.\nAdditionally, if gold standard references are provided, Llamore can evaluate the performance of the extraction process.\nThe design of Llamore prioritized two main objectives: being lightweight and ensuring broad compatibility. It is lightweight because it does not contain any language models internally; instead, it functions as an interface to a model selected by the user. This design also ensures compatibility with a wide range of both open and closed LLMs and VLMs.\nImplementation details include its availability on PyPI, allowing simple installation via the pip package manager. The extraction workflow involves defining an extractor object specific to the chosen model, such as using the OpenAIExtractor. This particular extractor provides compatibility with many open models (like those served by Ollama or VLLM) by interacting with their API endpoints that are designed to be compatible with the OpenAI API specification.\nThe user provides a PDF or text input to the extractor, receives the extracted references, and can then export these references to an XML file. For evaluation, the user imports the F1 class from the package and provides both the gold standard references and the extracted references to this class to compute performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#evaluation-methodology",
    "href": "chapter_ai-nepi_010.html#evaluation-methodology",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.10 Evaluation Methodology",
    "text": "11.10 Evaluation Methodology\n\n\n\nSlide 16\n\n\nThe evaluation methodology employed utilizes the F1 score, a well-established metric for comparing structured data. The F1 score is calculated based on Precision and Recall, which in turn depend on the number of matches between elements in an extracted reference and a corresponding gold standard reference. A match is defined based on the exact correspondence of specific bibliographic elements, such as analytic title, monographic title, surname, and publication date.\nPartial matches, like a forename with an extra dot, might not count as an exact match depending on configuration. Precision is calculated as the number of matches divided by the total number of elements predicted in the extracted reference, while Recall is the number of matches divided by the total number of elements in the gold reference. The F1 score is the harmonic mean of Precision and Recall. An F1 score of 1 indicates perfect extraction, where the reference is perfectly captured, while an F1 score of 0 signifies no matches were found.\nA key challenge in evaluating performance across a set of references is aligning the extracted references with their corresponding gold standard references. This is tackled by formulating the problem as an unbalanced assignment problem. Llamore uses a solver from the SciPy library internally to address this. The process involves computing the F1 score for every possible pairing between each extracted reference and each gold reference, constructing a matrix of scores.\nThe solver then finds the assignment of extracted references to gold references that maximizes the total sum of F1 scores, ensuring that each reference is assigned uniquely. The final overall score is the macro average of the F1 scores for the assigned pairs. The method penalizes missing gold references (those not matched by an extracted reference) and hallucinated extracted references (those not matched to a gold reference) by assigning them an F1 score of zero in the averaging process. This approach is noted as being similar to a method recently published by Packet et al.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#evaluation-results",
    "href": "chapter_ai-nepi_010.html#evaluation-results",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship",
    "section": "11.11 Evaluation Results",
    "text": "11.11 Evaluation Results\n\n\n\nSlide 20\n\n\nEvaluation was conducted to assess the effectiveness of the developed approach. Using the PLOS 1000 dataset, which comprises 1,000 PDFs from the biomedical field, Llamore’s performance was found to be comparable to that of Grobid. It is noted that Grobid was specifically trained on data from similar journal articles.\nHowever, in terms of efficiency and compute resources, Grobid is significantly superior, requiring orders of magnitude less computational power than LLMs like Gemini.\nWhen evaluated on the specialized humanities dataset created by the project, Grobid struggled significantly to extract references, indicating that this data is out of its training distribution. In contrast, the LLM-based approach implemented in Llamore performed significantly better on this complex dataset.\nThe conclusion drawn is that while less computationally efficient on standard datasets, the LLM-based approach implemented in Llamore is more effective for extracting references from the challenging, out-of-distribution data characteristic of humanities footnotes compared to Grobid.\nIt is acknowledged that the F1 scores achieved are not considered high overall, suggesting substantial room for improvement. The current implementation utilizes a very basic approach, essentially sending the raw PDF text to a pre-trained model. Future work could explore potential improvements through methods such as fine-tuning models or incorporating more contextual information to enhance performance.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Chatting with Papers",
    "section": "",
    "text": "12 Chatting with Papers",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#overview",
    "href": "chapter_ai-nepi_011.html#overview",
    "title": "11  Chatting with Papers",
    "section": "12.1 Overview",
    "text": "12.1 Overview\nThe project develops an AI solution for interacting with specific document collections, referred to as “chatting with papers.” Its primary objective is to address the problem of information overload in science dynamics by improving information retrieval and knowledge production processes using AI.\nThe system utilizes a mixed approach, combining Large Language Models (LLMs) with semantic artifacts. These artifacts include structured data represented as knowledge graphs and vector spaces derived from document content. The core components are codenamed Ghostwriter (the interface) and EverythingData (the backend processing pipeline).\nThe approach is based on Retrieval-Augmented Generation (RAG), integrating vector embeddings of document content with a metadata layer. This metadata layer is represented as a knowledge graph, which incorporates ontologies and controlled vocabularies, including aspects of responsible AI. The graph is expressed using the Croissant ML standard.\nThe system aims for a “local” or “tailored” AI solution, functioning as a distributed AI. In this architecture, the LLM acts as an interface and reasoning engine, connected to the RAG library (graph) and consuming embeddings (vectors) as context. A key feature is the use of entity extraction pipelines that link terms to knowledge graphs, specifically Wikidata. This linkage provides ground truth, supports multilinguality, and enables validation of LLM responses against structured identifiers.\nThe system splits papers into small blocks, each with a unique identifier. It employs LLM techniques to connect and retrieve these blocks, applying weights and knowledge graph information to predict relevant text pieces. It provides summaries and references to original sources, avoids hallucination by relying solely on the ingested data, and indicates when information is not found.\nThe interface allows users to ask natural language questions, receive summaries and document lists, and add missing information. The system supports multilingual queries by linking terms to Wikidata identifiers, which have multilingual translations. This approach is seen as a way to support the user’s thought process and help find relevant research questions rather than providing definitive answers.\nThe system is being considered for open-source release under the Linux Foundation. It is also being explored for integration with various data sources, including GitHub content, manuals, and guidelines, with potential applications in building research infrastructure portals. Validation against other systems like Neo4j Graph Builder or Microsoft Graph is being considered. The approach of using knowledge organization systems linked to identifiers is proposed as a method for benchmarking future generations of AI models and ensuring sustainability. The system uses downscaled LLMs (e.g., 1 billion parameters) capable of running locally. Recency bias in results is acknowledged, with a proposed solution involving storing facts with timestamps in the knowledge graph to allow for processing linked to specific dates. The approach is considered similar to Google Notebook ML due to reliance on similar ideas and collaboration with the same teams.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-overview-and-affiliations",
    "href": "chapter_ai-nepi_011.html#project-overview-and-affiliations",
    "title": "11  Chatting with Papers",
    "section": "12.2 Project Overview and Affiliations",
    "text": "12.2 Project Overview and Affiliations\n\n\n\nSlide 01\n\n\nThe project is titled “Chatting with Papers,” with the subtitle “the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics - and beyond.” The authors involved are Slava Tykhonov, Philipp Mayr, Jetze Touber, and Andrea Scharnhorst.\nTheir affiliations are GESIS, Cologne, Germany for Philipp Mayr, and DANS-KNAW, The Hague, The Netherlands for Slava Tykhonov, Jetze Touber, and Andrea Scharnhorst. The presentation includes the logos for GESIS and DANS. The text “LLM 4 HPSS” is also present, indicating the context within which this work is situated.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#science-dynamics-and-information-overload",
    "href": "chapter_ai-nepi_011.html#science-dynamics-and-information-overload",
    "title": "11  Chatting with Papers",
    "section": "12.3 Science Dynamics and Information Overload",
    "text": "12.3 Science Dynamics and Information Overload\n\n\n\nSlide 01\n\n\nThe evolution of sciences exhibits growth and increasing differentiation, presenting the challenge of reviewing, evaluating, and selecting relevant information. A fundamental precondition for creating new knowledge, whether in individual researchers or across academia, is the ability to find and understand existing information. Machines, particularly recent advancements in AI, have contributed to this growth in information volume.\nThe project investigates whether AI can also support the knowledge production process itself, framing this as a problem within the domain of Information Retrieval. The motivation stems from the need to manage the overwhelming volume of information researchers face.\nThe work is based on extensive experimentation by senior research engineer Slava Tykhonov at DANS across various projects, involving the construction of complex technical pipelines, characterized as a “back of things you can hardly unravel.” The project aims to apply and illustrate this technical structure using a specific use case, making it understandable to a broader audience.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#talk-structure-and-system-architecture",
    "href": "chapter_ai-nepi_011.html#talk-structure-and-system-architecture",
    "title": "11  Chatting with Papers",
    "section": "12.4 Talk Structure and System Architecture",
    "text": "12.4 Talk Structure and System Architecture\n\n\n\nSlide 02\n\n\nThe talk addresses the research question: Can an AI solution be constructed to facilitate interaction, or “chatting,” with papers from a specific, selected collection? The introduction covers foundational concepts including information retrieval, the dynamics of human-machine interaction, and Retrieval-augmented generation (RAG) within the context of generative AI.\nA specific use case involving papers from the method-data-analysis (mda) journal is presented. The workflow introduces a “local” or “tailored AI solution” architecture, comprising two main components known by the pet names Ghostwriter and EverythingData. Ghostwriter serves as the user interface, while EverythingData encompasses the entire backend processing pipeline. The presentation includes illustrations of both front end and back end operations, concluding with a summary and outlook on future directions.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-new-ir-interface-and-query-models",
    "href": "chapter_ai-nepi_011.html#ghostwriter-new-ir-interface-and-query-models",
    "title": "11  Chatting with Papers",
    "section": "12.5 Ghostwriter: New IR Interface and Query Models",
    "text": "12.5 Ghostwriter: New IR Interface and Query Models\n\n\n\nSlide 02\n\n\nThe Ghostwriter approach introduces a new interface for information retrieval. A primary challenge in this domain involves formulating the correct question, identifying the appropriate person or information source, and accurately interpreting the results. This is fundamentally linked to the classic information retrieval (IR) problem of finding the right query. The approach explores different models of query interaction, illustrated through comparisons.\nInteracting with a database requires explicit knowledge of its schema and typical values to obtain results, representing the classic IR problem. A model involving querying connected structured data, such as databases or graphs, is likened to interacting with a librarian. The system suggests similar or improved queries based on schema connections and provides lists of potential results for different query variations. This is exemplified by features like Google’s schema.org integration, which works well on the web but is less suited for local interactions.\nQuerying a Large Language Model is compared to interacting with a library or a round of experts. The LLM interprets the query as natural language input and provides suggestions for results, also expressed in natural language. The Ghostwriter approach combines a local LLM with a target data collection or space, embedding it within a network of additional data interpretation sources accessible via APIs. This is metaphorically described as chatting simultaneously with experts and librarians.\nThis combined approach creates a family of terms related to the query, identifies relevant structured information, and returns a list of results. When applied iteratively, this process assists users in reformulating their questions by enhancing their understanding of their actual query intent and the capabilities of the available data space. The metaphors of a “librarian” representing structured data, knowledge organization systems, and existing classifications, and an “expert” representing natural language, are central to describing these interaction models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-rag-architecture",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-rag-architecture",
    "title": "11  Chatting with Papers",
    "section": "12.6 Ghostwriter and EverythingData: RAG Architecture",
    "text": "12.6 Ghostwriter and EverythingData: RAG Architecture\n\n\n\nSlide 04\n\n\nThe Ghostwriter and EverythingData architecture is situated within the wider discourse of Retrieval Augmented Generation (RAG). The main ingredients of this system include a vector space and a graph. The vector space is constructed from the content of data files, with content encoded into embeddings that possess properties and attributes. These embeddings are computed using various machine learning algorithms and different Large Language Models.\nThe graph component represents a metadata layer that is integrated with various ontologies and controlled vocabularies, encompassing considerations for responsible AI. This graph is expressed using the Croissant ML standard. The vision behind this approach is to combine both the graph and vector components into a single model, a concept referred to as GraphRAG.\nThis is implemented locally as a form of Distributed AI, where the LLM serves as the interface between the human user and the AI system, simultaneously functioning as a reasoning engine. In implementation, the LLM is connected to a “RAG library,” which is the graph component. It navigates through datasets and consumes the embeddings (vectors) as context to inform its responses.\nRelated concepts and resources mentioned include the GenAI Knowledge Graph and “The GraphRAG Manifesto: Adding Knowledge to GenAI,” authored by Philip Rathle, CTO of Neo4j, with a link provided to the Neo4j blog post. The Wikipedia page for Retrieval-augmented generation is also referenced, along with a reference to Arno Simons’ presentation on tool boxes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#everythingdata-backend-and-vector-space",
    "href": "chapter_ai-nepi_011.html#everythingdata-backend-and-vector-space",
    "title": "11  Chatting with Papers",
    "section": "12.7 EverythingData Backend and Vector Space",
    "text": "12.7 EverythingData Backend and Vector Space\n\n\n\nSlide 04\n\n\nThe system’s input data consists of a collection of articles, specifically scraped from the MDA journal, although the system is designed to work with any collection of documents. This input is processed by the backend component, referred to as “tamed EverythingData.” The backend executes various operations, including storing the information in a vector store utilizing Quadrant. Additional processing steps involve term extractions, constructing embeddings, and other related operations.\nA crucial aspect of the backend is the integration of knowledge graphs, coupling the processed information to these graphs. This integration enhances the value of words, phrases, and embeddings by providing additional context and adding another layer of value to the existing context. The processed information is structured and fed into a vector space. The user interface interacts with this combined vector space and graph structure. Users formulate queries as natural language questions. The system responds by providing a list of relevant documents, consistent with standard information retrieval outputs, and a summary generated by the machinery based on the user’s question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-functionality-and-mechanisms",
    "href": "chapter_ai-nepi_011.html#ghostwriter-functionality-and-mechanisms",
    "title": "11  Chatting with Papers",
    "section": "12.8 Ghostwriter Functionality and Mechanisms",
    "text": "12.8 Ghostwriter Functionality and Mechanisms\n\n\n\nSlide 07\n\n\nThe Ghostwriter system is designed for chatting with papers, but its capabilities extend to interacting with any content from the web or even spreadsheets. When interacting with spreadsheets, the system can recognize specific values and provide responses without hallucinating, as it relies solely on the spreadsheet content as its source. The system utilizes a relatively simple LLM with 1 billion parameters, which is capable of answering complex questions by leveraging knowledge graphs.\nBy default, the system does not depend on knowledge pre-ingested into the LLM. Its primary goal is to provide answers based only on factual information present in the specific paper or papers that have been ingested. If the required information is not found within the ingested papers, the system explicitly states “I don’t know.” This strict reliance on the source material is the mechanism for avoiding hallucination, as the system has precise knowledge of where to locate information.\nThe underlying implementation involves splitting each paper into small blocks, with a separate identifier assigned to each block. LLM techniques are employed to intelligently connect and retrieve these blocks. The system applies weights and incorporates information from knowledge graphs to predict which specific pieces of text are most relevant to answer a given question. A user interaction feature includes an “Add paper” button, allowing users to contribute missing information; this added content will then be available for subsequent queries on the same topic.\nThe backend processing involves an entity extraction pipeline and annotations. Entities are linked to knowledge graphs, which is considered extremely important as it provides a ground truth mechanism for validating the accuracy of the LLM’s responses. Multilinguality support is a critical feature; the system can handle papers written in languages such as Chinese or German and respond to questions posed in English, aiming for reliable answers. The LLM’s final role is to synthesize the retrieved pieces of text to produce the results.\nThe fact extraction process involves splitting the user’s question into smaller pieces and utilizing a knowledge organization system (KOS). This KOS is a repeatable process that can reveal new levels of related terms underneath the initial query term. A key step is linking everything to Wikidata. This process transforms free-text strings into identifiers that are linked to multilingual translations available in Wikidata. This linking provides access to all associated properties and enables the system to understand and respond to questions asked in various languages. The query construction process involves translating the user’s question into potentially hundreds of languages, and all these translations are used as input to the LLM.\nThe knowledge graph linking provides a ground truth mechanism by decoupling knowledge from the questions and papers, storing this knowledge externally as a list of identifiers from Wikidata. This allows for a validation mechanism where different models, including those not yet trained, can be tested by asking the same questions and comparing the resulting lists of identifiers. Discrepancies in the identifier lists indicate that a particular model may not be suitable for the task. This approach is proposed as a method for creating benchmarks and supporting future generations of scientists. The project is collaborating with industry partners like Google and Meta to ensure the sustainability of this process, viewing the knowledge organization system as a potential future standard.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-demonstration",
    "href": "chapter_ai-nepi_011.html#ghostwriter-demonstration",
    "title": "11  Chatting with Papers",
    "section": "12.9 Ghostwriter Demonstration",
    "text": "12.9 Ghostwriter Demonstration\nA demonstration of the Ghostwriter interface is conducted within a browser environment. The first query example involves asking the system about “rational choice theory.” The system processes this request by thinking and retrieving relevant pieces of information. The output consists of a summary compiled from different papers and includes references pointing directly to the original source papers, confirming that the results originate from the specified sources.\nA second query example involves asking the system to “explain utility in Rational Choice Theory.” The system responds by selecting different pieces of information from the ingested papers, presenting different results while still referencing the same source documents. The system provides an API that enables automatic mode operation, facilitating the construction of pipelines within an agentic architecture where the system can be prompted, results collected, and subsequent queries issued. This API can be used to analyze papers to identify new information or knowledge contributions.\nThe interface includes a feature allowing users to add a page or information if the system does not provide results for a query; this added information is then incorporated and will appear in responses to the same question in the future. A key demonstration of the system’s capability is asking questions in English about a source paper that is written entirely in German, showcasing its multilinguality support.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-benefits-and-philosophy",
    "href": "chapter_ai-nepi_011.html#project-benefits-and-philosophy",
    "title": "11  Chatting with Papers",
    "section": "12.10 Project Benefits and Philosophy",
    "text": "12.10 Project Benefits and Philosophy\nA significant benefit of the project’s approach is the local availability of the system. This provides users with greater control compared to interacting with large, external systems, which can also be costly. The interaction with papers via the system is likened to chatting with an invisible college.\nIt is recommended that users approach this interaction with the same perspective as engaging with an invisible college, meaning the goal is not necessarily to find ultimate facts or definitive answers. Instead, the primary purpose of the system is to provoke and support the user’s thinking process. The human user retains the role of understanding the question and identifying the appropriate research question. The system’s function is to provide support for the user’s own cognitive process. The recommended perspective is to view these technological possibilities as tools that enhance and support human thinking.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-performance-and-local-deployment",
    "href": "chapter_ai-nepi_011.html#system-performance-and-local-deployment",
    "title": "11  Chatting with Papers",
    "section": "12.11 System Performance and Local Deployment",
    "text": "12.11 System Performance and Local Deployment\nSystem performance has been improved by downscaling the Large Language Models used. The implementation transitioned from a complex Llama model with 70 billion parameters to a smaller model with only 1 billion parameters. This current model is capable of running on a local computer. The ability to deploy and run LLMs locally on private or sensitive material is seen as a potential challenge to companies like Nvidia if this capability becomes widely known and adopted.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#from-development-to-production",
    "href": "chapter_ai-nepi_011.html#from-development-to-production",
    "title": "11  Chatting with Papers",
    "section": "12.12 From Development to Production",
    "text": "12.12 From Development to Production\nThe current status of the interface is described as a “playing ground,” used primarily to gain a better understanding of the system’s behavior and capabilities. However, similar underlying machinery is being applied in other, more serious projects intended for production environments. An example of such a production project is the Odyssey project in the Netherlands, which involves building a portal designed to bring together various data sources.\nProjects like Odyssey necessitate considerations for long-term sustainability and the handling of diverse data sources, while still applying the same core principles developed in the Ghostwriter/EverythingData work. These aspects are actively discussed at a high level within research infrastructure discussions in the Netherlands.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#validation-and-community-engagement",
    "href": "chapter_ai-nepi_011.html#validation-and-community-engagement",
    "title": "11  Chatting with Papers",
    "section": "12.13 Validation and Community Engagement",
    "text": "12.13 Validation and Community Engagement\nFuture validation of the system is envisioned through its development as a community project under the Linux Foundation. The Linux Foundation has approached the project team with interest in publishing the work. The project is expected to be released as an open source project potentially within the current month.\nThe community is anticipated to play a crucial role in helping to validate and improve the system, reflecting the belief that significant progress is impossible without community involvement. Currently, the team is in an experimental phase regarding validation. The next steps involve engaging in scientific discourse and publishing scientific papers about the work, marking the beginning of the serious academic validation process.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#data-ingestion-and-collections",
    "href": "chapter_ai-nepi_011.html#data-ingestion-and-collections",
    "title": "11  Chatting with Papers",
    "section": "12.14 Data Ingestion and Collections",
    "text": "12.14 Data Ingestion and Collections\nSetting up a collection, such as a Nodo collection, is considered not hard. This assessment is based on observations of the system’s capability to perform similar setup processes for information extracted from various other APIs. The system is designed to ingest data from any kind of source, including content from GitHub, manuals, guidelines, and papers.\nAn example collaboration involves building this system for Harvard University. The system deployed for Harvard currently contains approximately 300,000 documents, and Harvard University has commenced using it. The project team is receiving substantial feedback from users like Harvard. Based on this feedback, there is a strong belief that utilizing local models deployed on personal computers represents a preferable approach compared to being fully dependent on industry-provided solutions such as ChatGPT.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-goals-and-collaboration",
    "href": "chapter_ai-nepi_011.html#project-goals-and-collaboration",
    "title": "11  Chatting with Papers",
    "section": "12.15 Project Goals and Collaboration",
    "text": "12.15 Project Goals and Collaboration\nThe project’s primary goal is not centered on developing or selling software commercially. The preferred model is based on collaborations, typically triggered by individuals or groups who have concrete research questions that the system might help address.\nThe collaboration process involves seeking resources to conduct a try-out of the system for the specific use case, followed by handing over the system to the collaborating partners. These partners are then expected to tinker with, validate, and polish the system further. The team expresses anticipation for future collaborations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#recency-bias-mitigation",
    "href": "chapter_ai-nepi_011.html#recency-bias-mitigation",
    "title": "11  Chatting with Papers",
    "section": "12.16 Recency Bias Mitigation",
    "text": "12.16 Recency Bias Mitigation\nA potential problem identified is the possibility of recency bias in the system’s results. An example cited is querying a concept like “rational choice,” which originated in the 1930s or 1940s, but potentially receiving results predominantly from the 2000s. This recency bias is acknowledged as true.\nThe proposed solution involves collecting facts and storing them within the knowledge graph. A key detail of the knowledge graph structure is the ability to store a fact along with a timestamp if one is available. This allows for separate processing based on these timestamps. For queries related to temporal aspects, the system can provide a list of all facts linked to specific dates, rather than a single, potentially biased answer, offering a way to mitigate recency bias.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#comparison-to-google-notebook-ml",
    "href": "chapter_ai-nepi_011.html#comparison-to-google-notebook-ml",
    "title": "11  Chatting with Papers",
    "section": "12.17 Comparison to Google Notebook ML",
    "text": "12.17 Comparison to Google Notebook ML\nWhen compared to Google Notebook ML, the system is assessed as being quite similar. This similarity is attributed to the reliance on the same underlying ideas and collaboration with the same development teams.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "",
    "text": "13 RAG Systems in Philosophy and HPSS",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overview",
    "href": "chapter_ai-nepi_012.html#overview",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nThe presentation details the application of Retrieval Augmented Generation (RAG) systems to philosophical research and teaching, specifically within the Humanities, Politics, and Social Sciences (HPSS) domain. The core objective is to address limitations of standard Large Language Models (LLMs) when applied to disciplines requiring high linguistic and semantic accuracy and deep engagement with specific textual corpora.\nStandard LLMs face problems with access to full texts (despite potential inclusion in training data), limited context windows, and attribution of information. RAG systems are proposed as a solution by providing explicit access to domain-specific data sources, augmenting prompts with retrieved relevant text chunks, and enabling source citation.\nThe presentation outlines typical philosophical research questions that require detailed textual analysis. It describes the standard LLM query process and contrasts it with the RAG workflow.\nThe RAG workflow involves a retrieval query to data sources (such as vector databases or APIs), retrieval of relevant text chunks (typically via semantic search, potentially hybrid or classic search), and augmentation of the LLM prompt with these chunks before generation. This process directly addresses the problems of access, limited context window, and attribution.\nPotential applications in philosophy include didactic use (chatting with philosophical corpora for instructive questioning) and research use (looking up facts in handbooks, exploring unexamined corpora, finding passages for close reading, and directly finding detailed answers to research questions).\nAn example RAG system is presented using the Stanford Encyclopedia of Philosophy (SEP) as the data source. The system was developed through a qualitative study involving theoretically grounded trial and error to optimize performance for philosophical queries.\nKey aspects of this study included model choices (generative LLM, embedding model), tuning hyperparameters (number of documents to retrieve (top-k), max input/output token length, chunk size and overlap), and addressing methodological challenges like retrieval semantic mismatch through reranking. Evaluation of results, particularly for complex, unstructured philosophical answers, is identified as crucial and requiring domain expertise.\nThe implemented SEP RAG system features a frontend with configuration options for generative model (e.g., gpt-4o-mini), prompt token limits, number of texts to retrieve, and persona. It includes a comparative setup displaying answers from the LLM alone versus the RAG system for qualitative evaluation and benchmarking.\nThe output also lists retrieved texts, their source files, section headings, distance metrics, token lengths, and inclusion status based on prompt limits.\nA specific hyperparameter tuning example, chunk size, is discussed. Options explored included fixed word counts, paragraphs, and sections.\nThe study found that chunking into main sections yielded the best results for the SEP corpus, despite section lengths often exceeding the embedding model’s cutoff. This outcome is attributed to the highly systematic structure of the SEP, where section beginnings effectively summarize content. This highlights that effective chunking is corpus- and question-dependent.\nResults indicate that RAG systems offer advantages in integrating verbatim corpora and domain knowledge, leading to more detailed answers and a dramatic reduction in hallucinations compared to standard LLMs. They also enable the citation of relevant documents supporting the generated answer. Overall, the RAG setup is identified as being very well suited for assisting in a wide range of scientific tasks.\nHowever, several cautionary points are raised. RAG systems fundamentally require tweaking; appropriate settings for hyperparameters and methods are highly dependent on the specific corpus and the nature of the questions being asked. Evaluation is crucial and necessitates domain experts to define representative questions and expected answers. A key challenge is the decrease in answer quality when no relevant documents are found, requiring prompt adjustment.\nCounterintuitively, RAG systems often provide worse results for widely discussed overview questions, such as inquiries about the central arguments against scientific realism, compared to more specific factual queries. A hypothesis for this phenomenon is that RAGs tend to focus on the local information present in the retrieved chunks. The prompt directs the model to answer based on this local information, which can inadvertently distract from a broader perspective. This suggests a need for prompt adjustments for different question types.\nUltimately, there is a need for more flexible systems, potentially agentic RAG systems, that can discern between different kinds of questions and adapt their strategy accordingly.\nThe discussion further explores challenges related to philosophical contentiousness and how RAG systems might represent diverse viewpoints, the potential for using LLMs as judges for evaluation, and the specific ways domain expertise influences RAG design, particularly in chunking and defining relevant arguments.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#introduction-to-rag-systems",
    "href": "chapter_ai-nepi_012.html#introduction-to-rag-systems",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.2 Introduction to RAG Systems",
    "text": "13.2 Introduction to RAG Systems\n\n\n\nSlide 01\n\n\nThe presentation is delivered from the perspective of a philosopher of science who has engaged with the technical details of Large Language Model (LLM) systems. The focus is on applying these systems, specifically Retrieval Augmented Generation (RAG), within the domain of philosophy and the broader Humanities, Politics, and Social Sciences (HPSS).\nA core requirement in philosophical research is a high degree of linguistic and semantic accuracy, demanding deep engagement with specific textual corpora.\nTypical research questions in philosophy include inquiries such as “What is Aristotle’s theory of matter in the Physics?” or “Does Einstein’s idea of locality develop from his earlier to his later works?”, referencing specific periods and texts like his relativity works and the 1948 paper on Quantenmechanik und Wirklichkeit. While standard LLMs like ChatGPT can provide decent, differentiated answers to such questions at a general level, they present several problems for rigorous philosophical research.\nStandard LLMs lack direct access to the full text of source corpora. Although texts may have been included in their training data, the models cannot explicitly retrieve or quote them accurately. This often leads to hallucination when specific quotes are requested.\nWhile online search features can sometimes provide access, they are subject to limitations such as copyright restrictions, as encountered with papers like the EPR paper. The training mechanism of LLMs is designed to prevent verbatim learning, focusing instead on generalizable statistical rules of text production, which is counter to the philosophical need for direct engagement with original text sources and their fine-grained formulations.\nFurthermore, standard LLMs have a limited context window, such as the 128,000 tokens available in ChatGPT-4, which is insufficient for processing large philosophical corpora. Finally, there is a significant attribution problem, as standard LLMs do not provide sources or citations for the claims made in their answers. RAG systems are presented as a suitable setup to address these specific problems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-system-architecture-and-problem-solving",
    "href": "chapter_ai-nepi_012.html#rag-system-architecture-and-problem-solving",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.3 RAG System Architecture and Problem Solving",
    "text": "13.3 RAG System Architecture and Problem Solving\n\n\n\nSlide 02\n\n\nThe RAG system architecture is described as a setup capable of solving the identified problems with standard LLMs in philosophical contexts. The process begins with a user initiating a query through an application (APP).\nThe APP then sends a retrieval query to designated data sources. These data sources contain the appropriate corpus, such as Aristotle’s corpus or Einstein’s corpus, and can be implemented using technologies like vector databases or APIs. The retrieval mechanism typically employs semantic search to find relevant text chunks, although hybrid or classic search methods are also viable options.\nThe data sources return the retrieved chunks of text to the APP. The APP then augments the original LLM query by incorporating these retrieved chunks into the prompt.\nThis augmented query is sent to the LLM, which performs text generation based on the provided information. The LLM returns the generated answer to the APP, which finally delivers the answer to the user.\nThis RAG setup directly addresses the problems faced by standard LLMs. It solves the problem of access by providing explicit access to specific texts within the defined corpus, ensuring that the LLM works with the actual source material.\nIt mitigates the problem of the limited context window by providing only the most relevant text chunks to the LLM, effectively managing the input size within the model’s capacity. Furthermore, the RAG system solves the attribution problem by enabling the citation of sources for the provided text chunks, allowing users to verify the basis of the generated claims.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#applications-in-philosophy",
    "href": "chapter_ai-nepi_012.html#applications-in-philosophy",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.4 Applications in Philosophy",
    "text": "13.4 Applications in Philosophy\n\n\n\nSlide 07\n\n\nThe general idea behind applying RAG systems in philosophy is to enable users to chat with philosophical corpora. This interaction style is similar to using ChatGPT but provides significantly more detailed domain knowledge and is grounded in a verbatim text basis from the specified corpus.\nDidactic applications are a key area. RAG systems are useful for students approaching complex philosophical texts, such as Locke’s Essay Concerning Human Understanding.\nThey allow for repeated questioning, enabling students to start with general ideas and progressively delve deeper into specific details, like Locke’s epistemology or his theory of matter. This interactive process provides an instructive method for students to gain a deeper understanding of the texts.\nResearch applications are also emphasized. RAG systems are expected to be important for looking up facts in handbooks, serving functions previously performed by manually consulting books for orientation, remarks, or footnote information.\nThis addresses the unreliability of factual information obtained solely from standard LLMs and necessitates the development of high-quality RAG systems for reliable factual retrieval. Other research uses include exploring corpora that have not been extensively studied, efficiently finding specific passages for close reading, and potentially, in the future, directly finding detailed answers to complex research questions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#example-stanford-encyclopedia-of-philosophy-rag-system",
    "href": "chapter_ai-nepi_012.html#example-stanford-encyclopedia-of-philosophy-rag-system",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.5 Example: Stanford Encyclopedia of Philosophy RAG System",
    "text": "13.5 Example: Stanford Encyclopedia of Philosophy RAG System\n\n\n\nSlide 10\n\n\nAn example RAG system was developed using the Stanford Encyclopedia of Philosophy (SEP) as the data source. The content of the SEP was prepared by scraping it into markdown format. The initial aim of this project was to create a useful tool for the philosophical community.\nThe development process evolved into a qualitative study employing theoretically grounded trial and error. An observation during the initial setup was that a standard RAG configuration, based on typical textbook descriptions involving only retrieval and generation components, produced poor answers.\nThese answers were found to be worse than those obtained by querying a standard LLM like ChatGPT directly without retrieval augmentation.\nThis led to an iterative improvement process involving significant tweaking and optimization. This included tweaking the choice of models, specifically selecting the appropriate generative LLM and embedding model.\nHyperparameters were also tuned, including the number of documents to retrieve (top-k), the maximum input and output token lengths, and the chunk size and overlap used for text processing. Furthermore, methodological complexities were added, such as implementing reranking mechanisms to address issues of retrieval semantic mismatch, where initially retrieved chunks might not be the most relevant.\nThe method for evaluating these improvements was theoretically grounded trial and error, assessing by which measures the answers improved. Sound evaluation standards were identified as crucial.\nA key challenge in evaluating philosophical RAG systems is that the desired answers are typically free, unstructured text rather than simple atomic facts (like asking for Wittgenstein’s last place of living, which yields a city name). Evaluating complex propositions for their factual accuracy is not straightforward and requires significant domain expertise.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-implementation",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-implementation",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.6 SEP RAG System Implementation",
    "text": "13.6 SEP RAG System Implementation\n\n\n\nSlide 13\n\n\nThe implemented SEP RAG system consists of a frontend and a backend, which is written in Python code. The frontend provides a web interface with several configurable options.\nUsers can select the Generative Model to be used, such as gpt-4o-mini. There are settings for the Prompt Token Limit, including the maximum limit supported by the chosen model (e.g., 128000) and a configurable limit for the current session (e.g., 15000). The number of texts to retrieve, corresponding to the top-k value, is also configurable (e.g., 15). A Persona text area allows defining the desired behavior for the LLM, for instance, instructing it to act as “an expert philosopher” who answers “meticulously and precisely.”\nThe frontend includes an input field for the Philosophical Question, where users enter their query (e.g., “What is priority monism?”). A “Generate answer” button triggers the RAG process.\nThe output section is designed for qualitative evaluation, featuring a comparative setup. It displays the “Answer with LLM alone” as a benchmark on one side and the “Answer with RAG” on the other. The RAG answer includes source citation indicators, such as “[Text 0]”, linking parts of the answer to the retrieved texts. A “Benchmark” button is available for comparative evaluation.\nBelow the answers, a “Retrieved Texts Overview” table lists the texts found during the retrieval phase. This table includes columns for file names, section headings, distance metrics, token lengths (length_token for the chunk, total_token for the full section/file), and a flag indicating whether the text was included in the final prompt based on token limits. This table shows the article names and specific section headings that were retrieved and indicates which ones were utilized in the prompt and which were excluded due to prompt length constraints. The backend functionality is implemented in Python code, described as comprising a few thousand lines.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#hyperparameter-tuning-chunk-size",
    "href": "chapter_ai-nepi_012.html#hyperparameter-tuning-chunk-size",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.7 Hyperparameter Tuning: Chunk Size",
    "text": "13.7 Hyperparameter Tuning: Chunk Size\n\n\n\nSlide 15\n\n\nChunk size is identified as a key hyperparameter requiring optimization in RAG system development. Several options exist for defining text chunks.\nOne approach is using a fixed number of words, such as 500 tokens or words. This provides a clean criterion but disregards the inherent structure of the document, such as headings or sections. Alternative methods involve chunking by semantic units like paragraphs or sections, potentially considering different hierarchical levels of sections.\nFor the specific case of the SEP corpus, the optimization process revealed a surprising result: the best performance was achieved by chunking the text into its main sections, including their headings.\nThis finding was unexpected because the average length of SEP sections (approximately 3000 words) significantly exceeded the typical cutoff length of the embedding model used (512 words). Despite this discrepancy, chunking by main sections yielded superior results compared to smaller, fixed-size chunks or paragraphs.\nA hypothesis for this surprising outcome is that the SEP documents are highly systematically ordered. The initial parts of each section often effectively summarize the main theme and key ideas. These crucial introductory segments likely fall within the effective context window of the embedding model, even if the entire section is much longer.\nThis suggests that the structure and organization of the corpus play a significant role in determining the optimal chunking strategy, and this approach might not be as effective for less structured or heterogeneous texts.\nFuture work is planned to explore the use of embedding models with longer context windows, such as Cohere Embed 3, to see if they further improve performance with larger chunks.\nThe key lesson derived from this optimization process is that effective chunking is not a one-size-fits-all solution; it highly depends on the specific characteristics of the corpus being used and the nature of the questions being posed.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#results-discussion-and-challenges",
    "href": "chapter_ai-nepi_012.html#results-discussion-and-challenges",
    "title": "12  RAG Systems in Philosophy and HPSS",
    "section": "13.8 Results, Discussion, and Challenges",
    "text": "13.8 Results, Discussion, and Challenges\n\n\n\nSlide 18\n\n\nThe results and discussion highlight several advantages of RAG systems. They effectively integrate verbatim corpora and domain-specific or special knowledge, leading to more detailed answers and a dramatic reduction in hallucinations compared to standard LLMs.\nRAG systems also enable the citation of relevant documents supporting the generated answer. Overall, the RAG setup is identified as being very well suited for assisting in a wide range of scientific tasks.\nHowever, several cautionary points are raised. RAG systems fundamentally require tweaking; appropriate settings for hyperparameters and methods are highly dependent on the specific corpus and the nature of the questions being asked.\nThe evaluation of RAG systems is crucial and necessitates a representative set of questions along with expected answers. This process essentially requires domain experts, such as philosophers in this context, for both evaluation and initial setup, as the optimal configuration is specific to the domain, the type of corpus, and the kind of questions. A challenge remains regarding how to effectively evaluate RAG performance when dealing with unexplored corpora.\nSeveral challenges are also identified. A decrease in answer quality occurs if no relevant documents are found during retrieval, indicating a need to adjust the prompt in such cases.\nCounterintuitively, RAG systems often provide worse results for widely discussed overview questions, such as inquiries about the central arguments against scientific realism, compared to more specific factual queries. A hypothesis for this phenomenon is that RAGs tend to focus on the local information present in the retrieved chunks. The prompt directs the model to answer based on this local information, which can inadvertently distract from a broader perspective. This suggests a need for prompt adjustments for different question types.\nUltimately, there is a need for more flexible systems, potentially agentic RAG systems, that can discern between different kinds of questions and adapt their strategy accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems in Philosophy and HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Plural pursuit across scales",
    "section": "",
    "text": "14 Plural pursuit across scales",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#overview",
    "href": "chapter_ai-nepi_015.html#overview",
    "title": "13  Plural pursuit across scales",
    "section": "14.1 Overview",
    "text": "14.1 Overview\nThe presentation investigates the structure of quantum gravity research using computational methods to address questions in the philosophy of science, specifically the concept of “plural pursuit”. The core problem is formulating a quantum theory of gravity, which has led to multiple attempted solutions and a situation characterized as plural pursuit. Plural pursuit is defined as distinct yet concurrent instances of normal science dedicated to a common problem-solving goal, where each instance is articulated by a community tied to an intellectual disciplinary matrix. The research empirically tests whether quantum gravity research is an instance of plural pursuit, meaning independent communities pursuing different paradigms in parallel.\nThe methodology involves a bottom-up reconstruction of the research landscape and a top-down comparison with physicists’ intuitions. The bottom-up approach utilizes a dataset of 228,748 abstracts and titles from theoretical physics literature listed on Inspire HEP.\nThis involves a two-step clustering pipeline: linguistic analysis and social network analysis. Linguistic analysis spatializes documents into an embedding space, performs unsupervised clustering to identify 611 fine-grained topics, and assigns specialties to authors based on their most common topic. Social network analysis constructs a co-authorship graph of approximately 30,000 physicists and applies community detection to recover 819 communities.\nA key challenge is that computational notions of topics and communities are scale-dependent. To address this, hierarchical clustering is applied: Ward agglomerative clustering for topics and hierarchical stochastic block modelling for communities, yielding multi-level partitions. An adaptive topic coarse-graining strategy is introduced, based on the Minimum Description Length (MDL) criterion, to select an appropriate scale by merging topics as long as it retains useful information for understanding the social structure. This process reduces the initial 611 topics to 50 coarse-grained topics.\nThe bottom-up results show that the relationship between communities and topics is complex, exhibiting nested structures and lacking a clear one-to-one mapping at fine scales. The coarse-grained topics, derived using the MDL criterion, reveal that some small-scale linguistic topics are preserved due to their relevance to social structure, while others are merged. The analysis of the correlation matrix between these 50 topics and communities across different hierarchical levels indicates that some topics (e.g., string theory) correspond to communities at higher hierarchical levels, while others (e.g., loop quantum gravity) correspond to communities at lower, more fine-grained levels. Observations suggest instances where communities are tied to multiple topics or nested within larger structures, indicating a departure from a simple plural pursuit configuration.\nThe top-down approach surveys founding members of the International Society for Quantum Gravity to elicit their intuitive list of structuring approaches (e.g., string theory, supergravity, causal sets, loop quantum gravity, holography). An SVM classifier is trained on text embeddings (all-MiniLM-L6-v2) using hand-coded labels based on this list to predict which papers belong to which approach.\nComparing the top-down (supervised) approaches to the bottom-up (coarse-grained) topics shows agreement for approaches considered well-defined and conceptually autonomous. However, disagreement exists for phenomenological or less conceptually integrated frameworks. A notable finding is the convergence of the bottom-up analysis regarding string theory, supergravity, and holography. While historically and conceptually distinct, the bottom-up analysis lumps them together in the coarse-grained topic structure, aligning with the intuition of some physicists that the communities working on these areas have significant overlap and are not meaningfully separable at a certain scale.\nThe conclusions state that socio-epistemic systems operate at multiple scales, and notions of communities and disciplinary matrices are scale-dependent. Identifying plural pursuit configurations requires matching these structures across scales. The bottom-up reconstruction in quantum gravity research can confirm or re-assess physicists’ intuitions. The work demonstrates that computational methods can revisit and challenge philosophical insights, acting as a continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#introduction",
    "href": "chapter_ai-nepi_015.html#introduction",
    "title": "13  Plural pursuit across scales",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\n\n\n\nSlide 01\n\n\nThe research represents a joint effort with Mike Schneider from the University of Missouri. The primary objective is to address general questions within the philosophy of science by employing a combination of computational methods and social network analysis techniques. The specific case study chosen for this investigation is quantum gravity.\nA long-standing and significant problem in fundamental physics is the formulation of a quantum theory of gravity. This problem involves reconciling the established knowledge of physical phenomena at small scales, described by quantum mechanics, with the understanding of phenomena at very large scales, described by general relativity.\nNumerous attempted solutions exist to address this challenge. These attempted solutions include prominent approaches such as String theory, Supergravity, Loop quantum gravity, spin foams, Causal set theory, and Asymptotic safety, among others. The existence of multiple concurrent approaches attempting to solve the same fundamental problem leads to the introduction of a conceptual framework termed “plural pursuit”.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#plural-pursuit-definition-and-empirical-question",
    "href": "chapter_ai-nepi_015.html#plural-pursuit-definition-and-empirical-question",
    "title": "13  Plural pursuit across scales",
    "section": "14.3 Plural Pursuit: Definition and Empirical Question",
    "text": "14.3 Plural Pursuit: Definition and Empirical Question\n\n\n\nSlide 01\n\n\nPlural pursuit is defined as a situation characterized by distinct yet concurrent instances of normal science. These instances are dedicated to achieving a common problem-solving goal. In the context of the quantum gravity case study, this common goal is the reconciliation of quantum mechanics and gravitation.\nEach instance of normal science within this framework is articulated by a specific community that is tied to an intellectual disciplinary matrix. This concept aligns with established ideas in the philosophy of science, such as Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes, which describe the structured intellectual and social frameworks guiding scientific research.\nBased on this definition, the research poses an empirical question: Is quantum gravity research an instance of plural pursuit? This question is interpreted as investigating whether the field is composed of independent communities that are pursuing different paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-methodology-data-and-pipeline",
    "href": "chapter_ai-nepi_015.html#bottom-up-methodology-data-and-pipeline",
    "title": "13  Plural pursuit across scales",
    "section": "14.4 Bottom-Up Methodology: Data and Pipeline",
    "text": "14.4 Bottom-Up Methodology: Data and Pipeline\n\n\n\nSlide 03\n\n\nTo address the empirical question, the research proposes performing a bottom-up reconstruction of the research landscape within quantum gravity. This reconstruction aims to capture both the linguistic and intellectual structure of the field, as well as its social structure.\nThe data source for this reconstruction is the Inspire HEP database, from which a dataset comprising 228,748 abstracts and titles of theoretical physics literature was gathered. The analysis proceeds through a two-step clustering pipeline: Linguistic analysis and Social network analysis.\nThe Linguistic analysis component (L) involves several steps. Step L.1 spatializes the documents into an embedding space. Step L.2 performs unsupervised clustering on this embedding space, resulting in an initial partition of the literature into K = 611 topics.\nThis clustering is performed at a very fine-grained level, which is considered necessary to identify niche approaches within quantum gravity that may involve a relatively small number of papers, potentially as few as 100. Step L.3 involves specialty assignment, where each scientist or physicist is assigned a specialty corresponding to the topic that is most common across their publications. This process yields a partition of authors based on the linguistic and intellectual structure derived from the literature.\nThe Social network analysis component (S) begins with Step S.1, constructing a co-authorship graph. In this graph, nodes represent individual physicists, and edges represent co-authorship relationships between them. The network includes approximately 30,000 physicists.\nStep S.2 applies a community detection method to this co-authorship graph, recovering an initial partition of the network into C = 819 communities. This provides a partition of authors that reflects the social structure of the field as captured by collaborative relationships.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#plural-pursuit-mapping-and-challenges",
    "href": "chapter_ai-nepi_015.html#plural-pursuit-mapping-and-challenges",
    "title": "13  Plural pursuit across scales",
    "section": "14.5 Plural Pursuit: Mapping and Challenges",
    "text": "14.5 Plural Pursuit: Mapping and Challenges\n\n\n\nSlide 05\n\n\nWithin the framework of the computational constructs derived from the bottom-up analysis (communities and topics), plural pursuit is conceptualized as a one-to-one mapping between communities and topics. Ideally, this configuration would be represented by a block-diagonal correlation matrix where communities are listed on one axis and topics on the other. Such a matrix structure would imply that each community is entirely dedicated to a single topic, signifying a clear division of labor within the field.\nHowever, applying this concept directly to the very fine-grained partitions (611 topics, 819 communities) results in a correlation matrix that is complex and difficult to interpret, exhibiting a high degree of structure without clear block-diagonal patterns. This complexity arises from inherent issues with the fine-grained partitions.\nThe first issue is the arbitrary nature of the level of fine-graining for topics. For example, a broad research program like string theory, which is intuitively understood as a coherent entity, might be fragmented and scattered across numerous fine-grained topics in the initial clustering. The second issue is that large research programs may be pursued by multiple communities in parallel. This occurs because social communities are shaped not only by intellectual similarity but also by various micro-social processes, such as geographical proximity or local collaborations.\nThese issues fundamentally stem from the fact that the computational notions of both topic and community are scale-dependent. Furthermore, this technical challenge reflects a deeper conceptual problem: research programs themselves are often hierarchically nested. For instance, string theory can be conceptually divided into families and subfamilies like Superstring Theory, which further branches into Type II, Heterotic, Bosonic String Theory, Type I, Type IIA, Type IIB, Heterotic SO(32), and Heterotic E₈ × E₈. Therefore, to accurately identify instances of plural pursuit, it is necessary to address this issue of scale dependence and the inherent ambiguity it introduces.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "title": "13  Plural pursuit across scales",
    "section": "14.6 Hierarchical Reconstruction",
    "text": "14.6 Hierarchical Reconstruction\n\n\n\nSlide 08\n\n\nTo address the scale dependence inherent in the initial fine-grained partitions, the research proposes building a hierarchical reconstruction of the quantum gravity research landscape. This involves applying hierarchical clustering techniques to both the topic and community structures.\nFor the topics, a hierarchical clustering approach is used, specifically the Ward agglomerative clustering algorithm. This method starts with the initial 611 fine-grained topics and iteratively merges them one by one based on an objective function. This process builds a dendrogram representing the hierarchical relationships between topics at different levels of granularity.\nFor the community structure, a hierarchical clustering is also constructed. This is achieved using a hierarchical stochastic block model, which is capable of learning a multi-level partition of the network directly. This model identifies coarser and coarser communities at increasing levels of the hierarchy. The model used is referenced as Peixoto 2014.\nThe application of these hierarchical methods results in hierarchical structures for both topics and communities. These structures induce a notion of scale, which allows researchers to observe and analyze the socio-epistemic system of quantum gravity research at various levels of coarse-graining, moving from very fine distinctions to broader groupings.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-mdl-criterion",
    "href": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-mdl-criterion",
    "title": "13  Plural pursuit across scales",
    "section": "14.7 Adaptive Topic Coarse-Graining: MDL Criterion",
    "text": "14.7 Adaptive Topic Coarse-Graining: MDL Criterion\n\n\n\nSlide 09\n\n\nA challenge arises because the selection of a specific scale at which to observe the community and topic structures remains arbitrary at this stage. The choice of scale significantly impacts the resulting correlation matrix between communities and topics, leading to potentially different interpretations of the research landscape.\nTo address this, the research proposes an adaptive topic coarse-graining strategy. The core idea is to merge the initial K=611 fine-grained topics. This merging process continues only as long as it does not remove information deemed useful for understanding the social structure of the field.\nThis strategy is based on an information theoretic criterion: the Minimum Description Length (MDL) criterion. The MDL criterion seeks to find the partition σ that minimizes the quantity given by the formula arg min_σ [-log P(G|σ) - log P(σ)]. This formula balances two factors: the first term, -log P(G|σ), represents how well the linguistic partition σ explains the social structure, as captured by the graph G; the second term, -log P(σ), represents the complexity of the partition σ itself. The objective is to find a partition that is complex enough to explain the social structure effectively but not overly complex or fine-grained.\nIn practice, this involves navigating the hierarchical topic dendrogram. The process “zooms in” or maintains finer distinctions as long as doing so improves the MDL criterion. It stops coarse-graining when further complexity (finer partitions) no longer yields sufficient information gain about the social structure. Applying this strategy reduces the initial 611 topics to a set of 50 coarse-grained topics.\nAn observation regarding these resulting topics is that certain small-scale linguistic topics from the initial fine-grained partition are preserved. This indicates that these specific nuances, captured at a fine level, are important for understanding the social structure. Conversely, many other topics are lumped together into much larger groupings. This outcome justifies the initial step of starting with a very fine-grained classification, as some of these small topics are indeed relevant for explaining the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-results-topics-and-communities",
    "href": "chapter_ai-nepi_015.html#bottom-up-results-topics-and-communities",
    "title": "13  Plural pursuit across scales",
    "section": "14.8 Bottom-Up Results: Topics and Communities",
    "text": "14.8 Bottom-Up Results: Topics and Communities\n\n\n\nSlide 13\n\n\nThe adaptive coarse-graining strategy results in a set of 50 topics. These topics are assigned labels by retrieving representative engrams, providing some interpretability. The subsequent analysis focuses specifically on those topics identified as relevant to quantum gravity.\nThe core analysis involves confronting these 50 coarse-grained topics with the community structures derived from the hierarchical stochastic block model. This is visualized and analyzed using a correlation matrix, where the columns represent the 50 resulting topics and the rows represent communities identified at different levels of the community hierarchy. For each topic, the analysis attempts to identify the community that best corresponds to or explains that topic across the various hierarchical levels of the community structure.\nObservations from this analysis reveal several patterns. Some topics, such as a large topic identified in purple, do not appear to be tied to a specific community, suggesting they represent concepts or areas broadly relevant across the entire field rather than being the exclusive domain of a particular group. However, other topics show a strong correspondence with specific community structures at certain hierarchical levels.\nFor instance, the topic identified as string theory corresponds well to a community structure found at the third level of the community hierarchy. In contrast, other research programs in quantum gravity, such as loop quantum gravity, appear to correspond to communities found at much lower, more fine-grained levels of the hierarchy.\nFurthermore, the analysis provides evidence that challenges a simple model of plural pursuit characterized by a clear one-to-one mapping and division of labor. Nested structures are observed, such as a smaller community tied to the topic of holography that is itself part of a larger community associated with string theory. This indicates that different scales are entangled and that a clear separation of communities pursuing distinct intellectual domains is not consistently present across the landscape.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-approach-survey-and-classification",
    "href": "chapter_ai-nepi_015.html#top-down-approach-survey-and-classification",
    "title": "13  Plural pursuit across scales",
    "section": "14.9 Top-Down Approach: Survey and Classification",
    "text": "14.9 Top-Down Approach: Survey and Classification\n\n\n\nSlide 15\n\n\nThe second part of the research involves confronting the bottom-up reconstruction of the quantum gravity research landscape with the intuitions held by physicists themselves regarding how their field is structured.\nThis top-down perspective was gathered through a survey administered to the founding members of the International Society for Quantum Gravity. Participants were asked to provide a list of approaches to quantum gravity that they perceived as structuring the overall research landscape. The feedback from the survey, while not entirely unanimous, resulted in a comprehensive list of intuitive approaches, including asymptotic safety, causal sets, dynamical triangulations, group field theory, lqg (loop quantum gravity), spin foams, noncommutative geometry, swampland, modified dispersion relation, dsr, quantum modified bh, shape dynamics, tensor models, string theory, supergravity, and holography.\nFor a more detailed comparison, the research specifically focused on the last three approaches: string theory, supergravity, and holography. This particular focus was motivated by the observation that some physicists surveyed expressed disagreement about whether these should be considered separate approaches, despite their distinct historical origins and conceptual differences.\nTo facilitate the comparison between this intuitive, top-down view and the bottom-up analysis, a classifier was trained to predict which papers belong to which of these intuitive approaches. The classifier used was a Support Vector Machine (SVM). It was trained using text embeddings derived from the titles and abstracts of papers, specifically utilizing the all-MiniLM-L6-v2 model for generating these embeddings. The training data consisted of a set of papers with hand-coded labels indicating their corresponding intuitive approach.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-vs.-bottom-up-comparison",
    "href": "chapter_ai-nepi_015.html#top-down-vs.-bottom-up-comparison",
    "title": "13  Plural pursuit across scales",
    "section": "14.10 Top-Down vs. Bottom-Up Comparison",
    "text": "14.10 Top-Down vs. Bottom-Up Comparison\n\n\n\nSlide 16\n\n\nThe research proceeds to compare the top-down perspective, represented by the supervised classification into intuitive approaches, with the bottom-up reconstruction, specifically the 50 coarse-grained topics. This comparison is visualized using a heatmap where rows represent the top-down (supervised) approaches and columns represent the coarse-grained bottom-up topics.\nThe findings from this comparison reveal varying degrees of agreement. For certain top-down approaches, there is a strong correspondence with specific topics that emerged from the bottom-up analysis. However, for other approaches, the correspondence is weak or non-existent. An explanation for this disagreement is that the bottom-up analysis tends to align well with approaches that are considered well-defined and conceptually autonomous, while it shows less correspondence with approaches that are primarily phenomenological or not yet developed into full-fledged conceptual frameworks.\nA particularly notable finding concerns the relationship between string theory, supergravity, and holography. The bottom-up analysis reveals a large cluster identified as string theory that appears to encompass both supergravity and string theory. This observation converges with the intuition expressed by some physicists in the survey, who struggled with whether supergravity and string theory should be considered separate entities, noting the significant overlap between the communities working on them and questioning whether they could be meaningfully separated. The interpretation is that the coarse-graining process, by stripping away linguistic nuances that do not have consequences for the social structure, groups together areas like supergravity and string theory, even though they are conceptually distinct, thereby reflecting the social reality of highly overlapping communities.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions",
    "href": "chapter_ai-nepi_015.html#conclusions",
    "title": "13  Plural pursuit across scales",
    "section": "14.11 Conclusions",
    "text": "14.11 Conclusions\n\n\n\nSlide 18\n\n\nThe research draws several conclusions regarding the structure of scientific fields and the application of computational methods in the philosophy of science.\nFirstly, it concludes that socio-epistemic systems, which encompass both the social and intellectual aspects of scientific research, can be observed and analyzed at multiple scales. This implies that the fundamental notions of communities and disciplinary matrices, central to understanding the structure of science, are inherently scale-dependent.\nSecondly, identifying configurations of plural pursuit, which ideally involve a one-to-one mapping between communities and their intellectual substrate, necessitates matching these structures across different scales. A simple analysis at a single, arbitrary scale is insufficient to capture the complex relationships present.\nThirdly, specifically in the case of quantum gravity, the bottom-up reconstruction of the research landscape, utilizing data-driven methods, can serve to either confirm or re-assess certain intuitions held by physicists about how their field is structured. The analysis provides empirical evidence that can validate or challenge subjective perceptions.\nMore broadly, the research highlights the increasing power of computational methods as tools that can help revisit or challenge long-standing philosophical insights that were previously based primarily on intuition. This includes intuitions about concepts such as what constitutes a paradigm or a community in a given scientific context. Drawing an analogy, the research suggests that computation can be seen as the continuation of philosophy by other means, paraphrasing Clausewitz.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural pursuit across scales</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "",
    "text": "15 Text Granularity and Topic Model Performance",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overview",
    "href": "chapter_ai-nepi_016.html#overview",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.1 Overview",
    "text": "15.1 Overview\nThis study investigates the impact of text granularity (titles, abstracts, full-texts) on the performance of two distinct topic modeling approaches: Latent Dirichlet Allocation (LDA) and BERTopic. The research addresses the practical challenge of significant resource requirements for obtaining, preprocessing, and analyzing full-text corpora by comparing topic models derived from different text levels.\nA corpus of scientific articles in Astrobiology serves as the material for this study. Six topic models are generated: LDA on titles, abstracts, and full-texts, and BERTopic on titles, abstracts, and full-texts. These models are then analyzed and compared qualitatively and quantitatively using metrics such as Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.\nThe qualitative analysis involves comparing topic coherence and the stability of topics across models, referencing a previously established LDA full-text model with 25 topics and 4 thematic clusters. Quantitative results indicate that title-based models generally perform poorly, while abstract models show better coherence and diversity. Full-text models demonstrate superior joint recall.\nSpecifically, BERTopic Abstract emerges as a strong performer in coherence, and BERTopic Title in diversity, while LDA Fulltext and BERTopic Fulltext excel in joint recall. The study concludes that the optimal choice of text level and topic model depends on specific research objectives. Abstract-based models offer a good balance and consistency with full-text models, while title-based models, despite limitations, can identify robust core topics. The potential for leveraging structural information (titles, abstracts, full-texts) in future models is also discussed.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#introduction",
    "href": "chapter_ai-nepi_016.html#introduction",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.2 Introduction",
    "text": "15.2 Introduction\n\n\n\nSlide 01\n\n\nThis presentation is delivered by Francis Lareau, a Postdoctoral Fellow affiliated with the University of Sherbrooke and the University of Quebec in Montreal (UQAM). This work is a comparative study conducted with Christophe Malaterre from the University of Quebec in Montreal.\nThe study focuses on topic modeling, a technique for extracting themes from a corpus. Topic modeling is recognized as an important tool for analyzing large volumes of scientific literature, especially within the history, philosophy, and sociology of science (HPSS).\nA problem arises because existing studies utilize different textual structures for topic modeling, namely titles, abstracts, and full text. Obtaining, preprocessing, and analyzing full-text corpora demand significant resources. This prompts the central research question: Is applying topic modeling to titles or abstracts sufficient, or is full-text analysis necessary?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#study-design",
    "href": "chapter_ai-nepi_016.html#study-design",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.3 Study Design",
    "text": "15.3 Study Design\n\n\n\nSlide 02\n\n\nThis study addresses the pressing question of whether analyzing titles or abstracts is sufficient for topic modeling, given the substantial resources needed for full-text corpora acquisition, preprocessing, and analysis. The methodology involves a structured workflow.\nFirst, a corpus of scientific articles is constituted. Second, the distinct title, abstract, and full text sections are identified within this corpus. Third, two different topic modeling approaches, Latent Dirichlet Allocation (LDA) and BERTopic, are applied separately to each of the three identified text levels: titles, abstracts, and full texts.\nThis process generates a total of six distinct topic models. Finally, these six resulting topic models undergo both qualitative and quantitative analysis and comparison to evaluate their performance across the different text levels.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modeling-approaches",
    "href": "chapter_ai-nepi_016.html#topic-modeling-approaches",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.4 Topic Modeling Approaches",
    "text": "15.4 Topic Modeling Approaches\n\n\n\nSlide 03\n\n\nThe study compares two distinct topic modeling approaches: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches share fundamental postulates: documents can be represented by numerical vectors, topics are identifiable through linguistic regularities manifested as repetitions, and machine learning facilitates the automatic detection of these regularities.\nLatent Dirichlet Allocation (LDA) is characterized as a classical statistical method. It employs a classical vector representation technique based on counting words within documents. In the LDA framework, topics are conceptualized as latent variables that adhere to Dirichlet’s law. A key advantage of LDA is its ability to handle long texts, making it suitable for analysis across titles, abstracts, and full texts.\nIn contrast, BERTopic is described as a modern, modular approach, developed by Martin Grootendorst. It utilizes an LLM-based vector representation method, originally based on BERT, which gives the approach its name. Topics in BERTopic correspond to topological densities of documents, typically identified using clustering algorithms like HDBSCAN.\nHistorically, BERTopic did not handle long texts efficiently, but recent advancements have addressed this limitation. For this study, a specific embedding model, Stella EN 1.5B V5, was selected for the BERTopic implementation. This model was chosen based on its high ranking on the Massive Text Embedding Benchmark on Hugging Face and its capacity to handle approximately 131,000 tokens, addressing the long text limitation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#material-and-qualitative-analysis",
    "href": "chapter_ai-nepi_016.html#material-and-qualitative-analysis",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.5 Material and Qualitative Analysis",
    "text": "15.5 Material and Qualitative Analysis\n\n\n\nSlide 06\n\n\nThe material utilized in this study is an Astrobiology corpus, which was previously subjected to an in-depth topic analysis. This prior analysis, documented in Malaterre & Lareau (2023), resulted in the selection of a full-text LDA model comprising 25 topics. This model serves as a reference for the current comparative study.\nThe 25 topics of the reference model were analyzed meticulously. This analysis involved examining the most representative words and documents associated with each topic. Based on this examination, each topic was assigned a descriptive label derived from its key terms.\nThe relationships between topics were then assessed by calculating their mutual correlation, determined by the co-occurrence of topics within documents. Subsequently, a community detection algorithm was applied to the correlation data, identifying four distinct thematic clusters. These clusters were designated with letters (A, B, C, D) and assigned corresponding colors (red, green, yellow, blue) for visualization.\nThe results of this reference analysis are represented visually as a graph illustrating the correlations between the 25 topics. The graph displays the topic labels and the color variations indicating their thematic clusters. The thickness of the lines connecting topics represents the strength of their correlation, while the size of the circles representing topics indicates their overall presence across all documents in the corpus. This established and analyzed reference model provides a basis for qualitatively comparing the six topic models generated and investigated in the current comparative study.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-metrics",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.6 Quantitative Metrics",
    "text": "15.6 Quantitative Metrics\n\n\n\nSlide 07\n\n\nThe quantitative analysis compares the six topic models using four specific metrics:\n\nAdjusted Rand Index: This metric assesses the similarity between two different document clusterings. It is corrected for chance, meaning a value of zero corresponds to a random clustering.\nTopic Diversity: This measures the proportion of distinct top words utilized to describe the topics within a single topic model. A higher diversity indicates that topics are characterized by different sets of words.\nJoint Recall: This evaluates the extent to which the top words collectively represent the documents assigned to each topic. It provides an average measure of document-topic recall, indicating how well the topic’s representative words can retrieve the documents belonging to that topic.\nCoherence CV: This evaluates the semantic meaningfulness of the topic’s top words. It is computed as the average of the cosine relative distance between the top words within each topic, where a higher value suggests greater semantic relatedness among the words and thus more coherent topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#adjusted-rand-index-results",
    "href": "chapter_ai-nepi_016.html#adjusted-rand-index-results",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.7 Adjusted Rand Index Results",
    "text": "15.7 Adjusted Rand Index Results\n\n\n\nSlide 08\n\n\nThe Adjusted Rand Index is employed to quantitatively assess the similarities among the six generated topic models by comparing their document clusterings. A value of zero for this metric signifies a random clustering.\nThe results indicate that the LDA model trained on titles is the most distinct among all models, as evidenced by low Adjusted Rand Index values, specifically under 0.2, shown in the heatmap comparison.\nConversely, all other models exhibit a better overall match with each other, demonstrating values exceeding 0.2. A notable observation is that the BERTopic models tend to correspond more strongly with each other, with Adjusted Rand Index values generally above 0.35. Furthermore, the BERTopic Abstract model appears to be more central in its similarity profile, showing good correspondence to every other model, with values over 0.30, except for the LDA Title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#lda-model-comparison",
    "href": "chapter_ai-nepi_016.html#lda-model-comparison",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.8 LDA Model Comparison",
    "text": "15.8 LDA Model Comparison\n\n\n\nSlide 09\n\n\nA more detailed qualitative analysis focuses on comparing the LDA Full-text model with the LDA Abstract and LDA Title models, using heatmaps that visualize the number of shared documents between topics. A reddish diagonal pattern in these heatmaps signifies a good correspondence between topics across models.\nComparing the LDA Full-text model with the LDA Abstract model (Table A) reveals a good overall fit. This is evident from the prominent reddish diagonal, indicating that topics in one model largely correspond to single topics in the other with a high proportion of shared documents.\nHowever, the analysis also identifies specific topic transformations: three full-text topics disappear entirely in the abstract model (represented by long horizontal dark gray lines), three full-text topics split into multiple topics in the abstract model (short horizontal dark gray lines), and three abstract topics are formed by the merger of multiple full-text topics (short horizontal dark gray lines). Additionally, the LDA Abstract model exhibits one small class containing fewer than 50 documents.\nIn contrast, the comparison between the LDA Full-text model and the LDA Title model (Table B) indicates a poor overall fit, characterized by substantial reorganization of topics. This is visually represented by numerous vertical and horizontal dark lines in the heatmap, signifying that many full-text topics disappear and many new topics emerge in the title model, with little direct correspondence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#bertopic-model-comparison",
    "href": "chapter_ai-nepi_016.html#bertopic-model-comparison",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.9 BERTopic Model Comparison",
    "text": "15.9 BERTopic Model Comparison\n\n\n\nSlide 11\n\n\nThe qualitative comparison extends to the BERTopic models, assessing their correspondence with the LDA Full-text reference model using heatmaps showing shared documents.\nComparing the LDA Full-text model with the BERTopic Full-text model (Table C) shows an average overall fit. The analysis reveals that 8 topics from the LDA Full-text model disappear, and 6 topics split into multiple topics in the BERTopic Full-text model (indicated on the horizontal axis). On the vertical axis, 5 new topics appear in the BERTopic model, and 1 topic results from the merger of LDA topics. The BERTopic Full-text model also exhibits issues with class size, including 4 small classes and 1 very large class.\nThe comparison between the LDA Full-text model and the BERTopic Abstract model (Table D) indicates a relatively good overall fit. Four LDA topics disappear, and 6 topics split in the BERTopic Abstract model (horizontal axis). Two new topics appear, and 4 topics result from mergers in the BERTopic Abstract model (vertical axis).\nFinally, comparing the LDA Full-text model with the BERTopic Title model (Table E) shows an average overall fit. Seven LDA topics disappear, and 1 topic splits in the BERTopic Title model (horizontal axis). Seven new topics appear, and 1 topic results from a merger in the BERTopic Title model (vertical axis). The BERTopic Title model also presents class size issues, with 3 small classes and 1 large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparing-top-words",
    "href": "chapter_ai-nepi_016.html#comparing-top-words",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.10 Comparing Top Words",
    "text": "15.10 Comparing Top Words\n\n\n\nSlide 13\n\n\nA qualitative assessment involves comparing the top words associated with selected topics across the different models to understand the nature of the topics generated.\nWithin the LDA models (Full-text, Abstract, and Title), topics are observed to be relatively well-formed. A robust topic, exemplified by “A radiation spore,” demonstrates good correspondence in its top words across all three LDA models.\nSplitting of topics is also observed: the “A life civilization” topic from the full-text model splits across the abstract and title models, which is considered sensible as it relates to a general theme of research in astrobiology. The “B chemistry” topic from the full-text model also splits across the abstract and title models, though this particular split is noted as being more challenging to interpret without deeper analysis. Merging of topics occurs as well, such as the “B amino acid” and “B protein gene RNA” topics from the full-text model merging into a single topic in other models, which is deemed sensible as it forms a more general thematic area.\nComparing the BERTopic models (Full-text, Abstract, and Title) with the LDA Full-text model also reveals relatively well-formed topics across all BERTopic models. The robustness of the “A radiation spore” topic is again observed, appearing consistently across all BERTopic models and the LDA Full-text reference. The “A life civilization” topic is relatively stable across the BERTopic models, although some splitting occurs, leading to narrower topics specifically focused on extraterrestrial life. The “B chemistry” topic also splits across the BERTopic models, resulting in more narrow thematic topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#coherence-diversity-and-joint-recall-results",
    "href": "chapter_ai-nepi_016.html#coherence-diversity-and-joint-recall-results",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.11 Coherence, Diversity, and Joint Recall Results",
    "text": "15.11 Coherence, Diversity, and Joint Recall Results\n\n\n\nSlide 15\n\n\nQuantitative performance metrics are evaluated for all six topic models across a range of topic numbers, specifically from 5 to 50.\nThe Coherence CV metric, which assesses the meaningfulness of the topic top words, yields several findings:\n\nModels trained on titles exhibit the worst coherence.\nAbstract models demonstrate better coherence compared to full-text models.\nOverall, BERTopic models show better coherence than LDA models when applied to abstracts and titles, although this difference becomes less pronounced as the number of topics increases.\nBased on this metric, BERTopic Abstract is identified as the clear winner.\n\nRegarding Topic Diversity, which measures the proportion of distinct top words, the results show that diversity generally decreases as the number of topics increases:\n\nModels trained on titles offer the best diversity.\nBERTopic models exhibit better diversity than LDA models.\nThe winner for diversity is BERTopic Title, closely followed by BERTopic Full-text.\n\nThe Joint Recall metric evaluates how effectively the top words collectively represent the documents classified within each topic:\n\nTitles yield the worst joint recall.\nFull-text models perform better than their abstract and title counterparts.\nLDA models generally show better joint recall than BERTopic models.\nThe winners for Joint Recall are LDA Fulltext and BERTopic Fulltext, with BERTopic Abstract performing very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#model-performance-summary",
    "href": "chapter_ai-nepi_016.html#model-performance-summary",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.12 Model Performance Summary",
    "text": "15.12 Model Performance Summary\n\n\n\nSlide 17\n\n\nA summary table consolidates the performance results, offering an overall view of each model’s strengths and weaknesses across various assessment criteria. The models evaluated are:\n\nLDA Full-text (rated 4*)\nLDA Abstract (4.5*)\nLDA Title (2.5*)\nBERTopic Full-text (4.5*)\nBERTopic Abstract (4.5*)\nBERTopic Title (3*)\n\nThe assessment criteria include Overall fit, Top-words quality, Coherence, Diversity, and Joint recall. Performance scores are visually represented using circle icons, where a full black circle signifies the highest score and an empty circle indicates a low score. Red crosses highlight specific problems, such as class imbalance.\nThe analysis indicates that there is no single absolute best model; the optimal choice is contingent upon the specific research objectives. Different objectives necessitate different model characteristics. For instance, if the primary goal is the discovery of main topics without requiring precise classification of every document, issues like poor recall or large classes might be acceptable. In such a scenario, the BERTopic Full-text model performs well, although it exhibits some class imbalance. The BERTopic Title model, while generally less optimal, is capable of producing some robust topics that are also identified by the other models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "title": "14  Text Granularity and Topic Model Performance",
    "section": "15.13 Discussion and Future Directions",
    "text": "15.13 Discussion and Future Directions\nThe discussion highlights several key observations and potential future directions. The poor performance observed in title-based models is primarily attributed to the inherent lack of information in titles compared to abstracts or full texts. This limitation can lead to inaccurate classification of documents, although title models are still capable of identifying meaningful core topics.\nFull-text models exhibit distinct characteristics depending on the approach. LDA models applied to full text tend to produce topics that are more loosely defined and broader in coverage, potentially capturing transverse themes such as research methods. BERTopic full-text models, on the other hand, may result in some topics being too narrow, leading to poor document coverage, and can suffer from class-size imbalance problems.\nAbstract models demonstrate notable consistency, both between the LDA and BERTopic implementations and in their correspondence with the LDA full-text model. A significant finding is the overall robustness of topics, with very similar thematic areas being identified across the board, regardless of the specific model or text level used.\nFuture research possibilities include exploiting meta-analysis techniques to systematically identify the most robust topics that consistently appear across multiple models and text levels. Another direction involves using relative distance metrics to determine which model is the most central or representative among the set. Furthermore, the study suggests the potential for developing new topic modeling approaches that explicitly leverage the structural information present in documents (i.e., the distinction between full text, abstract, and titles) to extract more meaningful sets of topics or top words.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Text Granularity and Topic Model Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models",
    "section": "",
    "text": "16 Time-Aware Language Models",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#overview",
    "href": "chapter_ai-nepi_017.html#overview",
    "title": "15  Time-Aware Language Models",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nThe presentation describes the development and evaluation of a novel architecture for creating time-aware language models (TALMs), specifically targeting applications in historical analysis. The core problem addressed is the implicit nature of temporal understanding in current Large Language Models (LLMs), which is derived statistically from training data. The proposed solution involves explicitly adding a temporal dimension to the latent semantic token features within a Transformer architecture.\nThe technical approach modifies a standard Transformer decoder model by injecting time data, represented as a non-trainable, min-max normalized day of the year, into the token embeddings. This allows the model to learn how the probability distribution of tokens depends on time.\nA proof-of-concept implementation utilizes a small generative LLM trained on a specific dataset: daily weather reports from the UK Met Office digital archive for the years 2018-2024. This dataset consists of approximately 2,500 reports, each 150-200 words long, characterized by a limited vocabulary and repetitive language. The text processing involves text vectorization with standardization (lower and strip punctuation) and no sub-word tokenization, resulting in a vocabulary of 3,395 words.\nThe model architecture is a modest-sized Transformer decoder with 4 multihead attention blocks, totaling 39 million parameters (150 MB). This is significantly smaller than models like GPT-4 (1.8 trillion parameters across 120 layers). Training is performed on 2 x A100 GPUs, taking 11 seconds per epoch. The code for the vanilla and time-aware Transformer models is available on GitHub at https://github.com/j-buettner/time_transformer.\nTwo experiments demonstrate the model’s ability to learn temporal drift:\n\nSynonymic Succession: Synthetic drift is injected by time-dependent replacement of “rain” with “liquid sunshine” following a sigmoid probability curve over the year. The model successfully reproduces this time dependence in predicted token sequences.\nChanging Co-occurrence/Collocation Fixation: Synthetic time-dependent change is injected where “rain” followed by any word except “and” is replaced by “rain and snow” with increasing probability over the year. The model learns this changing co-occurrence pattern, demonstrating the fixation of the “rain and snow” collocation over time. Attention analysis shows increased attention from “snow” to “rain” in the predicted sequences.\n\nThe proof of concept indicates that Transformer-based LLMs can be made time-aware efficiently by adding a temporal dimension to the token embedding. Potential applications include providing a foundation for downstream tasks on historical data, enabling instruction-tuned models to “talk to a specific time,” and modeling dependence on other metadata dimensions (country, genre). Challenges include uncertainty regarding the efficiency of fine-tuning due to architectural changes, the need for data curation (including timestamping token sequences), and the loss of metadata-free self-supervised learning benefits. An alternative approach involving a targeted encoder model or changing the training task (e.g., predicting document date) is also considered.\nDiscussion points include the potential for modeling semantic shift over time using this approach, the importance of persistent identifiers for source tracking, existing literature on time-aware LLMs and semantic change detection (specifically mentioning encoder-based models and “temporal heads” in foundational models), and a theoretical discussion on whether explicit time injection is necessary given that temporal information is implicitly present in other factors.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#motivation-for-time-aware-language-models",
    "href": "chapter_ai-nepi_017.html#motivation-for-time-aware-language-models",
    "title": "15  Time-Aware Language Models",
    "section": "16.2 Motivation for Time-Aware Language Models",
    "text": "16.2 Motivation for Time-Aware Language Models\n\n\n\nSlide 01\n\n\nCurrent Large Language Models (LLMs) exhibit only an implicit understanding of time. This temporal understanding is derived statistically from the patterns observed within their training data. Introducing explicit time awareness into these models is identified as a beneficial enhancement, particularly for their application in historical analysis and potentially other fields where temporal context is crucial.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#text-processing-architectures",
    "href": "chapter_ai-nepi_017.html#text-processing-architectures",
    "title": "15  Time-Aware Language Models",
    "section": "16.3 Text Processing Architectures",
    "text": "16.3 Text Processing Architectures\n\n\n\nSlide 01\n\n\nThe primary neural network architectures employed for processing text have evolved. Historically, around 2017, Long Short-Term Memory (LSTM) networks were the dominant architecture for tasks such as next-token prediction. As of approximately 2025, the landscape has shifted, with Transformer networks becoming the primary architecture utilized for next-token prediction and other text processing tasks.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#explicit-time-awareness",
    "href": "chapter_ai-nepi_017.html#explicit-time-awareness",
    "title": "15  Time-Aware Language Models",
    "section": "16.4 Explicit Time Awareness",
    "text": "16.4 Explicit Time Awareness\n\n\n\nSlide 04\n\n\nThe concept is that Large Language Models can be endowed with explicit time awareness. This involves enabling the models to learn and subsequently reproduce patterns within their training data that change as a function of time. A proof of concept for this approach is based on the implementation using a small generative LLM.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#temporal-dependence-of-token-probabilities",
    "href": "chapter_ai-nepi_017.html#temporal-dependence-of-token-probabilities",
    "title": "15  Time-Aware Language Models",
    "section": "16.5 Temporal Dependence of Token Probabilities",
    "text": "16.5 Temporal Dependence of Token Probabilities\n\n\n\nSlide 05\n\n\nStandard Large Language Models estimate the probability distribution over their vocabulary for the next token, \\(x_n\\), conditioned on a sequence of preceding tokens, \\(x_1, ..., x_{n-1}\\). This is formally expressed as \\(p(x_n | x_1, ..., x_{n-1})\\).\nHowever, in real-world scenarios, the probability of a token given its context is not static; it is dependent on time, \\(t\\). This temporal dependence is represented as \\(p(x_n | x_1, ..., x_{n-1}, t)\\). Consequently, the probability of an entire sequence of tokens, \\(x_1, x_2, ..., x_n\\), generated at a specific time \\(t\\), is the product of these time-dependent conditional probabilities for each token in the sequence: \\(p(x_1, x_2, ..., x_n | t) = \\prod_{k=1}^{n} p(x_k | x_1, x_2, ..., x_{k-1}, t)\\). During inference, current LLMs can only reflect the temporal drift observed in the underlying distribution of token sequences through in-context learning, which is an implicit mechanism.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#modeling-time-dependent-probabilities",
    "href": "chapter_ai-nepi_017.html#modeling-time-dependent-probabilities",
    "title": "15  Time-Aware Language Models",
    "section": "16.6 Modeling Time-Dependent Probabilities",
    "text": "16.6 Modeling Time-Dependent Probabilities\n\n\n\nSlide 07\n\n\nA key challenge is explicitly modeling the time-dependent probability of the next token, \\(p(x_n | x_1, ..., x_{n-1}, t)\\). An approach involving time slicing, where separate models are trained for distinct time periods, is considered extremely data inefficient.\nThe proposed solution is a Time Transformer architecture. This method involves adding a temporal dimension to the latent semantic features of each token. The combined embedding for a token \\(x\\) at time \\(t\\) is represented as a vector \\(E(x, t) = \\{e_1(x), e_2(x), ..., e_{d-1}(x), \\phi(t)\\}\\), where \\(e_i(x)\\) are the standard semantic features and \\(\\phi(t)\\) is a feature representing time. This sequence of time-aware embeddings, \\([E(x_1, t), E(x_2, t), ..., E(x_{n-1}, t)]\\), is then fed into a Transformer model to predict the time-dependent probability of the next token, \\(p_\\theta(x_n | x_1, ..., x_{n-1}, t)\\).\nThe training objective for this model is to minimize the negative log-likelihood across the entire dataset, given by \\(\\min_\\theta - \\sum_{i=1}^N \\sum_{k=1}^{n^{(i)}} \\log p_\\theta(x_k^{(i)} | x_1^{(i)}, ..., x_{k-1}^{(i)}, t^{(i)})\\), where the summation is over all sequences \\(i\\) in the dataset, each with length \\(n^{(i)}\\) and associated time \\(t^{(i)}\\). This approach injects time directly into the representation of every token, enabling the model to learn precisely how strongly or weakly the temporal dimension influences each token’s probability.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#data-source-and-preparation",
    "href": "chapter_ai-nepi_017.html#data-source-and-preparation",
    "title": "15  Time-Aware Language Models",
    "section": "16.7 Data Source and Preparation",
    "text": "16.7 Data Source and Preparation\n\n\n\nSlide 10\n\n\nThe data source utilized for the proof-of-concept implementation consists of Met Office Weather reports. This dataset is characterized by a limited vocabulary and the use of simple, repetitive language, making it suitable for initial experimentation with a small model. The reports are provided by the Met Office, the UK’s national meteorological service, and past reports are accessible through their digital archive at https://digital.nmla.metoffice.gov.uk/.\nThe specific dataset used comprises daily reports covering the years 2018 through 2024, totaling approximately 2,500 reports. Each report is between 150 and 200 words in length. Text processing is performed using tf.keras.layers.TextVectorization with the standardization setting standardize=\"lower_and_strip_punctuation\". This process involves neglecting case and interpunctuation, and notably, no sub-word tokenization is applied. This results in a vocabulary size of 3,395 unique words. The choice of a small model and dataset relates conceptually to research exploring the capabilities of small language models, such as the work described in the paper “TinyStories: How Small Can Language Models Be and Still Speak Coherent English?”.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#transformer-model-architecture-and-training",
    "href": "chapter_ai-nepi_017.html#transformer-model-architecture-and-training",
    "title": "15  Time-Aware Language Models",
    "section": "16.8 Transformer Model Architecture and Training",
    "text": "16.8 Transformer Model Architecture and Training\n\n\n\nSlide 12\n\n\nThe baseline model architecture is a modest-sized Transformer decoder. It consists of an Embedding Layer, Positional Encoding, Dropout, four Decoder Layers, and a Final Dense Layer. Each Decoder Layer incorporates Multi-Head Attention, followed by Add & Norm, a Feed-Forward Network (FFN), and another Add & Norm step.\nThe model has a total of 39 million parameters, occupying approximately 150 MB, which is considerably smaller than large models like GPT-4, which has 1.8 trillion parameters distributed across 120 layers. Training is conducted using 2 x A100 GPUs, achieving a speed of 11 seconds per epoch. The code implementation for this model is available on GitHub at https://github.com/j-buettner/time_transformer.\nThe training process demonstrates that the model learns to reproduce the language style and patterns of the weather report dataset effectively. For instance, given the seed sequence “During the night, a band …”, the model generates text such as “… of rain moved into scotland northern ireland and northern england outbreaks of rain continued to move across northern england and wales it stayed largely dry with clear spells and a few scattered showers in the north and west elsewhere there were plenty of clear spells and a few fog patches and overall it was a mild night across the south of the uk ….”. Model performance is tracked using accuracy, visualized in a line graph showing training and validation accuracy over 50 epochs. The training accuracy exhibits a steady increase, while the validation accuracy increases initially before plateauing.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#time-transformer-architecture",
    "title": "15  Time-Aware Language Models",
    "section": "16.9 Time Transformer Architecture",
    "text": "16.9 Time Transformer Architecture\n\n\n\nSlide 15\n\n\nThe Time Transformer architecture is created with a minimal adjustment to the vanilla Transformer model. The input now includes both Text Input and Time Data. The Text Input is processed by an Embedding Layer, while the Time Data is processed by a dedicated Time Embedding layer.\nThe outputs from the standard Embedding Layer and the Time Embedding layer are combined. This combined embedding is then fed into the Casual Masking and Positional Encoding layers. The subsequent layers, including the Decoder Layers and the Final Dense Layer, retain the same structure as the vanilla model. The time dimension is represented as a non-trainable, min-max normalized value corresponding to the day of the year. The time embedding is calculated using the formula time embedding = (day of year - 1) / (365 - 1), normalizing the day of the year (1 to 365) to a range between 0 and 1.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-1-learning-synonymic-succession",
    "href": "chapter_ai-nepi_017.html#experiment-1-learning-synonymic-succession",
    "title": "15  Time-Aware Language Models",
    "section": "16.10 Experiment 1: Learning Synonymic Succession",
    "text": "16.10 Experiment 1: Learning Synonymic Succession\n\n\n\nSlide 16\n\n\nThe first experiment aims to evaluate the model’s ability to efficiently learn temporal drift within the underlying data distribution, specifically focusing on synonymic succession. This involves injecting synthetic drift into the training data by implementing a time-dependent replacement rule: the word “rain” is replaced by the phrase “liquid sunshine”. The probability of this replacement occurring follows a sigmoid curve across the days of the year, ranging from Day 0 to Day 365. The probability starts near 0.00 at the beginning of the year and increases to near 1.00 by the end of the year, with the most significant increase occurring around Day 180.\nThe evaluation method involves checking whether this injected time dependence is accurately reproduced in the token sequences predicted by the model, generating a separate sequence for each day of the year. The presentation also notes observed seasonal patterns in the real weather data, illustrated by bar charts. One chart shows the monthly occurrences of “Rain and Snow” versus “Rain Only”, indicating that “Rain and Snow” is more frequent in the later months (September to December), while “Rain Only” is more frequent in the earlier months (January to April). Another chart depicts the monthly occurrences of terms related to heat (“Hot”, “Warm”) versus terms related to cold and snow (“Snow”, “Sleet”, “Wintry”), clearly showing that “Snow”, “Sleet”, and “Wintry” terms are more prevalent in the winter months (January, February, March, November, December), while “Hot” and “Warm” terms are more frequent in the summer months (June, July, August).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-2-learning-changing-co-occurrence",
    "href": "chapter_ai-nepi_017.html#experiment-2-learning-changing-co-occurrence",
    "title": "15  Time-Aware Language Models",
    "section": "16.11 Experiment 2: Learning Changing Co-occurrence",
    "text": "16.11 Experiment 2: Learning Changing Co-occurrence\n\n\n\nSlide 18\n\n\nThe second experiment also aims to evaluate the efficient learning of temporal drift, focusing on a changing co-occurrence pattern, specifically termed ‘fixation of a collocation’ from a variable to an obligatory relationship. This involves injecting a synthetic time-dependent change into the training data: any instance of the word “Rain” followed by any word other than “and” is synthetically altered to become “rain and snow”. This process is described as analogous to the linguistic concept of the fixation of a collocation, citing “bread and butter” as a common example.\nThe evaluation is presented through a bar chart titled “Monthly Comparison of”Rain and Snow” vs. “Rain Only” Occurrences” based on the predicted sequences generated for each day of the year. The y-axis quantifies the occurrences of “rain” only versus “rain and snow” in these predictions. The chart visually demonstrates that the blue bars representing “Rain and Snow” occurrences are higher in the later months (September to December), while the green bars representing “Rain Only” occurrences are higher in the earlier months (January to April), effectively reproducing the injected synthetic drift pattern. Example predicted sequences for Day 1 and Day 363 illustrate this learned pattern, both showing “heavy rain and snow”. Further analysis is presented via an attention chart titled “Attention from ‘snow’ to previous 10 tokens (Head 5)”. This bar chart displays the attention weights from the token ‘snow’ to the preceding 10 tokens. The bar corresponding to “rain” exhibits the highest attention weight, approximately 0.45, indicating that the model has learned a strong associative link between “rain” and “snow” within this specific context.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "title": "15  Time-Aware Language Models",
    "section": "16.12 Proof of Concept, Applications, and Challenges",
    "text": "16.12 Proof of Concept, Applications, and Challenges\n\n\n\nSlide 21\n\n\nThe proof of concept demonstrates that Transformer-based Large Language Models can be efficiently made time-aware by augmenting the token embedding with a temporal dimension. This approach offers several potential applications. A foundational Time Transformer could serve as an excellent base model for various downstream tasks involving historical data.\nFurthermore, an instruction-tuned Time Transformer could enable users to interact with the model by specifying a particular time period, effectively allowing them to “talk to a specific time.” This capability might also lead to improved results in standard usage scenarios where the model is expected to reflect the present state of knowledge or language (“talk to the present”). The methodology is also potentially generalizable, suggesting that the dependence of underlying token sequence distributions on other contextual or metadata dimensions, such as country or genre, could be modeled using a similar approach.\nPotential next steps for this research include benchmarking the Time Transformer against alternative methods, such as an explicit time-token approach, and testing whether the proposed architecture leads to an increase in training efficiency. Further exploration of other aspects is also warranted.\nHowever, several challenges must be addressed for practical application. It is currently unclear whether fine-tuning the modified architecture is possible and efficient. The approach also necessitates significant data curation, particularly the accurate determination of the generation time for each token sequence, which represents a loss of the metadata-free benefit typically associated with self-supervised learning. Accurately timestamping historical data is identified as a key challenge. An alternative direction considered is the development of a modest, targeted encoder model for time-aware tasks.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "",
    "text": "17 LLMs for Chemical Knowledge Analysis",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#overview",
    "href": "chapter_ai-nepi_018.html#overview",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.1 Overview",
    "text": "17.1 Overview\nThe work focuses on leveraging Large Language Models (LLMs) for metadata enrichment and diachronic analysis of chemical knowledge within a large corpus of historical scientific texts. The primary objective is divided into two parts. Part I involves using LLMs to improve metadata for historical texts, specifically focusing on categorizing articles by scientific discipline and semantic tags (topics), and generating abstractive summaries. Part II presents a case study analyzing the evolution of the chemical space over time across different scientific disciplines, aiming to identify periods of interdisciplinarity and knowledge transfer.\nThe research utilizes the Royal Society Corpus (RSC) 6.0 Full, a diachronic corpus spanning over 300 years of scientific writing (1665-1996). This corpus contains almost 48,000 texts and nearly 300 million tokens. It was created with steps to improve OCR quality and correct spelling.\nFor metadata enrichment (Part I), the Hermes-2-Pro-Llama-3-8B model, a fine-tuned variant of Llama 3 optimized for structured output (JSON, YAML), is employed. The LLM acts as a “librarian” following a detailed system prompt that defines its role, objective, input format (OCR text, existing metadata like title, author, date, journal, text snippet), and specific tasks.\nThe tasks include suggesting alternative titles, writing 3-4 sentence TL;DR summaries for a high school level, identifying exactly five main topics (like Wikipedia keywords), and classifying the primary scientific discipline from a predefined list of nine categories (Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Social Sciences & Humanities) and a suitable second-level sub-discipline (not from the primary list). The expected output format is YAML.\nValidation checks show 99.81% valid YAML output and 94% of predicted primary disciplines fall within the predefined set. Some instances of hallucination were observed, such as “Earth Sciences” instead of “Environmental & Earth Sciences” or inventing “Music” as a discipline. Initial analysis of the LLM-classified data shows discipline distribution over time, highlighting the rise of chemistry around the late 18th century (chemical revolution) and the prominence of biology, physics, and chemistry from the 19th century onwards. A t-SNE projection of TL;DR summaries visualizes the distribution and overlap of disciplines in semantic space.\nFor the diachronic analysis of the chemical space (Part II), the focus is on chemistry, biology, and physics. Chemical terms are extracted using ChemDataExtractor, a Python module. A two-pass application of ChemDataExtractor is used: first on the whole text, then a second pass on the initial list of extracted substances to reduce noise, particularly in earlier periods.\nKullback-Leibler divergence (KLD) is the primary method for analyzing the chemical space evolution. KLD is applied in two ways: independently per discipline to trace evolution over time (comparing 20-year windows sliding by 5 years) and pairwise between disciplines (chemistry vs. physics, chemistry vs. biology) using 50-year periods. Results show similar KLD trends across disciplines with peaks and troughs, and decreasing KLD towards the timeline end, indicating less variation between past and future periods.\nAnalysis of substance contributions to KLD divergence reveals differences between periods. For example, there was a focus on elements in the late 18th century versus biochemistry in biology and noble/radioactive gases in chemistry/physics in the late 19th century. Pairwise KLD analysis using word clouds confirms thematic differences (biochemical substances in biology, organic chemistry substances in chemistry, metals/noble gases in physics). The pairwise comparison also helps detect “knowledge transfer” cases, where an element’s distinctiveness shifts from one discipline to another over time (e.g., tin shifting from chemistry to physics in the 18th century).\nFuture work includes evaluating the LLM output quality, comparing results with other LLMs, adding more disciplines to the chemical space analysis, and conducting more fine-grained diachronic analysis by adjusting time windows and comparison periods. Challenges include handling historical terminology and OCR quality issues in older texts, and the potential for LLM hallucinations or artifacts in classification. The extracted metadata is intended to feed into knowledge graphs for further structured analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.2 Introduction and Research Objectives",
    "text": "17.2 Introduction and Research Objectives\n\n\n\nSlide 01\n\n\nThe presentation is titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts”. The authors are Diego Alves, Sergei Bagdasarov, and Badr M. Abdullah, affiliated with the Department of Language Science and Technology at Saarland University. The work is presented at the Large Language Models for the History, Philosophy, and Sociology of Science (LLM 4 HPSIS) workshop.\nThe research is structured into two main parts:\n\nPart I investigates the application of Large Language Models (LLMs) to enhance the metadata associated with historical scientific texts. This involves categorizing articles based on their scientific discipline and identifying relevant semantic tags or topics. Additionally, the process includes generating abstractive summaries for the articles.\nPart II constitutes a case study focused on analyzing the evolution of the chemical space across different scientific disciplines over time. The objective is to pinpoint specific periods characterized by heightened interdisciplinarity and significant knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#data-source-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#data-source-the-royal-society-corpus",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.3 Data Source: The Royal Society Corpus",
    "text": "17.3 Data Source: The Royal Society Corpus\n\n\n\nSlide 01\n\n\nThe research investigates how scientific English evolved over time, particularly its optimization for expert-to-expert communication. It also analyzes phenomena such as knowledge transfer and the identification of influential papers and authors.\nThe primary data source is the Philosophical Transactions of the Royal Society of London. This journal was first published in 1665 by the Royal Society of London and holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation today. It played a pivotal role in the development of scientific communication, notably by establishing the peer-reviewed paper publication model.\nThe corpus contains numerous influential contributions throughout history. Examples include Isaac Newton’s “New Theory about Light and Colours” from the 17th century (1672), Benjamin Franklin’s description of the “Philadelphia Experiment” (the Electrical Kite) in an 18th-century letter (1752), and James Clerk Maxwell’s work “On the Dynamical Theory of the Electromagnetic Field” from the 19th century (1865). The corpus also includes less conventional papers, such as speculations about inhabitants of the Moon, though the research focuses on linguistic and thematic analysis rather than scientific validity.\nThe specific version of the corpus utilized is the RSC 6.0 Full. This version covers a period exceeding 300 years, from 1665 to 1996. It comprises almost 48,000 individual texts and contains nearly 300 million tokens. The corpus includes some pre-existing metadata attributes such as author, century, year, and volume. A previous study applied LDA topic modeling to infer research field categories, but this approach resulted in categories that sometimes mixed scientific disciplines, sub-disciplines, and types of text like “observations” and “reporting”.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llms-for-metadata-enrichment",
    "href": "chapter_ai-nepi_018.html#llms-for-metadata-enrichment",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.4 LLMs for Metadata Enrichment",
    "text": "17.4 LLMs for Metadata Enrichment\n\n\n\nSlide 05\n\n\nThe project aims to enhance the existing metadata and generate new metadata for the corpus using Large Language Models (LLMs). LLMs are applied for various information management tasks including text clean-up, summarization, facilitating access and retrieval, information extraction, categorization, and ultimately, feeding knowledge graphs.\nFor each article in the corpus, the desired outputs from the LLM processing include a hierarchical categorization specifying the primary discipline and a sub-discipline, a list of index terms or semantic tags representing the main topics, and a concise TL;DR summary.\nThe specific LLM utilized for this task is Hermes-2-Pro-Llama-3-8B, which belongs to the Llama 3 family developed by Meta. Llama 3 is available in different parameter sizes, including 8 billion (8B) and 70 billion (70B), with a 400 billion (400B) version currently under training. The model is accessible via Hugging Face and is reported to perform significantly better than previous models like Mistral and Llama 2 for instruction-following tasks. The Hermes-2-Pro-Llama-3-8B variant is specifically fine-tuned with instruction tuning, making it particularly adept at producing structured output formats such as JSON and YAML.\nThe LLM is instructed to act as a “librarian” responsible for organizing a collection of historical scientific articles from the Royal Society of London published between 1665 and 1996. Its objective is to read, analyze, and organize this large corpus to create a structured database that facilitates search, retrieval, and analysis by researchers, historians, and scientists. The input provided to the LLM consists of OCR-extracted text of the original articles along with existing metadata like the title, author(s), publication date, journal, and a short text snippet. The prompt acknowledges potential issues with OCR quality in the historical texts.\nThe LLM is assigned four specific tasks:\n\nA. Read and analyze the article to understand its content and context, then suggest an alternative title that better reflects the content.\nB. Write a short, 3-4 sentence TL;DR summary that captures the article’s essence and main findings. The summary must be concise, informative, highlight key points, and be written in simple language suitable for a high school student.\nC. Identify exactly five main topics for the article. These topics are conceptualized as Wikipedia Keywords for categorizing the text into scientific sub-fields or thematic groups within a scientific journal.\nD. Based on the identified topics, determine the primary scientific discipline from a predefined list of nine categories: Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, and Social Sciences & Humanities. The LLM must also identify a suitable second-level sub-discipline that is a branch of the primary discipline and not one of the primary disciplines itself.\n\nThe required output format is a valid YAML file. An example input based on an article by Isaac Newton is provided, showing the existing metadata and a text snippet. A corresponding example output in YAML format demonstrates the expected structure, including the article ID, revised title, a list of five topics (e.g., Optics, Refraction, Spectroscopy), the TL;DR summary, the primary scientific discipline (e.g., Physics), and the scientific sub-discipline (e.g., Optics & Light). The prompt explicitly states that the output must be valid YAML and contain no additional text.\nValidation checks were performed on the LLM’s output. A high percentage, 99.81%, of the produced outputs were valid YAML files (17486 out of 17520). Furthermore, 94% of the predicted primary scientific disciplines fell within the predefined set of nine categories. However, some instances of hallucination or incorrect assignments were observed. These included predicting “Earth Sciences” instead of the full “Environmental & Earth Sciences” category, inventing novel categories such as “Music”, and occasionally including the index number from the predefined list as part of the discipline string (e.g., “3. Environmental & Earth Sciences”). The LLM also sometimes assigned sub-disciplines like “Neurology” or “Zoology” as primary disciplines. Despite these issues, the majority of papers were correctly assigned to the predefined primary disciplines.\nInitial analysis was conducted using the LLM-classified data. A stacked area chart visualizes the distribution of scientific disciplines over time across the corpus. This analysis reveals a more homogeneous distribution of disciplines up to the end of the 18th century. A notable peak in chemical articles is observed in the late 18th century, which is associated with the historical chemical revolution. From the 19th century onwards, biology, physics, and chemistry emerge as the three main pillars represented in the Royal Society’s publications.\nA first analysis of the generated TL;DR summaries was performed using t-SNE projection to visualize the semantic space. The t-SNE plot shows the distribution of articles based on their summary texts, colored by their assigned discipline. It indicates overlap between chemistry, physics, and biology, with chemistry appearing centrally located in the overlapping region. Disciplines such as humanities, astronomy, and mathematics tend to form more isolated clusters in this projection space. The analysis can be performed diachronically to observe shifts and changes in the semantic overlap between disciplines over time.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-the-chemical-space",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-the-chemical-space",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.5 Diachronic Analysis of the Chemical Space",
    "text": "17.5 Diachronic Analysis of the Chemical Space\n\n\n\nSlide 13\n\n\nPart II of the research presents a diachronic analysis of the chemical space, focusing specifically on three disciplines: Chemistry, Biology, and Physics, as these are the most frequently represented in the corpus.\nChemical terms are extracted from the texts using ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. The application of ChemDataExtractor involved a two-pass method to mitigate noise, particularly prevalent in earlier periods where the initial pass on the whole text tagged non-chemical entities (like animals) as chemical terms. The second pass applied ChemDataExtractor specifically to the list of substances identified in the first pass, which helped to reduce the amount of noisy output.\nKullback-Leibler divergence (KLD) is the method employed to analyze the evolution and comparison of the chemical space. KLD is applied in two distinct ways. The first method involves an independent analysis for each discipline to trace the evolution of its chemical space along the timeline. This is done by comparing the word distributions of chemical terms within two time windows: a period of 20 years before a specific date is compared with a period of 20 years after that date. This comparison window is then slid along the timeline in 5-year increments. This process yields KLD values for each discipline over time, illustrating internal changes in their chemical vocabulary.\nThe second method involves pairwise comparisons between disciplines, specifically comparing chemistry with physics and chemistry with biology. This analysis is based on 50-year periods. The output consists of KLD values that quantify the difference in chemical term distributions between the two disciplines within those periods.\nThe results from the independent KLD analysis per discipline show a similar trend across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same historical periods. Towards the end of the timeline, the KLD plots become flatter, and the overall KLD decreases. This indicates less variation in the chemical space between past and future periods within each discipline during later years.\nAnalysis focusing on the peak observed in the late 18th century reveals that KLD allows for zooming in to identify the specific chemical substances contributing most significantly to the divergence. In biology and physics during this period, one or two elements exhibit extremely high KLD values, acting as primary drivers of change. Across chemistry, biology, and physics, the same elements are observed to be responsible for the changes seen in the late 18th century.\nThe analysis of a later period, the second half of the 19th century, shows significant changes. The graphs for biology and physics become much more populated, indicating a wider range of substances contributing to the divergence. The individual contributions of elements are also more uniform. Thematic differences in substances emerge: biology’s chemical space evolves towards biochemistry, while chemistry and physics show a focus on noble gases and radioactive elements, which were discovered towards the end of the 19th century.\nPairwise comparisons, such as between chemistry and biology or chemistry and physics in the second half of the 20th century, further confirm these thematic differences. Word clouds generated from the distinctive substances in these comparisons show that the biology word cloud contains more substances related to biochemical processes in living organisms, while the chemistry word cloud features substances associated with organic chemistry, such as hydrocarbons and benzene. Comparing chemistry and physics reveals more metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals, in the physics word cloud.\nThe pairwise comparison method also proves useful for detecting instances of “knowledge transfer”. This is defined as a case where an element is ranked as distinctive of one discipline in an earlier period but becomes more distinctive of another discipline later in time. An illustration using tin (Sn) shows its distinctiveness shifting from chemistry to physics between the first and second halves of the 18th century. Similar shifts are observed for other elements in the early 20th century. Elements becoming distinctive of biology in the 20th century are typically related to biochemical processes.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-work",
    "title": "16  LLMs for Chemical Knowledge Analysis",
    "section": "17.6 Conclusion and Future Work",
    "text": "17.6 Conclusion and Future Work\n\n\n\nSlide 22\n\n\nIn conclusion, the research successfully utilized a Large Language Model (LLM) to improve the categorization and topic modeling of texts within the corpus. Building upon the metadata generated by the LLM, a diachronic analysis of the chemical space was conducted across three disciplines.\nFuture work includes several areas for improvement and further exploration:\n\nIt is necessary to evaluate the quality of the output produced by the Llama model to assess its accuracy and reliability.\nComparing the results obtained from Llama with those from other LLMs is also planned to understand model-specific variations.\nThe chemical space analysis can be expanded by including more disciplines, such as conducting a direct comparison between chemistry and biology.\nFurthermore, a more fine-grained diachronic analysis is intended, which will involve experimenting with different time sliding windows and comparison periods for the Kullback-Leibler divergence calculations.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>LLMs for Chemical Knowledge Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "",
    "text": "18 Interpretable Models for Linguistic Change",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#overview",
    "href": "chapter_ai-nepi_019.html#overview",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.1 Overview",
    "text": "18.1 Overview\nThe presentation details a research project focused on modeling context and the interplay between different types of context to trace linguistic change, specifically in English scientific writing. The project utilizes methods from both traditional linguistic analysis and deep learning. The core objective is to develop interpretable models that bridge these approaches to understand how language changes over time and across different contextual dimensions. The research investigates the chemical revolution period (1760s-1820s) in the Royal Society Corpus (RSC) as a case study, focusing on the shift from the phlogiston theory to the oxygen theory.\nPrevious work involved modeling context using separate approaches. The current work aims to combine these approaches and analyze their interactions. The theoretical framework draws upon language variation and register theory (Halliday 1985, Biber 1988), which posits that situational context determines language use and linguistic context exhibits variation. It also incorporates principles from rational communication and information theory (Jaeger and Levy 2007, Piantadosi et al. 2011), suggesting that linguistic variation modulates information content for efficient communication.\nMethods for detecting periods of change include continuous comparison using Kullback-Leibler Divergence (KLD) on probability distributions of linguistic units (words, POS trigrams) over time (Degaetano-Ortlieb and Teich 2018, 2019). This method identifies periods of increased divergence, indicating significant linguistic shifts. Analysis of what changes involves examining the specific lexical items and grammatical patterns contributing to high KLD, revealing “waves of increased expressivity” potentially linked to new concepts. The effects of change are observed across linguistic levels, including lexical items (lemmas) and grammatical units (POS trigrams).\nParadigmatic context and change are analyzed using semantic space models (Fankhauser et al. 2017, Bizzoni et al. 2019), visualizing semantic similarity and frequency of terms like “phlogiston” and “oxygen” across different time periods. Identifying who leads or spreads change utilizes Cascade models (Hawkes processes) (Bizzoni et al. 2021), which model influence spread within a network, identifying innovators (e.g., Priestley) and spreaders (e.g., Pearson).\nInvestigating how change is realized linguistically and why it occurs from a communicative perspective involves analyzing Surprisal (Shannon 1949), which correlates with cognitive effort (Hale 2001, Levy 2008, Crocker et al. 2016). Linguistic structures that reduce surprisal and encoding effort, such as shifts from prepositional phrases (“consumption of oxygen”) to compounds (“oxygen consumption”), are analyzed over time in relation to community adoption (number of authors).\nThe proposed framework for modeling context for language variation and change addresses limitations of current methods (e.g., static network approaches) by treating context as a central signal. It proposes using Graph Convolutional Networks (GCNs) for modeling complex relational data. A pilot study on the chemical revolution outlines a multi-stage process:\n\nData Sampling: Using the RSC, applying tf-idf and KLD to identify keywords in the target period (1760s-1820s).\nNetwork Construction: Building time-aware networks. This involves creating word- and time-aware feature vectors using BERT for word embeddings and one-hot encoding for categorical metadata (author, journal, period). Node feature matrices are created for 20-year periods. Change in node features is measured using KLD across periods, resulting in a diachronic series of graphs. Network size is managed using community detection algorithms (e.g., Riolo & Newman 2020).\nLink Prediction: Predicting how, when, and by whom words are used. Word profiles are augmented with semantic embeddings (from BERT), contextual metadata (author, journal, period), and grammatical information (POS, syntactic role). A Transformer-GCN model learns patterns in these profiles to predict new links, with GCN capturing structural relationships and Transformer attention highlighting influential contextual features.\nEntity Alignment: Inspecting and interpreting change. This involves identifying Network Motifs (small, overrepresented subgraphs) using the Kavosh algorithm, which groups isomorphic graphs to find motifs across networks. Entity alignment (e.g., using GCNs for tasks like aligning concepts across different datasets or languages) is a future perspective.\n\nLimitations and perspectives include computationally tracing conceptual versus linguistic change, integrating metadata as a core signal, determining the optimal unit of language change (word, concept, grammar, discourse), identifying recurring linguistic pathways for concept emergence, and ensuring interpretability of complex models. Future work includes expanding to multilingual corpora (e.g., French, German journals) and other text types (letters, monographs) and investigating the expression of attitude (positive/negative) towards concepts within the network structure.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#context-and-theoretical-framework",
    "href": "chapter_ai-nepi_019.html#context-and-theoretical-framework",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.2 Context and Theoretical Framework",
    "text": "18.2 Context and Theoretical Framework\n\n\n\nSlide 01\n\n\nThe research focuses on the computational analysis of semantic change across different environments, specifically modeling context and the interplay between various types of context. A key case study involves the chemical revolution as documented in the Royal Society Corpus (RSC). This historical event centers on the replacement of the 100-year-old phlogiston theory by Lavoisier’s theory of oxygen. Previous research efforts modeled context using separate approaches, while the current work aims to combine these methods to analyze the interactions between different contextual dimensions.\nThe framework identifies six key types of context: Situational (Where), Temporal (When), Experiential (What), Interpersonal (Who), Textual (How), and Causal (Why). The theoretical foundation draws upon two main areas. Firstly, language variation and register theory, as described by Halliday (1985) and Biber (1988), posits that situational context dictates language use and that linguistic context inherently exhibits variation. Examples of such variation include phrases like “…air which was dephlogisticated…”, “…dephlogisticated air…”, and “…oxygen…”. Secondly, rational communication and information theory, developed within the IDeaL SFB 1102 project and referenced in works by Jaeger and Levy (2007) and Piantadosi et al. (2011), suggests that linguistic variation serves to modulate information content, leading to optimization effects that facilitate efficient communication with reasonable effort.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-linguistic-change",
    "href": "chapter_ai-nepi_019.html#detecting-linguistic-change",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.3 Detecting Linguistic Change",
    "text": "18.3 Detecting Linguistic Change\n\n\n\nSlide 03\n\n\nThe research addresses the problem of detecting periods of change in language use by identifying these periods directly rather than relying on comparisons between predefined time segments. The primary method for detecting change utilizes Kullback-Leibler Divergence (KLD). This approach compares the probability distributions, p(unit|context), of linguistic units over time using a continuous comparison method (Degaetano-Ortlieb and Teich 2018, 2019). The interpretation of KLD values is direct: similar distributions result in low divergence, while differing distributions yield higher divergence. The continuous comparison employs a sliding time window, for instance, comparing a 20-year period designated as “PAST” with the subsequent 20-year period labeled “FUTURE”.\nTo analyze what changes, the method plots KLD over time for various linguistic items, including both lexical items and POS trigrams. Peaks observed in these KLD plots are interpreted as “waves of increased expressivity,” suggesting the emergence of new concepts or significant shifts in the linguistic treatment of existing ones. The analysis includes a wide range of lexical items such as “electricity”, “electrify”, “’s”, “limb”, “ditto”, “air”, “dephlogisticated experiment”, “nitrous”, “acid”, “gas”, “oxide”, “be”, “hydrogen”, “current”, “urine”, “cell”, “corpuscule”, “glacier”, “tide”, “the”, “of”, “sin”, and “cos”. A specific period of interest, approximately from 1765 to 1805, is highlighted, encompassing terms like “dephlogisticated experiment”, “nitrous”, “acid”, “air”, “gas”, “oxide”, “be”, “hydrogen”, “current”, “urine”, and “cell”. This period aligns with significant historical events like the discovery of hydrogen (inflammable air) by Henry Cavendish in 1766 and the discovery of oxygen (dephlogisticated air) by Joseph Priestley in 1774.\nThe analysis observes effects across different linguistic levels. KLD is applied to lexical items, using the lemma as the unit of analysis, and also to grammatical units, specifically POS trigrams. The findings indicate that peaks in KLD for lexical items, occurring around 1775-1805, correspond roughly to peaks observed in the KLD analysis of POS trigrams. Examples of POS trigrams analyzed include “NN NN IN (zenith distance of)”, “VBZ JJR IN (is greater than)”, “DT NN IN (the end of)”, “NN NN NN (thunder and lightning)”, “IN JJ NN (of dephlogisticated air)”, “DT NNS IN (the effects of)”, “NN NN DT (oxide of iron)”, “NN NN IN (the quantity/number of)”, “VBZ JJR IN (is greater than)”, “NN NN IN (unite edge of)”, and “IN DT JJ (for the same)”. This suggests that linguistic change during this period manifested across both vocabulary and grammatical structure.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#paradigmatic-context-and-influence",
    "href": "chapter_ai-nepi_019.html#paradigmatic-context-and-influence",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.4 Paradigmatic Context and Influence",
    "text": "18.4 Paradigmatic Context and Influence\n\n\n\nSlide 06\n\n\nThe analysis of paradigmatic context and change employs methods described by Fankhauser et al. (2017) and Bizzoni et al. (2019). This technique involves visualizing semantic space at different time periods, such as 1780, 1800, and 1840. Terms are represented as points within this space, where their position indicates semantic similarity. The visualizations provide additional details: the size of the circle representing a term indicates its relative frequency, and color is used to represent clusters of terms. Observing the shifts in term positions and clustering over time reveals semantic change, exemplified by the appearance of “oxygen” and the changing position and frequency of terms like “phlogiston” and “dephlogisticated”. The corpora used for this analysis are available at corpora.ids-mannheim.de.\nTo identify who is leading or spreading change, the research utilizes Cascade models, specifically Hawkes processes, as detailed by Bizzoni et al. (2021). These models are applied to model the spread of influence or linguistic innovations within a network, such as a network of authors. The models enable the identification of individuals acting as “Innovators,” such as Priestley, and those acting as “Early Adopters” or “Spreaders,” including Pearson and Davy. The results are visualized using a heatmap that shows author influence over time. In this visualization, the color intensity represents the degree of influence, and dashed lines are used to indicate the spread of influence across different time points.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#linguistic-realization-and-communicative-perspective",
    "href": "chapter_ai-nepi_019.html#linguistic-realization-and-communicative-perspective",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.5 Linguistic Realization and Communicative Perspective",
    "text": "18.5 Linguistic Realization and Communicative Perspective\n\n\n\nSlide 09\n\n\nThe research investigates how linguistic change is realized and why these changes occur from a communicative perspective. The approach involves analyzing change within the linguistic context using the concept of Surprisal, as introduced by Shannon (1949). The underlying principle is that the surprisal of a linguistic unit is proportional to the cognitive effort required to process it, a relationship supported by work from Hale (2001), Levy (2008), and Crocker et al. (2016). A core hypothesis is that linguistic changes occur to reduce cognitive effort and facilitate more efficient communication.\nThe analysis tracks the surprisal of different linguistic structures over time. Examples of structures examined include clausal forms like “…the oxygen (which was) consumed”, prepositional phrases such as “…the consumption of oxygen…”, and compound forms like “…the oxygen consumption…”. The observation is that the surprisal of longer, more effortful structures, exemplified by the prepositional phrase “Prepositional consumption of oxygen”, tends to decrease over time. This decrease coincides with the emergence and establishment of shorter, less effortful structures, such as the compound “Compound oxygen consumption”, within the linguistic community. A correlation is observed between the decrease in surprisal for these shorter forms and an increasing number of authors adopting and using them. This process indicates that shorter encoding emerges and becomes established in the community, effectively reducing cognitive effort as reflected by lower surprisal values. This work is related to the MA thesis of Viktoria Lima-Vaz (2025) and a submission by Degaetano-Ortlieb et al. (July 2024).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#framework-for-context-and-language-dynamics",
    "href": "chapter_ai-nepi_019.html#framework-for-context-and-language-dynamics",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.6 Framework for Context and Language Dynamics",
    "text": "18.6 Framework for Context and Language Dynamics\n\n\n\nSlide 12\n\n\nThe proposed framework for modeling context for the analysis of language variation and change is motivated by the understanding that language change is driven by shifts in social context, including evolving goals, social structures, and domain conventions. Current limitations in the field include that existing semantic change studies and KLD applications often track shifts but do not adequately model the interaction between various contextual signals. Furthermore, static network approaches are limited in their ability to capture dynamic interactions over time.\nThe proposed framework posits that context serves as a central signal for modeling language dynamics. Graph Convolutional Networks (GCNs) are proposed as one possible technological direction due to their powerful capability for modeling complex relational data. A pilot study focusing on the chemical revolution is outlined, utilizing the Royal Society Corpus (RSC) and targeting the period between the 1760s and 1820s.\n\n18.6.1 Stage I: Data Sampling\nThis stage employs methods such as tf-idf and KLD to identify relevant keywords within the target period. KLD is used to define words that are distinct for each period, with words contributing highly to KLD being considered relevant.\n\n\n18.6.2 Stage II: Network Construction\nThis stage aims at building word- and time-aware feature vectors. This involves using BERT for generating word vectors and one-hot encoding for categorical metadata such as Author, Journal, and Period. A node feature matrix is created for each 20-year period. Change in these node feature vectors is measured using KLD to assess dissimilarity across periods, resulting in a diachronic series of graphs. To manage network complexity, community detection algorithms, such as those described by Riolo & Newman (2020), are used for network size definition.\n\n\n18.6.3 Stage III: Link Prediction\nThis stage seeks to predict how, when, and by whom words are used. This involves using word profiles augmented with semantic embeddings (e.g., from BERT), contextual metadata (e.g., author, journal, period), and grammatical information (e.g., part of speech, syntactic role). A Transformer-GCN model is employed, which learns patterns in these augmented profiles and predicts new links. The GCN component captures structural relationships within the network, while the Transformer attention mechanism highlights the most influential contextual features.\n\n\n18.6.4 Stage IV: Entity Alignment\nThis stage is designed to inspect and interpret the observed change. It utilizes Network Motifs, defined as small, overrepresented subgraphs that reflect meaningful interaction structures. The Kavosh algorithm is used, which groups isomorphic graphs to identify these motifs within networks. Entity alignment is also considered as a future application, potentially involving tasks like aligning similar graphs across different time periods (t1, t2).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#limitations-and-future-work",
    "href": "chapter_ai-nepi_019.html#limitations-and-future-work",
    "title": "17  Interpretable Models for Linguistic Change",
    "section": "18.7 Limitations and Future Work",
    "text": "18.7 Limitations and Future Work\n\n\n\nSlide 16\n\n\nThe research acknowledges several limitations and outlines future perspectives. Key questions include what it truly means to computationally trace conceptual change and whether models can capture deeper epistemic shifts beyond mere linguistic drift. Another challenge is understanding how context becomes integrated into the meaning represented by language models, and whether metadata should be treated as external noise or a core signal. The optimal ‘unit’ of language change remains a question: are shifts best observed at the level of words, concepts, grammar, or discourse patterns? The possibility of identifying recurring linguistic pathways for concept emergence and determining if new ideas follow predictable linguistic trajectories is also explored. Finally, the limits of interpretability in complex models are considered, emphasizing the need to ensure that explanations are meaningful rather than merely plausible.\nFuture perspectives include expanding the data sources beyond the Royal Society Corpus to include multilingual corpora, such as French and German journals, and incorporating other text types like monographs and letters. A significant area for future work is addressing the expression of attitude or stance in language use, particularly the challenge of differentiating between positive and negative usage of terms like “oxygen” or “dephlogisticated air” within the context of heated historical debates. Potential approaches involve analyzing differences in network structure based on usage context, such as linking terms to critiques or “dispective” adjectives.\nMatching linguistic patterns to authors known to advocate for or against specific theories, potentially leveraging external historical knowledge from philosophy of science texts, is another avenue. Furthermore, insights from work on propaganda analysis, such as in the context of the Russian-Ukraine war, could be mapped onto historical texts to identify propagandistic strategies. The method would involve comparing network structures over time, focusing on structural features and identifying which nodes promote more edges.\nApplying the developed methods to current era corpora, such as a quantum gravity corpus, is also a future perspective. The goal is to observe community building and changing language in real-time or near real-time. This would require establishing a protocol for structuring the data, potentially in a relational database like SQL, to facilitate its translation into a graph format. Processing the data to leverage categorical values, such as author, journal, and topics, as features within the graph structure would be necessary. Depending on the structure of the dataset, data engineering may be required.\nFinally, Entity Alignment is identified as a future perspective, particularly for enabling multilingual or multi-corpus comparisons. This involves tasks such as aligning concepts, for example, the “oxygen” subgraph, across different datasets, such as those from English versus French journals. The method would utilize a graph convolutional network specifically for an entity alignment task, distinct from a link prediction task. The objective is to determine if entities are identified as the same based on their neighboring nodes and overall network structure.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpretable Models for Linguistic Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "",
    "text": "19 LLM for HPS Studies: Analyzing the NHGRI Archive",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#overview",
    "href": "chapter_ai-nepi_020.html#overview",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.1 Overview",
    "text": "19.1 Overview\nThe presentation addresses the limited understanding of science funding processes, which often relies solely on public outputs like publications and grants. It proposes analyzing born-physical archives of funding agencies to gain insights into the internal processes of science funding and innovation.\nThe case study focuses on the archive of the National Human Genome Research Institute (NHGRI), a key funder of the Human Genome Project and subsequent genomics initiatives. The NHGRI archive contains over 2 million pages of diverse internal documents, including meeting notes, handwritten correspondence, presentations, spreadsheets, newspapers, forms, proposals, and emails. This archive presents challenges due to its scale, complexity, and the presence of sensitive information like PII and handwriting.\nThe research employs a suite of computational methods and tools to process this archive. These include training a synthetic-data informed handwriting model for removal and recognition, utilizing multimodal models (vision, text, layout) for tasks like entity extraction and synthetic data generation, and implementing entity disambiguation and PII masking techniques.\nThe extracted data is used for various analyses, including reconstructing correspondence networks and computationally modeling funding decisions.\nKey findings include the identification of informal leadership structures within NHGRI, such as a “kitchen cabinet” during the International HapMap Project, discovered through unsupervised network community detection. Analysis of brokerage roles reveals differences in communication patterns between formal and informal groups.\nA computational model predicting organism sequencing funding decisions demonstrates that biological, project, reputation, and linguistic features are all informative. This highlights effects like the Matthew effect, where higher H-index and community size correlate with funding success.\nThe work underscores the importance of preserving born-physical archives and developing computational tools to make their content accessible and analyzable for historical and sociological research on science. The project is part of a larger consortium, “Born Physical, Studied Digitally,” which seeks collaborators to apply these methods to other archives, including federal court records and seismology data.\nThe research aims to inform policy, increase data accessibility, and answer fundamental questions about how science works, particularly regarding the influence of funding and the emergence of innovation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-of-current-understanding-of-science-funding",
    "href": "chapter_ai-nepi_020.html#limitations-of-current-understanding-of-science-funding",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.2 Limitations of Current Understanding of Science Funding",
    "text": "19.2 Limitations of Current Understanding of Science Funding\n\n\n\nSlide 01\n\n\nThe current understanding of science funders is limited, primarily focusing on public outputs. Since World War II, state-sponsored research has been the dominant model, based on a social contract where funders, acting on behalf of the public, invest in research with the expectation that it will translate into informing policy, clinical applications, and new technology.\nThis framework, analyzing public outputs such as publications and the activities of scholars, has provided insights into aspects of science, including the long-term impact of research, the size of research teams, the origins of interdisciplinary domains, and the career mobility of scientists. Bibliometric analysis of scientific articles has also contributed to this understanding.\nHowever, the scientific article offers a skewed and incomplete view of the scientific enterprise. Relying solely on bibliometrics to define science oversimplifies its inherent complexity. A deeper understanding requires examining the processes behind scientific outputs, moving beyond the flawed picture provided by the scientific article alone.\nThe conventional model depicts a flow from federal funding agencies (e.g., NASA, DARPA, NIH, USDA, DOE, NSF, USGS, NOAA) providing grants, which support scholars and lead to publications. Both scholars and publications contribute to knowledge, which in turn informs clinical applications, technology, and policy, ultimately impacting the public. Research drawing on these public outputs is supported by studies such as Wang et al. (2013), Uzzi et al. (2013), Börner et al. (2003), Guimera et al. (2005), Wu et al. (2019), Liu et al. (2018), and Sinatra et al. (2016).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#research-questions-and-expanded-model-of-science-funding",
    "href": "chapter_ai-nepi_020.html#research-questions-and-expanded-model-of-science-funding",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.3 Research Questions and Expanded Model of Science Funding",
    "text": "19.3 Research Questions and Expanded Model of Science Funding\n\n\n\nSlide 01\n\n\nUnderstanding the role of funders is central to comprehending “how science works.” This requires shifting the focus from the products of science, such as published articles, to the underlying processes.\nKey research questions emerge from this perspective, including whether science drives funding decisions or if funding shapes the direction of science. The analysis also seeks to identify points within the innovation pipeline, from initial ideation to long-term impact, where innovation emerges, spills over into other areas, or fails and falls through the cracks. A significant challenge is that failed projects, which do not typically result in publications, remain largely invisible in analyses focused solely on articles. Further questions concern the nature of assistance provided by funders beyond financial support and the potential presence of biases in funding allocation.\nAn expanded model of science funding incorporates additional elements to capture these processes. Public data serves as an input informing grant decisions. A circular relationship exists between grants and technology development, indicating that grants lead to technology development, which can in turn influence future grants. Similarly, knowledge informs community engagement, which can feed back into the knowledge creation process. Cooperative agreements are introduced as an output from grants that also serve as an input to community engagement, highlighting collaborative mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#case-study-the-human-genome-project-and-nhgri",
    "href": "chapter_ai-nepi_020.html#case-study-the-human-genome-project-and-nhgri",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.4 Case Study: The Human Genome Project and NHGRI",
    "text": "19.4 Case Study: The Human Genome Project and NHGRI\n\n\n\nSlide 02\n\n\nThe Human Genome Project (HGP) serves as a key case study, recognized as the first “big science” initiative in the field of biology. This project is relevant in the context of Netpi, which examines big science in particle physics, by providing a parallel in biology. The HGP era involved the collaboration of tens of countries and thousands of researchers with the primary goal of sequencing the human genome.\nThe HGP is notable for several reasons. It garnered unprecedented public attention for a biology project, shifting focus from laboratory organisms like Drosophila and C. elegans to a project with direct human relevance. Its impact continues today, as most omics methods in modern biology rely on a reference genome, and the field of genomics itself largely arose from the sequencing of the human genome.\nThe project also pioneered new data sharing practices that are now widely adopted and marked a significant integration of computational methods with biology. The HGP was primarily led by two major organizations: the Welcome Trust in the UK and the National Human Genome Research Institute (NHGRI), which served as the US National Institutes of Health (NIH) arm specifically for the HGP. Francis Collins, who led the NIH and directed NHGRI, was a key figure in this endeavor.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#nhgri-as-an-innovative-funding-agency",
    "href": "chapter_ai-nepi_020.html#nhgri-as-an-innovative-funding-agency",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.5 NHGRI as an Innovative Funding Agency",
    "text": "19.5 NHGRI as an Innovative Funding Agency\n\n\n\nSlide 04\n\n\nAnalysis indicates that NHGRI stands out as one of the most innovative funding agencies within the National Institutes of Health. This assessment is based on several quantitative metrics used for comparison across various NIH institutes. NHGRI consistently shows the highest performance in the share of its funded manuscripts that rank among the top 5% most cited. It also leads in the number of citations its funded research receives from patents and the total citations accumulated after ten years. Furthermore, NHGRI’s funded research scores highest on a disruption metric, which measures the extent to which subsequent publications cease citing earlier work, suggesting a shift in research trajectories.\nBased on these metrics, NHGRI is consistently identified as a leader in innovation within the biomedical community. However, while these metrics demonstrate that NHGRI is innovative, they do not explain the underlying reasons or processes that contribute to this innovation. Understanding why NHGRI is innovative requires a deeper examination of its internal operations and decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#interdisciplinary-team-and-research-goals",
    "href": "chapter_ai-nepi_020.html#interdisciplinary-team-and-research-goals",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.6 Interdisciplinary Team and Research Goals",
    "text": "19.6 Interdisciplinary Team and Research Goals\n\n\n\nSlide 05\n\n\nThe research is conducted by an interdisciplinary team comprising individuals with diverse expertise, including engineers, historians, physicists, ethicists, and computer scientists. Notable members of the team include former leaders of the NIH and NHGRI, such as Francis Collins. The project involves partnerships with organizations such as the NIH National Human Genome Research Institute, NVIDIA, and the NSF.\nThe research pursues several key goals. It aims to understand the specific factors and processes that contributed to the rise of the field of genomics. The team also seeks to identify failure modes within research and funding processes and analyze how innovation diffuses or spills over into different areas. A central objective is to study the dynamics of interaction between scientific funding agencies and academic scholars and scientists to understand how these relationships can foster better scientific outcomes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-content-and-challenges",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-content-and-challenges",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.7 The NHGRI Archive: Content and Challenges",
    "text": "19.7 The NHGRI Archive: Content and Challenges\n\n\n\nSlide 06\n\n\nThe NHGRI Archive constitutes a rich collection of content, though its structure is complex. Due to the notable historical nature of the Human Genome Project, many internal forms and documents from the 1980s, 1990s, and subsequent years were preserved.\nThis archive contains a variety of document types, including meeting notes detailing the daily coordination of the genome project, handwritten notes from correspondences, agendas, and conferences, as well as presentations, spreadsheets, newspaper clippings remarking on the period, forms, proposals, and printed copies of emails.\nThe archive is substantial in scale, exceeding 2 million pages, and continues to grow by 5% annually through ongoing digitization efforts. A significant challenge arises from the difficulty of studying this born-digital and born-physical artifact at scale using traditional methods, posing a barrier for individual researchers or even teams.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#distinction-between-internal-documents-and-public-data",
    "href": "chapter_ai-nepi_020.html#distinction-between-internal-documents-and-public-data",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.8 Distinction Between Internal Documents and Public Data",
    "text": "19.8 Distinction Between Internal Documents and Public Data\n\n\n\nSlide 07\n\n\nInternal documents within the NHGRI archive are fundamentally different from the data publicly available to scholars. Publicly accessible data primarily consists of Requests for Applications (RFAs) and publications, found in resources like PubMed and NIH RePORTER. The content of these public sources differs significantly from that found internally.\nThe internal documents provide detailed descriptions of the numerous large-scale genomic projects funded by NHGRI. These projects, often involving tens or hundreds of millions of dollars and thousands of researchers, were designed to create essential resources for the genomics community, thereby contributing significantly to the rise of the field.\nA t-SNE plot visualizing the document space illustrates this distinction, showing distinct clusters corresponding to various genomic projects such as LSAC, modENCODE, eMERGE, ENCODE, Ethical, Legal, and Social Implications Research, NHGRI-EBI GWAS Catalog, H3Africa, International HapMap Project, Human Genome Project, and PAGE. The clusters representing RFAs and Publications are clearly separated from those representing the internal project documents, highlighting the unique nature of the archive’s content.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#methodology-handwriting-processing",
    "href": "chapter_ai-nepi_020.html#methodology-handwriting-processing",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.9 Methodology: Handwriting Processing",
    "text": "19.9 Methodology: Handwriting Processing\n\n\n\nSlide 08\n\n\nA significant methodological challenge arises from the presence of a large volume of handwriting within the NHGRI archive, a consequence of its born-physical origins. Processing handwriting with AI presents difficulties, not only in terms of technical proficiency but also ethically, due to the unknown nature of the content it may contain. To address this, a custom handwriting model is trained.\nThe purpose of this model is twofold: it can remove handwriting from documents, which aids in improving the accuracy of Optical Character Recognition (OCR) for the remaining printed text, and it enables the creation of a dedicated pipeline specifically for handwriting recognition. The model architecture utilized includes components resembling a U-Net architecture, as depicted in a diagram. The ethical considerations surrounding the use of AI with handwriting are further explored in a separate ethics case study.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#methodology-multimodal-models-and-synthetic-data-generation",
    "href": "chapter_ai-nepi_020.html#methodology-multimodal-models-and-synthetic-data-generation",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.10 Methodology: Multimodal Models and Synthetic Data Generation",
    "text": "19.10 Methodology: Multimodal Models and Synthetic Data Generation\n\n\n\nSlide 08\n\n\nThe methodology incorporates multimodal models, drawing upon research from the document intelligence community, including work by Huang et al. (2022) and Zhang et al. (2022). These models are designed to smartly combine multiple modalities: vision (image), text, and layout. The layout modality plays a crucial role by supervising the joint embedding process and discretizing the document structure, represented internally using angle brackets and numbers.\nThe models typically include components such as a joint embedding space, a Masked Autoencoder Training Objective, a Text Decoder, and a Vision Decoder. This multimodal approach enables various tasks, including entity extraction, where specific pieces of information are identified and highlighted within a document. It also facilitates the generation of synthetic data, producing artificial documents or images. The synthetic data generation capability is utilized to create synthetic training datasets, which are valuable for developing and training new classifiers.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#methodology-entity-and-pii-recognition-and-disambiguation",
    "href": "chapter_ai-nepi_020.html#methodology-entity-and-pii-recognition-and-disambiguation",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.11 Methodology: Entity and PII Recognition and Disambiguation",
    "text": "19.11 Methodology: Entity and PII Recognition and Disambiguation\n\n\n\nSlide 09\n\n\nA critical task involves the recognition of entities and Personally Identifiable Information (PII). The NHGRI archive is considered a “living archive” because it contains information about real individuals, including sensitive data such as credit card numbers and social security numbers. A particular challenge is that some of these individuals remain active in government and academia today. Therefore, it is essential to implement rigorous processes for removing, masking, and disambiguating individuals within this large archive.\nThe methods employed for entity and PII recognition demonstrate good performance, as indicated by F1 scores. Performance metrics show that F1 scores for different entity types—PERSON, ORG, EMAIL, LOC, and IDNUM—increase significantly as more samples are used for finetuning. The F1 scores for EMAIL, IDNUM, and LOC approach 1.0 with sufficient finetuning, indicating high accuracy. Visual examples show highlighted entities on documents, such as an “identifier” on a boarding pass and various entities like “Organization,” “Address,” “Name,” “Email,” “IDNUM,” and “LOC” on a letter.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#case-study-reconstructing-a-correspondence-network-from-emails",
    "href": "chapter_ai-nepi_020.html#case-study-reconstructing-a-correspondence-network-from-emails",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.12 Case Study: Reconstructing a Correspondence Network from Emails",
    "text": "19.12 Case Study: Reconstructing a Correspondence Network from Emails\n\n\n\nSlide 10\n\n\nOne case study involves reconstructing a correspondence network within NHGRI based on the archive’s content. The data source for this analysis consists of printed scanned copies of emails. The method involves extracting entities from these scanned documents and linking them to build the network.\nThe analysis utilized 62,511 email conversations, which were derived from a collection of 5,414 scanned emails. In the resulting network visualization, each node represents a physical paper copy of an email from the archive.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#network-analysis-affiliation-association",
    "href": "chapter_ai-nepi_020.html#network-analysis-affiliation-association",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.13 Network Analysis: Affiliation Association",
    "text": "19.13 Network Analysis: Affiliation Association\n\n\n\nSlide 10\n\n\nThe network analysis includes associating the nodes, representing individuals or emails, with their respective affiliations. Two primary affiliation types are identified: Non-NIH Affiliation, visually represented by red nodes in the network graph, and NIH Affiliation, represented by blue nodes. These affiliations are linked to the specific organization, funding agency, or company with which individuals were associated during the Human Genome Project era.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#network-analysis-community-detection-and-informal-structures",
    "href": "chapter_ai-nepi_020.html#network-analysis-community-detection-and-informal-structures",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.14 Network Analysis: Community Detection and Informal Structures",
    "text": "19.14 Network Analysis: Community Detection and Informal Structures\n\n\n\nSlide 10\n\n\nNetwork analysis techniques, including community detection methods such as the stochastic block model, are applied to the reconstructed correspondence network. This analysis focuses on understanding the interactions between academia and NIH personnel. A specific case study examines emails exchanged during the International HapMap Project.\nThe International HapMap Project is described as another “big science” genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on genetic variation and is significant as the basis for genome-wide association studies (GWAS). This was a large-scale project involving numerous universities and agencies, posing a challenge for the funding agency in terms of coordination and management.\nThe formal structure for managing such projects typically includes a steering committee with representatives from participating universities. However, the analysis computationally discovered an informal structure not previously discussed: the “Kitchen Cabinet.” This informal leadership circle met prior to the official steering committee meetings, functioning to identify and address potential problems preemptively. This “Kitchen Cabinet” was identified from the archive data using unsupervised computational methods.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#network-analysis-brokerage-roles-and-leadership-comparison",
    "href": "chapter_ai-nepi_020.html#network-analysis-brokerage-roles-and-leadership-comparison",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.15 Network Analysis: Brokerage Roles and Leadership Comparison",
    "text": "19.15 Network Analysis: Brokerage Roles and Leadership Comparison\n\n\n\nSlide 12\n\n\nThe analysis extends to comparing different leadership circles based on their brokerage roles within the network. Brokerage roles are defined by the interaction patterns of each node (individual) with others in the network. Examples of these roles include a consultant, who receives information and disseminates it back within the same group, and a gatekeeper, who receives information but does not pass it back to the originating group. Other roles include coordinator, liaison, and representative.\nThe analysis compares the distribution of brokerage roles among three groups: the “Kitchen Cabinet” (representing informal leadership), the Steering Committee (representing formal leadership), and the Rest of HapMap (representing other project participants). The findings indicate that the “Kitchen Cabinet” primarily functioned in a consultant role, a pattern distinct from that observed in the other leadership circles during the project. Francis Collins is specifically noted as acting in a consultant role within the “Kitchen Cabinet.” A box plot visualization illustrates the distribution of brokerage roles for these three comparison groups.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#portfolio-analysis-modeling-funding-decisions",
    "href": "chapter_ai-nepi_020.html#portfolio-analysis-modeling-funding-decisions",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.16 Portfolio Analysis: Modeling Funding Decisions",
    "text": "19.16 Portfolio Analysis: Modeling Funding Decisions\n\n\n\nSlide 13\n\n\nThe research includes portfolio analysis focused on modeling funding decisions. A specific case study examines the decisions made regarding organism sequencing after the completion of the Human Genome Project. The problem involved scientists and leadership within the funding agency determining which organismal community’s genome should be sequenced next. This decision-making process required allocating limited funding among various competing proposals advocating for the sequencing of different organisms.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-model-for-funding-decisions-features",
    "href": "chapter_ai-nepi_020.html#computational-model-for-funding-decisions-features",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.17 Computational Model for Funding Decisions: Features",
    "text": "19.17 Computational Model for Funding Decisions: Features\n\n\n\nSlide 14\n\n\nA computational model is developed using machine learning to recapitulate the funding decisions made for organism sequencing projects. The model incorporates various features categorized as Biological, Project, Reputation, and Linguistic.\nBiological features include the genetic distance of the proposed organism to already known sequenced model organisms and the organism’s genome size.\nProject features encompass characteristics of the proposal and the proposing team, such as the team size, the time elapsed since the first submission of the proposal, the gender equity within the proposing team, whether the proposal is standalone or part of a larger initiative, and if the proposal was written internally within the agency.\nReputation features assess the standing of the individuals and community involved, including the H-index of the proposal authors, the size of the research community focused on the specific organism, the centrality rank of the proposers within the NHGRI network, and indicators of broader community support for sequencing the organism.\nLinguistic features analyze the text of the proposals, examining aspects such as the level of argumentation and repetitiveness.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-model-performance-and-feature-informativeness",
    "href": "chapter_ai-nepi_020.html#computational-model-performance-and-feature-informativeness",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.18 Computational Model Performance and Feature Informativeness",
    "text": "19.18 Computational Model Performance and Feature Informativeness\n\n\n\nSlide 15\n\n\nThe performance of the computational model for predicting funding decisions is evaluated using ROC curves. The model demonstrates that all categories of features are informative for predicting funding success. The Biological features achieve an ROC AUC of 0.76 ± 0.05, Project features achieve 0.83 ± 0.04, Reputation features achieve 0.87 ± 0.04, and Linguistic features achieve 0.85 ± 0.04. When all features are combined, the model achieves the highest predictive performance with an ROC AUC of 0.94 ± 0.03, significantly outperforming individual feature categories and a random classifier (ROC AUC 0.5). The informativeness of these features enables further analysis to understand how they influence the funding decision models.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#feature-interpretability-analysis",
    "href": "chapter_ai-nepi_020.html#feature-interpretability-analysis",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.19 Feature Interpretability Analysis",
    "text": "19.19 Feature Interpretability Analysis\n\n\n\nSlide 15\n\n\nFeature interpretability techniques are employed to analyze the computational model. The purpose is to understand how individual features contribute to or inform the model’s prediction of funding success. This approach relates to methods for explaining individual predictions, particularly in contexts where features may be dependent. Relevant work in this area, focusing on more accurate approximations to Shapley values for dependent features, is cited, specifically Aas, K., Jullum, M. & Løland, A. (2021) in Artificial Intelligence. A diagram visually represents features that are more likely to contribute positively to funding success prediction and those less likely to contribute positively.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#finding-matthew-effect-in-funding-decisions",
    "href": "chapter_ai-nepi_020.html#finding-matthew-effect-in-funding-decisions",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.20 Finding: Matthew Effect in Funding Decisions",
    "text": "19.20 Finding: Matthew Effect in Funding Decisions\n\n\n\nSlide 16\n\n\nAnalysis using feature interpretability reveals evidence of the Matthew Effect at play in the funding decisions. The Matthew Effect is observed in two key correlations: a higher maximum H-index among the proposal authors is associated with a greater likelihood of the proposal being approved for funding, and a larger size of the research community focused on the specific organism also correlates with a higher probability of funding success.\nThese correlations are visualized in scatter plots. The left plot shows an upward trend between the maximum H-index on the x-axis and a measure of contribution to funding success on the y-axis. The right plot similarly shows an upward trend between the logarithm of community size on the x-axis and the contribution to funding success on the y-axis. This confirms the expectation that funding agencies, aiming to maximize downstream impact in areas like clinical applications and technology, tend to favor proposals from more established researchers (higher H-index) and larger, more active communities. The findings align with existing understanding of the Matthew effect within the context of science funding.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#synthesis-and-broader-applications",
    "href": "chapter_ai-nepi_020.html#synthesis-and-broader-applications",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.21 Synthesis and Broader Applications",
    "text": "19.21 Synthesis and Broader Applications\n\n\n\nSlide 16\n\n\nThe work synthesizes the approach by demonstrating the potential achieved through combining born-physical archives with computational tools. This methodology is part of a broader initiative involving multiple partners beyond NHGRI, including custodians of federal court records from the United States and the EarthScope Consortium, which manages seismogram data. The core process involves translating data from these diverse sources using robust algorithms and cyber infrastructure.\nThe processed data and subsequent analysis have multiple applications. They can be used to inform policy decisions, increase the accessibility of previously locked-away data, and facilitate answering complex scientific questions. A diagram illustrates this process, showing Data and Metadata sources (NHGRI, federal court records, EarthScope Consortium) feeding into Knowledge Creation steps (Page stream segmentation, Handwriting extraction, Entity disambiguation, Layout modeling), which then enable Application Use (Scientific questions, Policy decisions, Data accessibility).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#importance-of-preserving-born-physical-archives",
    "href": "chapter_ai-nepi_020.html#importance-of-preserving-born-physical-archives",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.22 Importance of Preserving Born-Physical Archives",
    "text": "19.22 Importance of Preserving Born-Physical Archives\n\n\n\nSlide 17\n\n\nBorn-physical archives hold valuable data that is currently contained in forms such as shipping containers. These physical archives face risks of neglect and are vulnerable to damage. The research underscores the critical importance of preserving this data to ensure its availability for future generations of scholars and scientists. The work highlights the necessity of efforts dedicated to the preservation and accessibility of these historical records.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#consortium-and-call-for-collaboration",
    "href": "chapter_ai-nepi_020.html#consortium-and-call-for-collaboration",
    "title": "18  LLM for HPS Studies: Analyzing the NHGRI Archive",
    "section": "19.23 Consortium and Call for Collaboration",
    "text": "19.23 Consortium and Call for Collaboration\n\n\n\nSlide 18\n\n\nThe project is part of a larger consortium named “Born Physical, Studied Digitally.” The consortium is actively seeking to engage testers, partners, and users to collaborate on its initiatives. A call for collaboration is extended to individuals and institutions interested in working with the consortium.\nA specific note is made regarding the recent status of NHGRI, which was among the agencies proposed for dissolution in the past year. The presentation argues that NHGRI stands as one of the most innovative funding agencies in the history of science and emphasizes that the data contained within its archive holds substantial potential for answering important scientific questions.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLM for HPS Studies: Analyzing the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "20 From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#overview",
    "href": "chapter_ai-nepi_021.html#overview",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.1 Overview",
    "text": "20.1 Overview\nThe project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using Large Language Models (LLMs) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries.\nThe problem addressed is the lack of inherent structure in many valuable historical sources, which prevents complex analytical queries. The proposed solution is a two-stage pipeline. Stage 1 involves Open Information Extraction (OIE) using an LLM to extract subject-predicate-object triples from the text. This is followed by validation and refinement using an LLM ensemble acting as an adversary.\nThis stage includes a human-in-the-loop evaluation against domain expert extractions to assess quality using classical performance metrics. Stage 2 focuses on structuring the extracted triples into a knowledge graph. This stage is driven by research questions, defined as competency questions, which guide the creation of a domain-specific ontology.\nLLMs are used to draft both competency questions and the ontology, with human experts providing final refinement. Entity disambiguation is performed, including resolution to Wikidata instances. The structured data, along with metadata (source, scoring, chunk), is encoded into an RDF star graph. The pipeline includes a final validation step before export.\nCase studies include Zielinski’s Polish biographical dictionary (1930) and the “Who was who in the GDR” reference work. Initial experiments demonstrate the ability to build networks (e.g., editors and authors) and conduct structured analyses (e.g., correlation between state awards and political affiliation/roles).\nKey challenges identified are improving entity disambiguation and establishing proper benchmarking. Future work involves refining and completing the pipeline, systematic comparison with other graph extraction tools (Neo4j graph builder, Microsoft graph rack), developing graph RAG systems for natural language querying, and building multi-layered networks for deeper analysis. The approach emphasizes task decomposition, data/research question-driven structuring, and verifiability through human intervention at key points.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-extracting-structure-from-unstructured-sources",
    "href": "chapter_ai-nepi_021.html#introduction-extracting-structure-from-unstructured-sources",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.2 Introduction: Extracting Structure from Unstructured Sources",
    "text": "20.2 Introduction: Extracting Structure from Unstructured Sources\n\n\n\nSlide 01\n\n\nThe project aims to extract knowledge graphs from source material using Large Language Models (LLMs). The primary focus is on accessing and utilizing new types of sources for research that are currently computationally inaccessible due to their unstructured nature. While historical, philosophical, and social science (HPSS) research often utilizes structured data sources such as publication databases or email archives, a significant amount of valuable information resides in unstructured formats, particularly printed books and biographical dictionaries. The core problem addressed is the inability to perform structured queries on these unstructured sources.\nLLMs offer the potential to impose structure on this unstructured data. Historically, efforts like the Get Grass project attempted to address the computational accessibility of printed books. The current project specifically targets biographical sources, which are rich in detailed information about individuals but lack inherent structure. This absence of structure prevents researchers from asking complex, structured questions beyond simple facts like birth dates or work locations.\nThe goal is to enable queries about how professions formed networks over specific periods, how individuals migrating between locations contributed to the spread of ideas, or the specific roles of editors within a corpus in disseminating knowledge. The proposed solution involves using LLMs to construct knowledge graphs from this unstructured data in a controllable manner. A knowledge graph represents information as entities (such as persons, places, countries, or works) which become nodes in the graph. Relationships identified between these entities in the source material are represented as edges connecting the nodes. This structure allows for sophisticated structured querying.\nThe Neo4j graph database is used for representation and querying of the resulting knowledge graphs. The approach positions the LLM as one component within a larger processing pipeline, emphasizing its utility for specific tasks rather than seeking a universally perfect model. An example source snippet, an entry about the evangelical priest Henrik Bartsch born in 1832 who traveled and wrote books, illustrates the type of material processed. Traditional Natural Language Processing (NLP) approaches, such as NLTK, are often insufficient to extract the full contextual richness from such entries. The desired output is structured data in the form of statements or triples, capturing details like profession, birth date, birth place, and travel destinations from the text.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#two-stage-pipeline-stage-1---open-information-extraction",
    "href": "chapter_ai-nepi_021.html#two-stage-pipeline-stage-1---open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.3 Two-Stage Pipeline: Stage 1 - Open Information Extraction",
    "text": "20.3 Two-Stage Pipeline: Stage 1 - Open Information Extraction\n\n\n\nSlide 08\n\n\nThe process of transforming unstructured text into a structured knowledge graph is implemented as a two-stage pipeline. The first stage is responsible for extracting statements from the input text, while the second stage constructs the knowledge graph from these extracted statements, adhering to defined rules to ensure consistency and clear categorization.\nThe pipeline is built upon several core principles:\n\nIt employs task decomposition, breaking down the complex process into smaller, controllable, and verifiable steps.\nIt is data and research question driven, meaning the final structure of the knowledge graph is explicitly guided by the specific research aims of the project, rather than being solely dictated by a predefined ontology.\nVerifiability is integrated through human-in-the-loop steps at critical points in the process.\n\nThe input data for the pipeline often originates from messy, unstructured sources that typically require Optical Character Recognition (OCR) or scraping. Following these initial steps, the data undergoes preprocessing to achieve a semi-structured format before entering the pipeline.\nThe first central step of Stage 1 is Open Information Extraction (OIE). This step utilizes a Large Language Model (LLM) to extract all subject-predicate-object triples it can identify within the text, without relying on any preset categories or schemas. OIE is a rapidly evolving research area, with a significant shift towards using LLMs for this task.\nThe second step in Stage 1 involves LLM ensembling. A second LLM model is employed to validate and refine the initial statements extracted by the first model. This second model is specifically prompted to act as an adversary to the first, critically evaluating its output, correcting errors, identifying any missed triples, and assigning a confidence score to the extraction. This ensembling approach significantly enhances the quality of the extracted statements.\nThe final part of Stage 1 is evaluation. A sample of the validated output is evaluated against a corresponding sample created independently by domain experts. Classical performance metrics are calculated to quantify the quality of the extraction. This evaluation step represents the first instance of human-in-the-loop intervention, where domain experts judge the quality. A decision point is established: if the quality score meets a predefined threshold, the process moves to Stage 2; otherwise, Stage 1 is refined. The determination of what constitutes a “good enough” score is dependent on the specific use case and the characteristics of the dataset being processed.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#two-stage-pipeline-stage-2---knowledge-graph-structuring",
    "href": "chapter_ai-nepi_021.html#two-stage-pipeline-stage-2---knowledge-graph-structuring",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.4 Two-Stage Pipeline: Stage 2 - Knowledge Graph Structuring",
    "text": "20.4 Two-Stage Pipeline: Stage 2 - Knowledge Graph Structuring\n\n\n\nSlide 11\n\n\nFollowing the extraction of statements in Stage 1, Stage 2 of the pipeline focuses on imposing further structure on this knowledge to form a knowledge graph.\nThe first step in Stage 2 involves defining competency questions. These are specific questions that the final knowledge graph is designed to answer. The purpose of starting with competency questions is to ensure that the structure of the knowledge graph is tailored to the specific research questions driving the project, making it research-driven rather than solely dependent on a predefined, potentially overly broad, ontology. A reasoning model is utilized to draft an initial set of 20 to 30 competency questions. This draft is then refined and finalized by a human domain expert, incorporating a crucial human-in-the-loop step.\nBuilding upon the competency questions and the extracted triples, the next step is to construct the ontology for the knowledge graph. A reasoning model drafts this ontology, which is subsequently finalized and corrected by a domain expert, again involving human oversight.\nThe final step in Stage 2 encompasses several tasks: entity disambiguation, data encoding, and metadata inclusion. Entity disambiguation involves resolving variations in names or references to the same entity, such as mapping “Humboldt Uni Berlin” to its standardized form “Humboldt Universität zu Berlin”. Entities are also resolved to external, standardized instances, such as those found in Wikidata. The data is then encoded, and relevant metadata is included. This metadata comprises the original source data, the scoring data generated during Stage 1’s evaluation, and the initial text chunk from which the triples were extracted. The output format for the structured data is an RDF star graph.\nA final validation step is performed on the constructed knowledge graph. The resulting graph can then be exported in various formats depending on the intended use. Options include exporting it for network analysis into graph databases like Neo4j or storing it as a triple store to facilitate subsequent reasoning tasks. The pipeline is designed to handle the time dimension inherent in many biographical sources; if a time stamp is available for a statement in the source text, it is extracted along with the triple and modeled within the RDF star graph, which is crucial for analyzing developments and changes over time.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#use-cases-and-applications",
    "href": "chapter_ai-nepi_021.html#use-cases-and-applications",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.5 Use Cases and Applications",
    "text": "20.5 Use Cases and Applications\n\n\n\nSlide 15\n\n\nThe primary motivation for this pipeline is to provide a controlled method for converting unstructured data into structured data, thereby enabling researchers to ask overarching questions that were previously impossible.\nOne key use case involves Zielinski’s Polish biographical dictionary, a source compiled in 1930 as part of nation-building efforts during the formation of the Polish nation. This dictionary lists Poles who traveled abroad for exploration and other purposes. Although a PDF version is available, the printed format prevents structured querying. The pipeline enables researchers to ask specific questions about this corpus, such as how the migration of individuals facilitated the introduction of new ideas and fostered innovation in different locations, or the specific role played by editors listed in the dictionary in the spread of knowledge. An example result is a network graph, compiled by Alex Kay, visualizing the relationships between editors (represented as green nodes) and authors (represented as pink nodes). This type of network analysis, including the calculation of centrality measures and analysis across time, is not feasible manually from the printed PDF and provides novel information about the corpus.\nAnother use case is the biographical reference work “Who was who in the GDR”. This source documents approximately 4,000 prominent figures from East German history, including politicians, dissidents, scientists, and artists. First published in the early 1990s, it was digitized and made available online in the 2000s, allowing for text searches but not structured queries. The source is noted to have a bias towards including individuals based on their fame. An example of a structured question that can be explored after processing this source with the pipeline is identifying the differences between individuals who received state awards and those who did not, specifically concerning their political affiliations and roles within the state apparatus.\nAn initial experiment using an early version of the pipeline, applied to 1,000 randomly sampled biographies from this source (not the final validated set), yielded preliminary findings. It indicated strong correlations between state awards such as the Karl Marx Orden and Held der DDR with affiliation to the Socialist Unity Party of Germany (SED) and holding political power. In contrast, the Nationalpreis showed a weaker link to high positions compared to individuals who did not receive any award. This type of analysis is highly relevant for HPSS research as it allows for a structural investigation into the interconnection between science and politics. It enables researchers to examine how factors like education, political affiliation, or positions of power varied across different generations or cohorts of scientists, providing insights into the dynamics of this relationship over time.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-challenges-and-future-work",
    "href": "chapter_ai-nepi_021.html#conclusion-challenges-and-future-work",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "20.6 Conclusion, Challenges, and Future Work",
    "text": "20.6 Conclusion, Challenges, and Future Work\nIn summary, the project facilitates a transition from viewing biographical sources as collections of isolated entities to enabling complex structural queries across the data.\nThe main challenges currently faced include improving the accuracy and robustness of entity disambiguation and establishing proper benchmarking methodologies to systematically evaluate the pipeline’s performance.\nImmediate future work involves refining and completing the current pipeline, which is presently considered a proof of concept. A key next step is to systematically compare its performance against other existing graph extraction pipelines, such as the Neo4j graph builder and the Microsoft graph rack.\nLooking further ahead, future perspectives include utilizing the constructed knowledge graph in conjunction with the initially extracted text chunks to develop a graph Retrieval Augmented Generation (RAG) system. The objective of this graph RAG system is to allow users to query the entire dataset using natural language. Additionally, the project aims to develop methods for building multi-layered networks from the knowledge graph to support deeper structural analysis of the relationships within the data.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "20  References",
    "section": "",
    "text": "21 References\n{.unlisted}",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>References</span>"
    ]
  }
]