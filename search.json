[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held April 2-4, 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "",
    "text": "Overview\nThis workshop emerges from the confluence of two primary initiatives. The first is the ‘Network Epistemology in Practice’ (NEPI) project, within which Arno Simons has pioneered the training of large language models on physics texts, whilst Michael Zichert has explored their application to conceptual problems in physics. The second originates with Gerd Graßhoff, a key cooperation partner who has long championed the use of artificial intelligence in the history and philosophy of science, particularly for analysing processes of scientific discovery.\nRepresenting a fusion of these intellectual currents, the workshop is supported by an ERC grant awarded to the NEPI project. The project’s central objective is to investigate the internal communication of the ATLAS collaboration at CERN. To this end, the team employs network analysis to map the collaboration’s communication structures and uses advanced semantic tools, including large language models, to trace the flow of ideas across these networks.\nThe organisation of this event was expertly managed by Svenja Goetz, Lea Stengel, and Julia Kim. Essential technical support was provided by Oliver Ziegler and his Unicam team.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-and-dissemination",
    "href": "chapter_ai-nepi_001.html#recording-and-dissemination",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.1 Recording and Dissemination",
    "text": "2.1 Recording and Dissemination\n\n\n\nSlide 02\n\n\nA comprehensive recording protocol is in place for all workshop sessions to ensure a high-quality record of the proceedings. The technical arrangement, managed by Oliver Ziegler and the Unicam team, comprises a primary camera focused on the active speaker, four microphones to capture audience contributions, and an iPhone as a backup audio recorder. This configuration also facilitates a seamless experience for remote attendees joining via Zoom.\nFollowing the workshop, and with the explicit consent of the presenters, recordings of the talks and subsequent discussions will be made publicly available on the NEPI project’s YouTube channel. Participants consented to this policy upon registration and are welcome to contact the organisers with any queries.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#participant-interaction",
    "href": "chapter_ai-nepi_001.html#participant-interaction",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.2 Participant Interaction",
    "text": "2.2 Participant Interaction\n\n\n\nSlide 04\n\n\nTo foster productive dialogue, the workshop follows a structured protocol for interaction. During question-and-answer sessions, attendees are requested to formulate their questions concisely. The session chair will gather a small group of questions before inviting the presenter to offer a collective response, a method that streamlines the exchange of ideas.\nFor discussions that extend beyond the live sessions, a shared Etherpad (or CryptPad), accessible via a QR code, provides an asynchronous forum. This platform enables participants to post comments and questions in dedicated sections for each presentation. The Zoom chat also remains available for commentary throughout the event. Furthermore, the programme features a dedicated discussion session on the second day to explore common themes, complemented by generous breaks and social events designed to encourage informal networking.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-semantic-change-and-data-science",
    "href": "chapter_ai-nepi_001.html#keynote-semantic-change-and-data-science",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.3 Keynote: Semantic Change and Data Science",
    "text": "2.3 Keynote: Semantic Change and Data Science\n\n\n\nSlide 05\n\n\nThe first keynote address, delivered by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg, will explore large-scale text analysis for the study of cultural and societal change. Nina Tahmasebi leads the ‘Change is Key!’ research programme, to which Pierluigi Cassotti contributes as a researcher. Their collective work has made significant contributions to the field of semantic change detection.\nTheir research is notable for its dual focus, spanning not only technical innovations such as the development of evaluation benchmarks but also the broader application of data science methodologies to address complex questions within the humanities. This perspective resonates strongly with the workshop’s interdisciplinary aims. For logistical arrangements, coffee breaks will be held in the main venue, whilst lunch and the evening reception will take place in room H2051.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-cross-document-nlp",
    "href": "chapter_ai-nepi_001.html#keynote-cross-document-nlp",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.4 Keynote: Cross-Document NLP",
    "text": "2.4 Keynote: Cross-Document NLP\n\n\n\nSlide 06\n\n\nThe second keynote will be delivered by Professor Iryna Gurevych, who leads the Ubiquitous Knowledge Processing Lab at the Technical University of Darmstadt. Her presentation, titled How to InterText? Elevating NLP to the cross-document level, will delve into advanced natural language processing techniques.\nProfessor Gurevych’s research concentrates on information extraction, semantic text processing, and machine learning. A defining feature of her work is the application of these computational methods to generate new insights within the social sciences and humanities, an approach that aligns closely with the interdisciplinary objectives of this workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter details the foundational architecture of Transformer-based Large Language Models (LLMs), charting a course from their core principles to their specialised application in the History, Philosophy, and Sociology of Science (HPSS). The authors begin by deconstructing the Transformer model, examining its essential encoder and decoder components. They illuminate the intricate process by which input text is converted into rich numerical representations through embedding, positional encoding, multi-head attention, and feed-forward networks.\nFrom this fundamental design, the analysis proceeds to explore how distinct model families have emerged. The authors contrast the encoder-centric architecture of BERT, which excels at bidirectional language understanding, with the decoder-focused structure of GPT, renowned for its generative capabilities. Building upon this technical foundation, the chapter outlines several strategies for adapting these powerful models to the specific demands of HPSS research. These include four training-based methods for domain specialisation and the increasingly prominent technique of Retrieval Augmented Generation (RAG).\nFurthermore, the authors propose a clear taxonomy for classifying LLMs, using criteria such as architecture, fine-tuning methods, and deployment models. The chapter culminates by mapping these computational methodologies onto specific scholarly applications within HPSS, addressing key research areas such as source management, the analysis of knowledge structures, the dynamics of conceptual change, and the examination of scientific practices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#presentation-agenda",
    "href": "chapter_ai-nepi_003.html#presentation-agenda",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.1 Presentation Agenda",
    "text": "3.1 Presentation Agenda\n\n\n\nSlide 02\n\n\nThe discussion commences with an agenda, metaphorically framed as ‘Today’s Menu’, which outlines the main topics and logical progression of the chapter. This structure serves to establish clear expectations for the material to be explored.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.2 The Transformer Architecture",
    "text": "3.2 The Transformer Architecture\n\n\n\nSlide 03\n\n\nAt the core of contemporary large language models resides the Transformer architecture, a sophisticated design centred on an encoder-decoder structure. This framework governs the flow of information, systematically transforming a sequence of input words into a set of output probabilities. The accompanying diagram visualises this entire process, clarifying the pathway from initial input to final prediction and thereby illustrating the model’s fundamental mechanics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-encoder-component",
    "href": "chapter_ai-nepi_003.html#the-encoder-component",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.3 The Encoder Component",
    "text": "3.3 The Encoder Component\n\n\n\nSlide 04\n\n\nThe Transformer’s encoder component executes the critical function of converting input text into rich, context-aware numerical representations. This conversion unfolds through a precise, multi-stage process.\nAn embedding layer first maps each word to a vector, after which positional encoding is integrated to furnish the model with vital information about word order. These vectors are then processed by a multi-head attention mechanism, which dynamically assesses the significance of different words in relation to one another. Finally, the refined representations pass through feed-forward networks, completing their transformation into a state prepared for subsequent analytical tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#specialised-architectures-bert-and-gpt",
    "href": "chapter_ai-nepi_003.html#specialised-architectures-bert-and-gpt",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.4 Specialised Architectures: BERT and GPT",
    "text": "3.4 Specialised Architectures: BERT and GPT\n\n\n\nSlide 08\n\n\nFrom the foundational Transformer architecture, engineers have developed specialised models tailored for distinct computational tasks. This divergence has produced two prominent model families, BERT and GPT, both of which have proven valuable for research in the History, Philosophy, and Sociology of Science (HPSS).\nBERT (Bidirectional Encoder Representations from Transformers) leverages the encoder stack to cultivate a deep, bidirectional understanding of linguistic context. In contrast, GPT (Generative Pre-trained Transformer) primarily employs the decoder stack, enabling it to excel at unidirectional, generative tasks such as text creation. These architectural specialisations facilitate new forms of computational analysis within the humanities and social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#divergent-processing-capabilities",
    "href": "chapter_ai-nepi_003.html#divergent-processing-capabilities",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.5 Divergent Processing Capabilities",
    "text": "3.5 Divergent Processing Capabilities\n\n\n\nSlide 09\n\n\nAlthough they originate from the same Transformer model, the distinct architectural choices underpinning BERT and GPT yield fundamentally different processing capabilities. BERT’s reliance on the encoder permits it to analyse entire sequences simultaneously, making it highly proficient at tasks that demand nuanced language understanding.\nConversely, GPT’s decoder-centric design enables it to generate coherent and contextually relevant text in a sequential, unidirectional fashion. This clear division of labour—understanding versus generation—determines their respective applications and analytical strengths.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#adaptation-through-training",
    "href": "chapter_ai-nepi_003.html#adaptation-through-training",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.6 Adaptation Through Training",
    "text": "3.6 Adaptation Through Training\n\n\n\nSlide 11\n\n\nTo harness large language models for specialised fields like HPSS, the authors outline four distinct adaptation strategies centred on model training. These methods enable the fine-tuning of a general-purpose LLM, imbuing it with the specific knowledge and capabilities required for scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#adaptation-via-retrieval-augmented-generation",
    "href": "chapter_ai-nepi_003.html#adaptation-via-retrieval-augmented-generation",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.7 Adaptation via Retrieval Augmented Generation",
    "text": "3.7 Adaptation via Retrieval Augmented Generation\n\n\n\nSlide 12\n\n\nBeyond direct model training, Retrieval Augmented Generation (RAG) presents a powerful alternative for domain and task adaptation. This technique enhances an LLM’s performance by integrating an external knowledge source into its response-generation process.\nThe RAG mechanism operates in two stages. First, a retrieval component searches a specified corpus of documents to locate information relevant to a user’s query. Next, a generation component synthesises this retrieved information, using it as context to produce a more accurate, detailed, and factually grounded response.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#a-taxonomy-of-language-models",
    "href": "chapter_ai-nepi_003.html#a-taxonomy-of-language-models",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.8 A Taxonomy of Language Models",
    "text": "3.8 A Taxonomy of Language Models\n\n\n\nSlide 13\n\n\nThe authors propose a taxonomy to systematically organise the landscape of large language models. This classification framework distinguishes models according to four key characteristics:\n\nArchitecture: Whether the model is encoder-only, decoder-only, or a full encoder-decoder implementation.\nFine-Tuning Strategy: The specific methods applied to adapt the model for particular tasks or domains.\nEmbedding Type: The nature of the numerical representations the model employs.\nDeployment Abstraction: The level at which the model is accessed, from a simple API to a fully self-hosted instance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#computational-methods-in-hpss",
    "href": "chapter_ai-nepi_003.html#computational-methods-in-hpss",
    "title": "3  Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science",
    "section": "3.9 Computational Methods in HPSS",
    "text": "3.9 Computational Methods in HPSS\n\n\n\nSlide 14\n\n\nThe authors conclude by mapping these computational methods onto research practices in the History, Philosophy, and Sociology of Science. They organise the potential applications of LLMs into four primary domains of scholarly work:\n\nData and Source Management: Assisting in the organisation, curation, and processing of historical and scholarly materials.\nAnalysis of Knowledge Structures: Enabling the exploration of conceptual frameworks and intellectual networks within large corpora.\nInvestigation of Knowledge Dynamics: Facilitating inquiry into how concepts, theories, and disciplines evolve over time.\nExamination of Knowledge Practices: Allowing for the analysis of patterns in scientific communication, argumentation, and collaboration.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Transformer Architectures and LLM Adaptation for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "",
    "text": "Overview\nMax Neuchel, Andrea Loetgers, and Taya Knulla of Utrecht University have crafted OpenAlex Mapper, a sophisticated tool for visualising and querying scientific discourse. Their system processes extensive datasets of scholarly articles from OpenAlex, employing advanced dimensionality reduction to map publications into a two-dimensional conceptual space.\nThe team’s methodology involves sampling 300,000 English-language articles with well-formed abstracts, embedding them with a language model, and subsequently projecting them via a pre-trained UMAP model. A key feature of the tool is its capacity to accept arbitrary user queries to OpenAlex, download the results, and seamlessly integrate these new articles into the existing conceptual map. This functionality facilitates a dynamic exploration of research trends and the conceptual distribution of scientific knowledge, despite a minor technical issue encountered during a live demonstration.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#project-origins-and-data-acquisition",
    "href": "chapter_ai-nepi_004.html#project-origins-and-data-acquisition",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.1 Project Origins and Data Acquisition",
    "text": "4.1 Project Origins and Data Acquisition\n\n\n\nSlide 02\n\n\nThe OpenAlex Mapper project, a collaborative endeavour by Max Neuchel, Andrea Loetgers, and Taya Knulla, originates from the Department of Theoretical Philosophy at Utrecht University. The team initiated their work by acquiring a substantial dataset from the OpenAlex database, comprising 300,000 randomly selected articles.\nTo ensure data quality, the authors applied stringent selection criteria. Each article was required to possess a well-formed abstract and be published in English, thereby establishing a robust foundation for the subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#dimensionality-reduction-and-functionality",
    "href": "chapter_ai-nepi_004.html#dimensionality-reduction-and-functionality",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.2 Dimensionality Reduction and Functionality",
    "text": "4.2 Dimensionality Reduction and Functionality\n\n\n\nSlide 03\n\n\nTo process the extensive article dataset, the team applied Uniform Manifold Approximation and Projection (UMAP), a technique that effectively reduces the data’s dimensionality to two. This trained UMAP model is meticulously retained, ensuring consistent application across all operations.\nAs a dedicated tool, OpenAlex Mapper leverages this foundational model. It enables users to submit arbitrary queries directly to the OpenAlex database, which facilitates dynamic data retrieval and analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#embedding-and-projection-workflow",
    "href": "chapter_ai-nepi_004.html#embedding-and-projection-workflow",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.3 Embedding and Projection Workflow",
    "text": "4.3 Embedding and Projection Workflow\n\n\n\nSlide 04\n\n\nThe team established a precise workflow for integrating new data into OpenAlex Mapper. The process begins with the tool downloading query results directly from the OpenAlex database.\nSubsequently, it embeds these newly acquired articles using the identical language model employed during the initial dataset processing. Finally, the system projects these embedded representations through the pre-trained UMAP model, ensuring a consistent placement within the established two-dimensional conceptual space.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#conceptual-integration",
    "href": "chapter_ai-nepi_004.html#conceptual-integration",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.4 Conceptual Integration",
    "text": "4.4 Conceptual Integration\n\n\n\nSlide 05\n\n\nThrough this systematic process, newly processed articles acquire precise positions on the two-dimensional conceptual map. The underlying principle ensures that these positions are assigned as if the articles had been an integral part of the original layout.\nThis seamless integration facilitates a coherent and continuously evolving visualisation of scholarly discourse, thereby maintaining the integrity of the conceptual distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#the-role-of-umap-and-live-demonstration",
    "href": "chapter_ai-nepi_004.html#the-role-of-umap-and-live-demonstration",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.5 The Role of UMAP and Live Demonstration",
    "text": "4.5 The Role of UMAP and Live Demonstration\n\n\n\nSlide 06\n\n\nThe inherent features of UMAP significantly simplify the integration of new data points into the existing map, streamlining the process considerably. Following this technical explanation, the presenter proceeded with a live demonstration of the OpenAlex Mapper tool, aiming to illustrate its functionality in real-time.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#demonstration-challenges",
    "href": "chapter_ai-nepi_004.html#demonstration-challenges",
    "title": "4  OpenAlex Mapper presentation slide",
    "section": "4.6 Demonstration Challenges",
    "text": "4.6 Demonstration Challenges\n\n\n\nSlide 07\n\n\nAn unexpected technical issue arose during the live demonstration, which unfortunately prevented the tool from operating as intended. This anomaly, which had not been observed during extensive prior testing, was unforeseen. Nevertheless, the presenter promptly initiated a second attempt to run the demonstration and resolve the challenge.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenAlex Mapper presentation slide</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "",
    "text": "Overview\nThe authors present a comprehensive study on genre classification within historical patient periodicals, conducted as part of the ERC-funded ActDisease project. This programme investigates how patient organisations in 20th-century Europe shaped concepts of disease and medical practices, employing their magazines as a primary data source. A substantial corpus, the ActDisease Dataset, underpins this work, comprising 96,186 digitised pages from patient magazines across Germany, Sweden, France, and the United Kingdom.\nThe core technical challenge the authors address is the classification of diverse and often ambiguous textual genres found within these historical documents. This task is complicated by issues in Optical Character Recognition (OCR) and the limitations of traditional methods like topic modelling. To overcome these obstacles, the research team developed a sophisticated, expert-driven genre-labelling scheme and conducted extensive experiments using both zero-shot and few-shot learning paradigms.\nTheir study evaluates a range of multilingual encoder models, including XLM-Roberta, mBERT, and a specialised historical mBERT (hmBERT), alongside generative models such as Llama-3.1 8b. Key findings demonstrate that few-shot learning, particularly with Masked Language Model (MLM) fine-tuning, significantly improves performance. The hmBERT-MLM model emerges as the most effective, highlighting the value of domain-specific pre-training for historical texts. The work concludes that the rich generic diversity of popular magazines makes them a more complex text mining target than scientific journals, underscoring the necessity of robust genre classification for fine-grained historical analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#presentation-outline",
    "href": "chapter_ai-nepi_005.html#presentation-outline",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.1 Presentation Outline",
    "text": "5.1 Presentation Outline\n\n\n\nSlide 02\n\n\nThe authors structure their research programme into three principal sections. It begins with an introduction to the ActDisease project, followed by a detailed examination of the genre classification experiments. The presentation culminates in a summary of the key conclusions drawn from the research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.2 The ActDisease Project",
    "text": "5.2 The ActDisease Project\n\n\n\nSlide 03\n\n\nThe authors initiated the ‘ActDisease’ project, an ERC-funded research programme designed to explore the influence of patient organisations on medicine and society. Central to the project is an analysis of how these groups, active throughout 20th-century Europe, shaped concepts of disease, the personal experience of illness, and prevailing medical practices. To achieve this, the investigators use patient-published periodicals as the primary source material for their historical analysis. An image of Heligoland, Germany, provides visual context for the historical settings under investigation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-composition",
    "href": "chapter_ai-nepi_005.html#dataset-composition",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.3 Dataset Composition",
    "text": "5.3 Dataset Composition\n\n\n\nSlide 04\n\n\nThe project team compiled the ActDisease Dataset, a specialised corpus of digitised magazines published by patient organisations. This collection spans materials from Germany, Sweden, France, and the UK, amounting to a total of 96,186 pages. A summary table provides a detailed breakdown of the dataset, specifying the number of unique magazine titles, total page counts, and the range of publication years for different diseases within each country.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-and-ocr-challenges",
    "href": "chapter_ai-nepi_005.html#digitisation-and-ocr-challenges",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.4 Digitisation and OCR Challenges",
    "text": "5.4 Digitisation and OCR Challenges\n\n\n\nSlide 05\n\n\nThe digitisation of historical documents presented a significant technical hurdle. The process of Optical Character Recognition (OCR), in particular, often yields suboptimal results when applied to older, varied source materials. This challenge necessitates further research into effective post-OCR correction methods, which are crucial for enhancing the accuracy and utility of the digitised text for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-classification-challenges",
    "href": "chapter_ai-nepi_005.html#genre-classification-challenges",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.5 Genre Classification Challenges",
    "text": "5.5 Genre Classification Challenges\n\n\n\nSlide 06\n\n\nAnalysing patient periodicals introduces a distinct challenge in genre classification. The source materials contain a wide diversity of text types, ranging from medical advice to personal stories and advertisements. Consequently, conventional methods such as topic modelling or basic term-counting prove inadequate for accurately differentiating these nuanced genres, which often share overlapping vocabularies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#motivation-for-classification",
    "href": "chapter_ai-nepi_005.html#motivation-for-classification",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.6 Motivation for Classification",
    "text": "5.6 Motivation for Classification\n\n\n\nSlide 07\n\n\nThe authors’ focus on genre classification stems from its analytical utility. From a language technology standpoint, genre provides a framework for understanding the communicative purpose of a text. By classifying content into distinct genres, the team can conduct more rigorous historical investigations, separating, for instance, official announcements from personal testimonials. Ultimately, this capability enables a more fine-grained and context-aware analysis of the entire dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrating-genre-diversity",
    "href": "chapter_ai-nepi_005.html#illustrating-genre-diversity",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.7 Illustrating Genre Diversity",
    "text": "5.7 Illustrating Genre Diversity\n\n\n\nSlide 08\n\n\nA collage of documents illustrates the project’s central challenge, showcasing the diverse textual genres related to a specific disease, likely diabetes. This visual representation effectively communicates the variety of formats and styles—from scientific articles to personal letters and advertisements—that the authors must categorise. It underscores the complexity of performing automated analysis on such heterogeneous source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#textual-examples-of-variation",
    "href": "chapter_ai-nepi_005.html#textual-examples-of-variation",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.8 Textual Examples of Variation",
    "text": "5.8 Textual Examples of Variation\n\n\n\nSlide 09\n\n\nTo reinforce the concept of genre diversity, the authors provide several concrete textual examples from the ‘ActDisease’ domain, specifically concerning diabetes. These snippets highlight the distinct linguistic and structural features of different genres found within the patient magazines. By presenting these varied examples, the team clarifies the practical difficulties and the importance of developing a robust classification system.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.9 Defining Genre Labels",
    "text": "5.9 Defining Genre Labels\n\n\n\nSlide 11\n\n\nThe authors established a formal set of genre labels to structure their classification task. Rather than being algorithmically derived, these labels were defined by subject-matter experts to ensure historical and contextual relevance. The primary function of this schema is to enable the systematic separation of content according to its type, which is essential for nuanced historical analysis. Moreover, the team designed the labels with a view towards general-purpose applicability, aiming for a system that could be adapted for other historical text analysis projects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-classification-schema",
    "href": "chapter_ai-nepi_005.html#genre-classification-schema",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.10 Genre Classification Schema",
    "text": "5.10 Genre Classification Schema\n\n\n\nSlide 12\n\n\nA detailed classification schema formally defines the nine distinct text genres used in the project: Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and QA (Question & Answer). For clarity and consistency in annotation, a comprehensive table outlines the specific characteristics of each genre and provides representative examples drawn from the source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-methodology",
    "href": "chapter_ai-nepi_005.html#annotation-methodology",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.11 Annotation Methodology",
    "text": "5.11 Annotation Methodology\n\n\n\nSlide 13\n\n\nThe team developed a rigorous methodology for creating the ground-truth dataset, establishing the paragraph as the fundamental unit for genre annotation. Two student annotators, working with German patient magazines focused on diabetes, applied the predefined genre labels to the text. To ensure the reliability of this process, the authors calculated the inter-annotator agreement, achieving a Cohen’s Kappa score of 0.77, which signifies a substantial level of consistency.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-in-practice",
    "href": "chapter_ai-nepi_005.html#annotation-in-practice",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.12 Annotation in Practice",
    "text": "5.12 Annotation in Practice\n\n\n\nSlide 14\n\n\nA practical example demonstrates the annotation process. Three sample paragraphs extracted from the German magazine Der Diabetiker are presented in a table. Each paragraph is paired with its assigned genre label, clearly illustrating how the classification schema is applied to actual source text. This example serves to clarify the task for both training and evaluation purposes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splits-for-experiments",
    "href": "chapter_ai-nepi_005.html#dataset-splits-for-experiments",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.13 Dataset Splits for Experiments",
    "text": "5.13 Dataset Splits for Experiments\n\n\n\nSlide 15\n\n\nFor their machine learning experiments, the authors partitioned the annotated ActDisease data into specific training and held-out sets. They carefully designed this division to evaluate model performance in both few-shot and zero-shot learning scenarios. The training set consists exclusively of annotated German texts. In contrast, the held-out (test) set comprises texts in German, French, and Swedish, and crucially, it includes genres that are deliberately absent from the training data to test zero-shot generalisation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-and-language-distribution",
    "href": "chapter_ai-nepi_005.html#genre-and-language-distribution",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.14 Genre and Language Distribution",
    "text": "5.14 Genre and Language Distribution\n\n\n\nSlide 16\n\n\nAn analysis of the dataset reveals the distribution of text instances across different languages and genres. A comparison between the training and held-out sets highlights two important characteristics. First, there are significant imbalances in the prevalence of certain genres, a common feature of real-world data. Second, the held-out set intentionally includes novel genres absent from the training set, a design critical for rigorously assessing the zero-shot classification capabilities of the models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot",
    "href": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.15 External Datasets for Zero-Shot",
    "text": "5.15 External Datasets for Zero-Shot\n\n\n\nSlide 17\n\n\nTo enhance their zero-shot learning experiments, the team incorporated several publicly available, multilingual datasets for genre classification. These external resources, which include CORE, UDM, and FTD, provide a mix of document-level and sentence-level annotations. Leveraging these datasets allows for more robust training and evaluation of the models’ ability to generalise to unseen labels and data distributions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-label-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-label-mapping",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.16 Cross-Dataset Label Mapping",
    "text": "5.16 Cross-Dataset Label Mapping\n\n\n\nSlide 18\n\n\nA comparative table illustrates the mapping of genre labels across the four datasets used in the study: ActDisease, CORE, UDM, and FTD. This visualisation reveals considerable variation in the classification schemas, with different datasets using distinct and sometimes conflicting genre definitions. Such heterogeneity poses a significant challenge for zero-shot learning, as models trained on one schema must adapt to another, necessitating a thoughtful approach to label mapping.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-pipeline",
    "href": "chapter_ai-nepi_005.html#training-data-pipeline",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.17 Training Data Pipeline",
    "text": "5.17 Training Data Pipeline\n\n\n\nSlide 19\n\n\nThe authors designed a comprehensive and flexible pipeline for generating training data. This process integrates data from all four sources—ActDisease, CORE, UDM, and FTD—and subjects them to a series of preprocessing steps. Crucially, the pipeline incorporates configurable sampling strategies, allowing the team to create various training set compositions to test different hypotheses about model performance and data balancing systematically.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.18 Multilingual Encoder Models",
    "text": "5.18 Multilingual Encoder Models\n\n\n\nSlide 20\n\n\nThe experiments leverage several powerful multilingual encoder models as the basis for classification. The selected models include the widely used XLM-Roberta (xlmr) and multilingual BERT (mBERT). In addition, the team evaluates a specialised historical mBERT (hmbert), which has been pre-trained on a large corpus of historical texts. This selection allows for a comparison between general-purpose multilingual models and a model adapted for the specific domain of historical language.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#fine-tuning-setup",
    "href": "chapter_ai-nepi_005.html#fine-tuning-setup",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.19 Fine-Tuning Setup",
    "text": "5.19 Fine-Tuning Setup\n\n\n\nSlide 21\n\n\nThe experimental design for fine-tuning involved a systematic and large-scale approach. The authors created 16 unique training set configurations by varying the data sources and sampling strategies. They then fine-tuned each of the three base language models (XLM-R, mBERT, and hmBERT) on every one of these 16 configurations. This comprehensive methodology resulted in a total of 48 distinct fine-tuned models, enabling a thorough analysis of how training data composition affects performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-zero-shot-learning",
    "href": "chapter_ai-nepi_005.html#evaluating-zero-shot-learning",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.20 Evaluating Zero-Shot Learning",
    "text": "5.20 Evaluating Zero-Shot Learning\n\n\n\nSlide 22\n\n\nThe investigation now shifts its focus towards evaluating the models’ performance in a zero-shot learning context. This phase assesses the ability of the fine-tuned models to classify genres that they have not encountered during their training phase.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-evaluation-methodology",
    "href": "chapter_ai-nepi_005.html#zero-shot-evaluation-methodology",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.21 Zero-Shot Evaluation Methodology",
    "text": "5.21 Zero-Shot Evaluation Methodology\n\n\n\nSlide 23\n\n\nEvaluating zero-shot predictions requires a specialised methodology to handle inherent complexities. The primary challenges include managing the partial overlap between genre label sets from different source datasets and accounting for cross-lingual scenarios where a model is tested on a language not present in its training data. The authors’ evaluation protocol is designed specifically to navigate these issues, ensuring a robust and fair assessment of model generalisation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-results-overview",
    "href": "chapter_ai-nepi_005.html#zero-shot-results-overview",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.22 Zero-Shot Results Overview",
    "text": "5.22 Zero-Shot Results Overview\n\n\n\nSlide 24\n\n\nAn overview of the zero-shot experiment results reveals several key patterns. For the FTD dataset, employing a specific label mapping strategy yields a noticeable improvement in model performance. Across other datasets, however, results indicate the presence of class-specific and language-related biases. On the UDM dataset, the investigators observed intriguing performance variations between the different models for certain classification tasks. Notably, models fine-tuned using the CORE dataset demonstrate a particular aptitude for correctly identifying texts belonging to the Legal genre.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#comparative-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#comparative-confusion-matrices",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.23 Comparative Confusion Matrices",
    "text": "5.23 Comparative Confusion Matrices\n\n\n\nSlide 25\n\n\nFour confusion matrices provide a visual comparison of the performance of different genre classification models. The matrices detail the results for models such as hmbert_UDM, xlmr_CORE, xlmr_UDM, and xlmr_FTD, each representing a unique combination of base architecture and training data. These visualisations allow for a direct comparison of error patterns and classification accuracy under various experimental conditions when evaluated on the held-out ActDisease data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-f1-scores",
    "href": "chapter_ai-nepi_005.html#zero-shot-f1-scores",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.24 Zero-Shot F1 Scores",
    "text": "5.24 Zero-Shot F1 Scores\n\n\n\nSlide 26\n\n\nA summary table presents the zero-shot performance using the per-category F1 score as the primary metric. These scores, which measure the balance between precision and recall for each genre, are averaged across the various training data configurations. This allows for a consolidated view of how well each language model performs on individual genre categories in a zero-shot setting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#impact-of-data-configuration",
    "href": "chapter_ai-nepi_005.html#impact-of-data-configuration",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.25 Impact of Data Configuration",
    "text": "5.25 Impact of Data Configuration\n\n\n\nSlide 27\n\n\nThe authors analysed the average F1 performance of the classification models across three distinct target tasks, corresponding to the FTD, CORE, and UDM datasets. This analysis specifically investigates how performance is affected by two key factors in the training data construction: the application of data balancing techniques and the inclusion or exclusion of certain language families. The results illuminate the sensitivity of model performance to the composition of the training corpus.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-few-shot-learning",
    "href": "chapter_ai-nepi_005.html#evaluating-few-shot-learning",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.26 Evaluating Few-Shot Learning",
    "text": "5.26 Evaluating Few-Shot Learning\n\n\n\nSlide 28\n\n\nThe report now transitions to an evaluation of the models under a few-shot learning paradigm. This section assesses how effectively the models can learn to classify genres when provided with only a small number of training examples from the target ActDisease dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.27 Few-Shot Learning Performance",
    "text": "5.27 Few-Shot Learning Performance\n\n\n\nSlide 29\n\n\nThe evaluation of few-shot learning reveals clear performance trends. As expected, F1 scores for all models generally improve as the number of available training examples increases. A crucial finding is that an intermediate step of Masked Language Model (MLM) fine-tuning on the target domain’s text confers a distinct advantage, consistently boosting classification accuracy. Amongst all configurations, the hmBERT-MLM model, which combines historical pre-training with domain-specific MLM fine-tuning, achieves the highest performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-f1-score-details",
    "href": "chapter_ai-nepi_005.html#few-shot-f1-score-details",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.28 Few-Shot F1 Score Details",
    "text": "5.28 Few-Shot F1 Score Details\n\n\n\nSlide 30\n\n\nA detailed table presents the per-category F1 scores for the few-shot learning experiments. The results are broken down for each language model and are shown at two distinct levels of data availability (for example, with 16 and 32 training examples per class). In addition to the granular, per-category scores, the table also includes overall performance metrics, allowing for a comprehensive comparison of the models’ effectiveness in a data-scarce environment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#full-dataset-performance",
    "href": "chapter_ai-nepi_005.html#full-dataset-performance",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.29 Full-Dataset Performance",
    "text": "5.29 Full-Dataset Performance\n\n\n\nSlide 31\n\n\nWhen trained on the full dataset, the XLM-Roberta-MLM model demonstrates strong classification capabilities, as illustrated by a confusion matrix. The matrix reveals specific patterns of misclassification, which in turn provide insights into the nature of the data. For instance, it highlights the thematic and stylistic similarities between the Guide, Nonfiction Prose, and QA genres as they appear within the context of diabetes patient magazines, explaining why the model sometimes confuses them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-few-shot-prompting",
    "href": "chapter_ai-nepi_005.html#evaluating-few-shot-prompting",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.30 Evaluating Few-Shot Prompting",
    "text": "5.30 Evaluating Few-Shot Prompting\n\n\n\nSlide 32\n\n\nThe final experimental section shifts to an evaluation of few-shot prompting. This approach assesses the ability of large generative language models to perform genre classification based on instructions and a small number of examples provided directly in the input prompt.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#instruction-based-classification",
    "href": "chapter_ai-nepi_005.html#instruction-based-classification",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.31 Instruction-Based Classification",
    "text": "5.31 Instruction-Based Classification\n\n\n\nSlide 33\n\n\nFor the prompting experiment, the authors formulated a detailed set of instructions for a text genre classification task. The prompt explicitly defines the nine target genres, complete with illustrative examples for each, and specifies the required input and output format. The chosen language model for executing this instruction-based, few-shot task is Llama-3.1 8b.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-performance",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-performance",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.32 Llama-3.1 8b Performance",
    "text": "5.32 Llama-3.1 8b Performance\n\n\n\nSlide 34\n\n\nThe performance of the Llama-3.1 8b Instruct model on the few-shot prompting task is presented. The results include F1-scores for each genre, quantifying the model’s accuracy. A detailed confusion matrix is also provided, offering a granular view of the model’s classification decisions and revealing which genres were most frequently confused with one another.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusion-on-text-mining",
    "href": "chapter_ai-nepi_005.html#conclusion-on-text-mining",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.33 Conclusion on Text Mining",
    "text": "5.33 Conclusion on Text Mining\n\n\n\nSlide 35\n\n\nA primary conclusion from this work is that applying text mining techniques to popular magazines is an inherently more complex task than analysing more uniform corpora like scientific journals or books. The authors attribute this increased difficulty directly to the rich and varied multitude of genres that coexist within a single magazine issue, demanding more sophisticated analytical approaches.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#summary-of-conclusions",
    "href": "chapter_ai-nepi_005.html#summary-of-conclusions",
    "title": "5  Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project",
    "section": "5.34 Summary of Conclusions",
    "text": "5.34 Summary of Conclusions\n\n\n\nSlide 37\n\n\nThe authors’ research yields several key conclusions:\n\nThe generic diversity of historical magazines poses a substantial challenge for text mining.\nGenre classification proves to be an indispensable tool, enabling the kind of fine-grained analysis necessary for deep historical inquiry.\nModern, large-scale datasets can be successfully leveraged to improve the analysis of historical texts.\nContemporary generative models exhibit promising quality on these classification tasks.\nFew-shot learning with multilingual encoders is a highly effective strategy, with performance being particularly strong when using models specifically adapted for historical language.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification Experiments for Historical Patient Periodicals: The ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year initiative funded by an ERC Starting Grant and hosted at the Vrije Universiteit Brussel, investigates the profound influence of the ‘ancient wisdom’ tradition upon early modern natural philosophy. The project team is analysing a vast, multilingual corpus of 430,000 printed books published between 1540 and 1728, with texts sourced from Early English Books Online (EEBO), Gallica, and the Bavarian State Library. Adopting a computational approach to the History, Philosophy, and Sociology of Science (HPSS), the project aims to uncover previously overlooked intellectual networks by identifying both direct lexical and indirect semantic textual reuse.\nA key component of this work is the VERITRACE web application, an alpha-stage platform built upon an Elasticsearch backend. This powerful tool facilitates corpus exploration, advanced keyword searching, and sophisticated text matching. The project confronts significant challenges, including the variable quality of Optical Character Recognition (OCR) from raw library-provided files, the complexities of early modern typography across six languages, and the sheer scale of the data. To address these hurdles, the team utilises Large Language Models (LLMs) for two primary purposes. GPT-based models act as ‘judges’ to enrich and clean bibliographic metadata, whilst BERT-based models such as LaBSE generate vector embeddings to encode semantic meaning for passage comparison. The ultimate goal is to provide scholars with a powerful tool to discover new patterns in intellectual history, effectively creating an ‘early modern plagiarism detector’ that reveals the ‘great unread’ texts of the period.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#research-context-and-aims",
    "href": "chapter_ai-nepi_006.html#research-context-and-aims",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.1 Research Context and Aims",
    "text": "6.1 Research Context and Aims\n\n\n\nSlide 03\n\n\nAt the Vrije Universiteit Brussel, Professor Cornelis J. Schilt and his team are conducting the VERITRACE project. The central objective is to trace the influence of the ‘ancient wisdom’ tradition on the evolution of early modern natural philosophy and science. Their investigation centres on a core collection of 140 works representing this tradition, including such texts as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the historically significant Corpus Hermeticum.\nThis inquiry extends far beyond established connections, such as Isaac Newton’s documented engagement with the Sibylline Oracles or Johannes Kepler’s knowledge of the Corpus Hermeticum. A primary goal of the project is to delve deeper, uncovering a much broader and often neglected network of texts and authors. By focusing on what the team terms the ‘great unread’—a vast body of work by often lesser-known figures—the project aims to reveal a more comprehensive picture of this tradition’s impact.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#a-computational-methodology",
    "href": "chapter_ai-nepi_006.html#a-computational-methodology",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.2 A Computational Methodology",
    "text": "6.2 A Computational Methodology\n\n\n\nSlide 04\n\n\nThe project employs a computational framework for the History, Philosophy, and Sociology of Science (HPSS), applying large-scale, multilingual exploration to its core research questions. A central feature of this methodology is the identification of textual reuse across the extensive corpus. The team has developed systems to detect both direct lexical reuse, where wording is identical or highly similar, and indirect semantic reuse, where concepts are shared without verbatim overlap. This capability effectively functions as an ‘early modern plagiarism detector’.\nBeyond simply identifying copied text, this approach seeks to uncover networks of texts, passages, themes, and authors that traditional scholarship may have overlooked. By systematically mapping these connections, the team hopes to reveal new, large-scale patterns in intellectual history and the philosophy of science, thereby offering fresh perspectives on the period’s intellectual landscape.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-composition-and-scope",
    "href": "chapter_ai-nepi_006.html#corpus-composition-and-scope",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.3 Corpus Composition and Scope",
    "text": "6.3 Corpus Composition and Scope\n\n\n\nSlide 05\n\n\nTo conduct this investigation, the project team has assembled a large and diverse multilingual dataset focused exclusively on printed books and texts; handwritten materials are deliberately excluded to maintain a manageable scope. The corpus comprises approximately 430,000 books in six different languages, covering a period of nearly two centuries from 1540 to 1728. This timeframe was chosen to begin from a significant point in printing history and to conclude shortly after the death of Isaac Newton.\nThe data originates from three principal sources: the Early English Books Online (EEBO) collection, digitised materials from the French National Library accessed via Gallica, and the project’s largest contributor, the Bavarian State Library. This vast collection of texts is analysed using a suite of state-of-the-art digital techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-llm-solutions",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-llm-solutions",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.4 Core Challenges and LLM Solutions",
    "text": "6.4 Core Challenges and LLM Solutions\n\n\n\nSlide 06\n\n\nThe project confronts several core challenges inherent in working with historical texts at scale. A primary issue is the variable quality of the Optical Character Recognition (OCR), as the team receives raw text directly from libraries in formats like XML, HOCR, and HTML, without access to the ground-truth page images for verification. This poor OCR quality significantly affects all downstream processing. Furthermore, the work must navigate the complexities of early modern typography and semantics across at least six languages, alongside the sheer scale of managing hundreds of thousands of texts.\nTo address these issues, the team applies Large Language Models (LLMs) in two distinct ways. On the decoder side, GPT-based models function as ‘judges’ to help enrich and clean the vast collection of bibliographic metadata. This report, however, focuses on the encoder-side application, where the team uses BERT-based LLMs to generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages, forming the foundation for the project’s sophisticated text-matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-as-metadata-judges",
    "href": "chapter_ai-nepi_006.html#llms-as-metadata-judges",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.5 LLMs as Metadata ‘Judges’",
    "text": "6.5 LLMs as Metadata ‘Judges’\n\n\n\nSlide 09\n\n\nThe project team is exploring the use of LLMs to automate the highly tedious task of enriching and verifying bibliographic metadata. The manual process required each team member to compare 10,000 pairs of records to determine if they referred to the same underlying printed text. To streamline this, the team developed ‘The LLM Bench’, a system that employs a panel of open-source models like Llama to act as ‘judges’ in evaluating these potential matches.\nThe models receive extensive prompt guidelines and are tasked with producing a decision, a confidence level, and detailed reasoning. This approach, however, faces a significant hurdle: the models frequently hallucinate information. Whilst forcing more structured output can eliminate these hallucinations, it comes at a cost. The models then tend to provide generic, less useful responses, diminishing the value of their reasoning. Consequently, this application remains a work-in-progress as the team seeks to balance structured output with insightful analysis to create a genuinely useful automation tool.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-application-and-pipeline",
    "href": "chapter_ai-nepi_006.html#the-veritrace-application-and-pipeline",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.6 The VERITRACE Application and Pipeline",
    "text": "6.6 The VERITRACE Application and Pipeline\n\n\n\nSlide 12\n\n\nThe project’s core output is the VERITRACE web application, which is currently in an extremely early alpha version and not yet publicly available. This platform serves as a proof-of-concept, demonstrating the project’s intended capabilities. For its semantic analysis features, the application is currently testing a BERT-based model, LaBSE, to generate vector embeddings, although the team anticipates that this model may not be sufficient for the final product.\nUnderpinning the application is a complex data processing pipeline required to prepare the raw texts for the Elasticsearch database that serves as the backend. This multi-stage pipeline involves numerous steps, including text extraction, the creation of positional mappings, the segmentation of text into passages, and the assessment of OCR quality. Each of these steps demands careful optimisation to handle the data’s complexity and ensure the quality of the final indexed content.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-exploration-and-analysis",
    "href": "chapter_ai-nepi_006.html#corpus-exploration-and-analysis",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.7 Corpus Exploration and Analysis",
    "text": "6.7 Corpus Exploration and Analysis\n\n\n\nSlide 14\n\n\nThe VERITRACE web application features an ‘Explore’ section that provides users with high-level statistics about the corpus, which currently contains 427,305 metadata records in its prototype stage. This data is served directly from a MongoDB database. A ‘Metadata Explorer’ allows for deeper investigation, where users can inspect the rich metadata generated for each text.\nA crucial feature is the detailed language identification, which is performed on text segments as small as 50 characters. This granularity is essential for accurately cataloguing multilingual texts, which are common in the corpus. For instance, the system can identify a book as being 85% Latin and 15% Greek, information not typically available in standard library metadata. Furthermore, the system attempts to assess OCR quality on a page-by-page basis, providing a nuanced quality metric despite the challenge of not having ground-truth images for comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-keyword-search",
    "href": "chapter_ai-nepi_006.html#advanced-keyword-search",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.8 Advanced Keyword Search",
    "text": "6.8 Advanced Keyword Search\n\n\n\nSlide 15\n\n\nA central feature for scholars is the ‘Search’ section, which leverages the power of Elasticsearch to offer both basic and advanced querying. Even in its prototype stage with only 132 texts, the search index is a substantial 15 gigabytes, indicating that the full 430,000-text corpus will scale into the terabytes. Users can perform simple keyword searches, but the system’s strength lies in its advanced capabilities.\nScholars can construct fielded queries, for example, to find the keyword ‘Hermes’ only within works authored by ‘Kepler’. The system also supports complex nested queries with Boolean operators and, notably, proximity queries. This allows a user to search for instances where two terms, such as ‘Hermes’ and ‘Plato’, are mentioned within a specified word distance of each other, enabling more nuanced textual investigations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#analysis-and-reading-interfaces",
    "href": "chapter_ai-nepi_006.html#analysis-and-reading-interfaces",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.9 Analysis and Reading Interfaces",
    "text": "6.9 Analysis and Reading Interfaces\n\n\n\nSlide 17\n\n\nThe web application’s roadmap includes an ‘Analyse’ section, which is planned but not yet implemented. This future module will offer users a suite of advanced analytical tools, including topic modelling, Latent Semantic Analysis (LSA), and diachronic analysis to track changes in language and concepts over time.\nIn contrast, the ‘Read’ section is already functional. It provides scholars with direct access to the source material, recognising that they need to consult the original texts beyond the OCR data. This section integrates a Mirador viewer, which displays high-quality PDF facsimiles of every text in the corpus. This feature allows for a seamless reading experience, similar to using a modern digital library portal, with all relevant bibliographic metadata displayed alongside the text.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-text-matching-engine",
    "href": "chapter_ai-nepi_006.html#the-text-matching-engine",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.10 The Text-Matching Engine",
    "text": "6.10 The Text-Matching Engine\n\n\n\nSlide 19\n\n\nThe most innovative part of the application is the ‘Match’ section, designed to identify textual reuse. The tool is highly flexible, allowing users to compare a single text against another, or to compare a collection of documents—such as all of Kepler’s works—against another text. The ultimate ambition is to allow a single text to be compared against the entire corpus, a feature that poses immense computational challenges regarding user wait times.\nCrucially, the interface exposes technical parameters to the user, allowing them to tweak settings like the minimum similarity score. The tool offers two primary match types: lexical matching, which identifies shared vocabulary, and semantic matching, which uses vector embeddings to find conceptually similar passages even if they share no keywords. This latter function is vital for cross-lingual comparisons. Users can also select from different performance modes, including a standard mode, a computationally intensive ‘comprehensive’ mode, and a ‘faster’ mode for quick checks.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#case-study-matching-newtons-opticks",
    "href": "chapter_ai-nepi_006.html#case-study-matching-newtons-opticks",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.11 Case Study: Matching Newton’s Opticks",
    "text": "6.11 Case Study: Matching Newton’s Opticks\n\n\n\nSlide 20\n\n\nTo test the matching tool, the project team performed a series of sanity checks using the Latin (1719) and English (1718) editions of Isaac Newton’s Opticks. A lexical match between the two different-language texts correctly returned no results in standard mode, as expected. However, switching to the more intensive ‘comprehensive’ mode revealed three matches, correctly identifying small sections of English text present within the predominantly Latin volume. The interface presents these results with a quality score, a coverage score, and details on the millions of comparisons performed.\nConversely, a semantic match between the two editions produced reasonable results. The system successfully identified conceptually similar passages, such as discussions of colours, demonstrating its ability to work across translations. Nevertheless, some metrics, like the coverage score, appeared inaccurate, although this might reflect genuine differences between the two editions, as the Latin version is considerably longer. Despite these partial successes, the team considers the current embedding model not yet good enough for the project’s demanding requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-strategy",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-strategy",
    "title": "6  VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy",
    "section": "6.12 Future Challenges and Strategy",
    "text": "6.12 Future Challenges and Strategy\n\n\n\nSlide 26\n\n\nLooking ahead, the project team must confront several significant challenges. The choice of an embedding model is paramount; the current model is likely insufficient, forcing a decision between adopting a more accurate but resource-intensive pre-trained model or undertaking the complex task of fine-tuning a base model specifically on the unique historical corpus. A fundamental conceptual issue is semantic drift: how can a model effectively map the meaning of words that changed over two centuries and across multiple languages into a single, coherent vector space?\nPractically, the persistent problem of poor OCR quality cascades through the entire system, hindering fundamental tasks like sentence segmentation. As re-OCRing the entire 430,000-book corpus is infeasible, the team is considering targeted re-OCRing of the worst-performing texts or supplementing the corpus with high-quality versions from other sources. Finally, scaling and performance remain a major concern. With queries on a tiny 132-text prototype already taking 15 seconds, ensuring acceptable performance on the full corpus will require substantial optimisation and computational resources.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE: Tracing the Influence of Ancient Wisdom on Early Modern Natural Philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis presentation details a dual-pronged investigation into artificial intelligence. The authors first explore the evolution of explainable AI (XAI) before demonstrating its application in generating novel scientific insights within the humanities.\nThe initial part of the discourse charts the progression from first-generation XAI, which relied upon heatmap-based feature attributions for simple classification models, towards a more sophisticated paradigm the authors term ‘XAI 2.0’. This advanced approach focuses on structured interpretability, analysing second-order (pairwise) and higher-order (graph-based) feature interactions to understand the complex mechanisms of modern foundation models, including Large Language Models (LLMs). The team demonstrates how these methods can uncover biases in sentiment prediction, analyse how LLMs handle long-range dependencies, and reveal the surprisingly simple heuristics, such as noun matching, that models employ for tasks like sentence similarity.\nThe second part presents a series of case studies applying these AI techniques to historical research. One project involves classifying a corpus of early modern mathematical instruments, using heatmaps to derive visual definitions based on features like fine-grained scales. A more extensive project, the XAI-Historian, analyses the Sphera corpus of numerical tables from 1472–1650. By developing a specialised model for bigram detection and using XAI to verify its logic, the authors could analyse historical publishing patterns at scale. A key finding emerged from a cluster entropy analysis, which identified Wittenberg as a centre of low print diversity—a data-driven discovery that corroborated historical knowledge about the political control exerted over its publishing curriculum by Protestant reformers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#presentation-structure",
    "href": "chapter_ai-nepi_007.html#presentation-structure",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.1 Presentation Structure",
    "text": "7.1 Presentation Structure\n\n\n\nSlide 02\n\n\nThe discourse is structured into two principal sections. The first part addresses the field of Explainable AI and its role in understanding the inner workings of Large Language Models. Subsequently, the second part transitions to demonstrate how AI-driven methods can yield new scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-dawn-of-explainable-ai-xai-1.0",
    "href": "chapter_ai-nepi_007.html#the-dawn-of-explainable-ai-xai-1.0",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.2 The Dawn of Explainable AI: XAI 1.0",
    "text": "7.2 The Dawn of Explainable AI: XAI 1.0\n\n\n\nSlide 03\n\n\nThe initial exploration into explainable AI, which the authors designate XAI 1.0, was centred on the concept of feature attribution. This approach sought to establish a clear definition of what constitutes a model explanation within the machine learning community, thereby providing a foundation for subsequent, more complex methods of interpretability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-black-box-problem-and-post-hoc-explainability",
    "href": "chapter_ai-nepi_007.html#the-black-box-problem-and-post-hoc-explainability",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.3 The ‘Black Box’ Problem and Post-Hoc Explainability",
    "text": "7.3 The ‘Black Box’ Problem and Post-Hoc Explainability\n\n\n\nSlide 04\n\n\nHistorically, machine learning research focused predominantly on visual data, creating powerful but opaque ‘black box’ systems. These models could, for instance, correctly classify an object in an image but offered no insight into the basis for their decision. The field of explainable AI emerged to address this opacity, dedicating a decade of research to methods that trace a model’s predictions back to its inputs. A foundational technique is the heatmap, which visually highlights the input pixels most influential in a classification, thereby showing why a model recognised a rooster.\nThe imperative for such explainability is fourfold. It enables the team to:\n\nVerify that a model is functioning reasonably.\nDiagnose and correct its errors.\nLearn from the surprising or novel solutions that models can uncover.\nEnsure compliance with emerging regulatory frameworks, including the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-shift-to-generative-ai-and-foundation-models",
    "href": "chapter_ai-nepi_007.html#the-shift-to-generative-ai-and-foundation-models",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.4 The Shift to Generative AI and Foundation Models",
    "text": "7.4 The Shift to Generative AI and Foundation Models\n\n\n\nSlide 06\n\n\nThe landscape of AI has shifted dramatically from the standard classification models prevalent five years ago to the current era of Generative AI. Unlike their predecessors, today’s foundation models are multi-task systems. They can classify content, retrieve similar images, generate entirely new images, and answer questions across a vast range of topics.\nThis expanded capability introduces a significant challenge: grounding a model’s output, such as a generated answer, in specific input data becomes far more complex. Consequently, the authors argue that research must now advance beyond simple heatmap representations. The focus is shifting towards analysing feature interactions and adopting more mechanistic perspectives to understand these models, which effectively act as ‘world models’ that encode societal knowledge and patterns of textual evolution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "href": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.5 Illustrative Model Failures",
    "text": "7.5 Illustrative Model Failures\n\n\n\nSlide 07\n\n\nAI models are prone to making surprising and revealing errors. One well-known example involves an object classifier that incorrectly bases its identification of a boat on the surrounding water; the model learns this correlation because the water’s texture is a simpler feature to detect than the boat itself.\nAnother example highlights failures in multi-step planning. When the authors prompted a standard LLM, such as a Llama 3 model, to solve the Tower of Hanoi puzzle, it immediately violated the game’s rules by attempting to move the largest, inaccessible disk. This error demonstrates a fundamental misunderstanding of the problem’s physical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.6 XAI 2.0: Structured Interpretability",
    "text": "7.6 XAI 2.0: Structured Interpretability\n\n\n\nSlide 08\n\n\nTo address the limitations of earlier methods, the presenters introduce a new paradigm they term ‘XAI 2.0’, which champions the concept of structured interpretability. This approach aims to move beyond simple heatmap visualisations to uncover more complex relational patterns within a model’s decision-making process. The need for such methods is underscored by failures in standard models, like the Llama 3 variant that struggled with the Tower of Hanoi, although more recent reasoning models may show improvement.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-and-second-order-explanations",
    "href": "chapter_ai-nepi_007.html#first-and-second-order-explanations",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.7 First and Second-Order Explanations",
    "text": "7.7 First and Second-Order Explanations\n\n\n\nSlide 10\n\n\nStructured interpretability distinguishes between different orders of explanation. First-order explanations, akin to heatmaps, are useful for simple classifiers. For instance, when the team trained a model to classify historical tables, these explanations verified that the model correctly focused on numerical content to make its predictions.\nSecond-order explanations, however, analyse pairwise relationships between features. This becomes crucial for understanding tasks like similarity measurement. When explaining the similarity score between two images, an interaction-based method reveals the specific features that correspond. In an example with two identical tables, this approach correctly highlights the interactions between matching digits, confirming the model’s logic.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-in-graph-structures",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-in-graph-structures",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.8 Higher-Order Interactions in Graph Structures",
    "text": "7.8 Higher-Order Interactions in Graph Structures\n\n\n\nSlide 11\n\n\nThe authors’ more recent work extends this analysis to graph structures, such as citation networks or relationships between entities like books. In these contexts, higher-order interactions provide more meaningful explanations than simpler methods. For models trained on graph classification tasks, explanations manifest as ‘feature walks’ or subgraphs—sets of interconnected features that become relevant collectively. The ultimate goal of this research is to derive more complex insights into model behaviour and progress towards a circuit-level understanding of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#detecting-bias-in-llms-with-first-order-attribution",
    "href": "chapter_ai-nepi_007.html#detecting-bias-in-llms-with-first-order-attribution",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.9 Detecting Bias in LLMs with First-Order Attribution",
    "text": "7.9 Detecting Bias in LLMs with First-Order Attribution\n\n\n\nSlide 13\n\n\nApplying first-order attributions to language models reveals their underlying biases. In a standard sentiment prediction task using a movie review dataset, the team employed a heatmap-style method adapted for Transformers. The analysis showed that the model’s predictions were skewed by the names present in the text. A review was more likely to receive a positive classification if it contained male, Western names like ‘Lee’ or ‘Raphael’, whereas the presence of foreign-sounding names like ‘Saddam’ or ‘Chan’ correlated with a negative score. This demonstrates that explainable AI is a powerful tool for detecting such fine-grained, and often unintended, biases within language models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#analysing-long-range-dependencies-in-summarisation",
    "href": "chapter_ai-nepi_007.html#analysing-long-range-dependencies-in-summarisation",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.10 Analysing Long-Range Dependencies in Summarisation",
    "text": "7.10 Analysing Long-Range Dependencies in Summarisation\n\n\n\nSlide 14\n\n\nThe research group investigated how LLMs handle long-range dependencies when summarising extensive texts, such as Wikipedia articles, within an 8,000-token context window. By tracing the generated summary back to its sources in the input, they sought to determine if the models effectively use information from the entire document.\nTheir analysis revealed a strong recency bias: the model predominantly focuses on information from the latter parts of the context. Although it can access information from the beginning of the text, it is significantly less likely to do so, as shown by a logarithmic scale of attribution counts. This finding implies that LLM-generated summaries are not balanced representations of the source material but are skewed towards content presented closer to the end of the prompt.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explaining-sentence-similarity-with-second-order-methods",
    "href": "chapter_ai-nepi_007.html#explaining-sentence-similarity-with-second-order-methods",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.11 Explaining Sentence Similarity with Second-Order Methods",
    "text": "7.11 Explaining Sentence Similarity with Second-Order Methods\n\n\n\nSlide 17\n\n\nTo understand how models compute sentence similarity, the authors applied second-order explanations to a standard pretrained model like Sentence-BERT. Given two sentences, the method generates interaction scores between their tokens to reveal the basis for the calculated similarity score.\nThe analysis of a toy example (‘A cat I really like’ and ‘It is a great cat’) and other pairs revealed that the models do not employ complex semantic reasoning. Instead, they rely on surprisingly simplistic heuristics, operating like a ‘bag-of-tokens’ system. The primary strategy is simple noun matching, supplemented by noun-verb pairings and interactions with separator tokens. This suggests that in the process of compressing vast amounts of information, these models default to simple, and perhaps not immediately obvious, decision-making strategies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-for-complex-language",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-for-complex-language",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.12 Higher-Order Interactions for Complex Language",
    "text": "7.12 Higher-Order Interactions for Complex Language\n\n\n\nSlide 19\n\n\nThe presenters conceptually frame Graph Neural Networks (GNNs) as LLMs, as their message-passing mechanism is analogous to an LLM’s attention network. This perspective allows for the application of higher-order explanation methods to complex language phenomena.\nStandard first-order, or bag-of-words, explanations often fail in this regard. For instance, in the sentence ‘First, I didn’t like the boring pictures’, such a method would incorrectly assign a positive sentiment due to the word ‘like’, completely missing the negation. In contrast, a higher-order explanation method successfully captures the complex structure. It correctly assigns a negative value to the entire negated phrase and properly interprets the sentence’s overall sentiment hierarchy, demonstrating a more nuanced understanding of language.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-1-visual-definitions-of-instruments",
    "href": "chapter_ai-nepi_007.html#case-study-1-visual-definitions-of-instruments",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.13 Case Study 1: Visual Definitions of Instruments",
    "text": "7.13 Case Study 1: Visual Definitions of Instruments\n\n\n\nSlide 24\n\n\nIn a collaborative project with historians Matteo Valeriani and Jochen Büttner, the authors applied AI to a corpus of historical mathematical instruments. Their goal was to build a classifier that could distinguish between categories such as ‘machine’ and ‘mathematical instrument’. By employing heatmap-based explanations, the team sought to extract objective ‘visual definitions’ that the model used for its classifications.\nThis process necessitated close interaction with the domain experts to validate the meaningfulness of the AI-derived criteria. A key finding was that the model correctly identified fine-grained scales as a highly relevant and defining feature for the ‘mathematical instrument’ class.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-2-the-sphera-corpus-project",
    "href": "chapter_ai-nepi_007.html#case-study-2-the-sphera-corpus-project",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.14 Case Study 2: The Sphera Corpus Project",
    "text": "7.14 Case Study 2: The Sphera Corpus Project\n\n\n\nSlide 25\n\n\nThe team’s largest collaborative project with historians from the Bifold institute involved the Sphera corpus, a collection of early modern texts published between 1472 and 1650. The central challenge, brought forward by Matteo Valeriani and Jochen Büttner, was to analyse the corpus’s vast collection of numerical tables. Despite initial assessments that the data would be extremely difficult to process computationally, the research goal was to develop an automated method for matching tables based on semantic similarity—a task that had been impossible to conduct at scale using traditional methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-xai-historian-workflow",
    "href": "chapter_ai-nepi_007.html#the-xai-historian-workflow",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.15 The XAI-Historian Workflow",
    "text": "7.15 The XAI-Historian Workflow\n\n\n\nSlide 26\n\n\nThe authors developed a workflow to support what they term the XAI-Historian—a scholar who leverages AI and its explanations for data-driven hypothesis generation. Rather than applying a large, general foundation model, which performs poorly on such specialised, out-of-domain data, the team engineered a small, custom model.\nThis model was trained specifically to detect numerical bigrams within the historical tables. Crucially, they used explainable AI to verify that the model functioned as intended. By confirming that it correctly identified identical bigrams across different tables, they could trust its outputs and proceed with large-scale analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-3-cluster-entropy-and-innovation",
    "href": "chapter_ai-nepi_007.html#case-study-3-cluster-entropy-and-innovation",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.16 Case Study 3: Cluster Entropy and Innovation",
    "text": "7.16 Case Study 3: Cluster Entropy and Innovation\n\n\n\nSlide 28\n\n\nWith a trusted model in place, the team conducted case studies, including an analysis of innovation diffusion using cluster entropy. They analysed the publishing output of various European cities by clustering the representations of their printed tables. By calculating the entropy of each city’s output, they could quantify its diversity; low entropy signified a programme focused on reprinting, whilst high entropy indicated a more varied and innovative output.\nThe analysis yielded two notable low-entropy cases. It confirmed Frankfurt am Main’s known status as a reprinting hub. More significantly, it uncovered a historical anomaly in Wittenberg. The model detected an unusually low diversity in its print programme, a finding that perfectly matched historical knowledge about the active political control exerted by Protestant reformers, who strictly managed the city’s curriculum.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-and-future-potential",
    "href": "chapter_ai-nepi_007.html#conclusion-and-future-potential",
    "title": "7  Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities",
    "section": "7.17 Conclusion and Future Potential",
    "text": "7.17 Conclusion and Future Potential\n\n\n\nSlide 33\n\n\nIn conclusion, this research demonstrates the successful application of bespoke AI and explainability methods to complex historical data, yielding verifiable insights that augment traditional scholarship. Whilst the presentation concluded before detailing future challenges related to data scarcity and model capabilities, the work establishes a strong foundation for a new, computationally-assisted approach to humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and Understanding LLMs & AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "",
    "text": "Overview\nThe authors chart a course for integrating Artificial Intelligence into scholarly research, progressing from the foundational concepts of Large Language Model evolution towards a robust framework grounded in validation and critical thinking. Their work identifies key deficiencies in current AI, such as a lack of information verification, and proposes a solution centred on computational epistemology and epistemic agency.\nTo realise this vision, the team introduces a suite of tools and platforms. The Scholarium initiative, governed by a curated editorial board, provides access to validated historical sources, including the collected works of Euler. A digital academic workspace, featuring an AI Cockpit, demonstrates how scholars can interact with LLMs to analyse historical documents. The entire technical infrastructure is built upon the principles of Open Science Technology—embracing open source, open access, open data, and open collaboration—and exemplifies FAIR data principles through platforms like Zenodo, a repository for the academic community.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-evolution-of-llm-competence",
    "href": "chapter_ai-nepi_008.html#the-evolution-of-llm-competence",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.1 The Evolution of LLM Competence",
    "text": "8.1 The Evolution of LLM Competence\n\n\n\nSlide 02\n\n\nThe authors trace the evolution of competence in Large Language Models (LLMs) through three distinct conceptual stages. This progression begins with the foundational mechanism of attention, which allows models to weigh the significance of different words in a sequence.\nSubsequently, development advanced to incorporate context, enabling LLMs to understand and process information within a broader frame of reference. The most recent stage in this evolution introduces the capacity for thinking, characterising a move towards more sophisticated reasoning and problem-solving capabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#deficiencies-in-current-ai-systems",
    "href": "chapter_ai-nepi_008.html#deficiencies-in-current-ai-systems",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.2 Deficiencies in Current AI Systems",
    "text": "8.2 Deficiencies in Current AI Systems\n\n\n\nSlide 03\n\n\nA critical examination of current AI capabilities reveals fundamental principles that are largely absent from contemporary systems. These deficiencies primarily concern the need for robust critical thinking, a capacity that extends far beyond mere pattern recognition.\nFurthermore, a significant gap exists in the domain of information verification, as models often generate content without a reliable mechanism for confirming its accuracy. This analysis also highlights the inherent limitations of AI representations, acknowledging that they are not infallible and require careful scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#validation-in-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#validation-in-computational-epistemology",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.3 Validation in Computational Epistemology",
    "text": "8.3 Validation in Computational Epistemology\n\n\n\nSlide 04\n\n\nValidation emerges as a central theme in the authors’ framework for advancing trustworthy AI. This principle applies not only to the propositions an AI generates but also to the actions it recommends or undertakes, ensuring both are sound and justifiable.\nSituating this work within the field of Computational Epistemology provides a formal structure for analysing knowledge in computational systems. Moreover, it connects directly to the nature of Epistemic Agency, exploring how systems can responsibly and accurately acquire, assess, and use information.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#ai-assisted-document-analysis",
    "href": "chapter_ai-nepi_008.html#ai-assisted-document-analysis",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.4 AI-Assisted Document Analysis",
    "text": "8.4 AI-Assisted Document Analysis\n\n\n\nSlide 05\n\n\nA digital academic workspace, developed by the team, demonstrates the practical application of AI in scholarly research, particularly within the humanities. The user interface presents a historical document, such as a PDF, alongside an AI-powered analysis panel.\nIn a specific use case, the system processes a document related to a historical art commission. The AI successfully extracts and organises key information, identifying all the individuals involved and detailing their respective roles in the commission, thereby accelerating the research process.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-scholarium-platform",
    "href": "chapter_ai-nepi_008.html#the-scholarium-platform",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.5 The Scholarium Platform",
    "text": "8.5 The Scholarium Platform\n\n\n\nSlide 06\n\n\nThe Scholarium initiative provides a dedicated platform for high-integrity academic work, distinguishing itself through rigorous oversight. A Curated Scholarly Editorial Board governs the platform, ensuring that all included sources meet stringent academic standards.\nThe scope of its collection includes the major collected works of influential historical scientists. To illustrate its function, the system showcases a digital viewer for navigating the comprehensive works of Leonhard Euler, offering researchers direct access to this foundational scientific corpus.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-ai-cockpit-interface",
    "href": "chapter_ai-nepi_008.html#the-ai-cockpit-interface",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.6 The AI Cockpit Interface",
    "text": "8.6 The AI Cockpit Interface\n\n\n\nSlide 08\n\n\nThe team has engineered the AI Cockpit, a specialised user interface designed to streamline interaction with Large Language Models. This tool provides a focused environment for conducting complex, AI-assisted tasks.\nIts capabilities are demonstrated through its effective use in processing historical documents. The AI Cockpit can automatically extract salient information and generate concise summaries, enabling scholars to grasp the core content of archival materials with greater efficiency.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-principles-and-the-zenodo-repository",
    "href": "chapter_ai-nepi_008.html#fair-principles-and-the-zenodo-repository",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.7 FAIR Principles and the Zenodo Repository",
    "text": "8.7 FAIR Principles and the Zenodo Repository\n\n\n\nSlide 09\n\n\nThe authors highlight the Zenodo research data repository as a prime example of an infrastructure built upon FAIR principles. By design, it ensures that research outputs are Findable, Accessible, Interoperable, and Reusable, promoting transparency and collaboration across academic communities.\nAs a general-purpose repository, it supports a wide range of disciplines and data types. Its active and continuous use is evident from its interface, which showcases a dynamic list of recent uploads from researchers worldwide, underscoring its role in the scholarly ecosystem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#principles-of-open-science-technology",
    "href": "chapter_ai-nepi_008.html#principles-of-open-science-technology",
    "title": "8  AI, Computational Epistemology, and Open Science Infrastructure",
    "section": "8.8 Principles of Open Science Technology",
    "text": "8.8 Principles of Open Science Technology\n\n\n\nSlide 10\n\n\nThe technical support framework for this scholarly ecosystem rests upon the core principles of Open Science Technology. This commitment manifests in four key areas.\n\nIt embraces open-source software, ensuring that the underlying tools are transparent and customisable.\nIt champions open access to publications, removing barriers to knowledge.\nIt mandates open data, allowing for the verification of results and the reuse of datasets.\nIt fosters open collaboration, creating an environment where researchers can work together effectively.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI, Computational Epistemology, and Open Science Infrastructure</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "",
    "text": "Overview\nThis chapter presents a case study investigating the representation of the United Nations Sustainable Development Goals (SDGs) within major bibliometric databases. The authors employ a fine-tuned Large Language Model (LLM) to analyse and reveal the systematic biases inherent in how these platforms classify scientific publications.\nCentred on three key databases—Web of Science, Scopus, and OpenAlex—the investigation examines the significant inconsistencies in their respective SDG labelling methodologies. To probe these discrepancies, the research team developed a specialised analytical workflow, fine-tuning the DistilGPT2 model on a shared corpus of publications. This novel approach allowed them to assess the aggregate effects of classification choices on research policy and public perception.\nThe findings are stark. The resulting body of SDG-classified literature systematically overlooks the most disadvantaged populations, the poorest countries, and sensitive topics explicitly mentioned in the SDG targets. Conversely, the literature demonstrates a strong focus on economic superpowers and highly developed nations. This study thus highlights the performative nature of bibliometric classifications, underscoring the profound impact of these seemingly objective, science-informed practices.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#the-performativity-of-bibliometrics",
    "href": "chapter_ai-nepi_009.html#the-performativity-of-bibliometrics",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.1 The Performativity of Bibliometrics",
    "text": "9.1 The Performativity of Bibliometrics\n\n\n\nSlide 03\n\n\nBibliometric databases assume a critical role within the sociology of science, exerting considerable influence over the entire academic ecosystem. Their classification systems and metrics directly shape the behaviour of academics, funding bodies, and policymakers alike.\nThis influence, however, is far from neutral. The databases themselves respond to various political and commercial interests, a reality that imbues them with a performative character. Rather than passively reflecting scientific activity, they actively construct and mould it.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#rationale-and-methodology",
    "href": "chapter_ai-nepi_009.html#rationale-and-methodology",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.2 Rationale and Methodology",
    "text": "9.2 Rationale and Methodology\n\n\n\nSlide 05\n\n\nThe authors’ case study centres on three principal bibliometric databases: Web of Science, Scopus, and OpenAlex. Their work builds upon previous findings that revealed a minimal overlap amongst publications labelled with SDGs, a discrepancy attributed to how each service formulates its search queries.\nThis investigation examines the chain of dependencies linking metadata processing at the database level to its eventual impact on end-users such as academics, consultants, and policymakers. Consequently, the team’s primary objective was to deploy an LLM as an analytical tool. This model serves to conduct a generalised assessment of the aggregate effects these classification systems have on research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#initial-publication-overlap",
    "href": "chapter_ai-nepi_009.html#initial-publication-overlap",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.3 Initial Publication Overlap",
    "text": "9.3 Initial Publication Overlap\n\n\n\nSlide 08\n\n\nTo establish a comparative baseline, the authors performed a classification analysis on a shared corpus of publications jointly indexed across Web of Science, Scopus, and OpenAlex. Their initial results were entirely consistent with the findings of Armitage (2020), revealing a remarkably small overlap in publications that the different databases classified under the same SDG.\nFor instance, a publication indexed in Scopus may not be tagged as relevant to SDG 5 (Gender Equality), even whilst other databases classify it as such. Furthermore, the authors’ analysis uncovered significant classification anomalies. Web of Science, for example, categorises a substantial portion—approximately 10%—of publications under SDG 5 that originate from the field of mathematics, including topics such as geometrical differential equations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning",
    "href": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.4 LLM Selection and Fine-Tuning",
    "text": "9.4 LLM Selection and Fine-Tuning\n\n\n\nSlide 11\n\n\nThe research team developed a specific strategy for leveraging LLM technology. Their initial concept involved training a bespoke model exclusively on the corpus of publications classified under a given SDG by a specific database. Realising this approach would be prohibitively resource-intensive, the authors adopted a more pragmatic solution: fine-tuning an existing, open-source model.\nFor this purpose, the team selected DistilGPT2. Its basic architecture, limited parameters, and minimal pre-existing knowledge made it an ideal candidate, ensuring it held no significant prior understanding of the publication or prompt semantics—a quality that contrasts sharply with larger models. The authors designed the fine-tuning process for similarity, training the model using only publication titles and abstracts, where a new title serves as a prompt to generate a new abstract.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompt-generation-and-analysis",
    "href": "chapter_ai-nepi_009.html#prompt-generation-and-analysis",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.5 Prompt Generation and Analysis",
    "text": "9.5 Prompt Generation and Analysis\n\n\n\nSlide 15\n\n\nThe authors’ research design incorporated a systematic method for generating prompts to benchmark the LLM’s performance. Recognising that each SDG comprises between eight and twelve distinct targets, the team crafted ten diverse prompts for every single target. This process yielded a specific set of 80 to 120 prompts for each SDG, designed to probe different facets of the goals.\nThe fine-tuned DistilGPT-2 model then generated responses to these prompts, conditioned on the publication sets from each bibliometric database. To ensure a robust analysis, the team employed three distinct decoding strategies for text generation: top-k, nucleus, and contrastive search. Subsequently, they applied a word filter to the generated text to extract key noun phrases for the final discussion.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#analysis-of-sdg-4-quality-education",
    "href": "chapter_ai-nepi_009.html#analysis-of-sdg-4-quality-education",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.6 Analysis of SDG 4: Quality Education",
    "text": "9.6 Analysis of SDG 4: Quality Education\n\n\n\nSlide 17\n\n\nAn illustrative example from the analysis of SDG 4 (Quality Education) reveals the inherent biases of the underlying literature. The authors’ investigation, structured across four dimensions—Locations, Actors, Data/Metrics, and Focuses—shows a clear pattern of inclusion and exclusion.\nThe generated content frequently addresses locations such as South Africa, the U.S., Australia, and China, alongside actors like teachers, youth, and students. It also references metrics including PISA and socioeconomic status (SES), with a focus on curriculum, performance, and the English language.\nConversely, the model’s output systematically fails to address most other African nations, developing countries, and small island states. Critically, it overlooks vulnerable actors explicitly named in the SDG targets, such as persons with disabilities, indigenous peoples, and children in vulnerable situations. Key educational priorities like vocational training, scholarships, and the promotion of global citizenship are also conspicuously absent.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-omissions-across-sdgs",
    "href": "chapter_ai-nepi_009.html#systematic-omissions-across-sdgs",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.7 Systematic Omissions Across SDGs",
    "text": "9.7 Systematic Omissions Across SDGs\n\n\n\nSlide 19\n\n\nGeneralising the findings across the five SDGs studied reveals consistent and systematic omissions within the scientific literature as classified by the databases. A pronounced geographic bias exists: least developed countries receive scant attention, whilst the United States commands a near-monopoly of focus, followed by China, South Africa, the UK, and Australia.\nFurthermore, discriminated and vulnerable populations are systematically overlooked, a failing that persists across all analysed goals. The highlighted research methodologies tend to be general, such as thematic analysis or macroeconomic modelling.\nMost critically, the authors’ analysis shows that the most sensitive and challenging topics central to the SDGs—including human trafficking, exploitation, and migration—are largely absent from the discourse.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#findings-and-limitations",
    "href": "chapter_ai-nepi_009.html#findings-and-limitations",
    "title": "9  An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models",
    "section": "9.8 Findings and Limitations",
    "text": "9.8 Findings and Limitations\n\n\n\nSlide 20\n\n\nThe study’s central finding is that using an LLM as an intermediate analytical tool starkly reveals systematic oversights within the body of scientific publications classified under the SDGs. This curated literature consistently neglects the most disadvantaged individuals, the poorest nations, and specific topics that the SDG targets explicitly prioritise. In stark contrast, the research corpus directs its full attention to economic superpowers and highly developed countries.\nThese results clearly demonstrate the decisive, shaping power of a supposedly objective practice like bibliometric classification. Nevertheless, the authors acknowledge certain limitations. Their methodology exhibits high sensitivity to the LLM’s architecture, its training data, the chosen hyperparameters, and the decoding strategy, although they accounted for these factors in the experimental design.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Analysis of Sustainable Development Goal Research in Bibliometric Databases and the Application of Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "",
    "text": "Overview\nThis chapter details a significant new methodology for extracting accurate citation data from scholarly publications in the humanities, a domain historically underserved by commercial bibliometric databases. The authors, from the Max Planck Institute for Legal History and Legal Theory and the Max Planck Computing & Data Facility, articulate a core research problem: the profound inadequacy of platforms like Web of Science and Scopus for intellectual history. These databases struggle with non-English, pre-digital, and footnote-heavy literature, rendering them unsuitable for the field.\nTo surmount these obstacles, the team proposes an approach centred on Large Language Models (LLMs) and a robust evaluation framework. They developed a high-quality, TEI-annotated Gold Standard dataset from open-access humanities journals, which serves as a benchmark for their new Python package, Llamore. Designed as a lightweight interface for LLMs, Llamore performs reference extraction with notable precision. The chapter outlines Llamore’s architecture, its evaluation methodology, and its comparative performance. Whilst its results on a standard biomedical dataset (PLOS 1000) are comparable to the established tool Grobid, on the custom humanities dataset, Llamore significantly outperforms it, demonstrating the value of LLM-based approaches for complex, domain-specific tasks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "href": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.1 Citation Graphs in Intellectual History",
    "text": "10.1 Citation Graphs in Intellectual History\n\n\n\nSlide 03\n\n\nThe authors’ central research objective is to create comprehensive citation graphs that illuminate trends in intellectual history. Such graphs are powerful analytical tools, enabling scholars to uncover latent patterns in knowledge production, trace the influence of specific authors and ideas, and quantitatively measure their reception within a scholarly community.\nAn analysis of the most cited authors over a given period, for instance, provides a clear window into a field’s intellectual evolution. A key challenge, however, lies in the initial data production. The team acknowledges that a significant detour is required to generate the specialised data necessary to construct these graphs and, ultimately, to address their core research questions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-bibliometric-databases",
    "href": "chapter_ai-nepi_010.html#limitations-of-bibliometric-databases",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.2 Limitations of Bibliometric Databases",
    "text": "10.2 Limitations of Bibliometric Databases\n\n\n\nSlide 05\n\n\nA fundamental problem impedes research in this domain: the inadequacy of established bibliometric databases. For the specific field of intellectual history, the authors find that major platforms such as Web of Science and Scopus are effectively unusable due to critical data deficiencies.\nTheir coverage of essential academic journals is incomplete, they largely neglect scholarship from the pre-digital age, and they exhibit a strong bias towards English-language content. Beyond these content-related failings, these commercial services are also prohibitively expensive and impose restrictive licensing terms, fostering an undesirable dependency for the academic community.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#systemic-biases-and-technical-hurdles",
    "href": "chapter_ai-nepi_010.html#systemic-biases-and-technical-hurdles",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.3 Systemic Biases and Technical Hurdles",
    "text": "10.3 Systemic Biases and Technical Hurdles\n\n\n\nSlide 06\n\n\nThe team’s analysis of the German journal Zeitschrift für Rechtssoziologie exemplifies this poor coverage, revealing almost no citation data in major databases prior to the 2000s. This gap stems from systemic biases. Commercial database providers focus on STEM, medicine, and economics—fields perceived as more profitable than the humanities. Consequently, their infrastructure is optimised for metrics like the impact factor, which holds little relevance for tracking intellectual history.\nFurthermore, a significant technical hurdle arises from the very nature of humanities scholarship. Texts are frequently replete with dense, discursive footnotes that embed citations within extensive commentary and other textual noise. This complexity, which one presenter memorably termed a ‘footnote from hell’, frustrates conventional extraction tools.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-promise-of-language-models",
    "href": "chapter_ai-nepi_010.html#the-promise-of-language-models",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.4 The Promise of Language Models",
    "text": "10.4 The Promise of Language Models\n\n\n\nSlide 09\n\n\nTraditional approaches to citation extraction have proven insufficient for these complex documents. The authors note that creating the necessary training data involves a laborious manual annotation process, yet even with this investment, the performance of tools relying on established machine learning techniques like Conditional Random Forests is poor.\nIn contrast, their early experiments with Large Language Models (LLMs) yielded surprisingly effective results, with performance improving further in subsequent model generations. The advent of Visual Language Models (VLMs) now enables the direct processing of PDF documents, introducing a new frontier of methods that the research team is beginning to investigate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#a-robust-evaluation-framework",
    "href": "chapter_ai-nepi_010.html#a-robust-evaluation-framework",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.5 A Robust Evaluation Framework",
    "text": "10.5 A Robust Evaluation Framework\n\n\n\nSlide 13\n\n\nTo harness the power of LLMs responsibly, the authors contend that a robust testing and evaluation regime is non-negotiable. Their work operates on a core principle: one must not attempt to solve problems without first securing the data needed for validation.\nThis philosophy necessitates the development of a high-quality Gold Standard dataset to serve as a benchmark for accuracy. Moreover, the evaluation framework itself must be flexible enough to accommodate the fast-moving technological landscape. Finally, the testing tools must generate comparable metrics, allowing for rigorous and transparent performance comparisons against other existing or future solutions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-gold-standard",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-gold-standard",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.6 Developing a TEI Gold Standard",
    "text": "10.6 Developing a TEI Gold Standard\n\n\n\nSlide 16\n\n\nThe authors selected the Text Encoding Initiative (TEI) XML format to build their Gold Standard dataset, a decision grounded in its status as the pre-eminent standard in the digital humanities. Unlike purely bibliographic formats such as CSL or BibTeX, TEI offers a far more comprehensive and well-specified framework. Its richness allows for the encoding of crucial contextual phenomena, such as the author’s intention behind a citation, which goes beyond simple reference management.\nThis choice also enables the team to tap into a wealth of existing digital editions and text corpora already encoded in TEI. Crucially, using this interoperable standard ensures compatibility with key information extraction tools like Grobid. This alignment allows for direct performance comparisons, the use of Grobid’s training data, and the contribution of their new dataset back to the Grobid project.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#constructing-the-humanities-dataset",
    "href": "chapter_ai-nepi_010.html#constructing-the-humanities-dataset",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.7 Constructing the Humanities Dataset",
    "text": "10.7 Constructing the Humanities Dataset\n\n\n\nSlide 17\n\n\nThe team is actively constructing a new dataset comprising over 1,000 footnotes sourced from 20 different articles, which are expected to yield more than 1,500 reference instances. Midway through the project, their strategy shifted towards using Open Access journals; this change ensures that the entire pipeline—from the original PDFs to the final parsed data—can be made publicly available.\nThe selected articles provide linguistic and temporal diversity, spanning multiple languages and a broad historical period. The encoding workflow proceeds in stages: first, the reference string is segmented and isolated from non-bibliographic text within the footnote, and second, this string is parsed into a structured format. Notably, the team encodes every single reference as a unique occurrence, thereby preserving the context of each citation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-lightweight-python-package",
    "href": "chapter_ai-nepi_010.html#llamore-a-lightweight-python-package",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.8 Llamore: A Lightweight Python Package",
    "text": "10.8 Llamore: A Lightweight Python Package\n\n\n\nSlide 20\n\n\nTo implement their LLM-based approach, the team developed Llamore, a Python package for Large Language Models for Reference Extraction. This tool performs two primary functions: it extracts references from supplied text or PDF files, and it evaluates the performance of this extraction against a gold standard.\nTwo key objectives guided its design. First, it is lightweight, containing no models itself but rather serving as a flexible interface to a user-selected model. Second, it ensures broad compatibility with both open and closed-source LLMs and VLMs. Available via pip, Llamore’s workflow involves defining an extractor, which can connect to any OpenAI-compatible API and thus supports a wide range of model-serving frameworks like Olama. The extracted references can then be exported as TEI XML, and their accuracy assessed using the package’s built-in F1 evaluation class.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#evaluation-and-comparative-results",
    "href": "chapter_ai-nepi_010.html#evaluation-and-comparative-results",
    "title": "10  AI-NEPI: Citation extraction from scholarly publications in the humanities",
    "section": "10.9 Evaluation and Comparative Results",
    "text": "10.9 Evaluation and Comparative Results\n\n\n\nSlide 24\n\n\nThe project employs a rigorous evaluation methodology centred on the F1 score, a well-established metric for structured data comparison. This score is calculated from precision and recall, which are determined by counting the number of exact matches for each field within a reference.\nTo solve the complex problem of aligning the list of extracted references with the gold standard list, the authors frame it as an unbalanced assignment problem. They use a solver from the SciPy library to find the optimal one-to-one mapping that maximises the overall F1 score, penalising both missing and hallucinated references with a score of zero. Comparative tests reveal a telling divergence. On the PLOS 1000 biomedical dataset, Llamore performs on par with the highly optimised Grobid tool, albeit at a much higher computational expense. On the custom humanities dataset, however, Grobid’s performance falters, whereas Llamore demonstrates significantly superior extraction accuracy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI-NEPI: Citation extraction from scholarly publications in the humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "",
    "text": "Overview\nTo address the profound challenge of managing and comprehending vast quantities of scientific information, the authors have engineered an artificial intelligence solution comprising the Ghostwriter interface and the EverythingData backend. This system enables users to interact with a curated collection of academic papers through natural language queries, functioning as a sophisticated information retrieval tool. Its core methodology extends Retrieval Augmented Generation (RAG) by integrating knowledge graphs, a technique the team has termed ‘GraphRAG’, to deliver more contextual and accurate results.\nThe workflow begins by ingesting documents, such as a test collection of 100 articles from the mda journal, into a bespoke pipeline. This pipeline processes the full-text articles, partitions them into identifiable blocks, and performs term extraction and embedding construction. A key innovation involves linking extracted entities to the Wikidata knowledge graph, which enriches the semantic context and provides stable, multilingual identifiers.\nAll processed information populates a vector store managed by Quadrant, creating a queryable vector space. When a user poses a question, the system retrieves the most relevant text segments, generates a coherent summary, and crucially, provides direct references to the source documents. This design choice effectively prevents AI ‘hallucination’. The architecture supports multilingual queries and is designed to run locally on standard hardware using a compact one-billion-parameter LLM, ensuring complete user control and data privacy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-challenge-of-information-overload",
    "href": "chapter_ai-nepi_011.html#the-challenge-of-information-overload",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.1 The Challenge of Information Overload",
    "text": "11.1 The Challenge of Information Overload\n\n\n\nSlide 02\n\n\nScientific progress confronts a significant impediment in the escalating volume of published information, a deluge that threatens to overwhelm the modern researcher. Effectively navigating this landscape through advanced retrieval techniques is a fundamental prerequisite for generating new knowledge.\nThis project explores how artificial intelligence can assist in this process, stemming from experimental work on complex data pipelines conducted at Dans. The authors sought to apply these technical frameworks to a practical use case, aiming to demonstrate their capabilities in a manner that is both powerful and comprehensible to a broader audience.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-question-and-system-design",
    "href": "chapter_ai-nepi_011.html#research-question-and-system-design",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.2 Research Question and System Design",
    "text": "11.2 Research Question and System Design\n\n\n\nSlide 03\n\n\nThe investigation centres on a precise research question: is it feasible to construct an AI solution that enables a conversational interaction with a selected corpus of academic papers?\nTo achieve this, the development team engineered a system with two principal components. Ghostwriter functions as the user interface, providing the conversational layer for posing queries. Supporting this is EverythingData, a term encompassing the entire backend architecture responsible for data ingestion, processing, and retrieval.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-paradigm",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-paradigm",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.3 The Ghostwriter Paradigm",
    "text": "11.3 The Ghostwriter Paradigm\n\n\n\nSlide 04\n\n\nA new approach to information retrieval, embodied by the Ghostwriter interface, directly addresses the twin challenges of formulating precise questions and identifying relevant sources. The system’s design rests on a powerful metaphor that simplifies its operation: it allows a user to converse simultaneously with two conceptual entities.\nThe ‘librarian’ persona represents the world of structured data, including formal knowledge organisation systems and established classifications. In contrast, the ‘expert’ persona embodies the domain of pure natural language. The authors’ central claim is that Ghostwriter unifies these two modes of interaction, providing a single, coherent conversational experience.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#graphrag-augmenting-retrieval-with-knowledge-graphs",
    "href": "chapter_ai-nepi_011.html#graphrag-augmenting-retrieval-with-knowledge-graphs",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.4 GraphRAG: Augmenting Retrieval with Knowledge Graphs",
    "text": "11.4 GraphRAG: Augmenting Retrieval with Knowledge Graphs\n\n\n\nSlide 05\n\n\nThis work positions itself within the scientific discourse of Retrieval Augmented Generation (RAG), a prominent technique for enhancing language models with external knowledge. For a thorough introduction to this topic, the authors recommend a paper by Philip Rustle from Neo4j.\nThe system’s architecture combines two fundamental ingredients: a vector space for semantic similarity searches and a graph for representing structured relationships. This fusion results in what the team terms ‘GraphRAG’, an implementation that functions as a sophisticated reasoning engine.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-architecture-and-workflow",
    "href": "chapter_ai-nepi_011.html#system-architecture-and-workflow",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.5 System Architecture and Workflow",
    "text": "11.5 System Architecture and Workflow\n\n\n\nSlide 07\n\n\nThe system’s architecture processes document collections into a highly structured, queryable resource. It begins by ingesting a set of articles, for instance from the mda journal, into the EverythingData backend. This pipeline first populates a vector store using the Quadrant engine, then executes operations such as term extraction and the construction of semantic embeddings.\nA crucial innovation lies in coupling this vector store with knowledge graphs. This integration enriches the embeddings with structured relationships, adding a deeper layer of context. The final output is a sophisticated vector space, augmented with a graph structure, that users can query with natural language questions. In response, the system delivers both a generated summary and a precise list of the source documents used to formulate the answer.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#implementation-and-factual-grounding",
    "href": "chapter_ai-nepi_011.html#implementation-and-factual-grounding",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.6 Implementation and Factual Grounding",
    "text": "11.6 Implementation and Factual Grounding\n\n\n\nSlide 08\n\n\nThe engineers implemented the system to be versatile, capable of processing diverse web content including academic papers and spreadsheets. It operates using a lean one-billion-parameter Large Language Model, a design choice that prioritises efficiency and local deployment.\nA core principle of the design is its commitment to factual grounding; the system exclusively uses the provided source material to generate answers, thereby preventing hallucination. If the required information is absent from its knowledge base, it explicitly states its inability to answer. For demonstration, the team created a curated collection of 100 articles from the GESIS mda journal via web scraping. This highlights another key feature: the system begins with no knowledge and builds its expertise only from the documents users add to its collection.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#source-referenced-answer-generation",
    "href": "chapter_ai-nepi_011.html#source-referenced-answer-generation",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.7 Source-Referenced Answer Generation",
    "text": "11.7 Source-Referenced Answer Generation\n\n\n\nSlide 09\n\n\nThe system’s ability to generate reliable, source-backed answers is central to its design. When presented with a query, such as a request to explain the ‘male breadwinner model’, it produces a synthesised response accompanied by direct references to the source documents. This mechanism effectively prevents hallucination by tying every piece of generated text to a specific origin.\nTechnically, the authors achieve this by partitioning each paper into small, uniquely identified blocks. The retrieval process then employs a combination of LLM techniques and knowledge graph analysis to intelligently predict and retrieve the most relevant text blocks for answering the user’s question, applying weights to prioritise the best-fitting information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-knowledge-building",
    "href": "chapter_ai-nepi_011.html#iterative-knowledge-building",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.8 Iterative Knowledge Building",
    "text": "11.8 Iterative Knowledge Building\n\n\n\nSlide 10\n\n\nRather than guessing, the system explicitly communicates its limitations by stating when no direct information is available to answer a query. This transparency is complemented by a feature for iterative knowledge building.\nAn ‘add paper’ function empowers users to expand the system’s corpus directly. Consequently, if a user finds an answer in a document not yet in the collection, they can add it. The system will then successfully answer the same question on subsequent attempts, creating a dynamic and collaborative learning loop.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#knowledge-graph-construction",
    "href": "chapter_ai-nepi_011.html#knowledge-graph-construction",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.9 Knowledge Graph Construction",
    "text": "11.9 Knowledge Graph Construction\n\n\n\nSlide 11\n\n\nThe system’s technical backbone is a multi-stage pipeline designed for robust knowledge construction. It begins with an entity extraction process that annotates key concepts within the source texts. Crucially, these entities are then linked to external knowledge graphs, which establish a ‘ground truth’ for validating information.\nThis structure underpins the system’s powerful multilingual support, enabling a user to query documents in one language, such as German, using a different language, like English. In the final stage, the Large Language Model synthesises the various retrieved text fragments into a single, coherent summary for the user.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#hierarchical-knowledge-organisation",
    "href": "chapter_ai-nepi_011.html#hierarchical-knowledge-organisation",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.10 Hierarchical Knowledge Organisation",
    "text": "11.10 Hierarchical Knowledge Organisation\n\n\n\nSlide 12\n\n\nFact extraction begins by deconstructing a user’s query into its constituent conceptual pieces. The authors describe this procedure as part of a repeatable framework for knowledge organisation.\nBy applying this process iteratively, the system can generate progressively deeper and more detailed layers of information beneath a given term. This creates a rich, hierarchical representation of concepts, allowing for nuanced exploration of the subject matter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "href": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.11 Entity Linking with Wikidata",
    "text": "11.11 Entity Linking with Wikidata\n\n\n\nSlide 13\n\n\nTo ensure consistency and enable cross-lingual capabilities, the system links extracted entities to the Wikidata knowledge base. This critical step converts potentially ambiguous text strings into stable, language-agnostic identifiers.\nBecause each Wikidata identifier is connected to a network of multilingual translations and structured properties, the system can correctly interpret queries across different languages. The developers describe this mechanism as being analogous to a Transformer model, but one that is grounded in the structured knowledge of Wikidata.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#cross-lingual-query-expansion",
    "href": "chapter_ai-nepi_011.html#cross-lingual-query-expansion",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.12 Cross-Lingual Query Expansion",
    "text": "11.12 Cross-Lingual Query Expansion\n\n\n\nSlide 14\n\n\nThe system’s multilingual functionality is powered by a comprehensive query expansion process. For a given concept, such as the ‘breadwinner model’, the system generates translations across hundreds of languages.\nThis expanded set of multilingual terms, rather than just the original English phrase, constitutes the full query submitted to the Large Language Model. This method ensures that relevant information is retrieved regardless of the language in which it was originally written.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#a-decoupled-system-for-sustainability",
    "href": "chapter_ai-nepi_011.html#a-decoupled-system-for-sustainability",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.13 A Decoupled System for Sustainability",
    "text": "11.13 A Decoupled System for Sustainability\n\n\n\nSlide 15\n\n\nA key architectural decision was to decouple the extracted knowledge from the language models that process it. By converting queries into lists of stable Wikidata identifiers, the system creates an external, model-agnostic knowledge base.\nThis separation provides a powerful method for benchmarking; one can evaluate any LLM, including those developed in the future, by assessing its ability to generate the correct set of identifiers for a specific query. The authors propose this knowledge organisation system as a sustainable foundation for the next generation of scientific tools, an effort being pursued in collaboration with industry partners like Google and Meta.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#demonstration-and-concluding-vision",
    "href": "chapter_ai-nepi_011.html#demonstration-and-concluding-vision",
    "title": "11  Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections",
    "section": "11.14 Demonstration and Concluding Vision",
    "text": "11.14 Demonstration and Concluding Vision\n\n\n\nSlide 16\n\n\nA live demonstration showcased the system’s capabilities. A query for ‘rational choice theory’ returned a broad summary with references, whilst a more specific query about ‘utility in rational choice theory’ generated a focused answer using different information from the same sources. The system’s multilingual prowess was proven by successfully answering a question in English about a paper written almost entirely in German.\nBeyond manual use, an API enables the construction of automated agentic workflows. The overarching vision is for the tool to serve as a local, controllable resource that facilitates a dialogue with scientific literature—akin to conversing with an ‘invisible college’. Its ultimate purpose is not to replace human intellect, but to provoke and support the researcher’s own thought process.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ghostwriter and EverythingData: An AI Solution for Interacting with Scientific Collections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "",
    "text": "Overview\nThis chapter presents a methodological exploration into applying Retrieval-Augmented Generation (RAG) systems to enhance philosophical research. It confronts the inherent limitations of standard Large Language Models (LLMs), including their inability to access full texts, their propensity for hallucination, their constrained context windows, and their lack of source attribution. The authors propose the RAG architecture as a robust solution, integrating a specific corpus—such as the works of Aristotle or Einstein—with an LLM to furnish verifiable, context-rich answers.\nA practical case study documents the development of a RAG system that uses the Stanford Encyclopedia of Philosophy (SEP) as its knowledge base. This project reveals that a naive implementation yields poor results, necessitating significant refinement of models, hyperparameters, and algorithms like reranking. The investigation highlights the critical role of hyperparameter optimisation, particularly chunk size. For the highly structured SEP corpus, chunking by main section surprisingly outperformed more granular methods.\nThe chapter concludes that whilst RAG systems offer profound advantages for scholarly tasks by reducing hallucinations and enabling citations, their effectiveness is highly contingent on the corpus, the nature of the research questions, and rigorous evaluation by domain experts. Future work points towards developing more flexible, agentic RAG systems capable of discerning between different query types.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#scholarly-challenges-of-large-language-models",
    "href": "chapter_ai-nepi_012.html#scholarly-challenges-of-large-language-models",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.1 Scholarly Challenges of Large Language Models",
    "text": "12.1 Scholarly Challenges of Large Language Models\n\n\n\nSlide 05\n\n\nStandard Large Language Models can produce differentiated answers to complex philosophical questions, such as analysing Aristotle’s theory of matter or tracing the evolution of Einstein’s concept of locality. Their utility for deep scholarly research, however, is constrained by several fundamental problems. A Retrieval-Augmented Generation system offers a structured approach to overcome these limitations.\nThe core challenge with LLMs is their lack of direct access to full-text sources. Although trained on vast datasets, their training mechanism is explicitly designed to learn generalisable patterns of text production, not to memorise and recall texts verbatim. Consequently, when prompted for a specific quotation, an LLM might either refuse or, more problematically, hallucinate a response. Such an outcome is antithetical to philosophical research, which demands precise engagement with the fine-grained formulations of original texts.\nFurthermore, the context window of LLMs, whilst growing, remains a significant constraint. A model like ChatGPT 4.0 offers a 128,000-token context, yet this is quickly exhausted when dealing with the large corpora typical of scholarly inquiry. RAG systems directly address these issues. The architecture retrieves relevant documents from a specified data source—for instance, the complete works of a philosopher—using methods like semantic or hybrid search. These retrieved text chunks then augment the user’s prompt, feeding the LLM with the exact textual evidence needed to formulate a grounded response. This process not only provides access to the verbatim text but also solves the critical problem of attribution, as the system can cite the sources for its claims, much like the functionality seen in tools such as Perplexity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#a-case-study-in-philosophical-rag",
    "href": "chapter_ai-nepi_012.html#a-case-study-in-philosophical-rag",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.2 A Case Study in Philosophical RAG",
    "text": "12.2 A Case Study in Philosophical RAG\n\n\n\nSlide 09\n\n\nThe central ambition in applying RAG to philosophy is to facilitate a deep, interactive dialogue with scholarly corpora, such as the complete works of John Locke. This approach promises significant benefits for both didactics and advanced research. For students, it offers an intuitive pathway into complex texts, allowing them to move from broad inquiries to specific epistemological questions. For researchers, RAG systems can function as reliable tools for looking up facts, exploring unexamined corpora, and identifying key passages for close reading. The ultimate goal is for these systems to assist in answering detailed research questions directly.\nTo explore this potential, the authors developed a prototype RAG system using the Stanford Encyclopedia of Philosophy (SEP) as its knowledge base, with the initial aim of creating a useful tool for the philosophical community. The team scraped the SEP content into Markdown to serve as the data source. However, a straightforward, textbook implementation of the RAG architecture yielded surprisingly poor results; the answers were often inferior to those generated by a standalone LLM like ChatGPT. This discovery shifted the project’s focus towards rigorous optimisation.\nImproving the system’s performance necessitated a process of extensive adjustment, involving modifications to the models, their hyperparameters, and the introduction of more complex algorithms such as reranking. This optimisation work is largely a matter of trial and error, guided by theoretical principles but ultimately validated by empirical results. A significant challenge emerged in this phase: the evaluation of output quality. Unlike historical queries for atomic facts, philosophical questions elicit complex, unstructured propositions. Developing robust standards to evaluate the factual correctness of these nuanced answers is a non-trivial task that remains a key hurdle.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#system-interface-and-comparative-analysis",
    "href": "chapter_ai-nepi_012.html#system-interface-and-comparative-analysis",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.3 System Interface and Comparative Analysis",
    "text": "12.3 System Interface and Comparative Analysis\n\n\n\nSlide 25\n\n\nThe project team developed a custom frontend to facilitate interaction with and evaluation of the Stanford Encyclopedia of Philosophy RAG system. The interface, comprising a few thousand lines of code, provides users with controls to configure key hyperparameters, including prompt length and the number of documents to retrieve, alongside a field for submitting their query.\nA central feature of the design is its comparative output display, which is structured to aid analysis. On the left, the system shows a benchmark answer generated by the chosen LLM operating in isolation. On the right, it presents the corresponding answer from the RAG-augmented system. This side-by-side layout enables a direct and immediate comparison, making it easier to assess the value added by the retrieval process.\nTo ensure transparency and aid in debugging, the system’s output concludes with a comprehensive list of the texts found during the retrieval stage. This list details the source article names and the specific section headings that the system identified as relevant. Crucially, it also indicates which of these text chunks were successfully included in the final augmented prompt and which were ultimately excluded due to the LLM’s prompt length limitations, providing clear insight into the retrieval and augmentation process.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-hyperparameters-and-chunk-size",
    "href": "chapter_ai-nepi_012.html#optimising-hyperparameters-and-chunk-size",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.4 Optimising Hyperparameters and Chunk Size",
    "text": "12.4 Optimising Hyperparameters and Chunk Size\n\n\n\nSlide 32\n\n\nThe authors conducted experiments to optimise key hyperparameters, focusing specifically on the ‘chunk size’ used for text retrieval. They considered three primary chunking strategies: using a fixed number of words, a common practice in computer science; splitting the text by paragraphs; or dividing it by semantic sections.\nTo some surprise, the investigation revealed that using the main sections of the Stanford Encyclopedia of Philosophy articles as the retrieval units yielded the best results. This outcome was unexpected because the embedding model’s context limit was just over 500 words, whereas the average length of a main section in the corpus was around 3,000 words.\nThe team’s hypothesis for this result centres on the unique structure of the data source. The SEP is a highly systematised and well-ordered work. It is likely that the initial 500 words of each major section effectively summarise its core arguments, providing the embedding model with enough information to assess its relevance to a query accurately. This finding underscores a critical lesson: the optimal chunking strategy is not universal but is instead highly contingent on the specific nature of the corpus and the research questions posed. Consequently, this approach might not transfer well to more heterogeneous texts. Future plans involve experimenting with embedding models that possess longer context windows to explore potential further improvements.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#conclusion-and-future-directions",
    "href": "chapter_ai-nepi_012.html#conclusion-and-future-directions",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.5 Conclusion and Future Directions",
    "text": "12.5 Conclusion and Future Directions\n\n\n\nSlide 40\n\n\nRetrieval-Augmented Generation systems present clear advantages for scientific and scholarly work. They successfully integrate verbatim corpora and specialised domain knowledge, a capability that dramatically reduces the incidence of LLM hallucinations. Moreover, their ability to cite relevant source documents makes them, in principle, exceptionally well-suited to assisting with academic tasks.\nTheir practical application, however, requires caution. RAG systems are not off-the-shelf solutions; they demand careful configuration, as the appropriate settings for retrieval and generation depend heavily on the specific corpus and the kinds of questions being asked. The evaluation of their output is therefore crucial, demanding a representative set of test questions and expected answers—a task for which domain experts are indispensable.\nSeveral open challenges remain. The quality of a RAG system’s answer degrades significantly if no relevant documents are found in the retrieval phase, a scenario that necessitates careful prompt engineering. A notable, counter-intuitive finding is that these systems often perform poorly on broad, overview questions. The likely reason is that the RAG process forces the LLM to focus on the local information contained within the retrieved text chunks. This local focus can distract the model from synthesising information and adopting the wide perspective needed to answer a general query effectively.\nTo address this, future development must move towards more flexible, agentic RAG systems. Such systems would be capable of discerning between different types of questions and dynamically adjusting their information processing strategy, paving the way for more sophisticated scholarly assistants.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "",
    "text": "Overview\nThis research introduces a novel computational framework to analyse the structure of scientific fields, employing quantum gravity as its primary case study. The authors developed a dual-pronged approach to map the research landscape, contrasting a bottom-up, data-driven reconstruction with a top-down model derived from the intuition of expert physicists.\nThe bottom-up analysis utilises a corpus of approximately 200,000 abstracts and titles from the literature of fundamental physics. It combines linguistic analysis, which uses the Bertopic pipeline to identify fine-grained intellectual topics, with a social network analysis of a co-authorship graph of 30,000 physicists to detect research communities. A central challenge the authors address is the scale-dependency inherent in defining ‘topics’ and ‘communities’. To resolve this, they implemented a hierarchical clustering method for both structures and an adaptive coarse-graining strategy guided by the Minimum Description Length (MDL) criterion. This technique optimises the topic structure by balancing its descriptive complexity against its power to explain the social network.\nThe top-down approach involved surveying founding members of the International Society for Quantum Gravity to compile a list of recognised research programmes. A supervised classifier, trained on hand-coded labels, then categorised papers according to this expert-defined structure. By confronting these two perspectives, the study reveals a complex, multi-scale, and nested research landscape rather than a simple plural pursuit model. It confirms expert intuitions, for instance, regarding the tight integration of the string theory and supergravity communities, whilst providing a robust, scalable methodology for exploring the socio-epistemic dynamics of science.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#quantum-gravity-and-the-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#quantum-gravity-and-the-plural-pursuit",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.1 Quantum Gravity and the Plural Pursuit",
    "text": "13.1 Quantum Gravity and the Plural Pursuit\n\n\n\nSlide 03\n\n\nA long-standing challenge in fundamental physics centres on formulating a quantum theory of gravity, a unified framework intended to reconcile our understanding of phenomena at both the smallest and the largest scales. Physicists have proposed numerous solutions to this problem, amongst which string theory is the most prominent. To analyse a situation where many different research avenues are explored simultaneously, the authors introduce the conceptual framework of plural pursuit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.2 Defining Plural Pursuit",
    "text": "13.2 Defining Plural Pursuit\n\n\n\nSlide 04\n\n\nThe project formally defines plural pursuit as a situation characterised by distinct yet concurrent instances of ‘normal science’ that share a common problem-solving objective—in this case, the reconciliation of quantum mechanics and gravitation. Each instance of this normal science, the authors propose, should be articulated by a distinct social community that adheres to a particular intellectual disciplinary matrix.\nThis concept synthesises established philosophical frameworks, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’s research programmes. Consequently, the investigation frames a central empirical question: does the field of quantum gravity research actually constitute an instance of plural pursuit, comprising independent communities that pursue different paradigms in parallel?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#data-and-methodology",
    "href": "chapter_ai-nepi_015.html#data-and-methodology",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.3 Data and Methodology",
    "text": "13.3 Data and Methodology\n\n\n\nSlide 05\n\n\nTo conduct their analysis, the authors gathered a substantial dataset of around 200,000 abstracts and titles from the literature of fundamental physics. They then proceeded with a two-step methodology.\nThe first step involved a linguistic analysis of the field’s intellectual structure, for which they relied on the Bertopic pipeline. This process begins by spatialising the documents into an embedding space, upon which they perform unsupervised clustering at a very fine-grained level. Such granularity, which yields 600 distinct topics, proves essential for identifying niche research approaches that may only encompass around 100 papers. Based on this classification, each physicist can be assigned a specialty corresponding to the most frequent topic in their publications, creating a partition of authors according to the intellectual structure.\nIn parallel, the second step performs a social network analysis. The authors constructed a co-authorship graph where nodes represent 30,000 physicists and edges signify co-authorship. Applying a community detection method to this network, they identified approximately 800 distinct communities, which provides an alternative partition of the authors, this time reflecting the social structure of the field.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.4 Conceptualising Plural Pursuit",
    "text": "13.4 Conceptualising Plural Pursuit\n\n\n\nSlide 06\n\n\nWithin the analytical framework constructed by the authors, the concept of plural pursuit translates into an intuitive, idealised model: a one-to-one mapping between social communities and intellectual topics.\nIf this relationship were visualised in a correlation matrix, with communities on one axis and topics on the other, a perfect instance of plural pursuit would manifest as a clean, diagonal pattern. Such a structure would signify a clear division of labour, where each community dedicates its efforts exclusively to a single, distinct topic.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-challenge-of-scale",
    "href": "chapter_ai-nepi_015.html#the-challenge-of-scale",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.5 The Challenge of Scale",
    "text": "13.5 The Challenge of Scale\n\n\n\nSlide 07\n\n\nWhen applied directly to the fine-grained partitions of the field, the correlation matrix reveals a messy and complex structure that is difficult to interpret. This complexity arises from several underlying issues.\nFirstly, the level of fine-graining in the topic partition is somewhat arbitrary; a broad research area like string theory, for instance, might be scattered across numerous smaller topics. Secondly, community formation is influenced by many micro-social processes, which can result in large research programmes being pursued by several distinct communities simultaneously. These challenges point towards a more fundamental problem: the computational definitions of ‘topic’ and ‘community’ are inherently scale-dependent. Moreover, research programmes are often nested conceptually, with families and subfamilies of inquiry, further complicating any attempt at a simple, flat classification.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.6 Hierarchical Reconstruction",
    "text": "13.6 Hierarchical Reconstruction\n\n\n\nSlide 08\n\n\nTo address the issue of scale, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For the intellectual structure, they begin with the 600 fine-grained topics and progressively merge them using an agglomerative clustering technique to build a topic hierarchy. For the social structure, they employ a hierarchical stochastic block model from the start, a method that learns a multi-level partition of the co-authorship network into increasingly coarse communities.\nThese hierarchical models effectively introduce a notion of scale, enabling the system to be observed at various levels of granularity. One can, for instance, colour the co-authorship network according to topic specialties at different levels of the linguistic hierarchy. Nevertheless, the problem of arbitrariness persists, as it is not yet clear which scale should be chosen for analysing either structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-problem-of-scale-selection",
    "href": "chapter_ai-nepi_015.html#the-problem-of-scale-selection",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.7 The Problem of Scale Selection",
    "text": "13.7 The Problem of Scale Selection\n\n\n\nSlide 10\n\n\nThe freedom to select an observational scale for the topic and community hierarchies presents a significant challenge. Depending on the level of granularity chosen for each structure, the resulting correlation matrix will look markedly different. Consequently, this arbitrary choice can lead to very different narratives and conclusions about how the field of quantum gravity is organised.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#an-adaptive-coarse-graining-strategy",
    "href": "chapter_ai-nepi_015.html#an-adaptive-coarse-graining-strategy",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.8 An Adaptive Coarse-Graining Strategy",
    "text": "13.8 An Adaptive Coarse-Graining Strategy\n\n\n\nSlide 11\n\n\nTo select an appropriate analytical scale, the authors developed an adaptive topic coarse-graining strategy. The core idea is to systematically remove degrees of freedom from the fine-grained topic partition by merging topics whose linguistic distinctions have no discernible impact on scientists’ collaborative behaviour.\nThis process is guided by the Minimum Description Length (MDL) criterion, an information-theoretic principle. The MDL criterion seeks a partition that optimally balances two competing factors: its power to explain the social structure of the field and its own simplicity. In practice, the algorithm navigates the hierarchical tree of 600 topics, progressively coarse-graining the structure. It stops when adding further complexity—that is, maintaining finer topic distinctions—no longer provides a worthwhile gain in information about the social network.\nThis procedure effectively reduces the initial 600 topics to a more meaningful set of 50. Crucially, some small, niche topics are preserved because they correspond to genuine social divisions, whilst many others are lumped together into larger intellectual domains.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#labelling-the-topic-landscape",
    "href": "chapter_ai-nepi_015.html#labelling-the-topic-landscape",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.9 Labelling the Topic Landscape",
    "text": "13.9 Labelling the Topic Landscape\n\n\n\nSlide 12\n\n\nFollowing the coarse-graining procedure, the resulting 50 topics are assigned descriptive labels by retrieving representative n-grams from their constituent papers. This step renders the computationally derived clusters interpretable. With this manageable and meaningful topic landscape, the analysis can then be narrowed to focus specifically on those topics that are clearly related to quantum gravity research.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#mapping-structures-across-scales",
    "href": "chapter_ai-nepi_015.html#mapping-structures-across-scales",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.10 Mapping Structures Across Scales",
    "text": "13.10 Mapping Structures Across Scales\n\n\n\nSlide 13\n\n\nWith a refined set of topics, the analysis returns to the correlation matrix to match these intellectual structures with social communities across different scales. For each of the 50 topics, the authors identify the community level in the social hierarchy that best explains it. The results are varied.\nSome topics, such as string theory, map very well to a specific community structure, in this case at the third level of the hierarchy. In contrast, other research programmes like loop quantum gravity appear to be tied to communities at a much more fine-grained hierarchical level. Furthermore, some very large topics are not associated with any single community, suggesting they represent concepts of general interest across the field.\nUltimately, the landscape does not reflect a clear case of plural pursuit. Instead, the analysis reveals nested structures—for example, a small community focused on holography that is part of the larger string theory community—and entangled scales, demonstrating a complex interplay rather than a clean division of labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#a-top-down-expert-led-approach",
    "href": "chapter_ai-nepi_015.html#a-top-down-expert-led-approach",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.11 A Top-Down, Expert-Led Approach",
    "text": "13.11 A Top-Down, Expert-Led Approach\n\n\n\nSlide 14\n\n\nTo complement the bottom-up analysis, the authors implemented a top-down approach grounded in expert knowledge. They surveyed the founding members of the International Society for Quantum Gravity, asking them to list the research approaches they believe structure the field.\nAlthough the experts did not all agree, their feedback was synthesised into a detailed list of programmes that partition the landscape. For the subsequent analysis, the investigation focuses on three of these: string theory, supergravity, and holography. This particular trio was selected because of an interesting disagreement amongst the physicists themselves about whether these should be treated as separate approaches. Some contend that supergravity and holography are fundamentally aspects of string theory, despite their distinct historical and conceptual origins.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-the-two-models",
    "href": "chapter_ai-nepi_015.html#confronting-the-two-models",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.12 Confronting the Two Models",
    "text": "13.12 Confronting the Two Models\n\n\n\nSlide 15\n\n\nBased on the expert-derived list, the authors trained a classifier to automatically assign papers to these top-down categories. Using the all-MiniLM-L6-v2 model on text embeddings of titles and abstracts, and training it with hand-coded labels, they could predict which papers belong to each approach.\nThe output of this supervised, top-down classification was then confronted with the results of the unsupervised, bottom-up reconstruction. This comparison yielded a key insight: the two models align well for approaches that are conceptually autonomous and well-defined. Conversely, the correspondence is poor for approaches that are more phenomenological in nature or do not represent fully-fledged conceptual frameworks.\nNotably, the bottom-up analysis generated a single, large string theory cluster that appears to encompass what experts separately labelled as ‘supergravity’ and ‘string theory’. It thereby computationally reflects the very ambiguity the physicists had expressed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#convergence-of-evidence",
    "href": "chapter_ai-nepi_015.html#convergence-of-evidence",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.13 Convergence of Evidence",
    "text": "13.13 Convergence of Evidence\n\n\n\nSlide 16\n\n\nThe computational findings converge with direct expert testimony. One physicist commented that the community of researchers working on supergravity as a standalone theory is likely very small. The practical overlap of personnel working on both supergravity and string theory is so large that, in their view, the two communities cannot be separated in any meaningful way.\nThis assessment perfectly mirrors the outcome of the bottom-up model. After the MDL-based procedure strips away linguistic nuances that lack social consequences, the model lumps these two areas together. This happens despite the fact that the initial, fine-grained linguistic analysis correctly identified them as conceptually distinct clusters.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusion",
    "href": "chapter_ai-nepi_015.html#conclusion",
    "title": "13  Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit",
    "section": "13.14 Conclusion",
    "text": "13.14 Conclusion\n\n\n\nSlide 17\n\n\nThe research yields several key conclusions. Firstly, it demonstrates that socio-epistemic systems must be observed at multiple scales, as core concepts like ‘community’ and ‘disciplinary matrix’ are inherently scale-dependent. Secondly, identifying configurations of plural pursuit—the one-to-one mapping of communities to their intellectual foundations—necessitates methods that can match these structures across different scales.\nFor the specific case of quantum gravity, the bottom-up reconstruction of the research landscape serves to either confirm or challenge the intuitions of physicists about their own field. More broadly, this work shows how powerful computational methods can enable us to revisit and test philosophical insights, such as the nature of a paradigm, that have long relied on intuition alone. In this spirit, the presentation concludes with a paraphrase of Clausewitz: computation is the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping the Landscape of Quantum Gravity: A Computational Analysis of Plural Pursuit</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "",
    "text": "Overview\nThis chapter investigates a central question in computational text analysis: whether robust topic modelling necessitates full-text documents, or if titles and abstracts provide sufficient data. The authors undertake a rigorous comparative analysis of two prominent techniques, Latent Dirichlet Allocation (LDA) and BERTopic. They applied these models to a corpus on Astrobiology, systematically segmenting the data into three distinct types: full-text documents, abstracts alone, and titles alone.\nTheir evaluation framework is twofold. A qualitative analysis explores thematic clustering and the coherence of top-words, whilst a comprehensive quantitative analysis employs four key metrics. These include the Adjusted Rand Index (ARI) to measure model similarity, Topic Diversity to assess the uniqueness of topics, Joint Recall to evaluate content coverage, and Coherence CV to gauge the interpretability of the generated topics.\nThe findings reveal a nuanced trade-off between the models and data types. BERTopic, for instance, excels in generating diverse topics, particularly from titles. Conversely, LDA models trained on full-text achieve the highest joint recall, indicating superior content coverage. The results suggest that the optimal choice of model and input data depends entirely on the specific analytical goals of the researcher, whether they prioritise thematic diversity, content coverage, or topic coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "href": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.1 Topic Modelling in HPSS",
    "text": "14.1 Topic Modelling in HPSS\n\n\n\nSlide 02\n\n\nTopic modelling has established itself as a significant analytical tool within the domains of History, Philosophy, and Sociology of Science (HPSS). Its utility is demonstrated across a range of applications that enhance scholarly inquiry. Within these fields, scholars employ this technique to identify influential authors and papers, trace the conceptual evolution of scientific ideas over time, and map the intellectual structure of entire disciplines.\nFurthermore, topic modelling enables the discovery of previously hidden thematic connections in large corpora, the analysis of long-term trends in scientific discourse, and the comparison of distinct research programmes. This capacity for large-scale analysis also makes it an invaluable resource for conducting comprehensive literature reviews.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-methodology",
    "href": "chapter_ai-nepi_016.html#research-question-and-methodology",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.2 Research Question and Methodology",
    "text": "14.2 Research Question and Methodology\n\n\n\nSlide 03\n\n\nThe authors’ investigation centres on a fundamental research question: does robust topic modelling depend on the analysis of full-text documents, or can comparable results be achieved using only titles or abstracts?\nTo answer this, the team implements a formal comparative methodology. This framework systematically evaluates two distinct topic modelling approaches—the probabilistic Latent Dirichlet Allocation (LDA) and the transformer-based BERTopic. They apply each model to three different granularities of text data: complete full-text documents, abstracts, and titles. Subsequently, the outputs from these combinations are assessed through both qualitative and quantitative analysis, providing a multi-faceted evaluation of their performance.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology",
    "href": "chapter_ai-nepi_016.html#methodology",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.3 Methodology",
    "text": "14.3 Methodology\n\n\n\nSlide 04\n\n\nThe investigation proceeds with a detailed examination of the methodologies employed to compare the two topic modelling techniques.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.4 Comparing LDA and BERTopic",
    "text": "14.4 Comparing LDA and BERTopic\n\n\n\nSlide 05\n\n\nAt their core, both Latent Dirichlet Allocation and BERTopic share common postulates; they generally rely on a bag-of-words representation and conceptualise topics as distinct distributions over a vocabulary. Nevertheless, their underlying mechanisms differ significantly. LDA is a generative probabilistic model that assumes each document is a mixture of various topics.\nIn contrast, BERTopic leverages modern transformer models to create contextual word and sentence embeddings. It then applies clustering algorithms to these rich semantic representations to identify topics. This allows it to capture nuances of meaning that frequency-based models like LDA may miss.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#framework-for-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#framework-for-qualitative-comparison",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.5 Framework for Qualitative Comparison",
    "text": "14.5 Framework for Qualitative Comparison\n\n\n\nSlide 06\n\n\nThe authors established a clear framework for the qualitative comparison of the models, using a specialised Astrobiology corpus as their primary dataset. Within this framework, they configured an LDA model to generate 25 distinct topics from the corpus.\nFollowing this initial modelling, the team further organised these 25 topics through a clustering process into four high-level thematic groups. To visualise the interplay and connections between these themes, the analysts created a correlation graph, mapping the relationships between the identified topic clusters.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.6 Quantitative Analysis Metrics",
    "text": "14.6 Quantitative Analysis Metrics\n\n\n\nSlide 07\n\n\nThe quantitative evaluation relies on four distinct metrics, each chosen to assess a specific aspect of model performance. The Adjusted Rand Index (ARI) measures the similarity between the clustering structures produced by different topic models. Topic Diversity calculates the percentage of unique words present in the top terms across all topics, providing a measure of model redundancy.\nJoint Recall is used to evaluate how effectively a model trained on a text subset, such as abstracts, can retrieve the topics found in the corresponding full-text model. Finally, Coherence CV assesses the human interpretability of a topic by computing the semantic similarity of its most prominent words.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results",
    "href": "chapter_ai-nepi_016.html#results",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.7 Results",
    "text": "14.7 Results\n\n\n\nSlide 08\n\n\nThe subsequent sections present the empirical results derived from the authors’ comprehensive qualitative and quantitative analyses.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#model-similarity-via-adjusted-rand-index",
    "href": "chapter_ai-nepi_016.html#model-similarity-via-adjusted-rand-index",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.8 Model Similarity via Adjusted RAND Index",
    "text": "14.8 Model Similarity via Adjusted RAND Index\n\n\n\nSlide 09\n\n\nThe authors used the Adjusted RAND Index (ARI) to quantify the similarity between the outputs of different models. Their results, presented in a matrix, show that models of the same family—such as LDA models trained on abstracts versus titles—exhibit greater similarity to one another than they do to models from the other family, like BERTopic.\nThis finding indicates that the choice of algorithm (LDA vs. BERTopic) has a more profound impact on the resulting topic structure than the choice of input text. Furthermore, the analysis reveals that models trained on abstracts more closely resemble their full-text counterparts than models trained on titles do, suggesting abstracts retain more of the core thematic structure.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#lda-performance-across-text-types",
    "href": "chapter_ai-nepi_016.html#lda-performance-across-text-types",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.9 LDA Performance Across Text Types",
    "text": "14.9 LDA Performance Across Text Types\n\n\n\nSlide 10\n\n\nAn analysis of Latent Dirichlet Allocation (LDA) performance across different text granularities reveals notable variations. Using heatmaps to visualise topic distributions, the authors compared models trained on full-text documents against those trained on only abstracts or titles.\nTheir results indicate that whilst some thematic correspondence exists, the topic structures generated from abstracts and titles frequently diverge from those derived from the full text. This suggests that relying on shorter text segments can lead to a different, and potentially less complete, thematic representation of the corpus when using LDA.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#bertopic-performance-across-text-types",
    "href": "chapter_ai-nepi_016.html#bertopic-performance-across-text-types",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.10 BERTopic Performance Across Text Types",
    "text": "14.10 BERTopic Performance Across Text Types\n\n\n\nSlide 11\n\n\nThe performance of BERTopic also varies significantly depending on the input text. Visualised through three distinct matrices, the team’s analysis shows that the BERTopic model trained on full-text documents tends to produce a high number of small and highly specific topics.\nIn contrast, when trained on abstracts, the model yields topics that are more stable and clearly defined. Training on titles, however, results in the formation of broader and more generalised thematic categories, demonstrating how the input data’s scope directly influences the granularity of the output.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#qualitative-analysis-of-lda-top-words",
    "href": "chapter_ai-nepi_016.html#qualitative-analysis-of-lda-top-words",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.11 Qualitative Analysis of LDA Top-Words",
    "text": "14.11 Qualitative Analysis of LDA Top-Words\n\n\n\nSlide 12\n\n\nA qualitative comparison of the top-words generated by LDA models highlights the impact of text granularity on topic interpretability. By examining the lists of top-words from models trained on full-text, abstracts, and titles, the authors observed clear divergences in topic coherence and thematic focus.\nFor instance, a distinct topic related to ‘life detection’ might appear clearly in both the full-text and abstract-based models. However, in the model trained solely on titles, the same theme could become less coherent, potentially merging with other, more general topics and losing its specific meaning.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparative-topic-formation",
    "href": "chapter_ai-nepi_016.html#comparative-topic-formation",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.12 Comparative Topic Formation",
    "text": "14.12 Comparative Topic Formation\n\n\n\nSlide 13\n\n\nContrasting the behaviour of LDA and BERTopic reveals fundamental differences in how they construct topics from text. The authors’ analysis shows that a single, broad topic identified by an LDA model can often be resolved into several more specific and nuanced topics by BERTopic, a phenomenon known as topic splitting.\nConversely, BERTopic’s ability to discern fine-grained semantic distinctions may result in multiple related topics that LDA, with its focus on word co-occurrence, merges into a single, more generalised category. These patterns of splitting and merging underscore the distinct operational logics of the two algorithms.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-results-topic-coherence",
    "href": "chapter_ai-nepi_016.html#quantitative-results-topic-coherence",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.13 Quantitative Results: Topic Coherence",
    "text": "14.13 Quantitative Results: Topic Coherence\n\n\n\nSlide 14\n\n\nThe quantitative analysis of topic coherence, measured using the CV score, produced nuanced results. When comparing BERTopic and LDA across titles, abstracts, and full-text data, no single model or text type demonstrated consistent superiority.\nInstead, topic coherence appears to be highly dependent on the specific model configuration. The number of topics a user specifies is a particularly influential variable, with coherence scores for both LDA and BERTopic fluctuating significantly as this parameter changes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-results-topic-diversity",
    "href": "chapter_ai-nepi_016.html#quantitative-results-topic-diversity",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.14 Quantitative Results: Topic Diversity",
    "text": "14.14 Quantitative Results: Topic Diversity\n\n\n\nSlide 15\n\n\nIn the evaluation of topic diversity, a clear pattern emerged. The authors found that BERTopic models consistently outperform their LDA counterparts, generating topic sets with less word overlap.\nNotably, the peak diversity scores were achieved when BERTopic was trained on titles alone. This finding suggests that for research goals where maximising the variety of distinct themes is paramount, the combination of the BERTopic algorithm and title-only data provides a highly effective strategy.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-results-joint-recall",
    "href": "chapter_ai-nepi_016.html#quantitative-results-joint-recall",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.15 Quantitative Results: Joint Recall",
    "text": "14.15 Quantitative Results: Joint Recall\n\n\n\nSlide 16\n\n\nThe analysis of joint recall, which measures how well a model captures the themes of the entire document, yields an unambiguous result. Models trained on full-text data consistently achieve the highest recall scores.\nSpecifically, the LDA model applied to full-text documents registered the top performance. This outcome demonstrates that for applications where comprehensive thematic coverage is the primary objective, there is no substitute for analysing the complete text of the documents.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#summary-matrix-of-model-performance",
    "href": "chapter_ai-nepi_016.html#summary-matrix-of-model-performance",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.16 Summary Matrix of Model Performance",
    "text": "14.16 Summary Matrix of Model Performance\n\n\n\nSlide 17\n\n\nA summary matrix provides a consolidated overview of the comparative analysis. It systematically contrasts the performance of LDA and BERTopic when applied to full-text, abstract, and title data. Using a range of evaluation metrics, the matrix employs a simple visual key—filled circles—to indicate which combination of model and data input excels for each specific measure, allowing for a quick, at-a-glance assessment of the relative strengths of each approach.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-performance-summary",
    "href": "chapter_ai-nepi_016.html#overall-performance-summary",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.17 Overall Performance Summary",
    "text": "14.17 Overall Performance Summary\n\n\n\nSlide 18\n\n\nThe final performance summary synthesises the findings across all evaluation criteria, including overall fit, top-word quality, coherence, diversity, and joint recall. This overview uses filled circles to denote strong performance and red crosses to flag identified weaknesses.\nThe LDA model trained on full-text, for example, is highlighted for its excellent joint recall and overall fit but shows limitations in topic diversity. Conversely, the BERTopic model trained on titles excels in producing diverse topics but at the cost of lower content coverage. This clearly illustrates a fundamental trade-off: methods that maximise topic diversity often do so at the expense of comprehensive recall, and vice versa.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusions",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusions",
    "title": "14  A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora",
    "section": "14.18 Discussion and Conclusions",
    "text": "14.18 Discussion and Conclusions\n\n\n\nSlide 19\n\n\nIn conclusion, the authors synthesise the distinct performance characteristics of LDA and BERTopic when applied to different sections of scholarly documents. As their similarity matrix demonstrated, the choice of algorithm is a more powerful determinant of the final topic structure than the granularity of the input text.\nScholars seeking the most comprehensive thematic coverage should favour full-text analysis, particularly with LDA, which excels in joint recall. However, for projects prioritising the discovery of a wide and diverse range of topics, BERTopic applied to titles proves to be a superior strategy. Ultimately, the study confirms that there is no single best approach; the optimal combination of model and data is entirely contingent on the specific objectives of the research.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Analysis of Topic Modeling Techniques for Scholarly Corpora</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "",
    "text": "Overview\nThis chapter details a novel approach for integrating explicit time awareness into Transformer-based Large Language Models (LLMs). The authors identify a core limitation in current models: they possess only an implicit, statistical understanding of time derived from their training data. This deficiency leads to an inability to resolve time-dependent contradictory information and contributes to a pronounced recency bias.\nTo address this, the team proposes the Time Transformer, an architecture that incorporates an explicit temporal dimension directly into the token embedding space. This minimal adjustment allows the model to learn the influence of time on language patterns without altering the fundamental training objective of maximising log likelihood.\nA proof of concept was developed using a small, decoder-only Transformer model built from scratch, featuring 39 million parameters. For training, the authors curated a specialised dataset of UK Met Office daily weather reports from 2018 to 2024, selected for its restricted vocabulary and repetitive linguistic structures.\nTwo experiments involving synthetically injected temporal drift demonstrated the model’s efficacy. The first involved a ‘synonymic succession’ where one word was progressively replaced by another over a year, a pattern the model successfully learned and reproduced. The second, a more complex ‘collocation fixation’, altered word co-occurrence probabilities over time, which the model also learned, as verified by analysing its internal attention mechanisms.\nWhilst the proof of concept is successful, the authors acknowledge significant challenges for broader application. These include the necessity of training new models from scratch and the extensive data curation required to assign accurate timestamps to all training sequences.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-problem-of-implicit-time",
    "href": "chapter_ai-nepi_017.html#the-problem-of-implicit-time",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.1 The Problem of Implicit Time",
    "text": "15.1 The Problem of Implicit Time\n\n\n\nSlide 03\n\n\nCurrent Large Language Models derive their understanding of time statistically, extracting implicit cues from vast training corpora. Whilst remarkably capable, this approach has inherent limitations. Models cannot easily resolve information that is contradictory without temporal context; for example, two statements identifying different dominant neural network architectures are both valid, but at different points in time. During training, these sentences compete directly for attention, forcing the model into a state where it cannot perfectly fulfil its objective because validating one statement necessitates penalising the other.\nConsequently, during inference, these models often exhibit a recency bias, favouring the most recently prevalent information. An input sequence about neural architectures will likely elicit the completion ‘Transformers’, even though ‘long short-term memories’ (LSTMs) is also present within its learned knowledge. To retrieve this older information, users must resort to prompt engineering—adjusting the input by adding a year or changing a verb’s tense. This process is imprecise and unreliable, as it depends on exploiting how the model has happened to learn temporal cues.\nA more robust solution requires models that are explicitly time-aware, capable of learning and reproducing evolving patterns as a direct function of time. To this end, the authors have developed a proof of concept that achieves this for generative language models.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "href": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.2 Formalising Temporal Dependence",
    "text": "15.2 Formalising Temporal Dependence\n\n\n\nSlide 05\n\n\nTo formalise the problem, it is necessary to consider the core function of a language model. Based on its training data, an LLM learns to estimate the probability distribution over its entire vocabulary for the next token, given a preceding sequence of tokens. It then outputs the most probable continuation.\nA critical factor, however, is that these probabilities are not static in the real world; they are inherently a function of time. For instance, the probability of the token ‘Transformers’ completing a specific sentence about neural architectures was effectively zero in 2017.\nDespite this reality, the training process typically treats the probability distributions for token sequences as static. This simplification means that when the model is later used for inference, it can only reflect the temporal drift in language patterns via in-context learning. The model’s ability to generate time-appropriate text is therefore contingent on the specific cues provided in the immediate prompt, rather than on a fundamental, built-in understanding of temporal dynamics.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-time-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#the-time-transformer-architecture",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.3 The Time Transformer Architecture",
    "text": "15.3 The Time Transformer Architecture\n\n\n\nSlide 06\n\n\nTo model time-dependent probability distributions effectively, the authors propose a new architecture, the Time Transformer, as a more efficient alternative to data-intensive methods like time-slicing. The core idea is elegant in its simplicity. Every natural language processing task begins by converting tokens into a vectorial representation, or embedding, within a latent space that is learned during training. The Time Transformer augments this process by adding a single, additional dimension to the embedding that explicitly encodes the token’s time of utterance.\nIn this model, every token in an input sequence is assigned a specific time value, meaning its vector representation will differ slightly based on when it was recorded. When these time-aware embeddings are processed by the Transformer, the resulting output probability distributions for subsequent tokens are inherently time-dependent.\nCrucially, the training objective remains the standard maximisation of log likelihood. The beauty of this approach lies in the Transformer’s innate ability to process statistical relationships; it learns precisely how much influence the temporal dimension should have on each token, allowing some words to remain stable over time whilst others change significantly.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-curation-and-pre-processing",
    "href": "chapter_ai-nepi_017.html#dataset-curation-and-pre-processing",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.4 Dataset Curation and Pre-processing",
    "text": "15.4 Dataset Curation and Pre-processing\n\n\n\nSlide 07\n\n\nTo test the Time Transformer concept, the authors required a dataset with a restricted language and a small vocabulary, which would allow a modest model to learn its linguistic patterns. They identified UK Met Office daily weather reports as an ideal candidate. These reports, available online as monthly PDFs from the UK’s national meteorological service, feature highly repetitive language. As a potential alternative, the TinyStories dataset was also noted.\nThe team scraped all daily reports from 2018 to 2024, creating a corpus of roughly 2,500 documents. The text was then chunked and tokenised using Keras TextVectorization. A simple pre-processing scheme was employed, which ignored case and punctuation and did not use subword tokenisation. This straightforward approach was sufficient for the simple language of the reports, which, across seven years of data, comprised a vocabulary of only 3,400 words.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#baseline-model-and-training",
    "href": "chapter_ai-nepi_017.html#baseline-model-and-training",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.5 Baseline Model and Training",
    "text": "15.5 Baseline Model and Training\n\n\n\nSlide 08\n\n\nBefore implementing the temporal component, the authors constructed a baseline ‘vanilla’ Transformer from scratch to confirm that a small model could learn the language patterns of the weather report dataset. This modest, decoder-only model features four decoder layers. Each layer is composed of a multi-head attention block with eight heads, followed by a normalisation layer, a non-linear feed-forward layer, and a final normalisation layer. A concluding dense layer performs the classification task of assigning probabilities across the vocabulary.\nThe resulting model is very small by modern standards, with only 39 million parameters and a file size of 150 MB. It was trained on an HPC cluster in Munich using two A100 GPUs, with each training epoch completing in just 11 seconds. After training, the model demonstrated its ability to autoregressively generate coherent and realistic weather reports from a simple seed phrase, proving it had successfully captured the linguistic patterns of the source data. The code for this model and the subsequent experiments is publicly available.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#implementing-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#implementing-the-time-transformer",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.6 Implementing the Time Transformer",
    "text": "15.6 Implementing the Time Transformer\n\n\n\nSlide 10\n\n\nAdapting the baseline model into the Time Transformer requires a remarkably minimal architectural change. The implementation reserves just one dimension within the model’s 512-dimensional latent semantic space to carry temporal information. For this proof of concept, every token is assigned a non-trainable, min-max normalised value corresponding to the day of the year on which its source report was published.\nThis specific encoding—the day of the year—was deliberately chosen to exploit the seasonal variations naturally present in the weather data, such as the higher frequency of ‘snow’ in winter and ‘hot’ in summer. This design choice enables the model to directly associate linguistic patterns with cyclical time. Nevertheless, the framework is flexible, and any other form of time embedding could be integrated as needed.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-one-synonymic-succession",
    "href": "chapter_ai-nepi_017.html#experiment-one-synonymic-succession",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.7 Experiment One: Synonymic Succession",
    "text": "15.7 Experiment One: Synonymic Succession\n\n\n\nSlide 11\n\n\nThe first experiment sought to determine if the new architecture could efficiently learn a temporal drift injected into its training data. The authors designed a ‘synonymic succession’ by systematically replacing the word ‘rain’ with ‘liquid sunshine’. This replacement was governed by a sigmoid function tied to the day of the year, where the probability of replacement was zero at the beginning of the year and gradually increased to one by the end.\nAfter training on this modified dataset, the model was tasked with generating one weather prediction for every day of the year. By counting the monthly frequencies of ‘rain’ and ‘liquid sunshine’ in the output, the team confirmed the experiment’s success. The generated text, with some expected statistical variation, faithfully reproduced the engineered pattern: ‘rain’ appeared almost exclusively in the early months, ‘liquid sunshine’ dominated the later months, and the transition occurred precisely in the middle of the year.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-two-collocation-fixation",
    "href": "chapter_ai-nepi_017.html#experiment-two-collocation-fixation",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.8 Experiment Two: Collocation Fixation",
    "text": "15.8 Experiment Two: Collocation Fixation\n\n\n\nSlide 13\n\n\nSeeking a more complex challenge than simple word replacement, a second experiment was designed to test if the model could learn a change in word co-occurrence. This ‘collocation fixation’ synthetically altered the weather language over the course of the year. Specifically, instances of the word ‘rain’ not already followed by ‘and’ were replaced with the phrase ‘rain and snow’, with the probability of this replacement increasing throughout the year. From the model’s perspective, this changes the language such that by the year’s end, the appearance of ‘snow’ becomes almost entirely conditioned on the preceding token being ‘rain’.\nOnce again, the model successfully learned the injected pattern. Generated forecasts for the end of the year almost always featured ‘rain and snow’ together, whereas forecasts for the beginning of the year showed more varied patterns. Deeper analysis into the model’s internals provided further proof. The authors found that specific attention heads had specialised to detect this relationship, paying significantly more attention to the token ‘rain’ late in the year when deciding whether to generate ‘snow’.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#conclusions-and-future-challenges",
    "href": "chapter_ai-nepi_017.html#conclusions-and-future-challenges",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.9 Conclusions and Future Challenges",
    "text": "15.9 Conclusions and Future Challenges\n\n\n\nSlide 14\n\n\nThe research successfully demonstrates as a proof of concept that Transformer-based LLMs can be made explicitly time-aware. This is achieved simply by adding a temporal dimension to the initial token embeddings. Whilst this opens up fascinating application possibilities, several potential next steps and significant challenges remain. One promising avenue for future work is to investigate whether the explicit time signal could actually make training more efficient, as it provides a clear signal for patterns the model would otherwise have to decipher from implicit clues.\nHowever, major hurdles exist for widespread application. Firstly, because this is a novel architecture, one cannot simply fine-tune an existing pre-trained model; it requires training from scratch, a computationally prohibitive task for large-scale models. Secondly, the approach sacrifices the simplicity of metadata-free learning. It necessitates a rigorous data curation process to assign an accurate date to every text sequence, a complex and often ambiguous task, especially for historians.\nAs a final thought, a more practical application might be to use this principle not for a full generative model, but to build a targeted, BERT-like embedder for specific, time-sensitive analytical tasks.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "",
    "text": "Overview\nThis research programme explores the interplay between different forms of context in shaping language variation and semantic change. It operates within the Cascade project, a Marie Curie Doctoral Network dedicated to the computational analysis of semantic change across diverse environments. The project employs the chemical revolution—a pivotal paradigm shift from the phlogiston theory to modern oxygen theory—as a pilot study, drawing upon the Royal Society Corpus as its primary data source.\nThe investigation synthesises principles from linguistics and information theory, including language variation, register theory, and rational communication, to understand how language adapts to scientific evolution. The team employs a multi-faceted computational toolkit to dissect this process. Previous work by the authors demonstrated the utility of Kullback-Leibler divergence for detecting periods of significant linguistic change, cascade models (Hawkes processes) for identifying key innovators like Priestley and spreaders like Pearson, and word2vec for visualising the displacement of the ‘phlogiston’ semantic space by ‘oxygen’ terminology.\nMoreover, the authors applied the concept of surprisal to model the cognitive effort associated with new terms, demonstrating how structural compression fosters more efficient terminology as concepts become established. The current project, led by PhD student Sofía Aguilar, aims to synthesise these distinct approaches. Aguilar proposes a novel framework that combines BERT embeddings with a Transformer-Graph Convolutional Network (GCN) to model the latent interactions between semantic content and contextual metadata, such as authors and journals, to cultivate a more holistic understanding of conceptual change.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-framework-for-scientific-discourse",
    "href": "chapter_ai-nepi_019.html#a-framework-for-scientific-discourse",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.1 A Framework for Scientific Discourse",
    "text": "17.1 A Framework for Scientific Discourse\n\n\n\nSlide 19\n\n\nThe authors have developed a robust theoretical framework to analyse transformations in scientific discourse, situating their work within the Cascade project, a Marie Curie Doctoral Network. Its central objective is to model how different forms of context interact to drive semantic change. To this end, the project uses the chemical revolution as a pilot case study, drawing on texts from the Royal Society Corpus.\nThis investigation rests upon two key linguistic principles. First, the principles of language variation and register theory posit that situational context fundamentally shapes language use. This theory also explains how the linguistic system’s inherent flexibility permits various encodings for a concept, such as the evolution from ‘dephlogisticated air’ to the term ‘oxygen’. Second, the framework integrates rational communication and information-theoretic accounts. These accounts propose that such variation is not arbitrary but rather a mechanism for modulating information content, enabling writers to achieve communicative efficiency whilst maintaining a reasonable cognitive load.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-change-with-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-change-with-kullback-leibler-divergence",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.2 Detecting Change with Kullback-Leibler Divergence",
    "text": "17.2 Detecting Change with Kullback-Leibler Divergence\nTo pinpoint the moments and mechanisms of linguistic change, the team employs Kullback-Leibler (KL) divergence, a method that transcends the static, period-based comparisons typical of traditional corpus linguistics. Instead, the authors model change as a continuous process. Their underlying assumption is that linguistic divergence increases with temporal distance; language from periods far apart should differ more than language from adjacent periods.\nIn practice, this involves constructing a continuous timeline of language use. The authors calculate KL divergence repeatedly using sliding bins that encompass a set number of previous and past years. This dynamic process yields a diachronic comparison that illuminates moments of significant transformation, visualised as peaks of divergence, and periods of stability, represented by troughs of convergence.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#lexical-and-grammatical-evolution",
    "href": "chapter_ai-nepi_019.html#lexical-and-grammatical-evolution",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.3 Lexical and Grammatical Evolution",
    "text": "17.3 Lexical and Grammatical Evolution\nThe team’s analysis of linguistic evolution extends across both lexical and grammatical levels to build a comprehensive picture of change. At the lexical level, their diachronic comparison reveals that peaks in divergence are primarily driven by the emergence of new terminology. During the chemical revolution, for example, scientists first wrote about experimenting with ‘air’ that was ‘dephlogisticated’ long before the concept and term ‘oxygen’ became established.\nConcurrently, the investigation tracks grammatical shifts by retaining function words, which are often discarded in such analyses. Through the analysis of part-of-speech (POS) trigrams, the authors identify grammatical patterns that co-occur with and support lexical innovation. These structures, such as adjectival patterns like ‘dephlogisticated air’ or nominal phrases like ‘oxide of iron’, also generate divergence peaks, demonstrating that grammar plays an active, supportive role in the expression of new scientific concepts.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#visualising-semantic-shifts",
    "href": "chapter_ai-nepi_019.html#visualising-semantic-shifts",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.4 Visualising Semantic Shifts",
    "text": "17.4 Visualising Semantic Shifts\nTo visualise the profound conceptual shifts of the chemical revolution, the authors modelled the semantic space using the word2vec method. This approach maps the relationships between terms, offering a visual representation of the paradigmatic context. The results of this analysis proved decisive.\nTheir model showed how the established ‘phlogiston’ semantic space was systematically displaced by terminology associated with the new oxygen theory. Over time, the term ‘phlogiston’ and its related concepts occupied a progressively smaller area within the semantic map, eventually disappearing from the dominant scientific discourse.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#identifying-innovators-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#identifying-innovators-with-cascade-models",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.5 Identifying Innovators with Cascade Models",
    "text": "17.5 Identifying Innovators with Cascade Models\nBeyond tracking what changes, the team sought to identify who drives these transformations. To determine the key actors leading and disseminating new scientific terminology, the authors employed cascade models, a methodology derived from Hawkes processes. This technique, which originated in fields like earthquake detection and is now applied to social media analysis, models how events or behaviours trigger subsequent events in a network.\nBy applying these models to the usage of chemical words in the Royal Society Corpus, the authors could distinguish innovators from those they influenced. Their results identified distinct roles within the scientific community. Priestley emerged as the central innovator who initiated the change. He, in turn, heavily influenced George Pearson, who was identified as a critical early adopter and a spreader responsible for disseminating the new terminology. The models also categorised other figures, such as late adopters, thereby providing a clear map of the innovation’s diffusion.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#surprisal-cognition-and-terminology",
    "href": "chapter_ai-nepi_019.html#surprisal-cognition-and-terminology",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.6 Surprisal, Cognition, and Terminology",
    "text": "17.6 Surprisal, Cognition, and Terminology\nTo understand the drivers of terminological evolution, the authors adopt a communicative perspective centred on the concept of surprisal. Surprisal quantifies the predictability of a word in a given context and is proportional to the cognitive effort needed to process it; a highly surprising word is informative but difficult to process. For instance, the first mention of ‘oxygen’ by Priestley carried a high surprisal value for its contemporary readers.\nThe authors observed a distinct evolutionary pattern they term structural compression. Initially, a new technical term exhibits high surprisal. As it gains traction within the community, its usage becomes more common and its surprisal value steadily declines, eventually reaching a saturation point. This reduction in collective cognitive effort paves the way for a more compact and efficient linguistic form to emerge and solidify its place in the lexicon. Crucially, the authors note that this mechanism appears specific to terminological development, as it does not apply to the evolution of general-purpose phrases.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-unified-framework-for-contextual-interaction",
    "href": "chapter_ai-nepi_019.html#a-unified-framework-for-contextual-interaction",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.7 A Unified Framework for Contextual Interaction",
    "text": "17.7 A Unified Framework for Contextual Interaction\nBuilding on this body of work, Sofía Aguilar is leading a project to create a unified framework for modelling the interaction between diverse contextual factors. Her multi-stage methodology aims to cultivate a more holistic view of conceptual change. The process begins with data sampling, where Aguilar uses Kullback-Leibler divergence to identify the key terms and historical periods that define the phlogiston-oxygen debate.\nIn the subsequent network construction stage, the team represents the interactions that fostered the new concept. Aguilar’s approach uses BERT to create term embeddings, combining them with contextual metadata—such as authors and journals—into comprehensive node feature matrices for every 20-year period. To manage the high computational cost of this dense graph, Aguilar proposes using community detection to simplify the network structure.\nThe third stage focuses on predicting latent relationships. Aguilar employs a hybrid Transformer-Graph Convolutional Network (GCN) to infer connections not explicitly mentioned in the source texts. The GCN learns structural patterns from the node profiles to predict new links, whilst the Transformer’s attention mechanism identifies the most influential nodes driving these interactions.\nFinally, for validation, Aguilar’s framework performs entity alignment. This step assesses whether the predicted relationships are meaningful by searching for isomorphic graphs or recurring network motifs over time. By identifying stable structures, the team can confirm the persistence and significance of the discovered interaction patterns.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#limitations-and-future-perspectives",
    "href": "chapter_ai-nepi_019.html#limitations-and-future-perspectives",
    "title": "17  Modelling Context for Language Variation and Change",
    "section": "17.8 Limitations and Future Perspectives",
    "text": "17.8 Limitations and Future Perspectives\nThe authors conclude by outlining several critical limitations and forward-looking questions that define the future of computational conceptual history. A primary challenge is distinguishing genuine epistemic shifts from mere linguistic drift; can models truly trace the evolution of thought itself? Another open question concerns the role of context: how exactly do language models incorporate metadata, and should it be treated as a core signal of meaning or as external noise? The current project proceeds on the assumption that context is a core signal.\nFurthermore, the authors continue to debate the fundamental unit of language change. Are shifts best observed at the level of words, concepts, grammar, or broader discourse patterns? This leads to the question of whether it is possible to identify recurring linguistic pathways for the emergence of new concepts across different historical and scientific domains. Finally, as the models used to investigate these phenomena grow in complexity, the limits of their interpretability become a central concern, demanding new methods for understanding and validating their outputs.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context for Language Variation and Change</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview\nThe research team investigates the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the investigators reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]