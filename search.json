[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThe workshop, “Large Language Models for the History, Philosophy and Sociology of Science”, convened from 2-4 April 2025, at TU Berlin and online. Adrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert orchestrated the programme, which attracted considerable interest. Over 50 paper submissions were received, and approximately 220 participants registered for this three-day event. The European Research Council’s Network Epistemology in Practice (NEPI) grant (number 10104932) provided the essential funding.\nThe workshop’s conceptualisation arose from two distinct yet complementary initiatives. Firstly, within the NEPI project, Arno Simons pioneered the training of early large language models on physics texts, whilst Michael Zichert applied these models to analyse conceptual issues in physics. Secondly, Gerd Graßhoff, a long-standing collaborator, consistently advocated for integrating artificial intelligence into the history and philosophy of science, particularly for scrutinising scientific discovery processes. These converging interests culminated in a unified workshop, fostering broader discussion on AI-assisted methods.\nThe NEPI project itself meticulously investigates the internal communication dynamics of the Atlas collaboration at CERN, the renowned particle physics laboratory. The research team employs network analysis to map communication structures and utilises semantic tools, including large language models, to trace the flow of ideas within these complex networks. This endeavour aims to elucidate how large research collaborations collectively generate new knowledge.\nThe workshop featured two distinguished keynote speakers. Pierluigi Cassotti and Nina Tahmasebi, from the University of Gothenburg, presented their work on large-scale text analysis for cultural and societal change, focusing on semantic change detection and data science for the humanities. Iryna Gurevych, who heads the Ubiquitous Knowledge Processing Lab at Technical University Darmstadt, delivered a keynote on elevating Natural Language Processing to the cross-document level, covering information extraction, semantic text processing, machine learning, and NLP applications in the social sciences and humanities.\nLogistical arrangements for the workshop were comprehensive. The organisers recorded sessions with a camera focused on the presenter, four microphones, and an iPhone backup. They intend to upload videos of talks, including discussions, to the NEPI YouTube channel, subject to presenter consent. A structured Q&A protocol facilitated engagement, limiting questions to four per session to ensure efficiency. Furthermore, an Etherpad or Cryptpad provided an asynchronous platform for comments and questions, whilst the Zoom chat enabled real-time interaction. Ample networking opportunities were provided through scheduled lunch and coffee breaks, a modest reception, and a limited-seat workshop dinner.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-overview-and-participation",
    "href": "chapter_ai-nepi_001.html#workshop-overview-and-participation",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.1 Workshop Overview and Participation",
    "text": "2.1 Workshop Overview and Participation\n\n\n\nSlide 01\n\n\nAdrian Wüthrich, Gerd Graßhoff, Arno Simons, and Michael Zichert co-organised the workshop, “Large Language Models for the History, Philosophy and Sociology of Science”, held from 2-4 April 2025. This event welcomed participants both at TU Berlin’s Room H2005 and via a dedicated online platform.\nThe call for papers generated significant interest, attracting over 50 submissions. From this competitive pool, the organisers meticulously selected 16 papers for presentation during the workshop. Participation levels proved robust: in-person attendance quickly reached capacity, whilst a substantial online audience also registered. Overall, approximately 220 individuals enrolled for the workshop, with additional registrations continuing to arrive. Crucially, the organisers aimed to ensure that all individuals interested in these topics could actively participate in the discussions throughout the two-and-a-half-day programme.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-genesis-and-nepi-project-objectives",
    "href": "chapter_ai-nepi_001.html#workshop-genesis-and-nepi-project-objectives",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.2 Workshop Genesis and NEPI Project Objectives",
    "text": "2.2 Workshop Genesis and NEPI Project Objectives\n\n\n\nSlide 02\n\n\nThe workshop’s inception stemmed from two distinct yet complementary initiatives. Firstly, the Network Epistemology in Practice (NEPI) project provided a foundational impetus. Within this project, Arno Simons pioneered the training of one of the earliest large language models specifically on physics texts, aligning with the project’s core interests. Concurrently, Michael Zichert, also a member of the NEPI team, applied large language models to scrutinise conceptual issues prevalent in physics.\nSecondly, Gerd Graßhoff, a long-standing collaborator of Adrian Wüthrich, significantly contributed to the workshop’s genesis. Graßhoff has consistently championed the integration of artificial intelligence into the history and philosophy of science, particularly for analysing the intricate processes of scientific discovery. He independently conceived a workshop focused on novel AI-assisted methodologies for these disciplines. Recognising their shared objectives, the organisers subsequently decided to combine their efforts, culminating in the present workshop.\nThe European Research Council (ERC) grant, Network Epistemology in Practice (NEPI), bearing grant number 10104932, provides the funding for this endeavour. Within the NEPI project, the research team meticulously studies the internal communication of the Atlas collaboration at CERN, the prominent particle physics laboratory. This investigation aims to elucidate how one of the largest and most distinguished research collaborations collectively generates new knowledge. The team employs network analysis to discern the communication structures within this collaboration. Furthermore, they utilise semantic tools, including large language models, to trace the flow of ideas throughout these complex network structures. Indeed, the application of large language models represents a central interest for the project, with numerous other applications anticipated throughout the workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "href": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.3 Recording Protocols and Consent",
    "text": "2.3 Recording Protocols and Consent\n\n\n\nSlide 05\n\n\nThe organisers are currently recording the workshop sessions. Participants received prior notification of this during the registration process, thereby implying their consent. A single camera captures the presenter, ensuring focus remains on the speaker. For audio capture, four microphones are deployed, supplemented by an iPhone serving as a crucial backup recorder.\nFollowing the workshop, the organisers intend to upload videos of the talks, encompassing the accompanying discussions, to the NEPI YouTube Channel. This process, however, necessitates the explicit consent of each presenter. Crucially, whilst discussions are recorded, the audio and video capture solely focuses on the presenter, deliberately excluding the audience. Should any participant require additional information or wish to withdraw their consent, they are encouraged to approach the organisers. Ultimately, these recording efforts aim to establish a comprehensive record of the valuable discussions and presentations from this meeting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#engagement-protocols-and-interaction-channels",
    "href": "chapter_ai-nepi_001.html#engagement-protocols-and-interaction-channels",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.4 Engagement Protocols and Interaction Channels",
    "text": "2.4 Engagement Protocols and Interaction Channels\n\n\n\nSlide 07\n\n\nGiven the substantial number of participants and the constrained time allocated for presentations, the organisers have implemented a specific protocol for questions and comments. Participants are kindly requested to formulate their questions and comments concisely and directly. Following each presentation, the organisers will collect approximately four questions or comments, enabling the presenter to address them collectively, thereby optimising time and avoiding protracted back-and-forth exchanges. The organisers acknowledge that, despite the value of all inquiries, time limitations may preclude addressing every question in person.\nTo facilitate broader engagement beyond the live sessions, the organisers provide an Etherpad or Cryptpad. This platform allows participants to post comments or questions after sessions, offering presenters an opportunity to read and respond at their convenience. Consequently, this channel ensures continuous interaction even when sessions are not actively running. Furthermore, during live sessions, both online attendees and the in-person audience can utilise the Zoom chat feature to submit questions or comments at any time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#networking-opportunities-and-social-programme",
    "href": "chapter_ai-nepi_001.html#networking-opportunities-and-social-programme",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.5 Networking Opportunities and Social Programme",
    "text": "2.5 Networking Opportunities and Social Programme\n\n\n\nSlide 09\n\n\nBeyond the formal presentations, the workshop actively fosters informal networking amongst researchers and fellows. The programme incorporates ample lunch and coffee breaks, providing dedicated time for casual interaction. Additionally, a modest reception and a workshop dinner are scheduled, though seats for the dinner are strictly limited to confirmed participants.\nCoffee and refreshments are available on-site. For lunch and the reception, attendees will proceed to Room H2051, located down the hall and one floor below. The organisers will provide guidance to these locations, for instance, leading participants there after today’s final talk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.6 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change",
    "text": "2.6 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change\n\n\n\nSlide 11\n\n\nThe first keynote address, titled “Large-scale text analysis for the study of cultural and societal change”, featured Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi serves as the Principal Investigator for the “Change is Key” research programme in Gothenburg, whilst Pierluigi Cassotti contributes as a researcher within this project.\nThese scholars have garnered considerable recognition for their pioneering work in semantic change detection. Their contributions encompass not only technical advancements, such as the development of crucial benchmarks, but also broader methodological considerations concerning the application of data science methods to humanities questions. This dual expertise renders their work exceptionally pertinent to the workshop’s themes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "title": "Large Language Models for the History, Philosophy and Sociology of Science",
    "section": "2.7 Keynote 2: Elevating NLP to the Cross-Document Level",
    "text": "2.7 Keynote 2: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 12\n\n\nIryna Gurevych delivered the second keynote address, scheduled for the late afternoon of the following day, under the title “How to InterText? Elevating NLP to the cross-document level”. Gurevych heads the Ubiquitous Knowledge Processing (UKP) Lab at Technical University Darmstadt.\nHer extensive research primarily focuses on information extraction, semantic text processing, and machine learning. Crucially, her work also explores the practical applications of Natural Language Processing (NLP) within the social sciences and humanities. This specific area of expertise positions her contributions as an ideal complement to the workshop’s overarching objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "",
    "text": "Overview\nThis presentation systematically introduces the foundational architecture of large language models (LLMs), details their evolution and adaptation for scientific domains, and explores their burgeoning applications within the history, philosophy, and sociology of science (HPSS). Initially, the speaker, a co-organiser, provides a primer on the seminal Transformer architecture, explaining its encoder-decoder structure and its original purpose in language translation. Subsequently, the discussion differentiates between encoder-based models, such as BERT, which offer bidirectional full-context understanding, and decoder-based generative models, including GPT, capable of producing novel text.\nThe presentation then charts the proliferation of domain-specific LLMs across various scientific fields, outlining diverse adaptation strategies. These include pre-training, fine-tuning, and the sophisticated Retrieval Augmented Generation (RAG) pipeline. Crucially, the speaker categorises current LLM applications in HPSS, spanning data handling, knowledge structure analysis, and the study of knowledge dynamics and practices. Finally, the presentation offers critical reflections on HPSS-specific challenges, such as historical language evolution and sparse data, whilst advocating for enhanced LLM literacy and a steadfast adherence to HPSS methodologies. This approach highlights new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-foundational-transformer-architecture",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.1 The Foundational Transformer Architecture",
    "text": "3.1 The Foundational Transformer Architecture\n\n\n\nSlide 01\n\n\nThis presentation offers a foundational primer on large language models (LLMs), detailing their adaptation for scientific domains and summarising their current applications within the history, philosophy, and sociology of science (HPSS). Furthermore, it shares critical reflections intended to stimulate discussion throughout the workshop. Addressing a heterogeneous audience with diverse technical backgrounds, the speaker aims to ensure accessibility whilst maintaining scholarly rigour.\nAt the core of all contemporary large language models lies the seminal Transformer architecture, pioneered by Vaswani and colleagues in 2017. The engineers originally designed this model for language translation, facilitating conversions such as German to English. The architecture comprises two interconnected processing streams: an encoder on the left and a decoder on the right.\nThe encoder processes an entire input sentence concurrently. Within this stream, each word interacts bidirectionally with every other word, thereby constructing a comprehensive contextual representation of the complete sentence meaning. Conversely, the decoder generates output words sequentially. Crucially, whilst predicting the next word, the decoder can only access its predecessors, operating with a unidirectional context. Throughout both streams, internal layers progressively refine contextualised word embeddings, enhancing their semantic richness.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "href": "chapter_ai-nepi_003.html#differentiating-bert-and-gpt-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.2 Differentiating BERT and GPT Models",
    "text": "3.2 Differentiating BERT and GPT Models\n\n\n\nSlide 03\n\n\nImmediately following the Transformer’s introduction, researchers began re-engineering its individual streams to produce sophisticated pre-trained language models. This development ushered in a new domain of application, moving beyond mere translation to models capable of profound language understanding and generation, readily adaptable for various natural language processing (NLP) tasks with minimal additional training.\nFrom the encoder side, the BERT family of models emerged, standing for Bidirectional Encoder Representations from Transformers. BERT operates by allowing each word in the input stream to access the full context bidirectionally, thereby constructing a comprehensive understanding of the entire input at once. Conversely, the decoder side gave rise to the GPT models, or Generative Pre-trained Transformers, which now power applications like ChatGPT. These models, whilst constrained to accessing only their predecessors, possess the distinct capability to generate novel text, a function not inherently present in BERT-like models.\nConsequently, a fundamental distinction arises between these two model types: generative models, exemplified by GPT, primarily produce language, whereas full-context models, such as BERT, excel at coherently understanding sentences. Beyond these primary distinctions, engineers have also crafted models that combine encoder and decoder functionalities, or have devised advanced methods for utilising decoders in an encoder-like fashion, as seen in architectures like XLM and XLNet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "href": "chapter_ai-nepi_003.html#scientific-llm-evolution-and-adaptation-strategies",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.3 Scientific LLM Evolution and Adaptation Strategies",
    "text": "3.3 Scientific LLM Evolution and Adaptation Strategies\n\n\n\nSlide 06\n\n\nA comprehensive overview reveals the rapid evolution of large language models, particularly those tailored for specific science domains and tasks, spanning from 2018 to 2024. This landscape encompasses models categorised as Encoder-Decoder, Decoders, and Encoders, available as both open-source and closed-source solutions. Notably, encoder models, akin to BERT, exhibit a greater prevalence than their decoder counterparts. Early popular models in this scientific context included BioBERT, Specter, and SciBERT. Currently, a diverse array of domain-specific models serves fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and social science.\nResearchers employ several methods to adapt these models to specific scientific language. Pre-training constitutes the initial phase, where a model learns language by predicting the next token, as in GPT models, or by predicting randomly masked words, characteristic of BERT models. This process, however, demands immense computational resources and vast datasets, rendering full-scale pre-training impractical for many. Consequently, continued pre-training offers a viable alternative; researchers utilise an already pre-trained model, subsequently training it on domain-specific language, such as adapting a BERT model for physics texts.\nBeyond this, engineers can add extra layers atop pre-trained models, effectively training them for classification tasks like sentiment analysis or named entity recognition. Crucially, contrastive learning emerges as a pivotal method for generating sentence and document embeddings. Whilst word embeddings are readily available, the challenge lies in placing entire documents or sentences within the same embedding space. Contrastive learning addresses this, with Sentence BERT serving as a widely adopted example.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-and-key-distinctions",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.4 Retrieval Augmented Generation and Key Distinctions",
    "text": "3.4 Retrieval Augmented Generation and Key Distinctions\n\n\n\nSlide 10\n\n\nRetrieval Augmented Generation (RAG) represents a sophisticated pipeline system, fundamentally distinct from a singular large language model, as it orchestrates multiple models in concert. This architecture, for instance, underpins ChatGPT’s internet search capabilities. The process commences with a user query, such as “What are LLMs?”. Subsequently, a BERT-type model encodes this query into a sentence embedding. This embedding then facilitates a search within a comprehensive document database, identifying the most semantically similar passages. Finally, the RAG pipeline seamlessly integrates these retrieved sentences into the prompt of a generative model, which then formulates an answer based on this newly enriched context.\nBeyond RAG, advanced reasoning models and agents are emerging; these are not isolated LLMs but rather intricate systems that combine LLMs with a diverse array of other tools. Consequently, a clear understanding of key distinctions proves crucial for navigating the LLM landscape. These include the fundamental architectural differences, such as encoder-based, decoder-based, and encoder-decoder-based designs, alongside various fine-tuning strategies. Moreover, one must differentiate between word embeddings and sentence embeddings, as these represent fundamentally distinct levels of abstraction. Ultimately, discerning between standalone LLMs, complex pipelines like RAG, and sophisticated agents becomes paramount for effective application.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#applications-and-trends-in-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.5 Applications and Trends in HPSS Research",
    "text": "3.5 Applications and Trends in HPSS Research\n\n\n\nSlide 13\n\n\nA current survey explores the burgeoning uses of large language models as tools within history, philosophy, and sociology of science (HPSS) research. This investigation has identified four primary categories for sorting these applications:\n\nLLMs assist in dealing with data and sources, facilitating the parsing and extraction of information such as publication types, acknowledgements, and citations.\nThey contribute to analysing knowledge structures, enabling entity extraction for scientific instruments, celestial bodies, and chemicals, alongside mapping science policy discourses and interdisciplinary fields.\nLLMs illuminate knowledge dynamics, particularly through the study of conceptual histories of words.\nFinally, they support the analysis of knowledge practices, including citation context analysis—an older HPSS tradition now also employed for evaluatory purposes.\n\nA notable trend indicates an accelerating interest in LLMs, with findings predominantly appearing in information science journals like Scientometrics and Jasis. Increasingly, however, papers featuring LLM applications are emerging in journals traditionally less inclined towards computational methods. This expansion suggests that the semantic power of these models now attracts qualitative researchers and philosophers. Furthermore, the degree of customisation in LLM deployment varies widely, spanning from straightforward off-the-shelf use of ChatGPT to the development of entirely new model architectures.\nDespite this enthusiasm, several concerns recur. Researchers frequently cite overwhelming computational resource requirements, the inherent opaqueness of models, and persistent shortages of training data and benchmarks. Moreover, they grapple with trade-offs between different model types, acknowledging that no single model serves all purposes; rather, its adequacy depends entirely on the specific research objective. Nevertheless, a positive trend towards greater accessibility is evident, exemplified by BERTopic, a topic modelling tool gaining widespread adoption due to its user-friendliness and robust developer maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "href": "chapter_ai-nepi_003.html#critical-reflections-and-future-directions-for-hpss",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections",
    "section": "3.6 Critical Reflections and Future Directions for HPSS",
    "text": "3.6 Critical Reflections and Future Directions for HPSS\n\n\n\nSlide 12\n\n\nCrucially, scholars must acknowledge the specific challenges inherent to HPSS when engaging with large language models. Foremost amongst these is the historical evolution of concepts and language; models trained predominantly on modern language may exhibit inherent biases, necessitating either the training of custom models or the judicious use of existing ones with a keen awareness of their limitations. Furthermore, HPSS adopts a reconstructive and critically reflective perspective, reading between the lines of scientific texts to understand authorial context and subtle discursive strategies, such as boundary work. Current models are not inherently trained for such nuanced interpretation, demanding the development of methods that enable this distinctive HPSS “reading.” Practical data problems also persist, including sparse datasets, the prevalence of multiple languages, and the complexities of old scripts.\nConsequently, building robust LLM literacy becomes imperative. Researchers must thoroughly understand these tools, encompassing both their underlying theory and their practical implications. Whilst the necessity for extensive coding in natural language processing may diminish over time, a foundational understanding remains vital. This literacy prevents the superficial application of off-the-shelf tools, which, whilst producing visually appealing graphs, often yield no deeper insight.\nUltimately, HPSS researchers must remain true to their established methodologies. This involves translating complex HPSS problems into specific NLP tasks—such as classification, generation, or summarisation—without inadvertently compromising the original research purpose. Nevertheless, these advancements present novel opportunities for bridging qualitative and quantitative approaches within the discipline. Moreover, reflecting upon HPSS’s own history and the pre-history of these models, including pioneering efforts like co-word analysis developed by figures such as Callon and Rip in the 1980s, offers valuable theoretical grounding, particularly given the resonance of Actor-Network Theory (ANT) concepts with contemporary LLM developments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Neuchel, alongside Andrea Loetgers and Taya Knuuttila, unveiled OpenAlex Mapper, a novel tool designed to facilitate transdisciplinary investigations within the history, philosophy, and sociology of science (HPSS). The team, comprising researchers from Utrecht University and the University of Vienna, developed this instrument, funded by an ERC grant on ‘possible life’. Their presentation meticulously detailed the tool’s technical architecture, its operational workflow, and its diverse applications in scholarly analysis.\nOpenAlex Mapper addresses the inherent challenges of generalising findings from small samples and case studies prevalent in HPSS research. It leverages a fine-tuned Specter 2 language model, specifically adapted to discern disciplinary boundaries, to process a vast dataset of 300,000 randomly sampled English-language abstracts from the OpenAlex database. Subsequently, the system employs Uniform Manifold Approximation and Projection (UMAP) to reduce these high-dimensional embeddings into a two-dimensional, interactive base map. Users can submit arbitrary queries to OpenAlex; the tool then embeds the retrieved abstracts and projects them onto this pre-existing map, revealing their disciplinary locations.\nCrucially, OpenAlex Mapper supports qualitative, heuristic investigations by grounding them in extensive quantitative data, whilst always linking back to original textual sources. Demonstrations showcased its utility in mapping the distribution of model templates, such as the Ising, Hopfield, and Sherrington-Kirkpatrick models, across scientific fields. Furthermore, the tool effectively visualises the spread of key concepts, exemplified by ‘phase transition’ and ‘emergence’, and analyses the interdisciplinary adoption patterns of specific methods, including ‘Random Forest’ and ‘Logistic Regression’. The developers acknowledged certain limitations, notably OpenAlex’s data quality, the current English-only language model, the requirement for abstracts or robust titles, and the inherent stochasticity and dimensionality reduction trade-offs of the UMAP algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-introduction-and-technical-foundations",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-introduction-and-technical-foundations",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper: Introduction and Technical Foundations",
    "text": "4.1 OpenAlex Mapper: Introduction and Technical Foundations\n\n\n\nSlide 01\n\n\nMaximilian Neuchel, Andrea Loetgers, and Taya Knuuttila introduced OpenAlex Mapper, a sophisticated tool they developed through collaborative research. Neuchel, a PhD candidate at Utrecht University’s Theoretical Philosophy Department, collaborated with Loetgers and Knuuttila from the University of Vienna’s Philosophy Department. An ERC grant, specifically supporting research on ‘possible life’, funded this endeavour.\nTheir presentation systematically unfolded, first elucidating the tool’s core functions and high-level technical specifications. Subsequently, a live demonstration illustrated its practical application, culminating in a detailed discussion of its utility within the History, Philosophy, and Sociology of Science (HPSS) domain.\nA meticulously fine-tuned Specter 2 language model forms the heart of OpenAlex Mapper’s functionality. The team adapted this model, training it on a dataset of articles from closely related fields to enhance its capacity for discerning disciplinary boundaries and improve distinction. UMAP dimensionality reduction visually represents this training process. Crucially, these modifications represent minor adjustments to the language model, rather than a comprehensive retraining effort.\nFor base-map preparation, the team drew upon the extensive OpenAlex database. This resource, renowned for its size and inclusivity, surpasses proprietary databases such as Web of Science and Scopus. Its open data policy facilitates easy, batch-query access, making it an invaluable asset for large-scale scholarly analysis. From this vast repository, the authors sampled 300,000 random articles, selecting only those with well-formed English abstracts. The fine-tuned Specter 2 model then processed these abstracts, generating high-dimensional embeddings.\nTo render these complex embeddings visually interpretable, the system employs Uniform Manifold Approximation and Projection (UMAP). This dimensionality reduction algorithm transforms the high-dimensional data into a two-dimensional base map, simultaneously producing a trained UMAP model. Consequently, when users submit arbitrary queries through the OpenAlex search interface, the tool downloads the initial 1,000 records, embeds their abstracts using the same fine-tuned language model, and then projects these new embeddings onto the pre-existing 2D map via the trained UMAP model. This inherent feature of UMAP ensures consistent and accurate positioning of new documents within the established disciplinary landscape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-problem-addressed",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-and-core-problem-addressed",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration and Core Problem Addressed",
    "text": "4.2 Interactive Demonstration and Core Problem Addressed\n\n\n\nSlide 08\n\n\nOpenAlex Mapper, accessible online at https://m7n-openalex-mapper.hf.space, offers an interactive platform for scholarly exploration. An alternative version, featuring a higher latency GPU setup, also accommodates larger, more demanding queries. Users initiate their investigations by inputting search terms, such as ‘scale-free network models’ or ‘coriander’, leveraging the comprehensive capabilities of the OpenAlex search interface.\nBehind the scenes, the system efficiently downloads the initial 1,000 records corresponding to the user’s query, subsequently embedding their abstracts. Should the user enable the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as an interactive projection of these search results onto a grey base map, visually indicating where specific terms, authors, or concepts appear across various disciplines. This interactive functionality empowers users to delve deeper, for instance, by investigating the presence of ‘coriander’ in epidemiology or public health literature. Moreover, the tool offers visualisations of temporal distributions and the overlay of citation graphs, providing multifaceted insights.\nFundamentally, OpenAlex Mapper addresses a critical challenge in the History, Philosophy, and Sociology of Science: the generalisation and validation of findings derived from small samples and case studies. It aims to answer nuanced questions regarding the adoption and prevalence of specific models, such as ‘Where did the Hopfield model truly establish itself?’. By employing rigorous quantitative methods, the tool provides a robust foundation for qualitative, heuristic investigations. Crucially, it maintains a direct link to the actual textual sources, ensuring that all findings remain traceable and verifiable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "href": "chapter_ai-nepi_004.html#applications-in-scholarly-analysis",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.3 Applications in Scholarly Analysis",
    "text": "4.3 Applications in Scholarly Analysis\n\n\n\nSlide 17\n\n\nThe development of OpenAlex Mapper represents an ongoing research endeavour, with several compelling applications already emerging. Initially, Neuchel, Loetgers, and Knuuttila conceived the tool specifically for investigating model templates. This concept explores how models possessing similar structural properties manifest across disparate scientific disciplines, potentially imposing a structure on science that operates orthogonally to traditional disciplinary boundaries. For instance, the system has mapped the distribution of the Ising model (7,819 instances), the Hopfield model (589 instances), and the Sherrington-Kirkpatrick model (1,437 instances), revealing their presence in distinct, non-continuous regions across the base map. This work draws upon foundational ideas from Humphreys (2004) and Knuuttila and Loettgers (2023).\nBeyond model templates, the tool effectively visualises the distribution of key concepts. A notable example contrasts the concept of ‘phase transition’ with ‘emergence’, depicted in orange on the map. This capability proves particularly advantageous for broadening conceptual analysis into interdisciplinary contexts, circumventing the common difficulties associated with acquiring specific, cross-disciplinary datasets. Relevant scholarship in this area includes contributions from Malaterre, Chartier, and Lareau (2020), and Zichert and Wüthrich (2024).\nFurthermore, OpenAlex Mapper facilitates the analysis of method distribution across scientific fields. Examining the usage patterns of ‘Random Forest’ (2,000 instances) versus ‘Logistic Regression’ (1,997 instances) demonstrates clearly distinguishable patterns of adoption within interdisciplinary research. This observation prompts profound philosophical questions, such as why neuroscientists might favour Random Forest algorithms whilst researchers in psychiatry or mental health predominantly employ Logistic Regression. This application engages with discussions from Breiman (2001), Bzdok, Altman, and Krzywinski (2018), and Andrews (2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-directions",
    "title": "4  Philosophy at Scale: Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Directions",
    "text": "4.4 Limitations and Future Directions\n\n\n\nSlide 21\n\n\nDespite its considerable utility, OpenAlex Mapper operates with several acknowledged limitations. Foremost amongst these is the inherent quality of the OpenAlex database itself. Whilst not flawless, its data quality remains commendably reasonable when compared to alternative scholarly data sources.\nA significant constraint currently stems from the language model’s scope: it processes English-language content exclusively. This naturally limits the tool’s overall reach; nevertheless, for investigations focusing on the more recent history of science, this presents a less severe impediment. The developers recognise the potential for future enhancement through the integration of multilingual models, though they note the current scarcity of robust, science-trained multilingual models.\nFurthermore, the embedding process necessitates source data that includes either comprehensive abstracts or sufficiently descriptive titles. This requirement inherently restricts the range of documents the tool can effectively analyse.\nFinally, the Uniform Manifold Approximation and Projection (UMAP) algorithm, central to the tool’s visualisation, introduces its own set of challenges. As a stochastic algorithm, UMAP generates one specific output from a multitude of possibilities, implying that alternative visualisations could emerge from repeated runs. Moreover, the algorithm must inevitably make trade-offs during dimensionality reduction. Compressing the Specter language model’s 768 dimensions into a mere two introduces unavoidable distortions, manifesting as ‘pushing, pulling, and misaligning’ of data points within the two-dimensional space.\nFor those seeking further information, the presentation slides remain accessible online at maxnoichl.eu/talk. Additionally, a comprehensive working paper, titled ‘Philosophy at Scale: Introducing OpenAlex Mapper’, provides more detailed technical explanations of the system’s architecture and methodology.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Philosophy at Scale: Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThe research team at Uppsala University has undertaken a procedural investigation into genre classification for historical medical periodicals, a core component of the ERC-funded ActDisease project. This initiative meticulously examines the history of patient organisations in 20th-century Europe, focusing on their influence on disease concepts, illness experiences, and medical practices. The project primarily utilises a private, recently digitised collection of patient organisation magazines from Sweden, Germany, France, and Great Britain, encompassing 96,186 pages published between approximately 1890 and 1990.\nEmploying ABBYY FineReader Server 14, the digitisation efforts successfully processed most common layouts; nevertheless, complex layouts, slanted text, and rare fonts posed persistent challenges, consequently leading to OCR errors, particularly in German and French texts. Recognising the diverse and co-occurring text types within these periodicals, the team identified genre classification as a crucial methodological advancement. This approach addresses the limitations and potential biases of traditional topic models and term counts, which fail to account for the varied communicative purposes embedded within single pages.\nTo address the scarcity of annotated data, the project explored both zero-shot and few-shot learning paradigms. The researchers defined a bespoke set of nine genre labels—Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and QA—under the supervision of a specialist historian. Six project members undertook the annotation, achieving a high inter-annotator agreement of 0.95 Krippendorff’s alpha on paragraphs from Swedish and German periodicals.\nFor zero-shot experiments, the team leveraged publicly available datasets, including the Corpus of Online Registers of English (CORE), Functional Text Dimensions (FTD), and UD-MULTIGENRE (UDM), performing a rigorous cross-dataset genre mapping. The team fine-tuned multilingual encoders, specifically XLM-Roberta, mBERT, and historical mBERT, across 48 configurations. The findings indicated that models fine-tuned on FTD performed optimally with the custom mapping, whilst historical mBERT demonstrated particular efficacy in distinguishing between fiction and nonfiction prose in few-shot settings.\nFurthermore, the project investigated few-shot prompting with Llama 3.1 8b Instruct, revealing its capacity to handle certain genre labels effectively, though a limited number of examples proved insufficient for comprehensive representation across all categories. Ultimately, the research concludes that genre classification significantly enhances the accessibility of historical periodical sources for text mining, with few-shot learning of multilingual encoders, particularly historical mBERT with prior MLM fine-tuning, offering the most robust performance. Ongoing work encompasses developing a more fine-grained annotation scheme, generating synthetic data, and implementing active learning strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-historical-inquiry",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 The ActDisease Project: Historical Inquiry",
    "text": "5.1 The ActDisease Project: Historical Inquiry\n\n\n\nSlide 01\n\n\nThe research team has embarked upon the ActDisease project, formally titled “Acting out Disease: How Patient Organizations Shaped Modern Medicine.” This ERC-funded initiative meticulously investigates the historical trajectory of patient organisations across Europe during the 20th century. Its central purpose involves scrutinising how these organisations fundamentally influenced the evolution of disease concepts, the lived experience of illness, and prevailing medical practices.\nThe project’s scope encompasses ten distinct European patient organisations, drawing its primary source material from their periodicals—predominantly magazines—published in England, Germany, France, and Great Britain between approximately 1890 and 1990. Notably, the Hay Fever Association of Heligoland, established in 1897, exemplifies the type of historical entity under examination; Heligoland, Germany, served as its foundational site.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-dataset-collection",
    "href": "chapter_ai-nepi_005.html#the-actdisease-dataset-collection",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Dataset: Collection",
    "text": "5.2 The ActDisease Dataset: Collection\n\n\n\nSlide 06\n\n\nThe ActDisease project has assembled a private, recently digitised collection of patient organisation magazines, constituting a substantial dataset of 96,186 pages. This extensive archive spans various countries, diseases, and publication periods.\nSpecifically, the German collection comprises 10,926 pages on Allergy/Asthma from two magazines (1901-1985), 19,324 pages on Diabetes from one magazine (1931-1990), and 5,646 pages on Multiple Sclerosis from one magazine (1954-1990). Swedish materials include 4,054 pages on Allergy/Asthma (1957-1990), 7,150 pages on Diabetes (1949-1990), and 16,790 pages on Lung Diseases (1938-1991), each from a single magazine. French contributions encompass 6,206 pages on Diabetes (1947-1990) and 9,317 pages on Rheumatism/Paralysis from three magazines (1935-1990). Finally, the UK segment features 11,127 pages on Diabetes (1935-1990) and 5,646 pages on Rheumatism (1950-1990), each sourced from one magazine.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-processes-and-challenges",
    "href": "chapter_ai-nepi_005.html#digitisation-processes-and-challenges",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Digitisation Processes and Challenges",
    "text": "5.3 Digitisation Processes and Challenges\n\n\n\nSlide 07\n\n\nThe digitisation process for the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition (OCR). This tool generally performed well, accurately recognising most common layouts and fonts present in the historical periodicals.\nNevertheless, significant challenges persisted. Complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality frequently hindered optimal OCR performance. Consequently, the team observed persistent OCR errors, particularly prevalent in German and French texts, alongside instances of disrupted reading order. Notably, creative text segments, including advertisements, humour pages, and poems, exhibited a higher frequency of OCR inaccuracies. To mitigate these issues, the team conducted specific experiments, focusing on post-OCR correction of German texts through the application of instruction-tuned generative models. Danilova and Aangenendt documented this work in a publication presented at the RESOURCEFUL-2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Rationale for Genre Classification",
    "text": "5.4 Rationale for Genre Classification\n\n\n\nSlide 11\n\n\nThe research team observed a profound diversity in the textual content of the historical periodicals, yet these varied text types consistently appeared across all magazines. Crucially, distinct text types frequently co-occurred on a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This inherent textual complexity posed a significant challenge for conventional analytical methods.\nTraditional yearly and decade-based topic models and term counts, the team realised, failed to account for this side-by-side textual variation. Consequently, these methods likely introduced a bias towards the most frequently occurring text type, potentially distorting analytical outcomes. To overcome this limitation, genre emerged as a highly pertinent concept for distinguishing between text types, particularly as genres inherently align with the communicative purposes of authors, as Petrenz (2004) and Kessler (1997) define in language technology.\nImplementing genre classification directly supports the project’s core objective: exploring the dataset from multiple perspectives to formulate robust historical arguments. Specifically, this approach enables a nuanced study of communicative strategies as they evolved over time, allowing for comparisons across different countries, diseases, and publications, a point Broersma (2010) highlights. Furthermore, it facilitates a more granular analysis of term distributions and topic models, enabling insights within specific genre groups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Illustrative Genre Examples",
    "text": "5.5 Illustrative Genre Examples\n\n\n\nSlide 19\n\n\nThe ActDisease dataset encompasses a rich array of genres, each serving distinct communicative functions. The research team identified examples such as poetry, which often provided emotional engagement. Academic reports, exemplified by studies on the pancreas, conveyed scientific and medical information. Legal documents, including deeds of covenant, established formal agreements and regulations. Advertisements, such as those promoting chocolate for diabetics, aimed to market products or services.\nFurthermore, the collection featured instructive or guidance messages, offering practical advice like recipes, doctor’s recommendations, or dietary guidelines. Patient organisation reports documented internal affairs, detailing meetings and activities. Finally, narratives about patient lives provided first-hand accounts of illness experiences, offering a unique perspective on the human dimension of disease.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#experimental-design-limited-data",
    "href": "chapter_ai-nepi_005.html#experimental-design-limited-data",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Experimental Design: Limited Data",
    "text": "5.6 Experimental Design: Limited Data\n\n\n\nSlide 04\n\n\nConfronted with a scarcity of annotated data, the research team systematically explored two distinct methodological paradigms: zero-shot learning and few-shot learning. For zero-shot learning, the investigation posed two primary research questions: firstly, whether an efficient mapping of genre labels from existing public datasets to custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might fluctuate across different datasets and models.\nConversely, the few-shot learning inquiry focused on two further questions: how performance changes in relation to varying training set sizes across different models; and whether a prior fine-tuning process on the entire dataset could significantly boost classification performance. Danilova and Söderfeldt’s comprehensive experimental design forms the basis of a forthcoming publication, scheduled for presentation at the LaTeCH-CLFL 2025 workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Defining Genre Labels",
    "text": "5.7 Defining Genre Labels\n\n\n\nSlide 32\n\n\nThe research team meticulously defined the genre labels under the direct supervision of the project’s lead historian, an expert in patient organisations. This collaborative process aimed to create categories that would effectively segment the content within the historical periodicals, thereby facilitating deeper historical analysis. Crucially, the team endeavoured to formulate these labels with sufficient generality to enable the classifier’s application to comparable datasets in the future.\nNine distinct genres emerged from this process, each with a precise definition:\n\nAcademic: Encompasses research-based reports or scientific explanations, designed to bridge the gap between the scientific medical community and the magazine’s readership.\nAdministrative: Documents organisational activities, reporting on patient organisation events and internal affairs.\nAdvertisement: Specifically promotes commercial products or services.\nGuide: Provides step-by-step instructions, ranging from health tips to recipes.\nFiction: Aims to entertain and emotionally engage through stories, poems, or humour.\nLegal: Explains terms, conditions, or contracts.\nNews: Reports on recent events.\nNonfiction Prose: Narrates real events or describes cultural and historical topics, including memoirs and essays.\nQA (Question and Answer): Designates sections structured as questions with expert responses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-protocol-and-agreement",
    "href": "chapter_ai-nepi_005.html#annotation-protocol-and-agreement",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Annotation Protocol and Agreement",
    "text": "5.8 Annotation Protocol and Agreement\n\n\n\nSlide 37\n\n\nThe annotation process employed paragraphs as the fundamental unit, extracting them from the ABBYY OCR output. The team merged these paragraphs based on consistent font patterns—including type, size, bolding, and italicisation—within each page. The research team sampled content from two specific periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” focusing on their first and mid-year issues across all publication years.\nSix dedicated project members undertook the annotation task, comprising four historians and two computational linguists. All annotators possessed either native fluency or high proficiency in both Swedish and German. For each paragraph, the team meticulously collected two independent annotations. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha, signifying a remarkably high level of consistency amongst the annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-and-configurations",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-and-configurations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Dataset Splitting and Configurations",
    "text": "5.9 Dataset Splitting and Configurations\n\n\n\nSlide 39\n\n\nFor the experimental phase, the research team initially partitioned the annotated data into a primary training set of 1182 paragraphs and a held-out set comprising 552 paragraphs, which represented approximately 30% of the total annotated material. The team stratified both these sets by genre label to ensure representative distributions.\nWithin the few-shot experiments, the team systematically varied the training set size, employing six distinct configurations: 100, 200, 300, 400, 500, and the full 1182 paragraphs. The team randomly sampled each of these smaller training sets from the main training pool, whilst maintaining a balance across genre labels. The team subsequently divided the held-out set equally into validation and test portions, similarly balancing them by label. Notably, the team excluded the Legal and News genres from these few-shot experiments, as their limited data volume precluded sufficient training. Conversely, the zero-shot experiments leveraged the entirety of the held-out test set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease",
    "href": "chapter_ai-nepi_005.html#genre-distribution-within-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Genre Distribution within ActDisease",
    "text": "5.10 Genre Distribution within ActDisease\n\n\n\nSlide 39\n\n\nAnalysis of the genre distribution across both the training and held-out samples of the ActDisease dataset revealed a pronounced imbalance. Specifically, the Advertisement and Non-fictional Prose genres exhibited significant disparities in their representation across different languages. This imbalance necessitates careful consideration during model training and evaluation to prevent potential biases.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#leveraging-external-datasets",
    "href": "chapter_ai-nepi_005.html#leveraging-external-datasets",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Leveraging External Datasets",
    "text": "5.11 Leveraging External Datasets\n\n\n\nSlide 39\n\n\nFor the zero-shot experiments, the research team incorporated external, modern datasets previously utilised in automatic web genre classification. The Corpus of Online Registers of English (CORE), developed by Egbert et al. (2015), provided document-level annotations, encompassing English, with main categories also available in Swedish, Finnish, and French.\nSimilarly, Sharoff’s (2018) Functional Text Dimensions (FTD) dataset, also annotated at the document level, offered balanced content in English and Russian. Kuzman et al. (2023) had previously leveraged this dataset for web genre classification. Additionally, the team employed UD-MULTIGENRE (UDM), a subset of Universal Dependencies (de Marneffe et al., 2021), which features recovered genre annotations at the sentence level across 38 languages, as detailed by Danilova and Stymne (2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Cross-Dataset Genre Mapping",
    "text": "5.12 Cross-Dataset Genre Mapping\n\n\n\nSlide 39\n\n\nThe research team meticulously performed genre mapping across datasets, with two independent annotators undertaking the task. Only assignments achieving full agreement proceeded to the final mapping, ensuring robust alignment.\nThis process established correspondences between ActDisease genres and their equivalents in CORE, UDM, and FTD. For instance, “Academic” in ActDisease mapped to “research article” (RA) in CORE, “academic” in UDM, and “academic (A14)” in FTD. “Advertisement” aligned with “advertisement (AD)” in CORE, “description with intent to sell (DS)” in UDM, and “commercial (A12)” in FTD. Similarly, “Fiction” found its counterparts in “poem” (PO) and “short story” (SS) in CORE, “fiction” in UDM, and “fictive (A4)” and “poetic (A19)” in FTD. However, the team encountered a limitation: some ActDisease genres lacked suitable corresponding labels within the available external datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-and-encoder-models",
    "href": "chapter_ai-nepi_005.html#training-data-and-encoder-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Training Data and Encoder Models",
    "text": "5.13 Training Data and Encoder Models\n\n\n\nSlide 39\n\n\nThe training data generation process involved a meticulous pipeline encompassing mapping, preprocessing, chunking, and systematic sampling. The research team configured training sets in four distinct ways for each dataset: one configuration ([G+]) focused exclusively on Germanic languages; another ([B1]) balanced data according to ActDisease labels; a third ([G-]) incorporated all language families; and the final configuration ([B2]) balanced data by both ActDisease and original labels. This yielded four FTD, four CORE, four UDM, and four merged training samples, all of which the team subjected to fine-tuning.\nFor classification, the team employed several multilingual encoder models. XLM-Roberta, developed by Conneau et al. (2020), served as a state-of-the-art web genre classifier, as noted by Kuzman et al. (2023). mBERT (Devlin et al., 2019) provided a baseline for comparison with historical mBERT (Schweter et al., 2022). Crucially, historical mBERT, pretrained on an extensive corpus of multilingual historical newspapers, proved particularly relevant given its inclusion of the target languages. These BERT-like models have consistently demonstrated efficacy in prior web register and genre classification studies, as evidenced by Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). Ultimately, the fine-tuning process generated 48 distinct models, and the team subsequently averaged their performance metrics across all configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Zero-Shot Learning Evaluation",
    "text": "5.14 Zero-Shot Learning Evaluation\n\n\n\nSlide 11\n\n\nEvaluating the zero-shot predictions presented a unique challenge: the imperfect overlap of label sets precluded direct comparison of overall performance metrics. Consequently, the research team meticulously assessed the performance of each genre individually, complementing this analysis with a thorough examination of confusion matrices to mitigate potential biases. The X-GENRE web genre classifier, as detailed by Kuzman et al. (2023), served as a robust baseline, with predictions focusing exclusively on the most similar labels directly mappable to the ActDisease genres.\nThe experimental setup often involved a cross-lingual context; FTD and X-GENRE, for instance, operated without German or Swedish data, whilst UDM and CORE datasets exhibited partially cross-lingual characteristics. Overall, models fine-tuned on FTD consistently demonstrated superior performance when integrated with the ActDisease mapping. Conversely, other datasets revealed distinct class-specific biases. UDM, for example, exhibited a bias towards news, primarily because its news training data contained the highest proportion of Germanic instances, overwhelmingly German. Similarly, CORE displayed a bias towards the guide genre, as its training data for this category was uniquely multilingual.\nIntriguingly, certain models excelled in specific genre predictions. XLM-Roberta, when fine-tuned on UDM, achieved an average of 32% more correct predictions in the QA genre compared to mBERT and hmBERT. Conversely, hmBERT, also on UDM, produced an average of 16% more accurate predictions in the Administrative genre than XLM-Roberta and mBERT. Furthermore, CORE-based models consistently proved proficient at predicting the legal genre. Confusion matrices visually underscored these observed behavioural patterns, whilst detailed average per-category F1 scores provided a comprehensive quantitative assessment across all data configurations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 Few-Shot Learning Performance",
    "text": "5.15 Few-Shot Learning Performance\n\n\n\nSlide 39\n\n\nFew-shot learning experiments unequivocally demonstrated the advantage of further training models on the ActDisease dataset, particularly when incorporating Masked Language Model (MLM) fine-tuning. The F1 score consistently improved as the number of training instances increased, though it remained below 0.8 even with the full training set of 1182 instances.\nAmongst the models tested, hmBERT-MLM consistently outperformed its counterparts. A detailed examination of its performance revealed that, unlike other models, hmBERT-MLM retained its capacity to differentiate between fiction and nonfiction genres even when exposed to the full dataset. Conversely, other models, notably XLM-Roberta, exhibited a drastic decline in their ability to distinguish these two categories. Analysis of XLM-Roberta’s confusion matrix, when fine-tuned with MLM on the full dataset, indicated a frequent overprediction of nonfiction prose for fiction. This phenomenon likely stems from the shared thematic content within the ActDisease data, where both fictional and autobiographical narratives often revolve around patient experiences, leading to similar themes and narrative structures. Consequently, the research team proposes that an increased volume of data is essential to enhance performance in distinguishing these increasingly similar genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Few-Shot Prompting with Llama-3.1",
    "text": "5.16 Few-Shot Prompting with Llama-3.1\n\n\n\nSlide 39\n\n\nGiven the current insufficiency of data for comprehensive instruction tuning, the research team opted to evaluate few-shot prompting using Llama 3.1 8b Instruct, a widely recognised multilingual generative model with open weights. The prompt structure provided clear genre definitions, complemented by two or three illustrative examples for each category.\nThe results indicated that the model handled certain genre labels with reasonable efficacy. For instance, it achieved an F1-score of 0.84 for Legal content and 0.72 for Academic texts. However, the limited number of examples proved insufficient for robust representation across all genres. Notably, nonfictional prose yielded a lower F1-score of 0.49, whilst advertisement and administrative content also demonstrated suboptimal performance, with F1-scores of 0.73 and 0.60 respectively. The overall accuracy stood at 0.62, with a macro average F1-score of 0.59 and a weighted average of 0.63.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#key-findings-and-recommendations",
    "href": "chapter_ai-nepi_005.html#key-findings-and-recommendations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Key Findings and Recommendations",
    "text": "5.17 Key Findings and Recommendations\n\n\n\nSlide 39\n\n\nPopular magazines, unlike more specialised scientific journals or books, frequently encompass a multitude of genres, a characteristic that significantly complicates text mining efforts. The research team has concluded that genres inherently reflect deliberate choices in communicative strategies; consequently, accounting for these distinctions, whilst challenging, proves crucial for achieving accurate and detailed interpretations of text mining outcomes. Fundamentally, genre classification renders these rich historical sources accessible for advanced text mining.\nFor scenarios lacking dedicated training data, two viable strategies emerge. Firstly, one can successfully leverage existing modern datasets, provided the target categories maintain a general purpose. Alternatively, few-shot instruction of a proficient generative model offers another effective pathway. However, when some training data is available, few-shot learning of multilingual encoders, particularly those with prior Masked Language Model (MLM) fine-tuning—such as XLM-Roberta or historical multilingual BERT—demonstrates superior performance. Indeed, this latter approach emerged as the optimal solution for the project. Notably, historical multilingual BERT exhibited particularly strong gains, achieving a 24% improvement, which significantly surpassed the 14.5% gain for mBERT-MLM and 16.9% for XLM-RoBERTa.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#ongoing-and-future-research",
    "href": "chapter_ai-nepi_005.html#ongoing-and-future-research",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Ongoing and Future Research",
    "text": "5.18 Ongoing and Future Research\n\n\n\nSlide 39\n\n\nThe research team is actively pursuing several avenues to enhance the quality and scope of this work for both the project and the wider academic community. Currently, the team is engaging with specific historical hypotheses, leveraging the insights gained from genre classification. Furthermore, they are developing a new, more fine-grained annotation scheme for genres, a project notably financed by Swe-CLARIN. Methodologically, the team is exploring advanced techniques, including synthetic data generation and active learning, to further refine their classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#acknowledgements",
    "href": "chapter_ai-nepi_005.html#acknowledgements",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Acknowledgements",
    "text": "5.19 Acknowledgements\n\n\n\nSlide 19\n\n\nThe project gratefully acknowledges the invaluable contributions of its annotators and core team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. The European Research Council generously provided funding for this research under grant ERC-2021-STG 10104099. The Centre for Digital Humanities and Social Sciences offered crucial institutional support, supplying essential GPUs and data storage facilities. Finally, the team extends its gratitude to the diligent reviewers, Dr Maria Skeppstedt and other anonymous contributors, whose feedback significantly enhanced the work. Further details are available on the project website.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "Overview\nProfessor Cornelis J. Schilt and his team at the Vrije Universiteit Brussel have embarked upon the VERITRACE project, a five-year ERC Starting Grant initiative spanning 2023 to 2028. This ambitious undertaking endeavours to delineate the profound influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition on the development of natural philosophy and science. A dedicated team of five, comprising Professor Schilt as Principal Investigator, Dr. Eszter Kovács, Dr. Jeffrey Wolf, Niccolò Cantoni, and Demetrios Paraschos, drives this interdisciplinary effort, seamlessly combining historical scholarship with advanced digital humanities methodologies.\nThe project pinpoints foundational texts embodying this tradition, such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence already confirms the tradition’s significant impact; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum. Beyond these well-known examples, the VERITRACE team aims to uncover a broader, often overlooked network of texts and authors, collectively termed the ‘great unread’.\nTo achieve this, the project pioneers a computational approach to the history and philosophy of science (HPSS). This involves large-scale multilingual exploration, employing keyword search and sophisticated text matching to identify both direct lexical reuse and indirect semantic influence across a vast corpus. In essence, the system functions as an early modern plagiarism detector, meticulously designed to reveal hidden networks of texts, passages, themes, topics, and authors, thereby potentially uncovering novel patterns in intellectual history.\nThe VERITRACE team has assembled a substantial dataset comprising approximately 430,000 printed texts, spanning nearly two centuries from 1540 to 1728. These digital texts originate from three primary multilingual sources—Early English Books Online (EEBO), Gallica (from the French National Library), and the Bavarian State Library—encompassing at least six languages. The researchers apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to analyse this extensive corpus.\nSignificant challenges confront the team, notably the variable quality of Optical Character Recognition (OCR) text provided directly by libraries in raw formats (XML, HOCR, HTML), often without corresponding page images. Early modern typography and the evolving semantics across multiple languages further complicate processing. Moreover, the sheer volume of data, hundreds of thousands of texts printed across Europe, presents a formidable computational hurdle.\nThe project strategically leverages Large Language Models (LLMs) in two distinct capacities. GPT-based LLMs serve as ‘judges’ for enriching and cleaning metadata, whilst BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages, facilitating advanced text matching. An alpha version of the VERITRACE web application currently serves as an internal testing ground, showcasing the project’s ambitious capabilities. This application features a complex 15-stage data processing pipeline, transforming raw text into an Elasticsearch database. It offers functionalities for exploring corpus statistics, examining rich metadata, conducting advanced keyword searches, and, crucially, performing lexical and semantic text matching. Whilst the current BERT-based embedding model (LaBSE) demonstrates promise for semantic comparisons, particularly across languages, its limitations with historical and out-of-domain data necessitate further investigation into alternative models or fine-tuning strategies. Scaling the system to accommodate the entire corpus and managing the evolving semantics of historical languages remain key issues for future development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-overview-and-team-composition",
    "href": "chapter_ai-nepi_006.html#project-overview-and-team-composition",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 Project Overview and Team Composition",
    "text": "6.1 Project Overview and Team Composition\n\n\n\nSlide 02\n\n\nThe VERITRACE project, formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,” constitutes a five-year ERC Starting Grant initiative, active from 2023 to 2028. This significant undertaking, identified by grant number 101076836, operates from the Vrije Universiteit Brussel (VUB).\nProfessor Cornelis J. Schilt serves as the project’s Principal Investigator, leading a dedicated team of five scholars. The core team comprises Dr. Eszter Kovács, Niccolò Cantoni, and Demetrios Paraschos, alongside Dr. Jeffrey Wolf. Dr. Wolf, a historian specialising in science and medicine with an eighteenth-century focus, specifically contributes his expertise in digital humanities to the project. Whilst the team bases its operations in Brussels, individual members maintain residences in various locations, including Berlin. Further information regarding the project’s scope and progress is available on its official website, veritrace.eu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#tracing-ancient-wisdom-in-early-modern-philosophy",
    "href": "chapter_ai-nepi_006.html#tracing-ancient-wisdom-in-early-modern-philosophy",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 Tracing Ancient Wisdom in Early Modern Philosophy",
    "text": "6.2 Tracing Ancient Wisdom in Early Modern Philosophy\n\n\n\nSlide 01\n\n\nAt its core, the VERITRACE project endeavours to delineate the profound influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia, upon the evolution of natural philosophy and science during the early modern period. This tradition manifests in various foundational works, including the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most notably for scholars of chemistry’s history, the Corpus Hermeticum.\nThe VERITRACE researchers have assembled a close-reading corpus of 140 works, each embodying this ancient wisdom tradition. Historical evidence already confirms its significant impact; for instance, Isaac Newton demonstrably engaged with the Sibylline Oracles, whilst Johannes Kepler possessed familiarity with the Corpus Hermeticum. Beyond these established connections, the project seeks to delve deeper, aiming to uncover a far broader array of networks and texts that interacted with this tradition. One scholar has aptly termed this extensive, often neglected body of work the ‘great unread’, as it frequently comprises numerous texts by lesser-known authors, rarely forming the primary focus of historical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-to-historical-inquiry",
    "href": "chapter_ai-nepi_006.html#computational-approaches-to-historical-inquiry",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Approaches to Historical Inquiry",
    "text": "6.3 Computational Approaches to Historical Inquiry\n\n\n\nSlide 04\n\n\nTo address its central research questions, the VERITRACE project employs a robust computational framework, enabling large-scale multilingual exploration. A primary objective involves identifying textual re-use across a vast, multilingual corpus. This encompasses both direct lexical re-use, where authors incorporate direct, potentially uncited, quotations, and more indirect semantic re-use, involving paraphrases or subtle allusions that contemporary readers would have recognised as originating from specific works, such as the Corpus Hermeticum.\nEssentially, the VERITRACE system functions as an ‘early modern plagiarism detector’, meticulously designed to uncover previously ignored networks of texts, passages, themes, topics, and authors. Through this systematic analysis, the researchers anticipate identifying novel patterns within the intellectual history and philosophy of science. The project leverages specific tools, including advanced keyword search capabilities and sophisticated text matching algorithms, to facilitate these investigations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-construction-and-data-sources",
    "href": "chapter_ai-nepi_006.html#corpus-construction-and-data-sources",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Corpus Construction and Data Sources",
    "text": "6.4 Corpus Construction and Data Sources\n\n\n\nSlide 04\n\n\nThe VERITRACE project has meticulously assembled a large and diverse multilingual dataset, focusing exclusively on printed works, thereby excluding handwritten materials for manageability. This extensive corpus spans approximately two centuries, from 1540, a chosen starting point for various historical reasons, to 1728, shortly after Isaac Newton’s death. The dataset incorporates texts in at least six different languages.\nThree primary digital repositories constitute the main data sources: Early English Books Online (EEBO), Gallica, which provides access to materials from the French National Library, and the Bavarian State Library, which contributes the largest proportion of the corpus. Collectively, these sources yield approximately 430,000 books for analysis. The VERITRACE researchers intend to apply state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, to explore this rich historical collection.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-llm-applications",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-llm-applications",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Core Challenges and LLM Applications",
    "text": "6.5 Core Challenges and LLM Applications\n\n\n\nSlide 06\n\n\nThe VERITRACE project navigates several core challenges inherent in processing historical texts at scale. A primary concern stems from the variable quality of Optical Character Recognition (OCR) text. Libraries provide this raw text directly in formats such as XML, HOCR, and HTML, frequently without accompanying ground truth page images. This raw input significantly affects all subsequent data processing stages. Furthermore, early modern typography and the evolving semantics across at least six distinct languages introduce considerable complexity. The sheer volume of data—hundreds of thousands of texts published across Europe over two centuries—also presents a substantial logistical and computational challenge.\nThe project strategically employs Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as ‘judges’, assisting in the enrichment and cleaning of metadata. This application, whilst promising for automating tedious tasks, currently faces challenges such as output hallucinations and a tendency towards generic responses when structured output is requested. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, a crucial step for facilitating the project’s advanced text matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-case-study",
    "href": "chapter_ai-nepi_006.html#llms-for-metadata-enrichment-case-study",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 LLMs for Metadata Enrichment (Case Study)",
    "text": "6.6 LLMs for Metadata Enrichment (Case Study)\n\n\n\nSlide 08\n\n\nA significant internal case study explores the application of LLMs to enrich VERITRACE metadata, aiming to automate the laborious process of bibliographic record comparison. The core motivation involves mapping VERITRACE records to the Universal Short Title Catalogue (USTC), a high-quality metadata source, to create enriched records that require less manual cleaning. Whilst external identifiers facilitate some automated mapping, the majority of records necessitate manual review due to the uncleaned state of the VERITRACE data. This manual process proved exceptionally tedious, with each team member assigned 10,000 pairs of bibliographic records for comparison, determining if they represented the same underlying printed text.\nTo alleviate this burden, the VERITRACE researchers devised an LLM-based solution, conceptualised as a ‘bench’ or ‘panel of judges’. This system employs a chain of LLMs—including Primary, Secondary, and Tiebreaker models, with an Expert LLM handling edge cases—to evaluate pairs of bibliographic records. The LLMs are tasked with judging whether records from a low-quality source and a high-quality source represent the identical underlying text. Crucially, the models must provide both their decision (match or no match) and accompanying reasoning with confidence levels. The team then compares these LLM decisions against ground truth data, followed by a final review by VERITRACE scholars.\nCurrently, the project utilises open-source LLM models, such as Llama, for this task. However, significant challenges persist. The models frequently exhibit hallucinations in their output, fabricating records not present in the input. Furthermore, whilst requesting more structured output aims to reduce unhelpful responses, it often results in more generic and less insightful reasoning. Achieving the optimal balance between structured output and helpfulness remains an ongoing challenge, described as more ‘art’ than science. Despite these hurdles, the approach holds substantial theoretical potential for significant time savings, though it has not yet achieved full operational efficacy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-data-pipeline",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-data-pipeline",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 VERITRACE Web Application: Alpha Version and Data Pipeline",
    "text": "6.7 VERITRACE Web Application: Alpha Version and Data Pipeline\n\n\n\nSlide 13\n\n\nThe VERITRACE project has developed an alpha version of its web application, currently in its nascent stages and not yet publicly accessible. This internal prototype serves as a tangible demonstration of the project’s ambitious future capabilities. Engineers are presently testing a BERT-based Large Language Model, specifically LaBSE, to generate vector embeddings for every passage within the textual corpus. Whilst this model demonstrates functionality in certain scenarios, preliminary assessments suggest it may not ultimately suffice for the project’s comprehensive requirements.\nUnderpinning this application lies a complex 15-stage data processing pipeline. This pipeline meticulously transforms raw text, supplied by libraries in various formats including XML, HOCR, and HTML, into a structured Elasticsearch database, which serves as the web application’s backend. The pipeline encompasses numerous critical stages, such as extracting text into standardised files, generating precise mappings of textual positions, segmenting the content, and rigorously assessing the OCR quality of each input. The generation of vector embeddings occurs towards the latter stages of this intricate process. Crucially, each of these 15 stages demands individual optimisation to ensure the integrity and efficiency of the entire workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-explore-and-metadata-explorer",
    "href": "chapter_ai-nepi_006.html#web-application-features-explore-and-metadata-explorer",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Web Application Features: Explore and Metadata Explorer",
    "text": "6.8 Web Application Features: Explore and Metadata Explorer\n\n\n\nSlide 15\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Metadata Explorer, Search, Analyse, Read, and Match. The Explore section serves as an initial entry point, offering comprehensive statistics about the corpus. This data, drawn directly from a Mongo database, provides an overview of the collection, including the total count of 427,305 metadata records. Visualisations such as pie charts and bar charts illustrate language distribution, documents by data source, documents by decade, and publication places, enabling users to gain insights into the corpus’s composition.\nBeyond this statistical overview, the Metadata Explorer section allows users to delve into the rich metadata associated with each individual text. This includes detailed fields such as Document ID, Filename, Bibliographic Title, Author, Publication Place, Printer, Format, Language, and Subject. A crucial feature involves granular language identification, performed on every text down to approximately 50 characters. This addresses the prevalent multilingual nature of early modern works, ensuring accurate language representation beyond simple metadata declarations. For instance, a text might reveal a substantive multilingual composition, such as 15% Greek and 85% Latin. Furthermore, the system attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-search-and-analyse",
    "href": "chapter_ai-nepi_006.html#web-application-features-search-and-analyse",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Web Application Features: Search and Analyse",
    "text": "6.9 Web Application Features: Search and Analyse\n\n\n\nSlide 17\n\n\nFor most scholarly users, the Search section will likely serve as the primary interface, offering robust keyword search capabilities. For example, a simple query for “Hermes” within the current prototype corpus, which comprises 132 files, yields 22 documents containing 332 matches. Notably, even this limited prototype generates a 15 GB index, indicating that the full 400,000-text corpus will necessitate terabytes of storage.\nLeveraging Elasticsearch, the system supports highly sophisticated queries beyond basic keywords. Users can perform field-specific searches, such as retrieving all books by Kepler that contain “Hermes.” Furthermore, the platform accommodates complex boolean logic (ANDs, ORs), nested queries, and proximity searches, allowing users to specify, for instance, texts where “Hermes” and “Plato” appear within ten words of each other.\nWhilst not yet implemented, the Analyse section of the website is planned to offer advanced analytical tools. These will include Topic Modelling, Latent Semantic Analysis (LSA), and Diachronic Analysis, enabling scholars to explore thematic shifts and conceptual relationships over time.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-features-read-and-match",
    "href": "chapter_ai-nepi_006.html#web-application-features-read-and-match",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 Web Application Features: Read and Match",
    "text": "6.10 Web Application Features: Read and Match\n\n\n\nSlide 19\n\n\nThe VERITRACE web application incorporates a dedicated Read section, providing scholars with access to high-quality digital facsimiles of the texts. Utilising a Mirador viewer, users can engage with PDF versions of each document, complemented by the display of associated metadata.\nCrucially, the Match section facilitates the identification of textual reuse across the corpus. This powerful tool supports various comparison modes: users can compare a single document against another, perform multi-document comparisons (e.g., analysing textual overlap across all of Kepler’s works within the database), or even compare a single text against the entire corpus. The latter, whilst highly desirable, presents significant computational challenges regarding user wait times. The interface exposes numerous parameters for user customisation, such as minimum similarity scores, allowing scholars to fine-tune their searches.\nThe system offers two fundamental match types. Lexical matching employs keyword analysis to identify vocabulary similarities, proving ineffective for cross-language comparisons. Conversely, semantic matching leverages vector embeddings to discover conceptually similar passages, irrespective of shared vocabulary. This approach relies on a BERT-based multilingual embeddings model, trained on 109 languages, which encodes passages into a unified vector space, thereby enabling seamless cross-language comparisons. A hybrid matching option also exists, combining both lexical and semantic approaches with adjustable weights. Furthermore, users can select from different matching modes: a standard mode, a comprehensive mode that demands more computational power for exhaustive results, and a faster mode for quicker, though potentially less complete, outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-case-study-newtons-opticks",
    "href": "chapter_ai-nepi_006.html#text-matching-case-study-newtons-opticks",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Text Matching Case Study: Newton’s Opticks",
    "text": "6.11 Text Matching Case Study: Newton’s Opticks\n\n\n\nSlide 16\n\n\nA compelling case study involves comparing Isaac Newton’s Latin edition of Optice (1719) with its English counterpart, Opticks (1718). When performing a lexical match between these two texts, the standard mode yields no significant results, precisely as anticipated given their differing languages. However, employing the comprehensive mode reveals three matches, likely corresponding to English text embedded within the Latin edition, such as a preface.\nConversely, a semantic match between these translated works produces reasonable results, despite existing OCR issues. Passages demonstrate clear conceptual similarity, for instance, parallel discussions on colours. The system provides detailed match summary metrics, including a high quality score of 91.2%. Nevertheless, the coverage score registers at a comparatively low 36.9%. This low coverage, whilst initially appearing problematic, actually provides valuable insight: the Latin edition is considerably longer and exhibits notable divergences from the English version, suggesting the metric accurately reflects the textual relationship rather than an error. The interface for lexical matches further enhances usability by automatically highlighting matching terms within both the source and comparison passages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-development-horizons",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-development-horizons",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Future Challenges and Development Horizons",
    "text": "6.12 Future Challenges and Development Horizons\n\n\n\nSlide 21\n\n\nThe VERITRACE project faces several critical challenges as it progresses. The current BERT-based embedding model, LaBSE, whilst a starting point, is likely insufficient for the project’s comprehensive needs, primarily due to inherent trade-offs between accuracy, storage requirements, and inference time. The VERITRACE researchers are exploring alternative models such as XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting its own set of compromises. A fundamental question arises: given the distinct nature of the historical corpus, is fine-tuning a base model on this specific dataset essential to achieve adequate results?\nA further complexity involves the evolution of semantic meaning over time. The project must address how to accurately handle semantic shifts across centuries, particularly when comparing texts published in 1540 with those from 1700, often in different languages. This raises a crucial query: do texts from disparate historical periods truly reside within the same vector space when processed by modern models?\nThe pervasive issue of poor OCR quality also impacts every downstream process, fundamentally hindering accurate segmentation into sentences and passages. Re-OCR of the entire corpus is not feasible; therefore, the team must consider re-OCR for only the very poorest quality texts or investing time to locate existing high-quality versions. Finally, scaling and performance present a significant hurdle. Current queries on a mere 132 texts require approximately 15 seconds. Scaling this to the full corpus of 430,000 texts will undoubtedly introduce substantial performance issues, necessitating considerable computational power and innovative solutions to maintain acceptable query times. The VERITRACE project actively welcomes external advice on these multifaceted challenges.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis presentation delves into the critical domains of Explainable AI (XAI) and the application of AI-based scientific insights within the humanities. Initially, the discourse establishes the foundational principles of XAI, particularly its evolution from feature attribution in classification models to addressing the complexities of generative AI. The authors highlight the necessity of understanding model predictions, identifying biases, and ensuring regulatory compliance. Furthermore, the discussion meticulously details various orders of interpretability, progressing from first-order attributions, such as heatmaps, to more intricate second and higher-order interactions, including those within graph structures.\nSubsequently, the presentation transitions to practical applications, showcasing how these advanced AI and XAI methodologies facilitate novel research in the humanities. Specific case studies illustrate the extraction of visual definitions from historical corpora and the large-scale analysis of early modern astronomical tables. A significant workflow, termed “XAI-Historian”, empowers historians to generate data-driven hypotheses and discover new insights. This system employs specialised statistical models to derive bigram representations from challenging, out-of-domain historical data, enabling robust analysis. Crucially, the application of cluster entropy analysis reveals patterns of innovation spread across historical European publishing centres, identifying anomalies such as the politically controlled print programme in Wittenberg. The presentation concludes by acknowledging the challenges inherent in applying AI to heterogeneous, low-resource humanities data whilst underscoring the transformative potential of multimodal approaches and explainable machine learning for scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI 1.0: Feature Attributions",
    "text": "7.1 Explainable AI 1.0: Feature Attributions\n\n\n\nSlide 03\n\n\nExplainable AI (XAI) encompasses methods and approaches meticulously developed to decipher the internal workings of highly complex machine learning models. Historically, machine learning predominantly focused on visual data; interest in language, whilst present, gained significant momentum only in recent years. A typical scenario involves a “Black Box AI System” receiving an input, such as an image, and subsequently generating a prediction, for instance, identifying a “Rooster”. Crucially, users often lack insight into the underlying basis for such classifications.\nTo address this opacity, the research team pioneered “Post-Hoc Explainability” techniques. Heatmaps, for example, visually delineate the specific pixels or features that primarily contributed to a given prediction. In the rooster example, a heatmap would highlight the bird’s head, clearly indicating the model’s focus. Beyond mere transparency, the broader rationale for explainability spans several critical objectives:\n\nIt enables verification of predictions, ensuring the model operates logically and produces reasonable outcomes.\nIt facilitates the identification of flaws and biases, offering insights into how models make mistakes.\nIt serves as a tool for learning about the underlying problem itself, as models occasionally uncover surprising and unconventional solutions.\nIncreasingly vital, explainability ensures compliance with evolving legislation, such as the European AI Act.\n\nSamek and colleagues (2017) provided foundational work in this area.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0",
    "href": "chapter_ai-nepi_007.html#generative-ai-and-xai-2.0",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Generative AI and XAI 2.0",
    "text": "7.2 Generative AI and XAI 2.0\n\n\n\nSlide 03\n\n\nThe landscape of artificial intelligence has profoundly shifted from conventional classification models to the era of Generative AI (Gen AI). These advanced models now exhibit multifaceted capabilities, encompassing not only classification but also the retrieval of similar images, the generation of novel images, and comprehensive question-and-answer functionalities across diverse topics. Consequently, grounding a prediction or an answer from a Large Language Model (LLM) system to its specific input has become considerably more challenging. The authors are therefore exploring new directions for XAI, moving beyond simple heatmap representations to consider intricate feature interactions and adopt a more mechanistic view of model operations. These contemporary foundation models function as both multi-task and world models, offering profound insights into societal structures and the evolution of text over time.\nNevertheless, these sophisticated models can still exhibit surprising errors. A well-known example from object classification illustrates this: a standard classifier predicted a boat based on the surrounding water, a correlated and texturally simpler feature, rather than the boat itself. Lapuschkin and colleagues documented this phenomenon in Nature Communications (2019). More recently, Mondal, Webb, and their team (2024) highlighted multi-step planning mistakes in LLMs. When tasked with the Tower of Hanoi puzzle, for instance, an LLM might immediately attempt to move the largest, inaccessible disc, demonstrating a fundamental misunderstanding of the problem’s physical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-first-order",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-first-order",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Structured Interpretability: First-Order",
    "text": "7.3 Structured Interpretability: First-Order\n\n\n\nSlide 05\n\n\nStructured interpretability extends the utility of XAI beyond basic visualisations. First-order explanations, for instance, prove particularly effective for elucidating the decisions of classification models. The research team applied this technique to a classifier designed for historical documents, specifically aiming to distinguish various subgroups of historical tables.\nTo validate the classifier’s efficacy, the authors employed heatmaps, meticulously checking if the predictions relied upon genuinely meaningful features. This analysis revealed that the model correctly focused on the numerical content within the tables. This focus served as an accurate proxy for identifying numerical tables, thereby confirming the model’s meaningful operation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#structured-interpretability-higher-order",
    "href": "chapter_ai-nepi_007.html#structured-interpretability-higher-order",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Structured Interpretability: Higher-Order",
    "text": "7.4 Structured Interpretability: Higher-Order\n\n\n\nSlide 10\n\n\nBeyond first-order attributions, the authors have explored more complex forms of structured interpretability, notably second and higher-order interactions. Second-order features primarily focus on pairwise relationships, with similarity proving particularly important. The team computed a dot product, yielding a similarity score, from the embeddings of two entities, such as images. Subsequently, interaction scores between specific features, like individual digits, elucidated the basis for these similarity predictions, confirming the model’s intended function.\nFurthermore, in more recent work, higher-order interactions have demonstrated greater significance, particularly within graph structures. These structures might represent citation networks, intricate networks of books, or various interconnected entities. When models are trained on classification tasks within such networks, the authors find that feature subgraphs or feature walks—essentially, sets of features—become collectively relevant. Identifying these complex interactions facilitates deeper insights into model behaviour, moving towards a more granular, circuit-level understanding of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#llms-biases-and-long-range-dependencies",
    "href": "chapter_ai-nepi_007.html#llms-biases-and-long-range-dependencies",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 LLMs: Biases and Long-Range Dependencies",
    "text": "7.5 LLMs: Biases and Long-Range Dependencies\n\n\n\nSlide 12\n\n\nFirst-order attributions offer crucial insights into the internal workings of Large Language Models (LLMs), particularly concerning biases and their handling of long-range dependencies.\n\n7.5.1 Biased Sentiment Predictions in Transformer LLMs\nThe authors investigated feature importance in LLMs by analysing how specific names influenced sentiment predictions in movie reviews, a common task within the language community. Employing heatmaps generated via a novel method tailored for Transformers, they uncovered notable biases. Male Western names, such as Lee, Barry, Raphael, or the Cohen Brothers, consistently correlated with a higher likelihood of positive sentiment predictions. Conversely, more foreign-sounding names, including Saddam, Castro, or Chan, tended to elicit negative sentiment scores. This demonstrates XAI’s considerable utility in detecting subtle, fine-grained biases embedded within these complex models, a phenomenon now widely recognised within the community. Ali and colleagues (2022) detailed this work in “XAI for Transformers”.\n\n\n7.5.2 First-Order Attributions for Long-Range Dependencies in LLMs\nFurther research explored how LLMs manage long-range dependencies, specifically when generating text summaries from extensive inputs, up to an 8,000-token context window. In a typical scenario involving Wikipedia articles, the model receives a lengthy text and then produces a summary. Analysis revealed that the model predominantly focuses on the latter portions of the provided context, prioritising information presented closer to the prompt. Whilst models can indeed draw upon long-range information from the very beginning of the context, they do so significantly less frequently, as evidenced by a log scale of token counts. Consequently, users should note that LLM-generated summaries may not provide a balanced overview of the entire input text, often emphasising more recently presented data. Jafari and colleagues (2024) presented these findings in “MambaLRP”.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#text-higher-order-interactions",
    "href": "chapter_ai-nepi_007.html#text-higher-order-interactions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Text: Higher-Order Interactions",
    "text": "7.6 Text: Higher-Order Interactions\n\n\n\nSlide 11\n\n\nExploring second and higher-order interactions offers deeper insights into how models process textual data.\n\n7.6.1 Second-Order Interactions for Text Similarity\nConsider a scenario involving a pair of sentences, processed by a sentence embedding model, such as a BERT model, to yield a similarity score. The challenge lies in comprehending the precise reasons for that score. Second-order explanations address this by providing granular interaction scores between individual tokens. Analysis of these scores frequently reveals noun matching strategies, encompassing both synonyms and identical noun tokens, alongside interactions involving separators and other token types. This suggests that whilst models compress vast amounts of information, they often rely on surprisingly simplistic underlying strategies to achieve their similarity predictions.\n\n\n7.6.2 Graph Neural Networks for Structured Predictions\nGraph Neural Networks (GNNs) offer a powerful framework for structured predictions, providing attributions in terms of “walks” that represent complex feature interactions. Intriguingly, GNNs, which inherently encode structural information, can be conceptualised as LLMs, given that their attention networks facilitate token message passing. This connection enables their application to the analysis of language structure.\n\n\n7.6.3 Interaction of Nodes Learns Complex Language Structure\nThe authors demonstrated this by training a GNN (or an LLM) on a movie review sentiment task, leveraging the hierarchical structure inherent in natural language. They then extracted “walks” to understand the model’s decision-making. First-order attributions proved insufficient, failing to capture the nuanced complexity of language; for instance, the phrase “first I didn’t like the boring pictures” might receive a high positive score solely due to the presence of “like,” neglecting the crucial negation. In stark contrast, higher-order explanations accurately assigned a negative score to the entire negative sentence and correctly captured the hierarchical structure of the subsequent positive statement. Schnake and colleagues (2022) published this work in “Higher-Order Explanations of Graph Neural Networks via Relevant Walks”.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 AI in the Humanities",
    "text": "7.7 AI in the Humanities\n\n\n\nSlide 20\n\n\nAI-based methodologies offer transformative potential for scientific insights within the humanities, as demonstrated through several compelling examples.\n\n7.7.1 Extracting Visual Definitions from Corpora\nThe research team embarked on a project to extract visual definitions from a corpus of mathematical instruments. Their objective involved classifying these instruments, distinguishing, for instance, between a machine and a purely mathematical instrument. Employing heatmap-based approaches for visual definitions, the team collaborated closely with historians, including Matteo Valeriani and Jochen Büttner. These domain experts provided crucial guidance and meticulously verified the definitions derived from the models. The analysis revealed that fine-grained scales present on the mathematical instruments proved highly relevant for the model’s classification decisions. El-Hajj, Eberle, and their colleagues (2023) published this work on explainability and transparency in digital humanities.\n\n\n7.7.2 Corpus-Level Analysis of Early Modern Astronomical Tables\nA larger collaborative project focused on the corpus-level analysis of early modern astronomical tables. This initiative involved the Sphaera Corpus (1472-1650) and the Sacrobosco Table Corpus (1472-1650), collectively comprising 76,000 pages of university textbooks. Historically, these tables, vital carriers of scientific knowledge and indicators of mathematisation processes, had never been analysed at scale. This challenge arose from the data’s extreme heterogeneity, limited annotations, and the inadequacy of conventional Optical Character Recognition (OCR) and foundation models. The primary objective was to develop an automated method for matching tables with similar semantics. Valeriani and colleagues (2019) and Eberle and colleagues (2024) detail the foundational corpora.\n\n\n7.7.3 Historical Insights at Scale: XAI-Historian Workflow\nTo address these challenges, the authors developed a comprehensive workflow designed to empower historians with insights at scale, coining the term “XAI-Historian”. This concept envisions a historian leveraging AI and explainable AI to generate data-driven hypotheses and uncover new case studies. The workflow encompasses three key stages: initial data collections from images of books, followed by an atomisation-recomposition phase involving input tables, bigram maps, and histograms, culminating in corpus-level analysis through historical table embedding and data similarity. Rather than relying on general foundation models, which proved ineffective on this out-of-domain historical data, a specialised statistical model was crafted to detect bigrams. This bespoke model’s reliability was rigorously verified by confirming consistent bigram detection, such as “38” across two distinct inputs, thereby establishing trust in its decisions. Eberle and colleagues (2024) and Eberle and colleagues (2022) document this pioneering work.\n\n\n7.7.4 Cluster Entropy Analysis to Investigate Innovation\nBuilding upon these capabilities, the research team employed cluster entropy analysis to investigate the spread of innovation across European publishing centres during the early modern period. Focusing on the output of specific cities within the Sphaera publication (EPISD-626), they quantified the diversity of each city’s print programme using entropy. A low entropy score indicated a tendency to reproduce identical content, whilst a higher score signified a more diverse programme. This approach, utilising model representations for distance-based clustering and entropy calculation, enabled large-scale analysis previously unattainable. The analysis identified two particularly interesting cases: Frankfurt/Main, which exhibited the lowest entropy, confirming its established reputation as a centre for reprinting editions; and Wittenberg, also displaying remarkably low entropy. This latter finding revealed a historical anomaly: the political control exerted by the Protestant reformers, notably Melanchthon, actively limited the print programme and curriculum, a discovery that aligned perfectly with existing historical intuition and scholarly support. Eberle and colleagues (2024) further elaborates on these findings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion",
    "href": "chapter_ai-nepi_007.html#conclusion",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 Conclusion",
    "text": "7.8 Conclusion\n\n\n\nSlide 26\n\n\nHumanities and Digital Humanities (DH) research has historically concentrated on the digitisation of source material. Nevertheless, automated analyses of these digitised corpora present significant challenges, primarily owing to their inherent heterogeneity and the scarcity of annotated labels.\nDespite these hurdles, multimodality, advanced Machine Learning (ML), and Explainable AI (XAI) collectively offer substantial potential to scale humanities research and foster entirely novel research directions. Foundation Models and Large Language Models (LLMs), coupled with effective prompting strategies, can automate various intermediate tasks, including labelling, data curation, and error correction. However, their utility remains limited when addressing more complex research questions.\nSignificant challenges persist, particularly concerning low-resource data, which acts as a considerable roadblock, impacting the applicability of scaling laws. Moreover, out-of-domain transfer, especially for historical and small-scale datasets, necessitates rigorous evaluation. Current LLM training and alignment predominantly focus on natural language tasks and code generation, underscoring the need for specialised approaches when engaging with the unique characteristics of humanities data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "",
    "text": "Overview\nThis presentation introduces a pioneering framework for historical and scientific inquiry. The authors meticulously designed this system to harness advanced Large Language Models (LLMs) whilst systematically mitigating their inherent limitations, particularly concerning factual hallucination and validation. They propose a novel “computational epistemology” paradigm, fundamentally prioritising robust validation mechanisms over the mere expansion of contextual understanding or the simulated “thinking” capabilities of contemporary LLMs.\nA bespoke research infrastructure, aptly named Scholarium, forms the bedrock of this methodology. This sophisticated system seamlessly integrates meticulously curated scholarly sources, exemplified by the exhaustive Opera Omnia Euler, with a structured registry database. This registry systematically records historical activities and communications, thereby offering a validated alternative to conventional embedding-based approaches. The developers implemented this intricate system within the Cursor environment, leveraging a suite of multimodal LLMs, including Gemini 2.5, Claude, and Llama, alongside a dedicated AI agent christened Bernoulli.\nFurthermore, the project strategically employs Zenodo, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository, for enduring data preservation and dissemination. Open Science Technology provides crucial technical support, operating an MCP API server that ensures global access to the curated data via standardised APIs for artificial intelligence models, thereby fostering principles of open access, open data, and open collaboration. Ultimately, the system aims to deliver complete, rigorously validated answers to complex historical queries, a critical capability currently absent in existing LLM applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#evolution-and-limitations-of-large-language-models",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.1 Evolution and Limitations of Large Language Models",
    "text": "8.1 Evolution and Limitations of Large Language Models\n\n\n\nSlide 01\n\n\nLarge Language Models have undergone a remarkably swift evolution, progressing through distinct conceptual phases. Initially, the paradigm centred on “Attention is all you need”, emphasising the core mechanism of transformer architectures. Subsequently, the focus shifted towards “Context is all you need”, prompting developments such as Retrieval-Augmented Generation (RAG) to expand contextual understanding. The latest conceptualisation now postulates “Thinking is all you need”, suggesting a further layer of cognitive capability.\nDespite this rapid advancement, current LLM iterations exhibit significant deficiencies, particularly concerning the rigorous demands of scholarly inquiry. Crucially, they lack an inherent opponent mechanism to effectively counter hallucination, a pervasive challenge in generative AI. Furthermore, embedding vectors, whilst powerful for semantic similarity, fundamentally fail to capture the true meaning of expressions. These models frequently formulate statements that, whilst sounding plausible, are factually incorrect. They also struggle to differentiate genuine knowledge from mere internet media or hearsay, often repeating unverified information. Consequently, LLMs currently cannot reliably seek the best-justified information or formulate coherent plans for complex scientific inquiry. Regrettably, existing technologies offer no immediate prospect of overcoming these fundamental limitations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-computational-epistemology",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.2 The Imperative of Validation and Computational Epistemology",
    "text": "8.2 The Imperative of Validation and Computational Epistemology\n\n\n\nSlide 03\n\n\nA critical need arises for robust validation mechanisms within advanced computational systems. Such validation must furnish comprehensive reasons, compelling arguments, and verifiable evidence both for and against the truth of any given proposition. Moreover, it must extend to providing justifications for or against the pursuit of specific actions.\nTo address this profound gap, the authors propose a nascent discipline: computational epistemology. This new subject systematically develops the methods and methodologies essential for bridging the validation deficit inherent in current AI approaches. Achieving genuine epistemic agency necessitates several key capabilities. The research team must identify propositions that extend beyond simple sentences, discerning their underlying meaning and scope. Furthermore, the system must accurately identify arguments embedded within diverse texts, historical sources, and complex inquiries. Crucially, it must also discern the intentions, plans, and actions of historical figures, meticulously documented within their surviving records.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "href": "chapter_ai-nepi_008.html#the-research-inquiry-environment",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.3 The Research Inquiry Environment",
    "text": "8.3 The Research Inquiry Environment\n\n\n\nSlide 04\n\n\nA specialised working environment facilitates rigorous historical inquiry. The interface presents an open historical source, such as a book title page, on its left pane. This particular context involves the construction of Sanssouci Castle under Frederick the Great and the contentious role of Leonhard Euler, one of the 18th century’s most eminent mathematicians, in what proved to be a significant construction failure. Historians continue to debate Euler’s precise responsibility for this setback.\nThe right pane features a text editor where users formulate specific inquiries. For instance, a user might pose the question: “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain). The system aims to provide a validated, qualifying answer, rigorously supported by proven evidence rather than mere hearsay. Consequently, the output lists individuals, such as Nahl, Benkert and Heymüller, and Giese, detailing their specific contributions, periods of work, earnings, and documented achievements or failures, with explicit references to associated files like “Manger1789_p81-91.xml”.\nThe Cursor environment, situated at the bottom right of the interface, enables the deployment of AI agents. Here, a dedicated agent, presumptuously named Bernoulli, assists in navigating and querying these complex sources. A significant challenge, however, extends beyond merely reading individual PDF sources; it necessitates searching all available, relevant sources, a task for which conventional token-based indexing proves inadequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "href": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-sources",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.4 Scholarium: Curated Scholarly Sources",
    "text": "8.4 Scholarium: Curated Scholarly Sources\n\n\n\nSlide 07\n\n\nThe system fundamentally relies upon a scholarly curated editorial board, which meticulously validates all integrated sources. A prime example of this rigorous approach is the Opera Omnia Euler, a monumental collection spanning 86 volumes. Scholars dedicated over 120 years to its comprehensive editing, a process completed just two years prior. This exhaustive work encompasses all 866 of Euler’s publications and his complete correspondence. Other significant scholarly works, including the Kepler Gesammelte Werke and Brahe Opera Omnia, further complement this foundational collection. Users access these resources via a dedicated “Euler Opera Omnia Viewer”, which provides intuitive navigation through collection, series, volume, and index dropdowns.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "href": "chapter_ai-nepi_008.html#scholarium-registry-as-an-embedding-alternative",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.5 Scholarium: Registry as an Embedding Alternative",
    "text": "8.5 Scholarium: Registry as an Embedding Alternative\n\n\n\nSlide 06\n\n\nA pivotal innovation within this framework is the Scholarium, which serves as a sophisticated alternative to conventional embedding-based approaches. Functioning as a meticulously curated database of content items, the Scholarium maintains a highly detailed inventory of historically proven activities, each entry rigorously validated against original sources.\nThis comprehensive registry systematically captures a diverse array of information:\n\nPersonal actions\nVarious communication acts, including letters, publications, and reports\nSpecific statements\nImplications, arguments, and inquiries\nNuanced use of language, terminology, and concepts\nApplication of specific concepts and their relations\nDeployment of models and methods\nUtilisation of tools and devices\nPrecise use of data, information, evidence, and sources\n\nAn integrated AI API, specifically leveraging the Model Context Protocol (MCP API), facilitates seamless interaction with this rich, structured data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "href": "chapter_ai-nepi_008.html#technical-infrastructure-and-fair-principles",
    "title": "8  Modeling Science: LLM for the History, Philosophy and Sociology of Science",
    "section": "8.6 Technical Infrastructure and FAIR Principles",
    "text": "8.6 Technical Infrastructure and FAIR Principles\n\n\n\nSlide 07\n\n\nThe project team queries its meticulously compiled records using a suite of accessible multimodal models. The authors have determined that the latest multimodal models, such as Gemini 2.5, prove optimal for the task requirements, adeptly combining information derived from both text and images. The Cursor environment integrates a range of LLM models, including Claude, Gemini, Llama, and LettreAI.\nFor enduring data preservation and publication, a long-term FAIR (Findable, Accessible, Interoperable, Reusable) repository is indispensable. Zenodo, hosted by CERN in Geneva, fulfils this critical role, ensuring the longevity and accessibility of the project’s data for many years.\nTechnical support for the underlying infrastructure is provided by Open Science Technology, a dedicated startup. This entity manages the operational aspects of the system, including the crucial MCP API server. This server facilitates worldwide access to the curated data via standardised APIs, enabling seamless interaction with artificial intelligence models. This comprehensive technical framework actively promotes principles of open collaboration, open source development, open access to resources, and open data sharing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science: LLM for the History, Philosophy and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "",
    "text": "Overview\nThe authors investigated the application of Large Language Models (LLMs) to assess biases within publications classified under the Sustainable Development Goals (SDGs). This inquiry spanned three major bibliometric databases: Web of Science, Scopus, and OpenAlex. The core motivation for this work arose from the critical role these databases assume within the sociology of science, influencing academic behaviour, funding, and policy. These platforms also reflect political and commercial interests, whilst possessing an inherent performative nature.\nThis investigation built upon previous research, which identified minimal overlap in SDG-labelled publications across different providers. The project aimed to elucidate the aggregate effects of LLM-based tools on the representation of SDG-related research. Furthermore, it served as a proof-of-concept exercise, demonstrating the automation of information extraction for research decision-making.\nThe authors selected five SDGs directly related to socioeconomic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure). For classification, they utilised a shared corpus of 15,471,336 jointly indexed publications, spanning January 2015 to July 2023.\nFor the LLM component, the research team fine-tuned DistilGPT2, a lightweight, open-source model. They trained this model separately on publication abstracts for each SDG and database combination, thereby creating 15 distinct models. This strategic choice minimised any pre-existing knowledge about SDGs within the model. Prompts, derived from SDG targets (typically 8-12 targets per SDG, with 10 diverse questions per target), served to benchmark the fine-tuned LLMs. The authors employed three decoding strategies: top-k, nucleus, and contrastive search. Noun phrase extraction from the LLM responses subsequently facilitated analysis across four dimensions: locations, actors, data/metrics, and focuses.\nKey findings revealed a systematic oversight within the data concerning disadvantaged individuals, the poorest countries, and underrepresented topics explicitly mentioned in SDG targets. Conversely, economic superpowers received considerable attention. The study thus highlighted the decisive impact of bibliometric SDG classification and the sensitivity of LLMs to training data, model architecture, parameters, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#background-and-motivation-sdg-classification-in-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#background-and-motivation-sdg-classification-in-bibliometric-databases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.1 Background and Motivation: SDG Classification in Bibliometric Databases",
    "text": "9.1 Background and Motivation: SDG Classification in Bibliometric Databases\n\n\n\nSlide 01\n\n\nThe authors initiated an investigation to employ Large Language Models (LLMs) as a technology for assessing biases within publications classified by three principal bibliometric databases. This work acknowledges that bibliometric databases, such as Web of Science, Scopus, and OpenAlex, function as critical digital infrastructures. They enable bibliometric analyses and impact assessments throughout the scientific community. Nevertheless, as Whitley (2000) and Winkler (1988) noted, these databases possess a performative nature, shaped by particular understandings of the science system and specific value attributions.\nMajor bibliometric databases have recently implemented classifications that align scholarly publications with the United Nations Sustainable Development Goals (SDGs). However, prior research, notably by Armitage et al. (2020), discovered significant discrepancies in SDG labelling across various providers, including Elsevier, Bergen, and Aurora. This revealed minimal overlap in the resulting datasets. Such differences in classification carry substantial implications; they can foster varying perceptions of research priorities, which, in turn, may influence resource allocation and policy decisions.\nFurthermore, these databases exert considerable influence over academics, researchers, funding bodies, and policymakers, whilst also responding to diverse political and commercial interests. The current study specifically considered Web of Science, Scopus, and OpenAlex, building upon earlier findings that highlighted the limited overlap in publications when different SDG search queries were applied.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-objectives-and-conceptual-framework",
    "href": "chapter_ai-nepi_009.html#case-study-objectives-and-conceptual-framework",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.2 Case Study Objectives and Conceptual Framework",
    "text": "9.2 Case Study Objectives and Conceptual Framework\n\n\n\nSlide 02\n\n\nOttaviani and StahlSchmidt (2024) conducted a detailed case study, focusing on the representation of UN Sustainable Development Goals within bibliometric data. A primary motivation involved assessing the aggregated effects on how SDG-related research is portrayed in bibliometric databases, particularly if LLM-based tools were to be introduced. To achieve this, the authors employed DistilGPT2, a minimally pre-trained Large Language Model. They separately trained this model on distinct subsets of publication abstracts, each corresponding to the SDG classifications provided by the diverse bibliometric databases.\nThe LLM technology served a dual purpose in this study. Firstly, it functioned as a detector of biases present in the data. Secondly, it acted as a proof-of-concept exercise, demonstrating the potential of LLMs in automating information extraction to inform research-related decision-making. The authors aimed to understand the aggregate effects stemming from metadata processing by bibliometric databases and how these subsequently influence various stakeholders, including researchers, policymakers, and consultants. Ultimately, the project sought to develop a generalisable exercise for assessing the potential impact of such technologies on research policy.\nA conceptualised chain of dependencies illustrates this process:\n\nSDG classification initially defines what constitutes “Research” on SDGs.\nVarious actors, including researchers, small and medium-sized enterprises (SMEs), governments, and other intermediaries, then process this research.\nSubsequently, this processed research informs “Decision-making to align with SDGs,” which in turn affects “Socioeconomic inequalities.”\nParallel to this, the LLM, acting as a “detector of ‘biases’,” influences the “Introduction of LLM in Research Policy,” which also has repercussions for “Socioeconomic inequalities.”\n\nTherefore, alterations in the metadata that define “research on SDGs” can significantly impact advice, choices, indicators, and implemented measures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#methodology-actors-data-and-sdg-selection",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.3 Methodology: Actors, Data, and SDG Selection",
    "text": "9.3 Methodology: Actors, Data, and SDG Selection\n\n\n\nSlide 04\n\n\nThe research design incorporated three principal bibliometric databases as key actors. These included two proprietary systems, Web of Science (operated by Clarivate, US) and Scopus (managed by Elsevier, UK), alongside the open-access database OpenAlex (formerly a Microsoft entity, US). The authors focused their analysis on five specific UN Sustainable Development Goals, chosen for their direct relevance to socioeconomic inequalities. They categorised these into two dimensions:\n\nThe socio dimension, encompassing SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities).\nThe economic dimension, which included SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).\n\nFor data processing, the team utilised a jointly indexed subset comprising 15,471,336 publications. They compiled this dataset by collecting publications shared across all three bibliometric databases, identified through exact DOI matching, covering the period from January 2015 to July 2023. Subsequently, the authors undertook an analysis of the performance of the three distinct classification standards for the five selected SDGs. This approach resulted in the creation of three separate subsets of publications for each SDG—one corresponding to each bibliometric database—forming the basis for the comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "href": "chapter_ai-nepi_009.html#comparative-analysis-of-sdg-classified-papers",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.4 Comparative Analysis of SDG-Classified Papers",
    "text": "9.4 Comparative Analysis of SDG-Classified Papers\n\n\n\nSlide 05\n\n\nOttaviani and StahlSchmidt (2024) conducted a comparative analysis, examining the overlap of papers classified under specific Sustainable Development Goals (SDGs) across Web of Science, OpenAlex, and Scopus, particularly for the socio-dimension SDGs. For SDG4 (Quality Education), Scopus classified the largest share of publications (339,063; 52.2%), followed by OpenAlex (218,907; 33.6%), and Web of Science (124,359; 19.1%). The intersection of all three databases for SDG4 contained 46,711 publications (7.2%).\nConcerning SDG5 (Gender Equality), Web of Science accounted for the majority of classifications (373,224; 57.4%), with Scopus classifying 82,277 (26.2%) and OpenAlex 38,066 (12.1%). Notably, only 21,770 publications (6.9%) were commonly classified under SDG5 by all three. The authors observed that some publications present in Scopus were not designated as SDG5 by that database. Furthermore, Web of Science’s SDG5 classifications included approximately 10% of publications from mathematics, such as those on geometrical differential equations, indicating potential discrepancies in classification criteria.\nFor SDG10 (Reduce Inequalities), Scopus again led in volume (236,665; 36.2%), with OpenAlex (213,419; 32.7%) and Web of Science (99,460; 15.2%) following. The common overlap for SDG10 was the smallest, at 13,319 publications (2.0%). These findings generally align with Armitage (2020), underscoring the consistently small overlap in SDG labelling across different bibliometric providers. This limited congruence highlights the varying interpretations and applications of SDG classifications.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-technology-selection-and-fine-tuning-strategy",
    "href": "chapter_ai-nepi_009.html#llm-technology-selection-and-fine-tuning-strategy",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.5 LLM Technology Selection and Fine-Tuning Strategy",
    "text": "9.5 LLM Technology Selection and Fine-Tuning Strategy\n\n\n\nSlide 07\n\n\nThe authors conceptualised the development of Large Language Models (LLMs) possessing knowledge derived exclusively from publications classified under a specific Sustainable Development Goal (SDG) by a particular bibliometric database. The initial ambition to train such LLMs from scratch solely on these publications proved to be a substantial undertaking due to its resource-intensive nature. Consequently, a compromise involved fine-tuning an existing, pre-trained LLM that possessed minimal prior knowledge, using the abstracts of the selected publications.\nLeading commercial and open-source pre-trained LLMs, such as GPT-4 (with 1.76 trillion parameters), were deemed ineligible for this work. Their unsuitability stemmed from their extensive pre-training datasets, which include sources like Wikipedia and Reddit conversations. This meant they already embedded considerable knowledge about SDGs and possessed strong, pre-existing semantic associations. To circumvent this, the team selected DistilGPT2. This model, a “very light” English-speaking variant of the open-source GPT-2, employs a “distillation” technique (Sanh, 2019) and has significantly fewer parameters (82 million). Its advantages included feasibility for use with proprietary data and its minimally instructed nature, which ensures that its behaviour is more strongly influenced by the fine-tuning data. The authors operated under the premise that DistilGPT2 had no significant prior semantic knowledge relevant to the publications or the prompts used. This approach led to the fine-tuning of 15 distinct DistilGPT2 models, each tailored to a specific combination of one of the three bibliometric databases and one of the five chosen SDGs (DistilGPT2{bibDB, SDG}).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompt-engineering-and-research-design-for-llm-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompt-engineering-and-research-design-for-llm-benchmarking",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.6 Prompt Engineering and Research Design for LLM Benchmarking",
    "text": "9.6 Prompt Engineering and Research Design for LLM Benchmarking\n\n\n\nSlide 10\n\n\nThe United Nations’ Sustainable Development Goals (SDGs) are structured with specific targets. For instance, SDG4 (Quality Education) includes targets such as ensuring universal completion of primary and secondary education (Target 4.1) and equal access to tertiary education (Target 4.3), amongst others (typically 8-12 targets for the SDGs analysed, as per the 2030 Agenda for SDGs, UN). To benchmark the fine-tuned LLMs, the authors developed a systematic approach to prompt engineering. For each individual target within an SDG, they crafted ten diverse questions, or prompts, each designed to probe different facets of that target. This methodology yielded a unique set of 80 to 120 prompts for every SDG under investigation. These prompts established a benchmark, enabling the assessment of the LLMs’ compliance with SDG objectives and the identification of potential “biases” in their responses. For example, for Target 4.1 of SDG4, prompts included questions such as, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” and others addressing strategies, outcome improvements, and challenges.\nThe research design followed a structured workflow for each bibliometric database (DB) and SDG combination:\n\nInitially, a set of publication abstracts, classified under a specific SDG by a particular database, served as the input for fine-tuning a DistilGPT-2 model.\nThe resultant fine-tuned model (Fine-tuned DistilGPT-2 SDG# DB#) then processed the set of prompts specifically designed for that SDG.\nThis processing employed three distinct decoding strategies—top-k, nucleus, and contrastive search—generating three sets of responses.\nFinally, a “prompts’ words filter” was applied to these responses, leading to the extraction of noun phrases (Noun phrases SDG# DB#), which formed the basis for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-results-for-sdg-4-unaddressed-targets-and-biases",
    "href": "chapter_ai-nepi_009.html#illustrative-results-for-sdg-4-unaddressed-targets-and-biases",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.7 Illustrative Results for SDG 4: Unaddressed Targets and Biases",
    "text": "9.7 Illustrative Results for SDG 4: Unaddressed Targets and Biases\n\n\n\nSlide 13\n\n\nThe authors analysed the responses generated by the fine-tuned Large Language Models (LLMs) by matching extracted noun phrases with the official SDG targets. This analysis spanned four key dimensions: Locations, Actors, Data/Metrics, and Focuses. For each SDG, the assessment aimed to determine, firstly, the LLM’s compliance with its targets and, secondly, any discernible biases. Notably, differences in the outputs corresponding to the different source bibliometric databases were also observed.\nTaking SDG4 (Quality Education) as an illustrative example, the LLMs, despite being prompted with target-specific questions, failed to address several critical areas.\n\nIn terms of Locations, whilst countries like South Africa, the U.S., Australia, China, and Hong Kong were mentioned (reflecting the content of unique database subsets), there was a significant omission of African countries (beyond South Africa), Developing Countries more broadly, Least Developed Countries, Other Developing Countries, and Small Island Developing States—all explicitly or implicitly relevant to SDG4 targets.\nRegarding Actors, terms like “Classroom” and “Family” appeared, and general categories such as “All Women and Men,” “Children,” “Teachers,” and “Youth” were addressed. However, crucial vulnerable groups specified in SDG4 targets, including “The Vulnerable,” “Persons With Disabilities,” “Indigenous Peoples,” “Children In Vulnerable Situations,” and “All Learners,” were not adequately covered in the LLM responses.\nThe Data/Metrics dimension saw mentions of “Survey,” “PISA,” “Evaluation,” “Self-Efficacy,” and “Thematic Analysis,” indicating a focus on certain research methodologies and assessment tools.\nFor Focuses, whilst aspects like “Quality Primary and Secondary Education” and “Access” were addressed, numerous target areas remained unmentioned. These unaddressed focuses included “Affordable And Quality Technical, Vocational And Tertiary Education,” “Relevant Skills” for employment, “Vocational Training,” “Scholarships,” “Safe, non-violent, inclusive learning environments,” “Sustainable Lifestyles,” “Human Rights,” “Global Citizenship,” “Appreciation Of Cultural Diversity,” “Free primary and secondary education,” and “Tertiary education.”\n\nThis pattern of overlooking specific target elements, particularly concerning sensitive locations, vulnerable actors, and key educational focuses, emerged as a recurrent finding.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-systematic-oversights",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-systematic-oversights",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.8 Cross-SDG Considerations and Systematic Oversights",
    "text": "9.8 Cross-SDG Considerations and Systematic Oversights\n\n\n\nSlide 14\n\n\nAcross the five Sustainable Development Goals (SDGs) analysed, several consistent patterns and systematic oversights emerged from the LLM responses.\n\nRegarding Locations, least developed countries received scant attention; for instance, Sub-Saharan Africa was only notably mentioned in the context of SDG8. The United States featured with such prominence that it suggested an “undoubted monopoly” in the data’s geographical focus. Following the U.S., South Africa and China were the most frequently cited locations, with the United Kingdom and Australia also appearing.\nConcerning Actors, a particularly troubling finding was the systematic overlooking of discriminated and vulnerable categories of people. This oversight persisted across all five SDGs examined, indicating a significant gap in the LLM-generated content relative to the inclusive aims of the SDGs themselves.\nIn the Metrics dimension, the LLMs frequently referenced various data sources, such as Demographic and Health Surveys (DHS) and World Values Surveys (WVS), alongside numerous other metrics, indicators, and benchmarks. A range of research methodologies also appeared, including theoretical and empirical approaches, thematic analysis, market dynamics, and macroeconomic studies. An interesting distinction arose from the source databases: for three of the SDGs, Web of Science-trained LLMs tended to reflect a more theoretical research approach, whereas those trained on Scopus and OpenAlex data exhibited a more empirical orientation.\nFinally, whilst Focuses were generally SDG-specific, the LLMs often failed to address the most sensitive topics embedded within the SDG targets. Examples of such overlooked critical issues include human trafficking, human exploitation, and migration, all of which are pertinent to the broader aims of socioeconomic equality and development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#summary-of-findings-and-limitations",
    "href": "chapter_ai-nepi_009.html#summary-of-findings-and-limitations",
    "title": "9  SDG-Research in Bibliometric DBs - LLMs for HPSS",
    "section": "9.9 Summary of Findings and Limitations",
    "text": "9.9 Summary of Findings and Limitations\n\n\n\nSlide 18\n\n\nThe investigation’s principal finding highlights that the introduction of Large Language Models (LLMs) as an analytical AI tool, positioned between the initial SDG classification of scientific literature and its use by policymakers, uncovers a systematic oversight within the data. Specifically, scientific publications, when classified by SDGs and processed by these LLMs, tend to neglect the most disadvantaged categories of individuals, the poorest countries, and various underrepresented topics that the SDG targets explicitly aim to address. Conversely, the analysis indicates that substantial attention is directed towards economic superpowers and highly developing nations. This outcome underscores how an ostensibly objective, science-informed practice, such as the bibliometric classification of SDGs, can have decisive and potentially skewed impacts on the perceived landscape of research.\nThe authors acknowledge several limitations inherent in this study.\n\nLLMs exhibit high sensitivity to various factors, including model architecture; although the choice of DistilGPT2 aimed to mitigate some aspects of this, more developed architectures could yield different outcomes.\nSensitivity to training data is another critical factor, partially addressed in this work by utilising three distinct bibliometric databases.\nFurthermore, (hyper-)parameters and the chosen decoding strategy significantly influence LLM outputs; the use of three different decoding strategies (top-k, nucleus, and contrastive search) attempted to account for this variability.\n\nThe study presents a general framework, and it remains possible that its application to very specific, applied cases could produce different results, though the authors express some reservation about this likelihood.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SDG-Research in Bibliometric DBs - LLMs for HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "",
    "text": "Overview\nThe authors have addressed the persistent challenge of extracting citation data from the complex footnotes prevalent in law and humanities scholarship. Historically, bibliometric databases offer inadequate coverage for these domains, primarily due to a lack of commercial interest, a focus on impact factors over intellectual history, and the inherent complexity of humanities footnotes. Traditional machine learning tools, moreover, consistently perform poorly when parsing these intricate structures. Consequently, this project explores the utility of Large Language Models (LLMs) and Vision Language Models (VLMs) as a more effective solution.\nA central tenet of this research involves establishing a robust testing and evaluation framework. To this end, the research team is developing a high-quality gold standard dataset, meticulously annotated using TEI XML encoding. This standard, well-established within the digital humanities, facilitates a comprehensive representation of citation phenomena, including contextual information. Furthermore, it ensures interoperability with existing tools such as Grobid, enabling direct performance comparisons.\nTo operationalise this approach, the authors crafted Llamore, a lightweight Python package. Llamore extracts citation data from raw text or PDFs, exporting it into TEI-formatted XML files. Crucially, it also evaluates extraction performance against gold standard references using an F1-score metric, which accounts for precision and recall through an unbalanced assignment problem. Initial evaluations reveal that whilst Llamore’s resource consumption exceeds that of traditional tools like Grobid for biomedical literature, it significantly outperforms Grobid when processing the challenging, footnoted humanities data. Future work aims to expand the training data, refine evaluation metrics, and enhance Llamore’s capabilities to capture contextual citation information and resolve complex stylistic variations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-citation-graph-generation-for-humanities",
    "href": "chapter_ai-nepi_010.html#challenges-in-citation-graph-generation-for-humanities",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.1 Challenges in Citation Graph Generation for Humanities",
    "text": "10.1 Challenges in Citation Graph Generation for Humanities\n\n\n\nSlide 01\n\n\nThe authors confront a significant challenge: Large Language Models and other algorithms currently struggle with the intricate footnotes characteristic of law and humanities scholarship. Generating comprehensive citation graphs from these sources constitutes a primary objective. Such graphs prove invaluable for intellectual history, enabling scholars to discern patterns and relationships within knowledge production, trace intellectual influences, and quantify the reception of published ideas. For instance, scholars can readily identify the most cited authors within a specific journal over a defined period, as demonstrated by an analysis of the Journal of Law and Society between 1994 and 2003.\nA fundamental impediment arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) literature within existing bibliometric data sources. Leading platforms, including Web of Science, Scopus, and OpenAlex, exhibit substantial deficiencies. Web of Science and Scopus, moreover, impose prohibitive costs and restrictive licensing terms, hindering open research. Whilst OpenAlex offers an open-access alternative, it too lacks comprehensive coverage for many A-journals, pre-digital content, and non-English language publications. For example, the Zeitschrift für Rechtssoziologie, established in 1980, shows negligible citation data before the 2000s within these databases.\nSeveral factors contribute to this persistent data gap. Commercial entities demonstrate limited financial interest in humanities scholarship, unlike their engagement with STEM, medicine, and economics. Furthermore, these databases prioritise “impact factor” metrics for scientific evaluation, a focus that diverges from the needs of intellectual history research. Crucially, the pervasive use of complex footnotes within humanities literature presents a unique parsing challenge, which traditional systems have struggled to overcome.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#addressing-footnote-complexity-with-large-language-models",
    "href": "chapter_ai-nepi_010.html#addressing-footnote-complexity-with-large-language-models",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.2 Addressing Footnote Complexity with Large Language Models",
    "text": "10.2 Addressing Footnote Complexity with Large Language Models\n\n\n\nSlide 07\n\n\nA second, equally pressing problem arises from the inherent complexity of humanities footnotes, often termed “footnotes from hell.” These structures frequently incorporate extensive commentary, extraneous content, and non-reference text, embedding the actual citations within considerable noise. Traditional instruments for extracting such information necessitate laborious manual annotation. Moreover, conventional machine learning tools, including those based on conditional random forests, consistently exhibit poor performance. For instance, the ExCite Performance study (Boulanger/Iurshina 2022) reported low extraction and segmentation accuracies across various training datasets, with combined data yielding an extraction accuracy of merely 0.22 and segmentation accuracy of 0.47.\nConsequently, the authors have turned to Large Language Models (LLMs) as a promising alternative. Initial experiments in 2022, utilising models such as text-davinci-003, demonstrated LLMs’ considerable capacity for extracting references from highly unstructured textual data. Newer models offer even greater potential, whilst Vision Language Models (VLMs) extend this capability to direct processing of PDF documents. The authors employ various methods, including prompt engineering, Retrieval-Augmented Generation (RAG), and fine-tuning, to optimise these models.\nNevertheless, a crucial concern persists regarding the trustworthiness of LLM-generated results, particularly the risk of hallucinations. A notable incident involved a lawyer who, relying on ChatGPT, submitted a federal court filing citing at least six non-existent cases. Addressing this fundamental issue demands a robust testing and evaluation solution. Such a solution requires a high-quality gold standard dataset, a flexible framework capable of adapting to the rapidly evolving technology landscape, and solid testing algorithms to generate comparable performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-dataset",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard-dataset",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.3 Developing a TEI-Annotated Gold Standard Dataset",
    "text": "10.3 Developing a TEI-Annotated Gold Standard Dataset\n\n\n\nSlide 13\n\n\nTo address the need for reliable evaluation, the authors have embarked upon compiling a comprehensive training and evaluation dataset, employing TEI XML encoding. This choice rests upon several compelling reasons. TEI XML represents a well-established, precisely specified, and comprehensive standard for text interchange within the digital humanities. Crucially, it encompasses a far broader range of phenomena than more restrictive bibliographical standards, such as CSL or BibTeX. Indeed, TEI extends beyond mere reference management, allowing for the encoding of citations, cross-references, and other contextual markup, which proves vital for classifying citation intention. Furthermore, adopting this standard enables the project to leverage existing digital editions, text collections, and corpora, thereby enhancing the generalisation and robustness of the developed mechanisms.\nNevertheless, the TEI standard presents its own set of challenges, both conceptual and technical. Conceptual difficulties arise in differentiating between pointers and references, whilst technical complexities involve managing constrained elements versus elliptic material. Despite these hurdles, the dataset’s establishment progresses steadily. The encoding process involves multiple stages: capturing PDF screenshots, segmenting reference strings to distinguish them from non-reference footnote text, and finally, generating parsed structured data. The dataset currently comprises 1,100 footnotes and endnotes, drawn from 25 articles across 10 Directory of Open Access Journals (DOAJ) titles. It specifically focuses on humanities scholarship, particularly legal and historical texts, and encompasses a diverse range of languages, including French, German, Spanish, Italian, and Portuguese, spanning the period from 1958 to 2018. The authors estimate the dataset will contain over 1,600 references, with individual occurrences encoded separately to preserve contextual information. Notably, the project adjusted its strategy midway, shifting to Open Access journals and incorporating PDFs to facilitate Vision Language Model (VLM) mechanisms and enable the full publication of the dataset.\nThe interoperability afforded by the TEI XML standard offers a significant advantage, enabling seamless integration with existing tooling. Grobid, a widely recognised tool for reference and information extraction, notably utilises TEI XML for its training and evaluation processes. Consequently, this shared data format permits direct performance comparisons with Grobid and facilitates the exchange of training data, benefiting both the project and the broader research community.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction-and-evaluation",
    "href": "chapter_ai-nepi_010.html#llamore-a-python-package-for-reference-extraction-and-evaluation",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.4 Llamore: A Python Package for Reference Extraction and Evaluation",
    "text": "10.4 Llamore: A Python Package for Reference Extraction and Evaluation\n\n\n\nSlide 14\n\n\nThe authors have developed Llamore, a Python package acronym for “Large LANguage MOdels for Reference Extraction.” This tool facilitates two primary functions: extracting citation data from raw text or PDF inputs using multimodal Large Language Models, and subsequently evaluating the extraction performance. The workflow proceeds from text or PDF documents, through Llamore, to produce references in TEI XML format. These extracted references then undergo comparison with gold standard references, yielding an F1-score as an evaluation metric.\nCrafting Llamore involved two key objectives. Firstly, the package needed to remain lightweight, comprising fewer than 2,000 lines of code. Crucially, Llamore operates as an interface to a model of the user’s choosing, rather than embedding any specific model directly. Secondly, this design ensures broad compatibility with both open and closed Large Language Models and Vision Language Models.\nImplementing Llamore proves straightforward. Users can install the package directly from PyPI using pip install llamore. For extraction, one imports the relevant extractor, such as GeminiExtractor or OpenaiExtractor, then instantiates it with an API key. The extractor processes either a PDF file path or a raw input string, returning a collection of references that can then be exported to a TEI XML file. Notably, the OpenaiExtractor provides compatibility with numerous open model serving frameworks, including Olama and VLLM, which offer OpenAI-compatible API endpoints. For evaluation, users import the F1 class, configure it (e.g., levenshtein_distance=0 for exact matches), and compute the macro average F1-score by supplying both the extracted and gold references.\nLlamore employs the F1-score, a widely recognised metric for comparing structured data, to assess extraction performance. This score combines precision (the ratio of matches to predicted elements) and recall (the ratio of matches to gold elements) into a single harmonic mean. A perfect extraction yields an F1-score of 1, whilst an F1-score of 0 indicates no matches. For instance, in comparing an extracted reference to a gold standard, Llamore identifies matches for analytic_title, monographic_title, authors.surname, and publication_date, whilst noting a minor discrepancy in authors.forename due to an extraneous character in the gold reference. Furthermore, Llamore addresses the complex task of aligning extracted references with gold references by framing it as an unbalanced assignment problem. The tool computes F1 scores for every possible combination, constructs a matrix, and then maximises the total F1-score whilst ensuring a unique assignment, utilising SciPy’s solver for this optimisation. Significantly, the system penalises both missing and hallucinated references by assigning them an F1-score of zero.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "href": "chapter_ai-nepi_010.html#performance-analysis-and-future-directions",
    "title": "10  Parsing Footnotes in Law and Humanities Scholarship with Large Language Models",
    "section": "10.5 Performance Analysis and Future Directions",
    "text": "10.5 Performance Analysis and Future Directions\n\n\n\nSlide 20\n\n\nInitial performance evaluations provide crucial insights into Llamore’s efficacy across diverse datasets. When tested on the PLOS 1000 Dataset, which comprises 1,000 biomedical PDFs and demands exact matches, Grobid achieved an F1 score of 0.61, whilst Llamore, utilising Gemini 2.0 Flash, attained a comparable F1 score of 0.62. However, for literature on which Grobid was specifically trained, it demonstrates superior efficiency, operating considerably faster and with fewer computational resources; Llamore’s compute requirements, conversely, are orders of magnitude greater.\nA more compelling distinction emerges when evaluating performance on the project’s bespoke humanities dataset, which features complex footnotes and also requires exact matches. Here, Grobid struggles significantly, yielding an F1 score of only 0.14, largely due to its training data being out of distribution for such intricate structures. In stark contrast, Llamore (Gemini 2.0 Flash) achieves an F1 score of 0.45, representing a threefold improvement in performance. Nevertheless, this current performance metric pertains solely to pure reference extraction, excluding the capture of contextual information or cross-referencing.\nFuture work outlines several key objectives. The authors plan to generate additional training data and further refine the test metrics. Crucially, they aim to extend Llamore’s capabilities to support citations in context, discerning whether a work is cited approvingly or critically. Furthermore, the tool will incorporate features for resolving op cit. references, identifying specific pages cited, and quantifying multiple citations to the same work. Addressing these enhancements will necessitate overcoming several challenges, including the wide variation in citation styles (e.g., differentiating between volumes and pages, or first page versus cited page), the complexities of multilingual terminology (e.g., diverse contributor roles like “eds” or “hrsg. v.”, and special terms such as passim, ibid, or n.d.), the intricacies of canonical citations prevalent in fields like Bible studies or Roman law, and the accurate handling of ellipses, abbreviations, and cross-references.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Parsing Footnotes in Law and Humanities Scholarship with Large Language Models</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview\nThe DANS and GESIS teams have pioneered an AI-driven solution to manage the escalating volume of scientific information. This initiative directly addresses the challenge of continuous growth and increasing differentiation within the sciences, which complicates the review, evaluation, and selection of pertinent content. Efficiently locating and comprehending existing information forms a fundamental precondition for generating new knowledge, whether individually or across broader academic communities. Consequently, the project investigates whether contemporary AI, particularly Large Language Models (LLMs), can genuinely support the knowledge production process through advanced information retrieval.\nThe core research question explores the feasibility of constructing an AI solution that facilitates conversational interaction with academic papers drawn from specific collections. The team has crafted a dual-component system: Ghostwriter, serving as the user interface, and EverythingData, which encompasses the comprehensive backend operations. This architecture seamlessly integrates principles of information retrieval, human-machine interaction, and Retrieval-Augmented Generation (RAG). A practical use case involves the method-data-analysis (mda) journal, demonstrating a ‘local’ and ‘tailored’ AI solution workflow.\nGhostwriter redefines information retrieval by enabling simultaneous interaction with structured data—akin to a librarian—and natural language—much like an expert. This approach leverages knowledge organisation systems and classification schemes whilst interpreting natural language queries. The underlying technical infrastructure, EverythingData, processes input document collections, such as articles from the mda journal, by ingesting them into a vector store, specifically Qdrant. It performs crucial operations, including term extraction, embedding construction, and, pivotally, coupling these with knowledge graphs. This integration enriches embeddings by contextualising them, thereby adding significant value to the information.\nThe system’s design prioritises factual accuracy, aiming to prevent hallucinations by relying exclusively on the ingested source material. It segments papers into small, identifiable blocks, employing LLM techniques to intelligently connect and retrieve relevant sections. Knowledge graphs further enhance this process, predicting which text segments will best address specific queries. A key innovation involves linking extracted entities to Wikidata, transforming free strings into multilingual identifiers. This mechanism supports immediate multilinguality, allowing users to query in one language and retrieve information from documents in another. Moreover, this decoupling of knowledge from the model, storing it as Wikidata identifiers, establishes a robust ground truth for benchmarking and validating future AI models. The project envisions this knowledge organisation system as the sustainable future for scientific information management, fostering collaborations with industry leaders like Google and Meta.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#addressing-information-overload-in-science-dynamics",
    "href": "chapter_ai-nepi_011.html#addressing-information-overload-in-science-dynamics",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Addressing Information Overload in Science Dynamics",
    "text": "11.1 Addressing Information Overload in Science Dynamics\n\n\n\nSlide 01\n\n\nThis collaborative endeavour, uniting DANS—the data archive of the Royal Netherlands Academy of Arts and Science—with GESIS, an archive actively engaged in research, directly addresses a critical challenge within contemporary science. The sciences exhibit continuous growth and increasing differentiation, which consequently complicates the processes of reviewing, evaluating, and selecting pertinent information. Generating new knowledge, whether within individual minds or across broader academic communities, fundamentally necessitates the capacity to locate and comprehend existing information effectively. Therefore, a central research question explores whether advanced AI systems can genuinely support the knowledge production process, specifically focusing on information retrieval.\nSlava Tikhonov, a senior research engineer at DANS, initiated this project following his extensive experimentation, meticulously constructing complex data pipelines. Rather than simple linear pipelines, these systems represent intricate “backs of things,” as one colleague aptly described, making them challenging to unravel and explain. Consequently, the project aims to illustrate these AI capabilities in a manner accessible to a broader audience. From a wider perspective, this initiative seeks to harness AI’s potential to manage the overwhelming deluge of information in which researchers increasingly find themselves immersed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-overview-and-core-components",
    "href": "chapter_ai-nepi_011.html#system-overview-and-core-components",
    "title": "11  Science dynamics and AI",
    "section": "11.2 System Overview and Core Components",
    "text": "11.2 System Overview and Core Components\n\n\n\nSlide 02\n\n\nA central research question drives this project: can the team construct an AI solution enabling conversational interaction with academic papers drawn from a specific selection? This inquiry integrates several key concepts: information retrieval, the dynamics of human-machine interaction, and the principles of Retrieval-Augmented Generation (RAG). The team selected the method-data-analysis (mda) journal as a pertinent use case, providing a concrete context for demonstrating the system’s capabilities.\nThe project introduces a workflow for a ‘local’ or ‘tailored’ AI solution, comprising two primary components. The developers affectionately named these Ghostwriter, which functions as the user interface, and EverythingData, which serves as a comprehensive summary for all underlying backend operations. The presentation further illustrates both the front-end user experience and the intricate back-end processes, culminating in a summary and outlook on future developments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-paradigm",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-novel-information-retrieval-paradigm",
    "title": "11  Science dynamics and AI",
    "section": "11.3 The Ghostwriter Interface: A Novel Information Retrieval Paradigm",
    "text": "11.3 The Ghostwriter Interface: A Novel Information Retrieval Paradigm\n\n\n\nSlide 03\n\n\nThe Ghostwriter approach introduces a novel paradigm for information retrieval, fundamentally altering how users interact with data. This system conceptualises queries across various levels of complexity and data representation. Initially, a direct query to a single database representation necessitates prior knowledge of its schema and typical values to yield results, akin to a user interacting solely with a database. Progressing beyond this, a query directed at a data collection or space, underpinned by connected structured databases or graphs, prompts the system to suggest similar or improved queries based on schema interconnections, whilst providing a list of potential results. This interaction mirrors a user consulting a librarian.\nFurther advancing, a query posed to a Large Language Model (LLM) interprets natural language input and generates results, also expressed in natural language. This scenario evokes the experience of engaging with a library or a panel of experts. Crucially, the Ghostwriter system integrates a local LLM with a target data collection, embedding it within a network of additional data interpretation sources accessible via APIs. This sophisticated setup generates a family of terms around the query, identifies related structured information, and ultimately returns a comprehensive list of results. This advanced interaction simulates a user simultaneously chatting with both experts and librarians.\nTraditionally, information retrieval has grappled with the challenge of formulating the precise query. Whilst Google features and schema.org enable machines to make informed guesses about user queries, these typically operate within a web-based context, not a local interaction. Ghostwriter, however, through its iterative application, empowers users to reformulate their questions, thereby fostering a deeper understanding of their actual intent and the available data space. This innovative interface, drawing on the metaphors of a “librarian”—representing structured data, Knowledge Organisation Systems (KOS), and historical classifications—and an “expert”—embodying natural language—claims to facilitate simultaneous conversational interaction with both.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Retrieval-Augmented Generation (RAG) Architecture",
    "text": "11.4 Retrieval-Augmented Generation (RAG) Architecture\n\n\n\nSlide 04\n\n\nScientifically, this project firmly situates itself within the broader discourse surrounding Retrieval-Augmented Generation (RAG). Philip Rattliff’s paper from Neo4j offers a highly recommended introduction to this intricate topic. The system’s architecture fundamentally relies upon two main ingredients: a vector space and a knowledge graph. The developers construct the vector space from the content of data files, encoding this information into embeddings. Various Machine Learning (ML) algorithms compute these embeddings, employing diverse Large Language Models (LLMs) in the process.\nConversely, the knowledge graph represents a sophisticated metadata layer. Engineers integrate this layer with a range of ontologies and controlled vocabularies, notably incorporating principles of responsible AI. The Croissant ML standard precisely expresses this graph. The overarching vision for this system involves seamlessly combining both graph and vector representations into a unified model, termed GraphRAG. The developers implement this approach ‘locally’, conceptualising it as a form of Distributed AI. Within this framework, the LLM assumes a dual role, functioning both as an interface between human users and the AI, and as a powerful reasoning engine. Operationally, the LLM connects to a ‘RAG library’—the knowledge graph—enabling it to navigate through datasets and consume embeddings, or vectors, as contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#operational-workflow-and-data-processing",
    "href": "chapter_ai-nepi_011.html#operational-workflow-and-data-processing",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Operational Workflow and Data Processing",
    "text": "11.5 Operational Workflow and Data Processing\n\n\n\nSlide 05\n\n\nThe system initiates its operational workflow by ingesting a collection of articles, specifically those sourced from the MDA journal. The developers scraped a limited number of these articles for the current implementation; however, the architecture accommodates any collection of documents as input. This raw information then enters the EverythingData component, which orchestrates a series of sophisticated operations.\nInitially, the system stores this information within a vector store, utilising Qdrant for this purpose. Subsequently, it performs crucial processes, including term extraction and the construction of embeddings. A pivotal aspect of this workflow involves coupling the processed information with knowledge graphs. This integration significantly enhances the value of words, phrases, and embeddings by contextualising them, effectively enriching the overall context. All this meticulously processed data then feeds into a unified vector space, forming the RAG-Graph. The Ghostwriter interface interacts directly with this vector space and its integrated graph. Users formulate questions in natural language, and in response, the system provides both a list of relevant documents and a concise summary text, reflecting its understanding of the query.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-implementation-and-hallucination-prevention",
    "href": "chapter_ai-nepi_011.html#ghostwriter-implementation-and-hallucination-prevention",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter Implementation and Hallucination Prevention",
    "text": "11.6 Ghostwriter Implementation and Hallucination Prevention\n\n\n\nSlide 07\n\n\nThe developer, whilst having engaged with early iterations of LLMs such as GPT-2 in 2020, expresses a nuanced perspective, focusing instead on deconstructing and repurposing their training processes. This work, initially conceived for academic papers, demonstrates remarkable versatility, extending its application to any web content or even spreadsheets, enabling users to query specific values. A cornerstone of its design lies in its robust mechanism for preventing hallucinations: the system exclusively draws information from the provided source material. Consequently, if a query pertains to information absent from the ingested data, the system transparently responds with “I don’t know.”\nNotably, this implementation employs a relatively simple 1 billion parameter LLM, yet it effectively addresses complex questions through the strategic integration of knowledge graphs. For instance, papers from the MDA, a GESIS journal, have been ingested into Ghostwriter, forming a distinct collection. A core principle guiding this system dictates that it does not rely on any knowledge pre-ingested into the LLM. Instead, its explicit goal is to furnish only factual information directly present within the specific paper under scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-iterative-refinement",
    "href": "chapter_ai-nepi_011.html#ghostwriter-in-practice-querying-and-iterative-refinement",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Ghostwriter in Practice: Querying and Iterative Refinement",
    "text": "11.7 Ghostwriter in Practice: Querying and Iterative Refinement\n\n\n\nSlide 10\n\n\nDemonstrating its capabilities, the system processes a query such as “explain male breadwinner model to me,” providing a comprehensive explanation of the concept. Crucially, it accompanies this explanation with a detailed list of references, each entry specifying a chat number, article title, direct URL, and a relevance score. This meticulous referencing ensures the system’s output remains grounded in verifiable sources, effectively preventing hallucinations by precisely identifying where information originates.\nInternally, the system operates by segmenting each paper into small, distinct blocks, assigning a unique identifier to every block. It then employs sophisticated LLM techniques to intelligently connect and retrieve these blocks, applying weights and leveraging knowledge graphs to predict which text segments will most accurately respond to a given question. This iterative approach is evident when a refined query, for instance, “explain how data was collected on male breadwinner model,” yields a response indicating “no direct information” if the content is not present within the ingested papers. Furthermore, a user-friendly “Add paper” button empowers users to contribute new articles, seamlessly integrating fresh content for subsequent queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#underlying-architecture-entity-extraction-and-multilinguality",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Underlying Architecture: Entity Extraction and Multilinguality",
    "text": "11.8 Underlying Architecture: Entity Extraction and Multilinguality\n\n\n\nSlide 11\n\n\nThe system’s robust underlying architecture comprises several interconnected components: entity extraction, knowledge graph linking, multilinguality support, and summary generation. An entity extraction pipeline meticulously annotates terms with semantic meaning, mapping them to controlled vocabularies and thereby transitioning information from the vector space into the knowledge graph. This process extends to linking extracted entities to broader knowledge graph representations, notably Wikidata. Knowledge graphs assume critical importance, serving as a “ground truth” against which the accuracy of LLM-generated answers can be rigorously validated.\nFurthermore, the system incorporates immediate multilinguality support, a vital feature enabling it to process papers in diverse languages, such as Chinese or German, whilst responding to queries posed in English. Ultimately, the LLM synthesises these disparate pieces of text to produce the final results, including summary or explanatory content. The fact extraction process begins by segmenting the user’s query into smaller components, which are then mapped to a Knowledge Organisation System (KOS). This KOS possesses the unique characteristic of iteratively generating new levels of terms, enriching the semantic understanding.\nCrucially, the system links these extracted entities to Wikidata, transforming free strings into unique identifiers. These identifiers, in turn, connect to multilingual translations, providing comprehensive properties that facilitate cross-language comprehension. For instance, the core concept of a query, such as “bread winner mo,” can be translated by LLM/Gemma3 into hundreds of languages. This mechanism establishes a robust ground truth by decoupling knowledge from both questions and papers, storing it externally as Wikidata identifiers. Consequently, researchers can benchmark different models by testing them against the same list of identifiers, precisely assessing their suitability and identifying any inaccuracies. The project’s proponents envision the Knowledge Organisation System as the future of sustainable information management, actively pursuing collaborations with industry leaders like Google and Meta to realise this vision.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-interacting-with-ghostwriter",
    "href": "chapter_ai-nepi_011.html#live-demonstration-interacting-with-ghostwriter",
    "title": "11  Science dynamics and AI",
    "section": "11.9 Live Demonstration: Interacting with Ghostwriter",
    "text": "11.9 Live Demonstration: Interacting with Ghostwriter\n\n\n\nSlide 15\n\n\nA live demonstration showcased the Ghostwriter interface, accessible via the GESIS Leibniz-Institut für Sozialwissenschaften website, specifically within its “Ask Questions” section. Users interact with an input field labelled “Enter your question:”, submitting queries via a prominent red “Ask” button. The system also provides collection management functionalities, including an “Add New Collection” dropdown and an “Available Collections” section, where the mda (Methods, Data, Analyses) journal collection was selected, indicating “Vectors 37,637” as its size.\nDuring the demonstration, a query for “Rational Choice Theory” yielded a concise summary compiled from various papers, accompanied by precise references detailing titles, URLs, and relevance scores. A subsequent, more specific query, “explain utility in Rational Choice Theory,” prompted the system to select distinct pieces of information, presenting varied results whilst consistently pointing to the same foundational papers. An available API further extends the system’s utility, enabling an automatic mode for agentic architectures, which can collect results and identify new knowledge. Users can also expand the system’s knowledge base via an “Add Page” section, allowing the input of webpage URLs or RSS feeds, with options for single webpages, website crawling, or RSS feed integration. Notably, the demonstration highlighted the system’s robust multilingual capabilities: a query posed in English successfully retrieved and processed information from a source paper written entirely in German, save for its abstract.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-vision-and-future-implications",
    "href": "chapter_ai-nepi_011.html#project-vision-and-future-implications",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Project Vision and Future Implications",
    "text": "11.10 Project Vision and Future Implications\nThis project champions a key benefit: the provision of local control and cost-effectiveness, contrasting sharply with reliance on large, external AI models. The interaction with the system is conceptualised as conversing with an “invisible college,” a dynamic exchange designed to provoke human thinking and assist in formulating precise research questions. Crucially, the AI’s role remains one of support for the human thought process; it does not aim to furnish ultimate factual answers or to independently formulate research questions for its users.\nFrom a strategic perspective, the project explicitly avoids a business model centred on developing and selling software. Instead, it prioritises collaborative engagements, particularly with partners who present concrete research questions. The team actively seeks resources to facilitate initial try-outs, intending thereafter to hand over the developed solutions to collaborators. This handover model empowers partners to further tinker with, validate, and polish the systems, fostering a sustainable ecosystem for scientific inquiry.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "",
    "text": "12.1 Overview\nPhilosophers frequently grapple with complex research questions that demand precise linguistic and semantic accuracy. Conventional Large Language Models (LLMs), whilst offering differentiated responses, encounter significant limitations concerning access to full texts, context window capacity, and reliable attribution. Consequently, the research team developed Retrieval-Augmented Generation (RAG) systems to overcome these challenges. This approach integrates a data source, a retrieval mechanism (employing semantic, hybrid, or classic search), and a prompt augmentation process, thereby enabling LLMs to access and cite original source material.\nThe system’s utility extends across both didactics and research. For pedagogical purposes, it facilitates interactive engagement with philosophical corpora, such as Locke’s Oeuvre, allowing students to delve deeply into complex texts. In research, RAG systems streamline fact-finding in handbooks, enable exploration of previously unexamined corpora, assist in identifying passages for close reading, and potentially provide detailed answers to intricate research questions.\nA practical implementation involved the authors coding a RAG system using the Stanford Encyclopedia of Philosophy as its data source. Initially conceived as a community tool, this project evolved into a qualitative study examining RAG system setup for philosophical inquiry. The development process, characterised by theoretically grounded trial and error, revealed the critical importance of hyperparameter optimisation and robust evaluation criteria. Notably, optimising chunk size proved crucial; selecting main sections as retrieval documents, despite their length, yielded superior results for this highly systematised source.\nRAG systems demonstrably integrate verbatim corpora and domain-specific knowledge, significantly reducing hallucinations and enabling the citation of relevant documents. Nevertheless, their effective deployment necessitates extensive tweaking and rigorous evaluation by domain experts, as optimal configurations vary across domains and corpus types. A peculiar observation indicates that RAGs sometimes perform less effectively on broad overview questions, as their focus on local information can obscure the larger perspective. Future developments aim to create more flexible, agentic RAG systems capable of discerning question types and adapting their approach accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.2 Addressing Philosophical Research Challenges with RAG Systems",
    "text": "12.2 Addressing Philosophical Research Challenges with RAG Systems\n\n\n\nSlide 01\n\n\nPhilosophers frequently pose intricate research questions, such as discerning Aristotle’s theory of matter within his Physics or tracing the evolution of Einstein’s concept of locality from his early relativity works to his 1948 paper, “Quantenmechanik und Wirklichkeit.” Whilst contemporary Large Language Models (LLMs) like ChatGPT offer seemingly decent, differentiated answers to such queries, they encounter fundamental limitations.\nCrucially, LLMs suffer from an “access problem.” Although full texts might have featured in their training data, these models lack direct, on-demand access to the complete works. Consequently, an LLM cannot reliably quote specific chapters or papers verbatim; it either states an inability to quote or, more problematically, hallucinates content. Whilst online search capabilities can sometimes retrieve quotes where copyright permits, reproduction remains a complex issue. Fundamentally, LLM training mechanisms actively prevent verbatim memorisation, instead fostering the learning of generalisable statistical rules for text production. Philosophical research, however, demands direct engagement with original text sources, requiring deep immersion in their fine-grained formulations.\nBeyond this, LLMs contend with a “limited context window.” For instance, ChatGPT-4o offers 128,000 tokens of context; whilst substantial, this capacity quickly proves insufficient when processing extensive philosophical corpora. Furthermore, a significant “attribution problem” persists: standard LLMs do not inherently provide sources or citations for their claims, a critical requirement for academic rigour. Researchers desire precise, numbered citations for each central statement, akin to features found in tools like Perplexity.\nRetrieval-Augmented Generation (RAG) systems directly address these pervasive challenges. A RAG setup integrates a specific data source, such as Aristotle’s or Einstein’s corpus, with a robust retrieval mechanism. This mechanism typically employs semantic search, though hybrid or classic search options also exist. Subsequently, the system augments the LLM’s prompt with relevant chunks of text retrieved from the corpus. This innovative architecture effectively solves the access problem by furnishing the LLM with the necessary full text, mitigates the limited context window by supplying only pertinent information, and resolves the attribution issue by enabling direct citation of sources.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#diverse-applications-of-rag-systems-in-philosophy",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.3 Diverse Applications of RAG Systems in Philosophy",
    "text": "12.3 Diverse Applications of RAG Systems in Philosophy\n\n\n\nSlide 11\n\n\nThe fundamental concept underpinning RAG systems in philosophy involves enabling interactive engagement with extensive philosophical corpora, such as Locke’s Oeuvre, whilst surpassing the capabilities of conventional LLMs. This approach furnishes users with significantly more detailed domain knowledge and a verifiable verbatim text basis.\nBeyond its research utility, the system offers substantial didactic advantages. Repeated questioning, a core feature, proves highly instructive for students. It allows them to approach complex texts, like Locke’s essay, by initially chatting with the corpus to grasp general concepts, then progressively deepening their inquiry into specific areas such as epistemology or the theory of matter. This interactive method provides an effective pathway for students to immerse themselves in philosophical texts.\nCrucially, RAG systems hold considerable promise for research.\n\nFirstly, they facilitate reliable fact lookup in handbooks, providing accurate information for orientation, remarks, and footnotes—a significant improvement over the often unreliable factual output of standalone LLMs.\nSecondly, researchers can employ RAGs to explore previously unexamined corpora; whilst digitisation of unpublished texts remains a prerequisite, the system then allows for a deeper, interactive overview of their content.\nThirdly, RAGs assist in identifying specific passages relevant for close reading, streamlining the research process.\n\nUltimately, these systems may even generate detailed answers to components of complex research questions, painting a compelling picture of future possibilities within philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "href": "chapter_ai-nepi_012.html#developing-the-stanford-encyclopedia-of-philosophy-rag-system",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.4 Developing the Stanford Encyclopedia of Philosophy RAG System",
    "text": "12.4 Developing the Stanford Encyclopedia of Philosophy RAG System\n\n\n\nSlide 14\n\n\nThe authors undertook the development of an example RAG system, utilising the Stanford Encyclopedia of Philosophy, a widely recognised online handbook, as its primary data source. Initially, the project involved scraping the encyclopedia’s content and converting it into Markdown format.\nThe project’s initial aim centred on crafting a practical tool for the philosophical community. However, during the coding and testing phases, the system’s performance proved unexpectedly poor; initial answers were inferior to those generated by ChatGPT alone. This necessitated a shift in focus, transforming the endeavour into a qualitative study on optimising RAG system configurations specifically for philosophical applications.\nThe development methodology adopted a theoretically grounded trial-and-error approach. This involved extensive tweaking of various parameters, including generative models, hyperparameters, and retrieval algorithms such as reranking, to enhance answer quality. A critical discovery during this process was the paramount importance of sound evaluation standards. Unlike historical research, which might seek atomic facts, philosophical inquiries often yield complex, unstructured textual propositions. Consequently, evaluating these answers demands a nuanced assessment of their factual accuracy, a task proving far from straightforward.\nThe implemented system features a user-friendly frontend. This interface presents input fields for selecting the generative model, defining prompt token limits, specifying a persona, and entering the philosophical question—for instance, “What is priority monism?” The output section provides a comparative display, benchmarking the answer generated by the LLM alone against that produced by the RAG system, thereby facilitating direct comparison. Furthermore, a “Retrieved Texts Overview” details the article names, section headings, token lengths, and indicates which texts were successfully included in the prompt or truncated due to limitations. Powering this interface, the backend comprises a few thousand lines of Python code, orchestrating the system’s complex operations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "href": "chapter_ai-nepi_012.html#hyperparameter-optimisation-the-critical-role-of-chunk-size",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.5 Hyperparameter Optimisation: The Critical Role of Chunk Size",
    "text": "12.5 Hyperparameter Optimisation: The Critical Role of Chunk Size\n\n\n\nSlide 15\n\n\nOptimising hyperparameters constitutes a critical phase in RAG system development, with chunk size serving as a prime example. Initially, the developers considered three primary options for defining text chunks: a fixed number of words—typically around 500 tokens, a clean criterion favoured in computer science—or alternatively, paragraphs or sections, whether at a low or high level of granularity.\nSurprisingly, empirical testing revealed that employing main sections as the retrieval documents yielded the most favourable results. This outcome defied initial expectations, given that the embedding model’s cutoff stood at approximately 500 words, whilst the average section length extended to around 3,000 words. The success of this seemingly counter-intuitive approach stems from the highly systematised nature of the Stanford Encyclopedia of Philosophy. Within this structured work, the initial 500 words of any given section typically encapsulate its core ideas. Consequently, retrieving entire sections, despite their length, provides sufficient context for the LLM to formulate accurate responses. Nevertheless, this specific optimisation may not generalise effectively to more heterogeneous or less rigorously sectioned textual corpora.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "href": "chapter_ai-nepi_012.html#key-findings-and-future-directions-for-rag-systems",
    "title": "12  RAG Systems for Philosophical Research",
    "section": "12.6 Key Findings and Future Directions for RAG Systems",
    "text": "12.6 Key Findings and Future Directions for RAG Systems\n\n\n\nSlide 18\n\n\nRAG systems offer compelling advantages for academic and scientific endeavours. They seamlessly integrate verbatim corpora with specialised domain knowledge, thereby dramatically reducing instances of hallucination. Furthermore, these systems inherently cite relevant documents for their answers, making them exceptionally well-suited for assisting in diverse scientific tasks.\nNevertheless, several cautions and challenges accompany their deployment. Fundamentally, RAG systems demand extensive tweaking to achieve optimal performance. Consequently, sound evaluation becomes paramount, necessitating a representative set of questions and anticipated answers. Crucially, domain experts must conduct this evaluation, as the optimal RAG setup remains highly specific to the particular domain, corpus type, and nature of the questions posed. A notable challenge arises when no relevant documents are retrieved, leading to a discernible decrease in answer quality; this scenario often requires prompt adjustment.\nIntriguingly, RAG systems frequently yield inferior results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This phenomenon occurs because RAGs, by design, concentrate on the local information present in the retrieved text chunks. For broad overview questions, this focus on granular facts can inadvertently distract from the larger, synthesised perspective required. Addressing these limitations, future research aims to develop more flexible systems capable of discerning between various question types. This progression will ultimately lead towards the development of sophisticated, agentic RAG systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG Systems for Philosophical Research</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "",
    "text": "Overview\nIn a collaborative endeavour, the authors, alongside Mike Schneider of the University of Missouri, address fundamental questions within the philosophy of science. Their methodology seamlessly integrates computational linguistic techniques with social network analysis. This investigation unfolds through three distinct phases: initially, the team presents a case study focusing on quantum gravity, thereby establishing its philosophical framework; subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape; finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.\nThe enduring challenge of formulating a quantum theory of gravity, which seeks to reconcile knowledge of small and large scales, forms the core problem. Numerous attempted solutions exist, including string theory, supergravity, loop quantum gravity, spin foams, causal set theory, and asymptotic safety. To characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. They define this as distinct yet concurrent instances of normal science dedicated to a common problem-solving goal. Each instance articulates through a social community intertwined with an intellectual disciplinary matrix, drawing upon Kuhnian paradigms, Laudan’s research traditions, and Lakatos’ research programmes. This framework poses an empirical question: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?\nTo answer this, the authors undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape, encompassing both its linguistic/intellectual and social structures. They amassed a substantial dataset of 228,748 theoretical physics abstracts and titles from Inspire HEP. Their methodology involved two principal stages: linguistic analysis, employing the Bertopic pipeline for spatialisation into an embedding space, unsupervised clustering (K=611 topics), and specialty assignment to physicists; and social network analysis, utilising a co-authorship graph of 30,000 physicists with community detection (C=819 communities).\nA key challenge arose from the scale-dependency of computational notions of topics and communities, exacerbated by the inherently nested nature of research programmes. To address this, the authors developed a hierarchical reconstruction strategy, employing Ward agglomerative clustering for topics and hierarchical stochastic block modelling for communities. They then devised an adaptive topic coarse-graining strategy, guided by the Minimum Description Length (MDL) criterion, to identify the optimal scale by balancing model fit to social structure against model complexity.\nThe bottom-up analysis yielded 50 coarse-grained topics, which the team then correlated with community structures. Findings revealed that whilst some topics aligned well with specific communities, others proved universally relevant. Notably, the bottom-up approach identified a large string theory cluster encompassing supergravity, aligning with physicists’ intuitions that these, despite historical differences, are not meaningfully separable at certain scales. This suggests that linguistic nuances without social consequences are effectively stripped away. The study concludes that socio-epistemic systems operate at multiple scales, necessitating cross-scale matching for identifying plural pursuit. It underscores the transformative potential of computational methods in revisiting and challenging long-held philosophical insights.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "href": "chapter_ai-nepi_015.html#philosophical-framing-and-research-trajectory",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.1 Philosophical Framing and Research Trajectory",
    "text": "13.1 Philosophical Framing and Research Trajectory\n\n\n\nSlide 01\n\n\nIn a collaborative endeavour, the authors, alongside Mike Schneider of the University of Missouri, address fundamental questions within the philosophy of science. Their methodology integrates computational linguistic techniques, previously explored, with social network analysis. This investigation unfolds through three distinct phases. Initially, the team presents a case study focusing on quantum gravity, establishing its philosophical framework. Subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape. Finally, the study confronts this empirically derived reconstruction with physicists’ own perceptions of their field’s structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity-and-the-concept-of-plural-pursuit",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit",
    "text": "13.2 The Problem of Quantum Gravity and the Concept of Plural Pursuit\n\n\n\nSlide 01\n\n\nFundamental physics grapples with the enduring challenge of formulating a quantum theory of gravity, a theoretical endeavour seeking to reconcile our understanding of phenomena at both minute and vast scales. Numerous solutions have emerged, notably string theory, which remains the most prominent. Further approaches encompass Supergravity, Loop quantum gravity, spin foams, Causal set theory, and Asymptotic safety. To characterise this multifaceted situation, the authors introduce the concept of ‘plural pursuit’. This concept delineates situations where distinct, yet concurrent, instances of ‘normal science’ converge on a shared problem-solving objective, specifically the reconciliation of quantum mechanics and gravitation. Crucially, each such instance of normal science articulates through the interplay of a social community and an intellectual disciplinary matrix—a framework drawing upon Kuhnian paradigms, Laudan’s research traditions, and Lakatos’ research programmes. Consequently, an empirical question arises: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently pursuing disparate paradigms?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-data-and-methodologies",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.3 Bottom-Up Reconstruction: Data and Methodologies",
    "text": "13.3 Bottom-Up Reconstruction: Data and Methodologies\n\n\n\nSlide 03\n\n\nTo address this empirical query, the authors first undertook a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This reconstruction encompassed both the linguistic and intellectual fabric of the field, alongside its inherent social structure. The team amassed a substantial dataset, comprising 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP. Their methodology unfolded in two principal stages.\nInitially, a linguistic analysis elucidated the intellectual structure of the field. This phase crucially employed the Bertopic pipeline, a tool frequently discussed in contemporary computational linguistics. Documents underwent spatialisation into an embedding space. Subsequently, unsupervised clustering, executed at a highly fine-grained level (K=611 topics), identified distinct thematic areas. Such granularity proved essential for capturing niche quantum gravity approaches, some encompassing as few as 100 papers. Finally, the authors assigned each physicist a ‘specialty’, defined as the most prevalent topic across their collective publications. This process yielded a partition of authors into topics, reflecting the field’s intellectual architecture.\nConcurrently, a social network analysis illuminated the field’s social dynamics. This analysis commenced with a co-authorship graph, where nodes represented individual physicists and edges denoted collaborative relationships. The network encompassed approximately 30,000 physicists. Applying a community detection method, the authors identified 819 distinct communities. This yielded an alternative partition of authors into communities, mirroring the field’s social organisation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "href": "chapter_ai-nepi_015.html#scale-dependency-and-hierarchical-organisation-in-research-landscapes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes",
    "text": "13.4 Scale-Dependency and Hierarchical Organisation in Research Landscapes\n\n\n\nSlide 06\n\n\nWithin this analytical framework, plural pursuit signifies a direct, one-to-one correspondence between identified communities and their intellectual topics. An ideal configuration would manifest as a block-diagonal correlation matrix, where communities specialise in distinct domains, thereby exhibiting a clear division of labour. Conversely, applying this to the initial fine-grained partitions reveals a highly complex and ‘messy’ correlation heatmap, calculated using the normalised pointwise mutual information (npmi).\nSeveral factors contribute to this observed complexity. Firstly, the arbitrary granularity of topic partitioning can fragment conceptually unified areas; for instance, string theory, intuitively a single research programme, might appear scattered across numerous fine-grained topics. Secondly, extensive research programmes often involve parallel efforts by multiple communities, shaped by diverse micro-social processes. Crucially, the computational definitions of both ‘topic’ and ‘community’ inherently exhibit scale-dependency, permitting literature and social networks to be partitioned at varying granularities. Beyond mere technicality, this issue reflects a deeper conceptual reality: research programmes are intrinsically nested. String theory, for example, encompasses families and sub-families, such as Superstring Theory branching into Type II and Heterotic, which further subdivide into Type I, Type IIA, Type IIB, Heterotic SO(32), and Heterotic E_8 x E_8, alongside Bosonic String Theory. Consequently, identifying genuine instances of plural pursuit necessitates addressing this inherent ambiguity across different scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-and-adaptive-scale-selection",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection",
    "text": "13.5 Hierarchical Reconstruction and Adaptive Scale Selection\n\n\n\nSlide 09\n\n\nTo navigate these complexities, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, they employed Ward agglomerative clustering, iteratively merging the 611 fine-grained topics based on an objective function, thereby generating a comprehensive dendrogram. Similarly, for the community structure, the team implemented a hierarchical stochastic block model, as conceptualised by Peixoto (2014), which dynamically learns multi-level partitions into progressively coarser communities. These meticulously constructed hierarchical structures inherently introduce a notion of scale, enabling observation of the system at various granularities. For instance, one can observe the co-authorship network, where each physicist’s specialty is colour-coded, at differing levels of linguistic structure coarse-graining.\nNevertheless, a significant challenge persists: the selection of an appropriate scale remains largely arbitrary. To resolve this, the authors devised an adaptive topic coarse-graining strategy. This strategy posits that whilst topics capture subtle linguistic nuances, some possess no discernible consequence for scientists’ collaborative capacities. Consequently, their methodology systematically removes degrees of freedom from the fine-grained partition, provided this removal does not diminish useful information pertinent to understanding the social structure. This optimisation relies upon the Minimum Description Length (MDL) criterion, which seeks the partition that minimises a quantity balancing the linguistic partition’s explanatory power for the social structure against the complexity of the partition itself. The process involves iteratively refining the topic dendrogram, zooming in as long as the criterion improves, and halting when additional complexity yields insufficient informational gain regarding the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "href": "chapter_ai-nepi_015.html#bottom-up-analysis-emergent-topics-and-community-topic-correlations",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations",
    "text": "13.6 Bottom-Up Analysis: Emergent Topics and Community-Topic Correlations\n\n\n\nSlide 13\n\n\nUltimately, the analysis yielded 50 distinct, coarse-grained topics, each labelled by representative N-grams for conceptual clarity. Focusing specifically on quantum gravity-related topics, the authors then employed a correlation matrix to align these coarse-grained topics with community structures across various scales. For each emergent topic, their methodology sought to identify the community that best explained its prevalence across the different levels of the hierarchical community structure. Notably, some expansive topics, such as a very large purple cluster, exhibited no strong ties to specific communities, suggesting their universal relevance across the field.\nConversely, other topics, exemplified by string theory, demonstrated a robust correspondence, aligning with a research programme linked to a community structure at the third hierarchical level. Intriguingly, certain quantum gravity programmes, such as Loop quantum gravity, correlated with communities situated at much lower, more fine-grained levels within the hierarchy. Collectively, these observations suggest an absence of a clear-cut plural pursuit configuration. For instance, a smaller community, whilst nested within the broader string theory community, appeared intellectually bound to the distinct topic of holography. Evidently, nested structures and an entanglement of different scales characterise the research landscape, precluding a straightforward division of labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#reconciling-bottom-up-reconstruction-with-physicists-intuitions",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions",
    "text": "13.7 Reconciling Bottom-Up Reconstruction with Physicists’ Intuitions\n\n\n\nSlide 15\n\n\nSubsequently, the authors proceeded to confront this empirically derived reconstruction with physicists’ own perceptions of their field’s structure. They conducted a survey amongst the founding members of the International Society for Quantum Gravity, requesting a list of approaches that, in their view, structured the quantum gravity research landscape. From the collective feedback, a comprehensive list of approaches emerged, including asymptotic safety, causal sets, dynamical triangulations, group field theory, LQG, spin foams, noncommutative geometry, swampland, modified dispersion relation, DSR, quantum modified BH, shape dynamics, tensor models, string theory, supergravity, and holography. The analysis particularly focused on string theory, supergravity, and holography, given physicists’ differing opinions on their conceptual separation.\nTo facilitate this comparison, the team trained a Support Vector Machine (SVM) classifier, utilising text embeddings (specifically, all-MiniLM-L6-v2 applied to titles and abstracts) and hand-coded labels, to predict which papers belonged to each approach. The confrontation between these supervised, ‘top-down’ approaches and the coarse-grained ‘bottom-up’ topics manifested as a detailed heatmap, illustrating their degrees of overlap. For certain approaches, the alignment proved remarkably strong, particularly for those frameworks considered well-defined and conceptually autonomous. Conversely, the model performed less effectively for phenomenological or less fully developed conceptual frameworks. A significant finding revealed a large string theory cluster within the bottom-up analysis, encompassing both supergravity and string theory. This observation converged strikingly with physicists’ intuitions, as articulated by one survey respondent: “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on”supergravity” and “string theory” is so large that I’m not sure the communities can be separated in a meaningful way.” Evidently, once linguistic nuances lacking social consequences are stripped away, conceptually distinct areas may coalesce, even though initial linguistic clusters accurately reflect these differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#conclusions-and-philosophical-implications",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.8 Conclusions and Philosophical Implications",
    "text": "13.8 Conclusions and Philosophical Implications\n\n\n\nSlide 21\n\n\nSocio-epistemic systems demonstrably manifest across multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Consequently, identifying configurations of plural pursuit—characterised by a one-to-one mapping between communities and their intellectual substrate—necessitates the careful alignment of these structures across varying scales. In the specific context of quantum gravity, a bottom-up reconstruction of the research landscape offers a powerful means to either confirm or re-assess existing physicists’ intuitions. Crucially, the increasing potency of computational methods empowers researchers to revisit and even challenge long-held philosophical insights, particularly those concerning the nature of paradigms and communities within scientific fields. Indeed, as one might paraphrase, computation emerges as the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "",
    "text": "Overview\nThe authors conducted a comparative study, systematically evaluating the performance of Latent Dirichlet Allocation (LDA) and BERTopic across distinct textual levels: titles, abstracts, and full texts. This investigation addresses a pressing question within topic modelling, a crucial analytical tool for scrutinising large volumes of scientific literature, particularly in the history, philosophy, and sociology of science. Topic modelling extracts themes from corpora, enabling the identification of research trends, paradigm shifts, substructures, interrelations of themes, and the evolution of scientific vocabulary.\nThe study’s core objective was to ascertain whether analysing titles or abstracts suffices for topic modelling, or if full-text analysis remains indispensable. This inquiry is particularly pertinent given the substantial resources required for obtaining, preprocessing, and analysing comprehensive corpora. To achieve this, the authors constituted a corpus of scientific articles, meticulously identifying and segmenting title, abstract, and full-text sections. Subsequently, they applied both LDA and BERTopic approaches to each textual level. A dual analytical framework, encompassing both qualitative and quantitative methods, then facilitated the comparison of the resulting topic models. This rigorous methodology involved assessing model similarities, topic diversity, joint recall, and coherence, whilst leveraging a well-known astrobiology corpus for qualitative validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "href": "chapter_ai-nepi_016.html#research-question-and-study-design",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.1 Research Question and Study Design",
    "text": "14.1 Research Question and Study Design\n\n\n\nSlide 01\n\n\nThis research addresses a pivotal question within the domain of topic modelling: does analysing titles or abstracts provide sufficient data, or does full-text analysis remain a prerequisite? Topic modelling, a technique for extracting thematic content from textual corpora, has emerged as an indispensable tool for scrutinising extensive scientific literature, particularly within the history, philosophy, and sociology of science. Indeed, scholars employ it for diverse tasks, including identifying research trends, discerning paradigm shifts, uncovering substructures, mapping thematic interrelations, and tracing the evolution of scientific vocabulary.\nCrucially, existing studies apply topic modelling across various textual structures, encompassing titles, abstracts, and complete articles. This practice, however, raises a significant concern: obtaining, preprocessing, and analysing full-text corpora demand considerable resources. Consequently, the efficiency of utilising shorter textual forms becomes a pressing inquiry for the research team.\nTo investigate this, the authors meticulously constituted a corpus of scientific articles. They then precisely identified and isolated the title, abstract, and full-text sections within each document. Subsequently, they applied two distinct topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. A comprehensive analytical framework, integrating both qualitative and quantitative methods, facilitated the systematic comparison of the resultant topic models. This rigorous design ensured a thorough evaluation of performance across different model types and textual granularities.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "href": "chapter_ai-nepi_016.html#topic-modelling-methodologies",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.2 Topic Modelling Methodologies",
    "text": "14.2 Topic Modelling Methodologies\n\n\n\nSlide 05\n\n\nThe authors employed two principal topic modelling methodologies: Latent Dirichlet Allocation (LDA) and BERTopic. Both approaches fundamentally postulate that documents can be represented as numerical vectors. Within this framework, topics become identifiable through the detection of linguistic regularities, specifically repetitions, whilst machine learning algorithms facilitate the automatic discovery of these patterns.\nLDA, a classical statistical technique, constructs simple vector representations by counting words within documents. In this established approach, topics manifest as latent variables, adhering to Dirichlet’s law. Crucially, LDA readily accommodates extensive textual content, allowing for its application to titles, abstracts, or full texts.\nConversely, BERTopic represents a more recent, modular methodology. It leverages Large Language Model (LLM)-based vector representations, originally drawing upon BERT, which lends the approach its name. Here, topics emerge as clusters of documents. Historically, BERTopic struggled with processing lengthy texts; however, for this investigation, the authors integrated a novel embedding technique. This advancement significantly enhanced BERTopic’s capacity, enabling it to process approximately 131,000 tokens, thereby facilitating its application to full-text analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison-framework",
    "href": "chapter_ai-nepi_016.html#corpus-and-qualitative-comparison-framework",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.3 Corpus and Qualitative Comparison Framework",
    "text": "14.3 Corpus and Qualitative Comparison Framework\n\n\n\nSlide 07\n\n\nThe study’s qualitative comparisons drew upon a meticulously analysed astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a comprehensive evaluation, the authors selected a full-text LDA model comprising 25 distinct topics to serve as a foundational reference.\nThe authors meticulously analysed these 25 topics, examining their most representative words and documents. This process facilitated the generation of a concise label for each topic, derived directly from its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric based on the topics’ presence within individual documents. A community detection algorithm then identified four distinct thematic clusters, designated A, B, C, and D, and visually distinguished by red, green, yellow, and blue hues respectively.\nA graphical representation visually conveyed these findings, illustrating the correlations amongst the 25 topics, complete with their assigned labels and cluster affiliations. In this visualisation, the thickness of the connecting lines denoted the strength of the correlation between topics, whilst the size of each circular node indicated the topic’s overall prevalence across the entire document collection. This established analytical framework provided a robust basis for the qualitative assessment of the six topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-analysis-metrics",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.4 Quantitative Analysis Metrics",
    "text": "14.4 Quantitative Analysis Metrics\n\n\n\nSlide 08\n\n\nFor quantitative analysis, the authors employed four distinct metrics to rigorously compare the topic models.\n\nFirstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between any two document clusterings, whilst correcting for chance agreement. This metric precisely assessed the degree to which documents tended to cluster together across different models.\nSecondly, Topic Diversity quantified the proportion of distinct top words within a given topic model, thereby evaluating whether individual topics were indeed characterised by unique vocabularies.\nThirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words. This metric critically assessed how effectively the top words collectively represented the documents assigned to each topic.\nFinally, Coherence CV, calculated as the average cosine relative distance between top words within topics, determined whether the constituent words of a topic exhibited a meaningful semantic relationship.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-analysis",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.5 Results: Adjusted Rand Index Analysis",
    "text": "14.5 Results: Adjusted Rand Index Analysis\n\n\n\nSlide 09\n\n\nThe Adjusted Rand Index (ARI) provided initial insights into the similarities amongst the six topic models. A score of zero on this metric signifies a random clustering, establishing a baseline for comparison. Analysis revealed that the LDA model applied to titles exhibited the most pronounced dissimilarity from all other models, consistently registering ARI values below 0.2 within the heatmap.\nConversely, the remaining models generally achieved a superior overall match, with ARI scores exceeding 0.2. Notably, BERTopic models demonstrated a stronger mutual fit, consistently yielding values above 0.35. Amongst these, the BERTopic abstract model emerged as particularly central, correlating effectively with every other model, save for the outlier LDA title model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-lda-model-inter-comparisons",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.6 Results: LDA Model Inter-Comparisons",
    "text": "14.6 Results: LDA Model Inter-Comparisons\n\n\n\nSlide 09\n\n\nA more granular analysis of the LDA models provided detailed insights into their inter-relationships. Comparing LDA Full-text with LDA Abstract (Table A) revealed a generally strong fit. A distinct reddish diagonal in the table indicated that each topic from one model largely corresponded to a topic in the other, sharing a high proportion of common documents. Despite this overall alignment, some dynamic shifts occurred: three full-text LDA topics entirely disappeared, whilst another three split into multiple topics within the abstract model. Concurrently, three novel abstract topics emerged, and three abstract topics resulted from the merger of others. Furthermore, one small class within the abstract topics contained fewer than 50 documents.\nIn stark contrast, the comparison between LDA Full-text and LDA Title (Table B) demonstrated a poor fit, necessitating substantial reorganisation. This disparity manifested as numerous full-text topics vanishing and a proliferation of new topics appearing within the title model, underscoring the limited correspondence between these two textual granularities for LDA.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "href": "chapter_ai-nepi_016.html#results-bertopic-model-inter-comparisons",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.7 Results: BERTopic Model Inter-Comparisons",
    "text": "14.7 Results: BERTopic Model Inter-Comparisons\n\n\n\nSlide 11\n\n\nAnalysis of the BERTopic models, particularly in comparison with LDA Full-text, revealed varied levels of correspondence. Comparing LDA Full-text with BERTopic Full-text (Table C) indicated an average overall fit. Within this comparison, eight topics from the LDA model disappeared, whilst six topics split into the BERTopic model. Conversely, five new topics emerged within the BERTopic model, and one topic resulted from mergers. Furthermore, the document distribution showed four small classes alongside one notably large class.\nThe comparison between LDA Full-text and BERTopic Abstract (Table D) demonstrated a relatively good fit. Here, four topics disappeared, six topics split, two new topics appeared, and four topics resulted from mergers.\nFinally, examining LDA Full-text against BERTopic Title (Table E) again indicated an average overall fit. In this instance, seven topics disappeared, whilst one topic split. Simultaneously, seven new topics emerged, and one topic resulted from a merger. The document distribution for this comparison revealed three small classes and one large class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-lda-top-word-correspondence",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.8 Results: LDA Top-Word Correspondence",
    "text": "14.8 Results: LDA Top-Word Correspondence\n\n\n\nSlide 13\n\n\nAn examination of the top words within the LDA models revealed that topics generally maintained a relatively well-formed structure across all iterations. The authors identified several robust topics exhibiting strong correspondence across every LDA model; “A-Radiation spore” serves as a prime example of such consistency.\nConversely, certain topics from the full-text model fragmented across the abstract and title models. For instance, “A-Life civilization” split into multiple sub-topics, a division that logically aligned with the broader theme of research in astrobiology. However, the fragmentation of “B-Chemistry” proved more challenging to interpret without deeper investigation.\nFurthermore, the analysis uncovered instances where topics from the full-text model merged into new, consolidated topics within the abstract and title models. The fusion of “B-Amino-acid” and “B-Protein-gene-RNA” exemplified this phenomenon, forming a more generalised and coherent thematic unit.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "href": "chapter_ai-nepi_016.html#results-bertopic-top-word-correspondence",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.9 Results: BERTopic Top-Word Correspondence",
    "text": "14.9 Results: BERTopic Top-Word Correspondence\n\n\n\nSlide 14\n\n\nContinuing the assessment of top words, the three BERTopic models consistently yielded relatively well-formed topics. Notably, “A-Radiation spore” again demonstrated remarkable robustness, maintaining its coherence across all BERTopic iterations. Similarly, “A-Life civilization” remained comparatively stable across the models, albeit with some observed splitting.\nThis fragmentation of “A-Life civilization” specifically led to the emergence of narrower topics, focusing precisely on extraterrestrial life. Furthermore, the splitting of “B-Chemistry” across the BERTopic models also resulted in more specialised, narrower thematic categories.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-coherence-performance",
    "href": "chapter_ai-nepi_016.html#results-coherence-performance",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.10 Results: Coherence Performance",
    "text": "14.10 Results: Coherence Performance\n\n\n\nSlide 15\n\n\nAn evaluation of the models’ coherence, a metric assessing the meaningfulness of topic top words, revealed distinct performance patterns across a range of 5 to 50 topics. Titles consistently yielded the poorest coherence scores, indicating a less meaningful grouping of their constituent words. Conversely, abstract models generally demonstrated superior coherence compared to their full-text counterparts.\nAcross the board, BERTopic models exhibited better coherence than LDA, particularly for abstract and title analyses. However, this performance gap narrowed as the number of topics increased. Ultimately, the BERTopic Abstract model emerged as the unequivocal leader in terms of coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-diversity-performance",
    "href": "chapter_ai-nepi_016.html#results-diversity-performance",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.11 Results: Diversity Performance",
    "text": "14.11 Results: Diversity Performance\n\n\n\nSlide 16\n\n\nAssessing the diversity of top words representing the topics, a clear trend emerged: diversity generally diminished as the number of topics increased. Titles, surprisingly, offered the highest diversity amongst all models, suggesting a broader range of unique words characterising their topics.\nFurthermore, BERTopic consistently outperformed LDA in terms of diversity. Ultimately, the BERTopic Title model secured the top position for diversity, with BERTopic Full-text closely trailing.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "href": "chapter_ai-nepi_016.html#results-joint-recall-performance",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.12 Results: Joint Recall Performance",
    "text": "14.12 Results: Joint Recall Performance\n\n\n\nSlide 17\n\n\nThe joint recall metric, which evaluates the efficacy of top words in collectively representing documents classified within each topic, revealed distinct performance hierarchies. Titles consistently yielded the poorest recall scores, indicating a limited ability of their top words to capture the full scope of associated documents. Conversely, full-text models demonstrated superior recall compared to both their abstract and title counterparts.\nBetween the two primary approaches, LDA generally exhibited better joint recall than BERTopic. Ultimately, LDA Full-text and BERTopic Full-text emerged as joint leaders in this category, with BERTopic Abstract following very closely behind.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "href": "chapter_ai-nepi_016.html#overall-model-performance-summary",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.13 Overall Model Performance Summary",
    "text": "14.13 Overall Model Performance Summary\n\n\n\nSlide 17\n\n\nThe authors compiled the comprehensive results into a summary table, visually representing each model’s performance across various assessments using a graded circle system: black denoted the highest score, whilst white indicated the lowest. Crucially, this synthesis underscored the absence of an absolute “best” model, as varying research objectives inherently dictate differing needs and, consequently, distinct model choices.\nConsider, for instance, an objective focused solely on discovering main topics, where precise document classification is not paramount. In such a scenario, issues like poor recall or significant class imbalance might prove negligible. Here, full-text BERTopic performed commendably, despite exhibiting some class imbalance. Similarly, whilst far from optimal, title BERTopic nonetheless generated several robust topics that consistently appeared across other models. Conversely, the authors strongly advise against employing LDA Title, given its consistently poor performance across nearly all assessment criteria.\nUltimately, the study recommends conducting topic modelling on either abstract or full-text data, utilising both LDA and BERTopic, provided such an approach does not result in the misclassification of documents pertinent to the identified topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "href": "chapter_ai-nepi_016.html#discussion-and-future-directions",
    "title": "Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities",
    "section": "14.14 Discussion and Future Directions",
    "text": "14.14 Discussion and Future Directions\n\n\n\nSlide 17\n\n\nThis research yielded several crucial findings, informing future approaches to topic modelling. Firstly, title models consistently demonstrated poor performance. This deficiency likely stems from the inherent lack of information within titles, which can lead to the false classification of documents. Nevertheless, the BERTopic title model surprisingly revealed several meaningful topics, suggesting a potential balance between well-defined topics and comprehensive document coverage remains achievable.\nSecondly, full-text models, whilst offering comprehensive data, sometimes struggle to process vast quantities of information effectively. With LDA, topics can become more loosely defined and broader in scope, occasionally encompassing secondary themes such as methodology. Conversely, BERTopic, when applied to full text, can generate overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently performed well with summary information. Notably, the results obtained from LDA full text exhibited strong consistency with both abstract models, underscoring their utility. Fourthly, the study revealed a remarkable robustness of topics across all models. The authors identified very similar topics across the board, a consistency that facilitates the application of meta-analytic methods to pinpoint the most robust thematic elements. Moreover, leveraging the relative distance across models could enable the identification of an optimal solution, as exemplified by the BERTopic abstract model in this study, which performed exceptionally well across numerous metrics.\nFinally, the findings prompt consideration of new model paradigms. It appears feasible to exploit the inherent structural information—encompassing full text, abstracts, and titles—to extract more semantically meaningful sets of topics or top words, thereby advancing the precision and utility of topic modelling.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparative Topic Modelling: LDA and BERTopic Across Textual Granularities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "",
    "text": "Overview\nThe authors present a novel approach for imbuing large language models (LLMs) with explicit temporal awareness, directly addressing a fundamental limitation of current architectures. Presently, LLMs derive their understanding of time implicitly from statistical patterns within training texts. However, this method proves insufficient for tasks demanding precise temporal context. The research team proposes the “Time Transformer”, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce changing linguistic patterns as a function of time, thereby resolving ambiguities that arise from temporally contradictory information within training data.\nTo validate this concept, the authors developed a modest Transformer model and trained it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. This dataset, characterised by its restricted vocabulary and repetitive language, provided an ideal testbed for demonstrating the Time Transformer’s efficacy. Their experiments involved injecting synthetic temporal drifts into the training data. These included both synonymic succession (for instance, replacing “rain” with “liquid sunshine”) and co-occurrence changes (such as rain becoming rain and snow). The Time Transformer consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.\nBeyond this compelling proof of concept, the Time Transformer holds significant implications for historical analysis. It offers a robust foundation for downstream tasks on historical data and enables instruction-tuned models to “talk to a specific time.” Whilst the architectural modification necessitates training from scratch, posing computational challenges for large-scale applications and introducing data curation complexities, the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking the Time Transformer against explicit time-token approaches and investigates the utility of a modest, targeted encoder model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "href": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "15.1 Addressing Implicit Temporal Understanding",
    "text": "15.1 Addressing Implicit Temporal Understanding\n\n\n\nSlide 01\n\n\nLarge language models (LLMs) currently derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora. Whilst these models exhibit a remarkable grasp of temporal concepts, this reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.\nA critical challenge emerges when training data contains temporally contradictory information. For instance, consider two sentences: “The primary architectures for processing text through NNs are LSTMs” (true in 2017) and “The primary architectures for processing text through NNs are Transformers” (true in 2025). Without explicit temporal context, an LLM treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, these models often exhibit a “recency bias,” favouring more recent information in next-token prediction. Existing workarounds, such as prompt engineering—inserting explicit temporal cues like “In 2017”—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge. A more robust solution necessitates an architecture that enables LLMs to explicitly learn and reproduce these changing patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "15.2 Formalising Time-Dependent Probabilities",
    "text": "15.2 Formalising Time-Dependent Probabilities\n\n\n\nSlide 05\n\n\nTo formalise the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, x_n, given a sequence of preceding tokens, x_1, ..., x_{n-1}. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as p(x_n | x_1, ..., x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a specific time t is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing LLMs largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.\nTo overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution p(x_n | x_1, ..., x_{n-1}, t). Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. The authors propose a more elegant and efficient method, termed the “Time Transformer”, which introduces a simple yet profound modification: an additional dimension, φ(t), is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, E(x, t), which then serves as input to the Transformer architecture. Consequently, the Transformer processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution p_θ(x_n | x_1, ..., x_{n-1}, t). The training objective remains the standard maximisation of log likelihood across all sequences. This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "href": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "15.3 Empirical Validation: Data and Architecture",
    "text": "15.3 Empirical Validation: Data and Architecture\n\n\n\nSlide 16\n\n\nTo empirically validate the Time Transformer concept, the authors required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, proved an ideal choice. The authors scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. They intentionally simplified the tokenisation process, neglecting sub-word tokenisation, case, and interpunctuation. This resulted in a compact vocabulary of only 3,395 unique words across the entire seven-year period. An alternative dataset, TinyStories, was also considered for its similar characteristics.\nThe authors employed a modest Transformer architecture, termed the “Vanilla model,” to underpin their experimental setup. This model incorporates an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers culminating in a Final Dense Layer for output. Specifically, the architecture features four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures like GPT-4, which boasts 1.8 trillion parameters distributed across 120 layers. Training took place on an HPC cluster in Munich, utilising two H100 GPUs. Remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The code for this implementation is publicly available on GitHub, though the authors developed it primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a remarkable ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as “During the night, a band …”, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "href": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "15.4 Time Transformer: Implementation and Results",
    "text": "15.4 Time Transformer: Implementation and Results\n\n\n\nSlide 15\n\n\nImplementing the Time Transformer required only a minimal architectural adjustment to the previously described Vanilla model. The authors reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as (day of year - 1) / (365 - 1). The authors chose this specific encoding to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.\nThe first experiment aimed to demonstrate the Time Transformer’s capacity for learning synthetic temporal drift through synonymic succession. The authors injected a time-dependent replacement rule into the training data: rain was replaced by liquid sunshine according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating weather predictions for each day of the year and subsequently counting the monthly frequencies of rain versus liquid sunshine. The Time Transformer accurately reproduced the injected sigmoid pattern, exhibiting rain predominantly early in the year and liquid sunshine towards the end, with the transition occurring precisely mid-year.\nThe second experiment explored the Time Transformer’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the “fixation of a collocation.” Here, the authors synthetically replaced instances of rain not immediately followed by and with rain and snow. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of rain and snow against rain only. The Time Transformer successfully acquired this pattern, generating rain and snow almost exclusively in the latter part of the year, whilst early-year occurrences of rain (sometimes accompanied by snow) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year rain and snow on the presence of a “cold system,” underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "title": "The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness",
    "section": "15.5 Proof of Concept, Applications, and Challenges",
    "text": "15.5 Proof of Concept, Applications, and Challenges\n\n\n\nSlide 21\n\n\nThe authors’ research unequivocally establishes a proof of concept: Transformer-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation Time Transformer, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to “talk to a specific time,” potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.\nSeveral promising avenues for future research emerge. Benchmarking the Time Transformer against explicit time-token approaches will quantify its performance advantages. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.\nNevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained LLMs remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to BERT, built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Time Transformer: Imbuing Large Language Models with Explicit Temporal Awareness</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergey, with contributions from Badr Abdullah, have pioneered a comprehensive methodology to enrich the metadata of historical scientific texts and undertake diachronic analyses of chemical knowledge. This initiative addresses the inherent complexities of managing extensive historical corpora. The authors structured their project into two distinct phases: initially, they leverage Large Language Models (LLMs) to refine text metadata, focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Subsequently, their work delves into a meticulous case study, analysing the evolution of chemical discourse across various disciplines over time. This second phase specifically identifies periods of heightened interdisciplinarity and knowledge transfer, employing advanced computational methods. The Philosophical Transactions of the Royal Society of London, a foundational and continuously published scientific journal spanning over three centuries, serves as the primary data source, providing a rich historical context for this innovative research.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "href": "chapter_ai-nepi_018.html#research-objectives-and-scope",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Research Objectives and Scope",
    "text": "16.1 Research Objectives and Scope\n\n\n\nSlide 01\n\n\nDiego Alves, Sergey, and LLM expert Badr Abdullah meticulously investigate the application of Large Language Models (LLMs) for enriching historical scientific texts. Their project, aptly titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts,” is structured into two principal components.\nInitially, the first part focuses on deploying LLMs to enhance the metadata associated with historical texts. This phase encompasses the precise categorisation of articles by scientific discipline, the assignment of pertinent semantic tags or topics, and the generation of concise, abstractive summaries.\nSubsequently, the second part undertakes a detailed analysis of the chemical space as it evolved across various scientific disciplines over time. A crucial objective here involves identifying specific periods that correspond to peaks of interdisciplinarity and significant knowledge transfer, thereby illuminating the dynamic nature of scientific discourse.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "href": "chapter_ai-nepi_018.html#historical-scientific-corpus-philosophical-transactions",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 Historical Scientific Corpus: Philosophical Transactions",
    "text": "16.2 Historical Scientific Corpus: Philosophical Transactions\n\n\n\nSlide 01\n\n\nThe authors’ project investigates the evolution of scientific English, alongside phenomena such as knowledge transfer and the identification of influential papers and authors. Central to this inquiry is the Philosophical Transactions of the Royal Society of London, a corpus of singular historical import.\nFirst published in 1665, this journal holds the distinction of being the oldest scientific periodical in continuous publication, maintaining its esteemed reputation to this day. It proved instrumental in shaping scientific communication, notably by establishing the practice of peer-reviewed paper publication as a primary means for disseminating scientific knowledge. The corpus contains numerous foundational contributions, including Isaac Newton’s “New Theory about Light and Colours” from the 17th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” from the 18th century, and James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” from the 19th century. Beyond these renowned works, the collection also features more curious, speculative texts, such as discussions on lunar inhabitants, though the authors’ research focuses on linguistic and thematic analysis rather than factual validation.\nAlves and Sergey utilise the latest version of this extensive collection, the RSC 6.0 full corpus. This dataset spans over 300 years of scientific communication, from 1665 to 1996, encompassing nearly 48,000 texts and approximately 300 million tokens. Whilst the corpus includes pre-encoded metadata, such as author, century, year, and volume, a previous study employed LDA topic modelling to infer research field categories. This earlier classification, however, often conflated distinct scientific disciplines, sub-disciplines, and even text types, such as “observations” and “reporting,” necessitating a more refined approach to metadata enrichment.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-metadata-enrichment",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Large Language Models for Metadata Enrichment",
    "text": "16.3 Large Language Models for Metadata Enrichment\n\n\n\nSlide 10\n\n\nTo address the limitations of existing metadata, Alves, Sergey, and Abdullah endeavoured to harness Large Language Models (LLMs) for comprehensive metadata enrichment. These models offer myriad applications, including text clean-up, summarisation, information extraction, and the feeding of knowledge graphs, alongside their primary utility in categorisation and facilitating access and retrieval.\nSpecifically, the authors tasked the LLM with four distinct operations:\n\nIt performed hierarchical categorisation, assigning both a primary discipline and a suitable sub-discipline to each article.\nIt identified key index terms, functioning as semantic tags or topics.\nThe model generated concise TL;DR summaries, typically 3-4 sentences in length, designed to capture the essence and main findings of an article in simple language, accessible even to a high school student.\nFinally, the LLM proposed alternative, more reflective titles for the texts.\n\nThe research team selected Hermes-2-Pro-Llama-3-8B, an 8-billion-parameter model from the Llama 3 family, for this task. This particular variant, readily available on Hugging Face, demonstrated superior performance compared to Mistral and Llama 2, having undergone instruction-tuning specifically for producing structured outputs in formats such as JSON and YAML.\nA meticulously crafted system prompt guided the LLM’s operations. The prompt first defined the model’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” It then articulated the objective: “read, analyze, and organize a large corpus… create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input was described as OCR-extracted text from original articles, accompanied by existing metadata including title, author(s), publication date, journal, and a short text snippet.\nThe prompt detailed the four specific tasks:\n\nRead and analyse the provided article to understand its content and context, then suggest an alternative title that better reflects its content.\nWrite a short 3-4 sentence TL;DR summary that captures the article’s essence and main findings, ensuring conciseness, informativeness, and simple language.\nIdentify exactly five main topics, conceptualised as Wikipedia Keywords for categorising the text into scientific sub-fields.\nGiven the extracted topics, identify the primary scientific discipline from a predefined list (Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, Social Sciences & Humanities) and a suitable second-level sub-discipline, with the crucial constraint that the sub-discipline could not be one of the primary disciplines.\n\nAn example input, Isaac Newton’s 1672 letter, illustrated the expected YAML output. The LLM successfully transformed the original lengthy title into “A New Theory of Light and Colours,” assigned topics such as “Optics” and “Refraction,” generated a concise TL;DR summary, and accurately classified the article under “Physics” with the sub-discipline “Optics & Light.” A final instruction reinforced the requirement for a valid YAML output, with no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "href": "chapter_ai-nepi_018.html#llm-performance-and-diachronic-corpus-analysis",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 LLM Performance and Diachronic Corpus Analysis",
    "text": "16.4 LLM Performance and Diachronic Corpus Analysis\n\n\n\nSlide 15\n\n\nAlves, Sergey, and Abdullah conducted initial sanity checks on the LLM’s performance, which produced highly promising outcomes. A remarkable 99.81% of the generated outputs—specifically 17,486 out of 17,520—conformed to the specified YAML format, underscoring the model’s adeptness in structured data generation. Furthermore, 94% of the predicted scientific disciplines aligned precisely with the predefined set of nine categories.\nNevertheless, the LLM exhibited some minor “hallucinations” or errors. For instance, it occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences.” The model also innovated novel categories, such as “Music,” and sometimes incorporated the numerical index of a discipline directly into its name, for example, “3. Chemistry.” Moreover, certain sub-disciplines, like “neurology” and “zoology,” were incorrectly classified as primary disciplines. Despite these minor deviations, the LLM accurately assigned the vast majority of papers to their correct categories.\nLeveraging this enhanced metadata, the authors conducted a diachronic analysis of the Royal Society articles, examining their distribution across disciplines over time. Before the close of the 18th century, the distribution of articles across disciplines appeared relatively homogeneous. However, the late 18th century witnessed a distinct surge in chemical articles, a phenomenon directly correlating with the Chemical Revolution. Subsequently, from the 19th century into the 20th, Chemistry, alongside Biology and Physics, emerged as one of the Royal Society’s primary pillars of scientific inquiry.\nFurther analysis involved visualising the TL;DR summaries using t-SNE projections. This revealed significant overlaps between Chemistry, Physics, and Biology, with Chemistry often situated centrally within this interdisciplinary space. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters within the projection. This visualisation technique holds considerable promise for future research, enabling the detailed observation of diachronic shifts and evolving overlaps between disciplines.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Diachronic Analysis of Chemical Space",
    "text": "16.5 Diachronic Analysis of Chemical Space\n\n\n\nSlide 19\n\n\nBuilding upon the LLM-derived classifications, the authors proceeded with a diachronic analysis of the chemical space, focusing specifically on Chemistry, Biology, and Physics, given their prominence within the corpus.\nTo extract chemical terms, the research team employed ChemDataExtractor, a Python module designed for the automatic identification of chemical substances. Initial application of this tool to the entire text corpus yielded substantial noise. Consequently, the authors adopted a refined approach: ChemDataExtractor was subsequently applied to a pre-filtered list of extracted substances, a method that significantly reduced the noisy output and improved precision.\nFor analysing the chemical space, Alves and Sergey utilised Kullback-Leibler Divergence (KLD). This statistical measure quantifies the number of additional bits required to encode a dataset A when an (often sub-optimal) model based on dataset B is employed. Crucially, higher KLD values indicate greater differences between datasets, whilst lower values suggest relative similarity.\nThe authors applied KLD in two distinct ways. First, to trace the independent evolution of the chemical space within each discipline along the historical timeline, they compared 20-year periods before a specific date with 20-year periods after it, employing a sliding 5-year window. This technique allowed for a granular understanding of how chemical terminology and concepts shifted within Chemistry, Biology, and Physics individually. Second, for a broader interdisciplinary perspective, the team conducted a pairwise comparison of Chemistry with Physics and Chemistry with Biology, based on 50-year periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "href": "chapter_ai-nepi_018.html#kullback-leibler-divergence-results-and-interdisciplinary-dynamics",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics",
    "text": "16.6 Kullback-Leibler Divergence Results and Interdisciplinary Dynamics\n\n\n\nSlide 22\n\n\nThe authors’ analysis of the Kullback-Leibler Divergence (KLD) per discipline uncovered consistent trajectories across Chemistry, Biology, and Physics, with peaks and troughs in KLD values occurring roughly concurrently. Towards the end of the timeline, the KLD generally decreased, indicating less variation between future and past periods within each discipline. A pronounced peak manifested in the late 18th century, prompting further investigation into the specific chemical substances driving this change.\nExamining the period from 1776 to 1816, corresponding to the late 18th-century peak, Alves and Sergey observed that in both Biology and Physics, one or two elements exhibited exceptionally high KLD values, effectively driving the observed shifts. Intriguingly, the same core chemical elements appeared across Chemistry, Biology, and Physics during this era.\nHowever, a significant divergence became apparent when analysing the second half of the 19th century (1856-1906). During this period, the KLD graphs for Biology and Physics became considerably more populated, with individual elemental contributions showing greater uniformity. Biology’s chemical discourse evolved distinctly towards biochemistry, incorporating substances related to biochemical processes. Conversely, Chemistry and Physics increasingly focused on noble gases and radioactive elements, reflecting their discovery and burgeoning importance in the late 19th century.\nInterdisciplinary comparisons, visualised through word clouds for the latter half of the 20th century, further elucidated these thematic differences. In the comparison between Chemistry and Biology, the biological word cloud featured a greater prevalence of substances associated with biochemical processes in living organisms. In contrast, the chemical word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. When comparing Chemistry with Physics, the latter’s word cloud prominently displayed metals, noble gases, rare earth metals, semi-metals, and radioactive metals, underscoring distinct disciplinary thematic focuses.\nCrucially, this pairwise KLD analysis facilitated the detection of “knowledge transfer” events. Knowledge transfer, in this context, describes instances where an element initially distinctive of one discipline in an earlier period subsequently became more characteristic of another. For example, tin, which was distinctive of Chemistry in the early 18th century, clearly shifted to become distinctive of Physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. Furthermore, elements transitioning from Chemistry to Biology in the 20th century consistently related to biochemical processes, reinforcing the observed disciplinary specialisation.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#conclusion-and-future-research-directions",
    "title": "Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Conclusion and Future Research Directions",
    "text": "16.7 Conclusion and Future Research Directions\n\n\n\nSlide 21\n\n\nAlves, Sergey, and Abdullah effectively utilised a Large Language Model for the categorisation of articles and the development of refined topic models within the historical corpus. Building upon these LLM-generated results, the authors conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: Chemistry, Biology, and Physics. Furthermore, they performed a detailed interdisciplinary comparison of this evolving chemical landscape.\nDespite these significant achievements, considerable scope persists for future work. For the first part of the project, the authors plan to test alternative LLMs and undertake a rigorous evaluation of the current model’s outputs to ensure robustness and accuracy. Regarding the diachronic analysis of the chemical space, future efforts will involve a more fine-grained interdisciplinary analysis, potentially incorporating diachronic sliding windows of varying lengths. Expanding the scope to include additional disciplines, such as a comparison between Chemistry and Medicine, also presents a compelling avenue. Finally, exploring the evolution of chemical space using surprisal, a measure of unexpectedness, could yield further insights into the dynamics of scientific discovery and knowledge transfer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nThe Cascade project, a distinguished Marie Curie doctoral network, explores the computational analysis of semantic change, focusing intently on modelling diverse contextual factors and their intricate interplay. PhD student Sophia Aguilar leads significant contributions to this investigation. Whilst previous work modelled distinct context types in isolation, the current objective seeks to integrate these approaches, thereby illuminating their complex interactions. The chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a pivotal pilot study for this endeavour.\nLinguists engaged in this research meticulously examine how language adapts to real-world transformations, drawing critically upon register theory and principles of rational communication. The study aims to achieve several key objectives: to detect precise periods of linguistic change, to analyse the specific lexical and grammatical shifts occurring, to identify influential figures driving these transformations, and ultimately, to comprehend the underlying linguistic mechanisms and communicative drivers. To this end, the team proposes a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics. This innovative approach positions context as a central signal, thereby aiming to overcome the limitations of existing methods in capturing the nuanced interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-contextual-modelling-and-theoretical-underpinnings",
    "href": "chapter_ai-nepi_019.html#foundations-contextual-modelling-and-theoretical-underpinnings",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Contextual Modelling and Theoretical Underpinnings",
    "text": "17.1 Foundations: Contextual Modelling and Theoretical Underpinnings\nWithin the Cascade project, a prestigious Marie Curie doctoral network, the research team investigates the computational analysis of semantic change. PhD student Sophia Aguilar spearheads efforts to model context comprehensively, meticulously examining the interplay between its various dimensions. This work builds upon earlier studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing extensively upon the Royal Society Corpus (RSC). This historical period witnessed a significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation vividly documented in resources such as chemistryworld.com and visually represented by contemporary art, including the iconic painting of Lavoisier and his wife. The investigation aims to model a comprehensive spectrum of contextual factors: situational (where), temporal (when), experiential (what), interpersonal (who), textual (how), and causal (why).\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers inherent variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint precisely when linguistic transformations occur, the investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal substantially higher divergence due to significant linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time (e.g., from 1725 to 1845) reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence\nBeyond temporal detection, the investigation delves into paradigmatic context and the dynamics of conceptual change, referencing seminal work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories such as corpora.ids-mannheim.de.\nTo understand precisely who spearheads and propagates these linguistic and conceptual shifts, researchers Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with notable contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, the researchers examine shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, visualised through graphs plotting surprisal against year, supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community using that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Contextual Dynamics\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. The proposed framework positions context as a central signal for modelling language dynamics, with Graph Convolutional Networks (GCNs) identified as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four distinct stages:\n\nData Sampling: The team employs KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: This stage begins by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the team refines network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering profound insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research team acknowledges several profound questions that delineate current limitations and chart future directions. A primary concern involves the very nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. The investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "",
    "text": "Overview\nThe research team investigates the intricate complexities of science funding, moving beyond conventional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This unique collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, which significantly improves Optical Character Recognition (OCR) and enables separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\n\n\n\nSlide 01\n\n\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War. It operates under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that a more profound comprehension emerges from investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\n\n\n\nSlide 04\n\n\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses by the research team reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\n\n\n\nSlide 06\n\n\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\n\n\n\nSlide 10\n\n\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\n\n\n\nSlide 12\n\n\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, the authors developed robust methods for masking, removing, or disambiguating such information as paramount. Their developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the research team reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\n\n\n\nSlide 15\n\n\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, the team identified figures like Francis Collins as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\n\n\n\nSlide 13\n\n\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features:\n\nBiological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05).\nProject-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04).\nFurthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04).\nLinguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\n\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "Unlocking Science’s Hidden Processes: A Computational Analysis of the NHGRI Archive",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\n\n\n\nSlide 16\n\n\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Unlocking Science's Hidden Processes: A Computational Analysis of the NHGRI Archive</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs, thereby enabling sophisticated querying and analysis. Their work directly addresses the challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. The authors’ core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline meticulously designed for specific tasks. This pipeline aims to impose structure upon unstructured data in a controllable manner.\nThe process commences with diverse sources, including Polish biographical materials and German biographical handbooks, such as “Wer war wer in der DDR?”. It then proceeds to extract entities—persons, places, countries, and works—and their relationships, representing them as nodes and edges within a knowledge graph. Tools like Neo4j facilitate the visualisation of these graphs. This structured representation, the authors contend, significantly streamlines complex queries, allowing for investigations into network formations amongst professionals during specific periods or the tracing of intellectual evolution. Their methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies precisely tailored to research questions, and disambiguated entities linked to external resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially to enable natural language querying through technologies such as GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\n\n\n\nSlide 01\n\n\nMalte, Raphael, and Alex K. confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly that found within printed books and archives, has remained largely inaccessible to computational analysis due to its inherent lack of digital structure. Whilst earlier tools, such as Get Grasso, aimed to digitise and process printed materials, the current investigation centres upon biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the investigators propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline designed to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—which the authors represent as nodes, whilst depicting the relationships between them as edges. Polish biographical collections and German-language resources, including the handbook “Wer war wer in der DDR?”, serve as primary examples of the source material. Platforms like Neo4j can facilitate the visualisation and management of these graphs. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\n\n\n\nSlide 04\n\n\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline identifies key entities and their interrelations, which it then assembles into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities such as his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships like “born in” or “travelled to” various locations including Italy (“Włochy”) or Egypt (“Egipt”). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves the creation of SHACL (Shapes Constraint Language) shapes for validation, the mapping of extracted information to the ontology, the performance of entity disambiguation (including linking to Wiki IDs), and finally, the validation of the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\n\n\n\nSlide 09\n\n\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality suffices to proceed to the next stage of the pipeline or if further refinement of the OIE steps becomes necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\n\n\n\nSlide 12\n\n\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This process results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\n\n\n\nSlide 15\n\n\nThe authors illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying the knowledge-graph approach to this corpus, the team can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network, which the authors derived from these compilations, reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors appear coloured green and others pink, clearly showing clusters of interconnected individuals.\nThe second case study focuses on the German biographical lexicon “Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien”. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and researchers and journalists frequently consult it; the presentation displays sample entries for Gustav Hertz and Robert Havemann. An analytical example, which the authors derived from this dataset, examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR. Further comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and the holding of specific high-ranking positions within structures such as the Politbüro or Ministerrat.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\n\n\n\nSlide 20\n\n\nThe project successfully demonstrates a method for progressing from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, the authors identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the team intends to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]