[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings - Enhanced Edition",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held in 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview\nThe workshop, “Large Language Models for the History, Philosophy and Sociology of Science,” convened to explore the application of advanced AI methods within historical, philosophical, and sociological inquiries into science. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collaboratively organised this distinguished event, which emerged from two distinct yet complementary initiatives.\nFirstly, the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932), provided a foundational interest in training large language models on physics texts and analysing conceptual issues within the discipline. Secondly, Gerd Graßhoff, a long-standing advocate for AI integration in the history and philosophy of science, particularly for understanding scientific discovery processes, proposed a workshop on novel AI-assisted methodologies. These converging interests consequently led to the joint organisation of the current event.\nThe NEPI project specifically investigates the internal communication dynamics of the ATLAS collaboration at CERN, aiming to elucidate how such a prominent, large-scale research collaboration collectively generates new knowledge. This research employs both network analysis to map communication structures and semantic tools, including Large Language Models, to trace the flow of ideas within these networks. The workshop attracted significant interest, receiving over 50 paper submissions, from which the organisers selected 16 for presentation. It quickly reached full capacity for in-person attendance and garnered a substantial online audience, totalling approximately 220 registered participants. The programme features keynotes from leading researchers: Pierluigi Cassotti and Nina Tahmasebi, who focus on large-scale text analysis for cultural and societal change, and Iryna Gurevych, who addresses the elevation of Natural Language Processing to the cross-document level. Logistical arrangements include structured question-and-answer sessions, an Etherpad for comments, a dedicated discussion session, and various networking opportunities. Recording protocols ensure the capture of presentations for future dissemination on the NEPI YouTube channel, subject to presenter consent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "href": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Genesis and Scope",
    "text": "2.1 Workshop Genesis and Scope\n\n\n\nSlide 02\n\n\nThe workshop, titled “Large Language Models for the History, Philosophy and Sociology of Science,” emerged from two distinct yet complementary initiatives. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collectively organised this event. One primary impetus stemmed from the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932). Within this project, Arno Simons pioneered the training of one of the earliest large language models specifically on physics texts, whilst Michael Zichert employed similar models to analyse conceptual issues prevalent in physics.\nConcurrently, Gerd Graßhoff, a long-standing collaborator and proponent of AI integration within the history and philosophy of science, particularly for analysing scientific discovery processes, conceived a workshop focused on novel AI-assisted methodologies. Consequently, these converging interests led to a joint endeavour, culminating in the present workshop. The NEPI project specifically investigates the internal communication of the ATLAS collaboration at CERN, the particle physics laboratory. Researchers aim to understand how one of the largest and most prominent research collaborations collectively generates new knowledge. This investigation employs network analysis to elucidate communication structures and semantic tools, including large language models, to trace the flow of ideas within these intricate networks.\nThe call for papers for this workshop garnered significant interest, attracting over 50 submissions, from which the organisers selected 16 for presentation. On-site attendance quickly reached full capacity, whilst a substantial online audience also registered, bringing the total participation to approximately 220 individuals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "href": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Recording Protocols and Consent",
    "text": "2.2 Recording Protocols and Consent\n\n\n\nSlide 03\n\n\nThe workshop sessions are currently undergoing recording. Attendees provided their consent for this during the registration process. A single camera captures the proceedings, specifically directed towards the presenter. Audio recording relies on four microphones, supplemented by an iPhone serving as a backup audio recorder. Subject to the presenters’ explicit consent, the videos of the talks, encompassing the subsequent discussion, will be uploaded to the NEPI YouTube Channel following the workshop. Crucially, the discussion segments will feature only the audio and video of the presenter, ensuring the privacy of the audience. Individuals requiring further information or wishing to withdraw their consent should approach the organisers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "href": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Workshop Logistics and Interaction Guidelines",
    "text": "2.3 Workshop Logistics and Interaction Guidelines\n\n\n\nSlide 04\n\n\nGiven the large group size and the limited time allocated for presentations and subsequent questions, participants are requested to keep their questions and comments concise and pertinent. Following each presentation, the organisers will collect up to four questions or comments, enabling the presenter to respond to them collectively, thereby optimising time. An Etherpad provides dedicated sections for each talk, alongside a general section, allowing participants to place their comments appropriately. Furthermore, a dedicated discussion session on the second day will facilitate the pooling and collective discussion of frequently arising questions and comments.\nBeyond the formal sessions, the workshop offers ample opportunities for informal networking amongst researchers and fellows. These include generous lunch and coffee breaks, a modest reception, and a workshop dinner. Notably, seating for the dinner is highly limited, reserved exclusively for participants who received confirmation of their attendance. Coffee breaks and refreshments are available on-site. Lunch and the reception will take place in Room H 2051, located down the hall, one floor below the main workshop area.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change",
    "text": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change\n\n\n\nSlide 05\n\n\nThe first keynote address, scheduled shortly, features Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi directs the “Change is Key” research programme, whilst Pierluigi Cassotti contributes as a researcher within the project. Their work has gained considerable recognition for its focus on semantic change detection. This research encompasses both technical aspects, such as the development of benchmarks, and broader methodological considerations, including the application of data science methods to questions within the humanities. This dual focus renders their expertise particularly relevant to the workshop’s objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level",
    "text": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 06\n\n\nIryna Gurevych will deliver the second keynote address tomorrow late afternoon. She leads the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University Darmstadt. Her research primarily concentrates on information extraction, semantic text processing, and machine learning. Crucially, her work also extends to the application of Natural Language Processing (NLP) techniques within the social sciences and humanities, aligning perfectly with the interdisciplinary focus of this workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "",
    "text": "Overview\nThis chapter offers a comprehensive introduction to large language models (LLMs) and their applications within the History, Philosophy, and Sociology of Science (HPSS) domain. It commences with a foundational primer on LLMs and their adaptation to scientific contexts, followed by a summary of their contemporary applications in HPSS. The author also shares critical reflections intended to stimulate workshop discussions.\nThe primer details the core Transformer architecture, explaining its encoder-decoder structure and its evolution into distinct model types, such as BERT (bidirectional, encoder-based for understanding) and GPT (unidirectional, decoder-based for generation). It then explores various strategies for adapting these models to specific scientific domains and tasks, including pre-training, fine-tuning, and the sophisticated Retrieval-Augmented Generation (RAG) pipeline, which integrates multiple models for enhanced contextual responses.\nA systematic categorisation of LLM applications in HPSS research is presented, covering data handling, knowledge structure analysis, knowledge dynamics, and knowledge practices. The discussion highlights accelerating interest in LLMs across diverse academic journals whilst acknowledging recurring concerns, such as computational resource demands, model opaqueness, and data scarcity. Crucially, the chapter concludes with critical reflections on HPSS-specific challenges, including the historical evolution of language and the need for reconstructive analysis, advocating for enhanced LLM literacy amongst scholars. It also emphasises the importance of aligning NLP tasks with core HPSS methodologies and exploring new opportunities for bridging qualitative and quantitative research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture-foundations-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture-foundations-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.1 The Transformer Architecture: Foundations of Large Language Models",
    "text": "3.1 The Transformer Architecture: Foundations of Large Language Models\nThe Transformer architecture constitutes the fundamental framework underpinning all contemporary Large Language Models. Engineers designed this model in 2017 for language translation tasks, such as converting German text into English. The architecture comprises two interconnected streams: an encoder and a decoder.\nThe encoder, positioned on the left, processes input words—for instance, a German sentence—by converting them into numerical representations. Crucially, it reads the entire input sentence simultaneously, enabling each word to interact with every other word. This comprehensive interaction allows the model to construct a full contextual representation of the sentence’s complete meaning.\nConversely, the decoder, located on the right, receives these processed numerical representations from the encoder. It then generates output words, such as an English sentence, feeding each newly produced word back into its input stream. This iterative process continues until the complete English sentence emerges. A key distinction of the decoder lies in its unidirectional nature: words can only access their predecessors, preventing them from “looking into the future” whilst predicting the subsequent word. Within both the encoder and decoder, multiple layers progressively refine contextualised word embeddings. Vaswani and colleagues introduced this foundational design in their seminal 2017 paper, ‘Attention is all you need.’",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#encoder-based-models-bert-and-bidirectional-context-understanding",
    "href": "chapter_ai-nepi_003.html#encoder-based-models-bert-and-bidirectional-context-understanding",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.2 Encoder-Based Models: BERT and Bidirectional Context Understanding",
    "text": "3.2 Encoder-Based Models: BERT and Bidirectional Context Understanding\nImmediately following the Transformer’s introduction, researchers began re-engineering its individual streams to develop pre-trained language models. These models excel at understanding language and readily adapt to various Natural Language Processing tasks with minimal additional training.\nThe encoder side of the Transformer gave rise to models such as BERT, which remains highly prevalent. BERT, an acronym for Bidirectional Encoder Representations from Transformers, operates by allowing each word within the input stream to interact with every other word. This bidirectional capability enables the model to construct a comprehensive contextual understanding of the entire input simultaneously. Consequently, BERT-like models primarily focus on coherently understanding sentences rather than generating new text. Devlin and colleagues introduced this architecture in their 2018 paper.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#decoder-based-models-gpt-and-generative-capabilities",
    "href": "chapter_ai-nepi_003.html#decoder-based-models-gpt-and-generative-capabilities",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.3 Decoder-Based Models: GPT and Generative Capabilities",
    "text": "3.3 Decoder-Based Models: GPT and Generative Capabilities\nConversely, the decoder side of the Transformer architecture led to the development of GPT models, or Generative Pre-trained Transformers. These models now power widely used applications such as ChatGPT. Their distinct structure allows words to access only their predecessors, establishing a unidirectional context. This design, however, confers a powerful generative capability, enabling them to produce new text and language—a function not inherently present in BERT models.\nBeyond these two primary types, various model architectures exist, including those that combine encoder and decoder components. Furthermore, researchers have devised sophisticated methods to enable decoder models to operate more akin to encoders, exemplified by models like XLNet and XLM. Fundamentally, understanding these distinctions is crucial: generative models, such as those in the GPT family, excel at producing language, whilst full-context models, like BERT, demonstrate superior capabilities in coherently understanding sentences. Radford and colleagues introduced the GPT architecture in 2018.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-large-language-models",
    "href": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.4 Evolution and Specialisation of Scientific Large Language Models",
    "text": "3.4 Evolution and Specialisation of Scientific Large Language Models\nA comprehensive overview charts the evolution of large language models from 2018 to 2024, specifically highlighting their development for scientific domains and tasks. This landscape categorises models into Encoder-Decoder, Decoder, and Encoder types, encompassing both open-source and closed-source variants. Notably, encoder models, akin to BERT, appear significantly more prevalent within scientific applications.\nEarly pioneering models, such as BioBERT, Specter, and SciBERT, gained considerable popularity. Today, researchers have developed a wide array of domain-specific models tailored for fields including biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. This proliferation underscores the substantial potential for scholars in the History, Philosophy, and Sociology of Science to either leverage existing models or craft their own specialised tools. Ho and colleagues provided this 2024 survey, detailing pre-trained language models for scientific text processing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.5 Strategies for Domain and Task Adaptation of Large Language Models",
    "text": "3.5 Strategies for Domain and Task Adaptation of Large Language Models\nAdapting large language models to specific scientific languages and tasks involves several key strategies. Pre-training represents the initial phase where a model acquires language by predicting either the next token, as seen in GPT models, or random masked words, characteristic of BERT models. This process, however, demands prohibitive computational resources and vast datasets, rendering it largely impractical for individual researchers.\nA more accessible approach involves continued pre-training, where researchers take an already pre-trained model, such as a BERT variant, and further train it on domain-specific language; for instance, applying it to physics texts. Beyond this, fine-tuning through the addition of extra parameters allows researchers to append new layers atop pre-trained models. These layers then undergo training for specific Natural Language Processing tasks, including sentiment classification or named entity recognition.\nWhilst prompt-based methods also exist, contrastive learning emerges as a pivotal technique. This method generates sentence or document embeddings from existing word embeddings, effectively mapping documents or sentences into the same embedding space as individual words. Sentence BERT stands out as a widely adopted and highly effective method within this domain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-for-domain-adaptation",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-for-domain-adaptation",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.6 Retrieval-Augmented Generation (RAG) for Domain Adaptation",
    "text": "3.6 Retrieval-Augmented Generation (RAG) for Domain Adaptation\nRetrieval-Augmented Generation, or RAG, offers a sophisticated pipeline for adapting models to specific scientific domains without necessitating direct model training. This system integrates multiple models, typically at least two, working in concert. Users frequently encounter RAG in contemporary generative AI tools, such as ChatGPT, where it underpins functionalities like internet search.\nThe RAG workflow commences when a user submits a query, for example, “What are LLMs?”. A BERT-type model encodes this query into a sentence embedding. Subsequently, this model searches a database of relevant documents to identify and retrieve the most similar passages. These retrieved passages are then seamlessly integrated into the prompt provided to a generative model. Drawing upon this newly supplied context, the generative model formulates and produces its answer. Crucially, this demonstrates that advanced reasoning models and agents are not monolithic LLMs but intricate systems that combine LLMs with a diverse array of other computational tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#fundamental-distinctions-in-large-language-model-paradigms",
    "href": "chapter_ai-nepi_003.html#fundamental-distinctions-in-large-language-model-paradigms",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.7 Fundamental Distinctions in Large Language Model Paradigms",
    "text": "3.7 Fundamental Distinctions in Large Language Model Paradigms\nUnderstanding the landscape of large language models necessitates grasping several fundamental distinctions.\nFirstly, models categorise by their core architectural types: encoder-based, such as BERT; decoder-based, exemplified by GPT; and hybrid encoder-decoder configurations.\nSecondly, researchers employ diverse fine-tuning strategies to adapt these models for specific tasks.\nA crucial differentiation lies in embeddings: word embeddings represent individual lexical units, whilst sentence embeddings capture the meaning of entire sentences. These two types operate at fundamentally different levels of abstraction. Beyond individual LLMs, the field distinguishes between pipelines, such as Retrieval-Augmented Generation (RAG), which combine multiple models, and agents, which represent complex systems integrating LLMs with a variety of external tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#categorisation-of-large-language-model-applications-in-hpss-research",
    "href": "chapter_ai-nepi_003.html#categorisation-of-large-language-model-applications-in-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.8 Categorisation of Large Language Model Applications in HPSS Research",
    "text": "3.8 Categorisation of Large Language Model Applications in HPSS Research\nAn ongoing survey documents the applications of large language models as tools within History, Philosophy, and Sociology of Science (HPSS) research. This survey has identified four primary categories for classifying these applications.\nThe first category, “Dealing with data and sources,” focuses on how scholars interact with and locate their data. This includes tasks such as parsing and extracting specific information, exemplified by identifying publication types, acknowledgements, or citations.\nThe second category, “Knowledge structures,” concerns the analysis of knowledge organisation. Here, applications involve entity extraction—for instance, identifying scientific instruments, celestial bodies, or chemicals—alongside mapping science policy discourses and delineating interdisciplinary fields.\n“Knowledge dynamics” constitutes the third category, addressing the evolution and change of knowledge over time. This encompasses conceptual histories of words—including the term “theory” in Digital Humanities or “virtual” and “Planck” in physics—and the reconstruction of arguments by identifying premises, conclusions, and causal relationships.\nFinally, “Knowledge practices” forms the fourth category, focusing on the production and utilisation of knowledge. A notable example is citation context analysis, an established HPSS tradition that, whilst often employed for evaluatory purposes today, offers significant utility for other HPSS tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#trends-and-challenges-in-hpss-applications-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#trends-and-challenges-in-hpss-applications-of-large-language-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.9 Trends and Challenges in HPSS Applications of Large Language Models",
    "text": "3.9 Trends and Challenges in HPSS Applications of Large Language Models\nScholars observe an accelerating interest in large language models within the History, Philosophy, and Sociology of Science, extending beyond traditional computational journals, such as Scientometrics and JASIS, into publications not typically associated with computational methods. This expansion stems from the remarkable semantic power of LLMs, which increasingly attracts qualitative researchers and philosophers.\nApplications demonstrate a wide spectrum of customisation, ranging from the straightforward, off-the-shelf use of tools such as ChatGPT to the intricate development of novel architectures and bespoke pre-training or fine-tuning. Despite this burgeoning interest, recurring concerns persist. These include the overwhelming computational resources demanded by LLMs, their inherent opaqueness, the scarcity of suitable training data, and the absence of standardised benchmarks. Furthermore, scholars consistently grapple with trade-offs between different model types, such as BERT-like versus GPT-like architectures, underscoring that no single model serves all purposes; rather, the appropriate model depends entirely on the specific research objective. Nevertheless, a discernible trend towards greater accessibility emerges, exemplified by tools like BERTTopic for topic modelling, which developers maintain meticulously to ensure ease of use.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#critical-reflections-on-integrating-large-language-models-into-hpss-research",
    "href": "chapter_ai-nepi_003.html#critical-reflections-on-integrating-large-language-models-into-hpss-research",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections",
    "section": "3.10 Critical Reflections on Integrating Large Language Models into HPSS Research",
    "text": "3.10 Critical Reflections on Integrating Large Language Models into HPSS Research\nIntegrating large language models into History, Philosophy, and Sociology of Science research necessitates careful critical reflection, particularly concerning HPSS-specific challenges. A primary concern involves the historical evolution of concepts and language. Given that LLMs typically undergo training on modern language, scholars must devise strategies for adapting them to historical contexts whilst remaining acutely aware of potential biases. Furthermore, HPSS scholarship adopts a reconstructive and critically reflective perspective, demanding that scholars read between the lines, comprehend authorial context, and discern subtle discursive strategies, such as boundary work. LLMs, however, are not inherently trained to detect such nuances. Practical data challenges, including sparse datasets, the presence of multiple languages, and old scripts, further complicate their application.\nConsequently, building LLM literacy becomes paramount. Scholars must thoroughly understand these tools, encompassing both their theoretical foundations and practical implications. This may entail acquiring coding skills, although natural language processing coding is progressively becoming more accessible. Crucially, scholars must avoid the superficial use of off-the-shelf tools without a profound comprehension of their outputs.\nFinally, maintaining fidelity to HPSS methodologies is essential. Scholars must translate HPSS problems into Natural Language Processing tasks whilst steadfastly preserving their core HPSS focus. This prevents NLP tasks, such as classification, generation, or summarisation, from inadvertently “hijacking” the original research purpose. Nevertheless, LLMs present novel opportunities for bridging qualitative and quantitative research approaches. Moreover, scholars should reflect upon the pre-history of these models within HPSS, recognising connections to earlier developments like co-word analysis, pioneered by Callon and Rip in the 1980s, which itself emerged from Actor-Network Theory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science: A Primer, Applications, and Critical Reflections</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila have developed OpenAlex Mapper, an innovative tool designed to mitigate critical generalisation and validation challenges pervasive in History, Philosophy, and Sociology of Science (HPSS) research. This presentation introduces the tool, clarifies its technical underpinnings, demonstrates its interactive capabilities, and examines its manifold applications within transdisciplinary contexts.\nThe methodology centres on fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, the team sampled 300,000 English abstracts from the OpenAlex database, a comprehensive and openly accessible repository of scholarly material. Engineers embedded these abstracts and reduced their dimensionality to two dimensions using Uniform Manifold Approximation and Projection (UMAP), thereby creating a foundational 2D base map. OpenAlex Mapper then allows users to submit arbitrary queries, downloading and embedding the corresponding records before projecting them onto this pre-trained UMAP model.\nThe interactive map facilitates in-depth investigation of specific terms, authors, temporal distributions, and citation networks. Significantly, the tool provides a rigorous quantitative framework that grounds qualitative, heuristic investigations, enabling researchers to trace the diffusion of models, map the distribution of concepts, and analyse method usage patterns across vast interdisciplinary samples. Examples include tracking the Hopfield model’s adoption, visualising model templates like Ising and Sherrington-Kirkpatrick, and contrasting the spatial distribution of concepts such as “phase transition” and “emergence,” alongside methods like Random Forest and Logistic Regression.\nDespite its utility, the system acknowledges several qualifications. It relies on the OpenAlex database, which, whilst robust, is not without imperfections, particularly concerning disciplinary representation. The current language model processes English-only sources, and the embedding step necessitates the presence of abstracts or well-formed titles. Furthermore, the UMAP algorithm, a stochastic process, introduces inherent trade-offs in dimensionality reduction, meaning the 768 dimensions of the Specter model cannot be perfectly represented in two, leading to potential misalignments. A working paper, Philosophy at Scale: Introducing OpenAlex Mapper, offers more detailed technical insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper Architecture and Workflow",
    "text": "4.1 OpenAlex Mapper Architecture and Workflow\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila crafted OpenAlex Mapper, a novel tool funded by an ERC grant focused on “possible life.” This innovation aims to introduce the tool, clarify its high-level technical operations, demonstrate its practical application, and ultimately discuss its utility for research within the History, Philosophy, and Sociology of Science (HPSS).\nThe core workflow of OpenAlex Mapper comprises several distinct stages. Initially, the team fine-tuned the Specter 2 embedding model, specifically to enhance its recognition of disciplinary boundaries. This process involved training the model on a dataset of articles originating from highly similar disciplinary backgrounds, with UMAP dimensionality reduction providing a visualisation of this training. Notably, these adjustments constituted minor modifications to the language model, rather than a comprehensive retraining effort.\nSubsequently, for base-map preparation, the researchers leveraged the OpenAlex database, a vast and inclusive repository of scholarly material that surpasses the scale of Web of Science or Scopus. OpenAlex offers fully open data, facilitating easy batch querying and free accessibility, which distinguishes it from many proprietary alternatives. From this extensive database, the team sampled 300,000 random articles, imposing minimal restrictions beyond requiring reasonably well-formed English abstracts. These abstracts then underwent embedding using the previously fine-tuned Specter 2 model. Engineers further reduced these embeddings to two dimensions through Uniform Manifold Approximation and Projection (UMAP), yielding both a 2D base map and a trained UMAP model.\nFor individual user queries, OpenAlex Mapper allows submission of arbitrary searches to the OpenAlex database. The tool downloads the relevant records—for instance, the first 1,000 for demonstration purposes—and embeds their abstracts using the identical fine-tuned language model. These new embeddings are then projected through the pre-trained UMAP model, ensuring that the queried articles acquire positions on the two-dimensional map consistent with their hypothetical presence during the original layout process. The resulting interactive map is accessible online and available for download via data mappers, offering features such as temporal distributions and citation graph overlays. Users can access the slides and interactive tool via maxnoichl.eu/talk, whilst a version with a higher latency GPU setup is also available for processing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration of OpenAlex Mapper",
    "text": "4.2 Interactive Demonstration of OpenAlex Mapper\nThe OpenAlex Mapper tool, accessible via https://m7n-openalex-mapper.hf.space, offers a straightforward user experience. Users initiate their investigation by searching the OpenAlex database directly through its comprehensive search interface, for example, by entering a query such as “scale-free network models.”\nIn the backend, the system efficiently downloads the initial 1,000 records pertinent to the search query, a limit imposed to optimise processing time. Subsequently, it embeds all abstracts from these downloaded records. If the user enables the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as a projection of the search results onto a pre-existing grey base map, visually representing the disciplinary landscape.\nCrucially, the map is fully interactive, empowering users to delve into specific data points. For instance, one can investigate the presence of a term like “coriander” within unexpected fields such as epidemiology or public health, gaining nuanced insights into interdisciplinary connections. The demonstration showcased queries for both “coriander,” a standard OpenAlex example, and “scale-free network models.” Furthermore, the developers have made an alternative setup available, featuring a higher latency GPU, which accommodates larger and more complex queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper primarily addresses the persistent challenges of generalisation and validation that arise from the reliance on small samples and case studies within History, Philosophy, and Sociology of Science (HPSS). Whilst traditional HPSS methods—such as the close reading of scholarly papers, direct interaction with scientists, and studies conducted by researchers with scientific training—offer invaluable detailed, close-up views of scientific processes, they often struggle to scale. Generalising these granular insights to the vast, global, and rapidly evolving landscape of contemporary science presents a significant hurdle.\nOpenAlex Mapper contributes by providing rigorous quantitative methods that effectively ground qualitative, heuristic investigations. A key feature of the tool is its capacity to trace all analytical results directly back to their original textual sources, ensuring transparency and scholarly rigour.\nThe tool supports several specific applications, offering compelling examples of its utility. Researchers can trace the diffusion of particular models, such as the Hopfield model, to ascertain where it genuinely “stuck” or achieved widespread adoption and sustained reference across diverse scientific domains. Furthermore, the system facilitates the investigation of “model templates”—conceptual frameworks defining models of similar structure that emerge in disparate scientific fields, potentially structuring science in ways orthogonal to established disciplines. Examples like the Ising, Hopfield, and Sherrington-Kirkpatrick models often appear at specific, non-continuous locations on the base map, providing crucial insights for ongoing debates concerning model transfer in science.\nBeyond models, OpenAlex Mapper enables the mapping of concept distribution. For instance, it can visually contrast the spread of “phase transition” (depicted in blue) with “emergence” (in orange), broadening such analyses into interdisciplinary contexts and circumventing common problems associated with acquiring specific datasets. Finally, the tool proves invaluable for analysing method usage. It reveals distinguishable patterns of specific methods within interdisciplinary contexts; for example, neuroscientists frequently employ Random Forest algorithms, whilst researchers in psychiatry or mental health often utilise Logistic Regression. This observation prompts profound philosophical questions regarding the underlying reasons for these patterns and their implications for debates on machine learning in science versus classical statistics, and the concept of “theory-free science.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Considerations",
    "text": "4.4 Limitations and Future Considerations\nWhilst OpenAlex Mapper offers significant analytical capabilities, its application is subject to several important qualifications. The system’s efficacy inherently depends on the OpenAlex database, which, despite its overall reasonable data quality compared to other available sources, is not without imperfections. Notably, certain disciplines, such as law, may exhibit underrepresentation within the database, potentially skewing comprehensive analyses.\nThe current language model processes English-only sources, which somewhat limits the tool’s global scope. Nevertheless, this constraint poses less of a problem for investigations focused on the more recent history of science. In principle, the integration of multilingual models could remedy this limitation, although the availability of high-quality, science-trained multilingual models remains scarce. Furthermore, the embedding step of the methodology necessitates that sources include either abstracts or well-formed titles, thereby restricting the range of processable data.\nCrucially, the method relies heavily on the Uniform Manifold Approximation and Projection (UMAP) algorithm, which presents its own set of imperfections. As a stochastic algorithm, UMAP generates one specific output amongst many possible configurations. Moreover, the algorithm must make inherent trade-offs during dimensionality reduction; the 768 dimensions of the Specter language model cannot be perfectly compressed into two, inevitably leading to some degree of “pushing and pulling and misaligning” of data points.\nFor those seeking further information, the presentation slides are available online at maxnoichl.eu/talk. Additionally, a working paper, titled Philosophy at Scale: Introducing OpenAlex Mapper, provides more exhaustive technical details regarding the tool’s development and operation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThis report systematically documents the methodology and findings of ActDisease, an ERC-funded research initiative. The project meticulously investigates the historical evolution of patient organisations in 20th-century Europe. The authors primarily utilise a substantial, recently digitised collection of patient organisation periodicals, encompassing 96,186 pages from Sweden, Germany, France, and the UK. Acknowledging the inherent challenges of digitisation, particularly Optical Character Recognition (OCR) errors in complex layouts and creative texts, the research team has explored post-OCR correction techniques using instruction-tuned generative models.\nA central objective involves classifying the diverse textual genres within these historical magazines. This crucial step facilitates fine-grained historical analysis that transcends the limitations of aggregate topic models. Under expert historical supervision, the authors defined nine distinct genres: Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and QA. This classification ensures both analytical utility and general applicability. An annotation project involving six project members achieved a high inter-annotator agreement (0.95 Krippendorff’s alpha) on paragraphs sampled from Swedish and German periodicals.\nGiven the scarcity of annotated data, the authors rigorously explored both zero-shot and few-shot learning paradigms. Zero-shot experiments leveraged publicly available, modern datasets, including the Corpus of Online Registers of English (CORE), Functional Text Dimensions (FTD), and UD-MULTIGENRE, through a meticulous genre mapping process. These experiments employed multilingual encoders such as XLM-Roberta, mBERT, and historical mBERT, revealing varying performance across genres and models, alongside class-specific biases.\nFew-shot learning, conducted on the ActDisease dataset, demonstrated clear performance advantages with increased training instances, particularly for historical mBERT when coupled with prior Masked Language Model (MLM) fine-tuning. Additionally, the authors investigated few-shot prompting with the Llama 3.1 8b Instruct model, observing its capacity to handle certain genre labels effectively, whilst highlighting the need for more comprehensive examples for others.\nThe findings underscore the inherent complexity of text mining popular magazines due to their rich genre diversity. They affirm genre classification as an indispensable tool for rendering these historical sources accessible for nuanced textual analysis. Future work encompasses refining annotation schemes, generating synthetic data, and implementing active learning strategies to further enhance classification quality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-source-materials",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-source-materials",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 The ActDisease Project: Objectives and Source Materials",
    "text": "5.1 The ActDisease Project: Objectives and Source Materials\n\n\n\nSlide 01\n\n\nThe ActDisease project, an initiative funded by the European Research Council, meticulously investigates the historical trajectory of patient organisations across 20th-century Europe. The authors specifically aim to understand how these organisations fundamentally shaped disease concepts, influenced illness experiences, and contributed to the evolution of medical practices. Their study concentrates on ten distinct European patient organisations, drawing examples from Sweden, Germany, France, and Great Britain, covering the period from approximately 1890 to 1990. Crucially, the primary source material comprises periodicals, predominantly magazines, published by these patient organisations. The project team notes the historical significance of locations such as Heligoland, Germany, which served as the founding place for the Hay Fever Association of Heligoland in 1897.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "href": "chapter_ai-nepi_005.html#actdisease-dataset-composition",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 ActDisease Dataset Composition",
    "text": "5.2 ActDisease Dataset Composition\n\n\n\nSlide 01\n\n\nThe ActDisease project has assembled a comprehensive private dataset, comprising a recently digitised collection of patient organisation magazines. This extensive corpus totals 96,186 pages, providing a rich foundation for historical inquiry. The dataset encompasses materials from various European countries, each focusing on specific diseases and spanning distinct chronological periods.\nSpecifically, the German collection includes:\n\nTwo magazines on Allergy/Asthma, covering 10,926 pages from 1901 to 1985.\nOne magazine on Diabetes, with 19,324 pages from 1931 to 1990.\nOne on Multiple Sclerosis, contributing 5,646 pages from 1954 to 1990.\n\nSwedish contributions feature:\n\nOne Allergy/Asthma magazine (4,054 pages, 1957-1990).\nOne Diabetes magazine (7,150 pages, 1949-1990).\nOne Lung Diseases magazine (16,790 pages, 1938-1991).\n\nFrom France, the dataset incorporates:\n\nOne Diabetes magazine (6,206 pages, 1947-1990).\nThree magazines on Rheumatism/Paralysis (9,317 pages, 1935-1990).\n\nFinally, the UK component includes:\n\nOne Diabetes magazine (11,127 pages, 1935-1990).\nOne Rheumatism magazine (5,646 pages, 1950-1990).\n\nThis detailed breakdown highlights the breadth and depth of the primary source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "href": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 Digitisation Challenges and Post-OCR Correction",
    "text": "5.3 Digitisation Challenges and Post-OCR Correction\n\n\n\nSlide 01\n\n\nThe digitisation process for the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition. This tool generally performed well, accurately recognising most common layouts and fonts encountered in the periodicals. Nevertheless, significant challenges persisted, particularly with complex page layouts, slanted text, rare font types, and inconsistencies arising from varying scan or photo quality.\nThese issues frequently resulted in OCR errors, notably in German and French texts, and often led to a disrupted reading order within documents. To address these limitations, Danilova and Aangenendt conducted specific experiments focusing on post-OCR correction of German texts, leveraging instruction-tuned generative models. This work is detailed in their forthcoming publication. A notable observation during this phase was the high frequency of OCR errors in creative textual elements, such as advertisements, humour pages, and poems, which often feature non-standard formatting or stylistic choices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#motivation-for-genre-classification-addressing-textual-diversity",
    "href": "chapter_ai-nepi_005.html#motivation-for-genre-classification-addressing-textual-diversity",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Motivation for Genre Classification: Addressing Textual Diversity",
    "text": "5.4 Motivation for Genre Classification: Addressing Textual Diversity\n\n\n\nSlide 02\n\n\nA significant challenge in analysing the ActDisease materials stems from their inherent textual diversity. The authors observed a wide array of text types within the periodicals, a diversity that remained consistent across all magazines. Crucially, these disparate text types frequently co-occurred on the same page; for instance, an administrative report might appear alongside an advertisement or a humour section.\nExisting analytical methods, such as yearly and decade-based topic models and term counts, inherently fail to account for this intricate co-occurrence of genres. Consequently, these aggregate analyses are likely to exhibit a bias, disproportionately reflecting the characteristics of the most frequent text type present on a page or within a given period. This limitation underscores the necessity for a more granular approach to textual analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#textual-diversity-and-analytical-limitations",
    "href": "chapter_ai-nepi_005.html#textual-diversity-and-analytical-limitations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Textual Diversity and Analytical Limitations",
    "text": "5.5 Textual Diversity and Analytical Limitations\n\n\n\nSlide 02\n\n\nThe ActDisease materials exhibit a profound textual diversity, with a wide array of text types appearing consistently across all magazines. Notably, these distinct text types frequently co-exist within the same page, presenting a complex analytical landscape. For example, a single page might feature an administrative report, an advertisement, and a humour section, each serving a unique communicative purpose.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#limitations-of-aggregate-topic-modelling",
    "href": "chapter_ai-nepi_005.html#limitations-of-aggregate-topic-modelling",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Limitations of Aggregate Topic Modelling",
    "text": "5.6 Limitations of Aggregate Topic Modelling\n\n\n\nSlide 02\n\n\nThe periodicals within the ActDisease dataset demonstrate a consistent textual diversity, with a variety of text types appearing across all magazines. A key challenge arises from the co-occurrence of distinct text types on a single page; for instance, an administrative report might share space with an advertisement or a humour section. This intricate intermingling of genres poses a significant limitation for conventional analytical methods, as yearly and decade-based topic models and term counts inherently fail to account for such fine-grained textual variations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#addressing-bias-in-textual-analysis",
    "href": "chapter_ai-nepi_005.html#addressing-bias-in-textual-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Addressing Bias in Textual Analysis",
    "text": "5.7 Addressing Bias in Textual Analysis\n\n\n\nSlide 02\n\n\nThe ActDisease materials consistently exhibit a wide range of text types across all magazines. A critical observation reveals that distinct text types frequently appear side-by-side on the same page, such as administrative reports, advertisements, and humour sections. Conventional analytical approaches, including yearly and decade-based topic models and term counts, do not adequately account for this intricate co-occurrence. Consequently, these methods are prone to bias, disproportionately emphasising the characteristics of the most frequently occurring text type within a given analytical unit.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-as-a-foundational-concept-for-historical-analysis",
    "href": "chapter_ai-nepi_005.html#genre-as-a-foundational-concept-for-historical-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Genre as a Foundational Concept for Historical Analysis",
    "text": "5.8 Genre as a Foundational Concept for Historical Analysis\n\n\n\nSlide 03\n\n\nRecognising the limitations of aggregate analysis, genre emerged as a particularly useful concept for distinguishing between various text types. Within Language Technology, Petrenz (2004) and Kessler (1997) define genre as a class of documents united by a shared communicative purpose. This conceptual framework directly supports the project’s core objective: to explore the historical data from multiple perspectives, thereby facilitating robust historical arguments.\nSpecifically, genre classification enables a nuanced study of communicative strategies as they evolve over time, as Broersma (2010) highlights. This approach allows for comparative analysis across different countries, diseases, and publications. Furthermore, it facilitates a more granular examination of term distributions and the application of topic models, ensuring that these analyses are conducted within specific genre groups, thereby enhancing their precision and historical relevance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-within-the-actdisease-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Illustrative Genre Examples within the ActDisease Dataset",
    "text": "5.9 Illustrative Genre Examples within the ActDisease Dataset\n\n\n\nSlide 03\n\n\nTo illustrate the practical application of genre classification within the ActDisease dataset, the authors provide several concrete examples. These instances demonstrate the diverse textual forms encountered in the historical periodicals and clarify how distinct genres manifest within the corpus.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-academic-reports",
    "href": "chapter_ai-nepi_005.html#genre-examples-academic-reports",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Genre Examples: Academic Reports",
    "text": "5.10 Genre Examples: Academic Reports\n\n\n\nSlide 03\n\n\nOne illustrative example of genre within the dataset is an academic report. This specific instance details studies conducted on the pancreas, showcasing the presence of scientific and research-oriented content within the periodicals.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-legal-documents",
    "href": "chapter_ai-nepi_005.html#genre-examples-legal-documents",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Genre Examples: Legal Documents",
    "text": "5.11 Genre Examples: Legal Documents\n\n\n\nSlide 03\n\n\nAnother distinct genre identified is legal documentation. An example provided is a deed of covenant, demonstrating the inclusion of formal legal texts within the historical periodicals.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-advertisements",
    "href": "chapter_ai-nepi_005.html#genre-examples-advertisements",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Genre Examples: Advertisements",
    "text": "5.12 Genre Examples: Advertisements\n\n\n\nSlide 04\n\n\nThe dataset also features advertisements as a prominent genre. One particular example showcases an advertisement for chocolate, specifically formulated and marketed for individuals with diabetes, reflecting the commercial aspects present in these historical publications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-instructiveguidance-texts",
    "href": "chapter_ai-nepi_005.html#genre-examples-instructiveguidance-texts",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Genre Examples: Instructive/Guidance Texts",
    "text": "5.13 Genre Examples: Instructive/Guidance Texts\n\n\n\nSlide 04\n\n\nInstructive or guidance texts form another key genre. Examples include practical advice, such as recipes, or medical guidance provided by doctors, including dietary recommendations. These texts aim to inform and direct the reader on specific actions or health-related matters.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-patient-organisation-reports",
    "href": "chapter_ai-nepi_005.html#genre-examples-patient-organisation-reports",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Genre Examples: Patient Organisation Reports",
    "text": "5.14 Genre Examples: Patient Organisation Reports\n\n\n\nSlide 04\n\n\nPatient organisation reports constitute a significant genre within the corpus. These documents typically detail the proceedings of meetings and outline the various activities undertaken by the organisations, providing insight into their operational aspects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-examples-patient-experiences",
    "href": "chapter_ai-nepi_005.html#genre-examples-patient-experiences",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 Genre Examples: Patient Experiences",
    "text": "5.15 Genre Examples: Patient Experiences\n\n\n\nSlide 04\n\n\nFinally, the dataset includes narratives focused on patient experiences. These texts recount personal stories and aspects of patient lives, offering qualitative insights into the lived realities of individuals with specific health conditions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-classification-experiments-zero-shot-and-few-shot-learning",
    "href": "chapter_ai-nepi_005.html#genre-classification-experiments-zero-shot-and-few-shot-learning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Genre Classification Experiments: Zero-Shot and Few-Shot Learning",
    "text": "5.16 Genre Classification Experiments: Zero-Shot and Few-Shot Learning\n\n\n\nSlide 04\n\n\nGiven the significant constraint of very limited annotated data, the authors systematically explored both zero-shot and few-shot learning paradigms for genre classification. The zero-shot investigations addressed two primary research questions: firstly, whether an efficient mapping of genre labels from publicly available datasets to the project’s custom labels could yield satisfactory performance on the test set; and secondly, how classification performance might vary across different external datasets and models.\nFor few-shot learning, the inquiry focused on understanding how performance changes with varying training data sizes across different models. A further crucial question examined whether prior fine-tuning on the entire dataset could substantially enhance classification performance. These experiments form the basis of a forthcoming publication by Danilova and Söderfeldt, presented at LaTeCH-CLFL 2025.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-custom-genre-labels-for-historical-analysis",
    "href": "chapter_ai-nepi_005.html#defining-custom-genre-labels-for-historical-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Defining Custom Genre Labels for Historical Analysis",
    "text": "5.17 Defining Custom Genre Labels for Historical Analysis\n\n\n\nSlide 05\n\n\nThe project meticulously defined its custom genre labels under the direct supervision of the main historian, an expert in patient organisations. This collaborative approach ensured that the labels would prove highly useful for segmenting content within the ActDisease materials, thereby facilitating more granular historical analysis. Furthermore, the design principle aimed for maximum generality, enabling the classifier’s potential application to similar historical datasets beyond the immediate scope of the project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defined-genres-and-their-communicative-purposes",
    "href": "chapter_ai-nepi_005.html#defined-genres-and-their-communicative-purposes",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Defined Genres and Their Communicative Purposes",
    "text": "5.18 Defined Genres and Their Communicative Purposes\n\n\n\nSlide 05\n\n\nThe authors established a comprehensive set of genres, each with a clearly defined communicative purpose:\n\nAcademic texts encompass research-based reports or explanations of scientific ideas, such as articles or formal reports, primarily aiming to convey information from the scientific and medical community to the magazine’s readership.\nAdministrative documents, including meeting minutes, reports, or announcements, serve to inform readers about the patient organisation’s events and activities.\nAdvertisements explicitly promote products or services for commercial ends.\nGuide texts offer step-by-step instructions, ranging from health tips and legal advice to recipes.\nFiction, conversely, seeks to entertain and emotionally engage through stories, poems, humour, or myths.\nLegal documents elucidate terms and conditions, encompassing contracts, rules, and amendments.\nNews reports recent events and developments.\nNonfiction Prose narrates real events or describes cultural and historical topics, exemplified by memoirs, essays, or documentaries.\nQA (Question and Answer) sections present structured questions alongside expert responses, directly sourced from the periodicals themselves.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-methodology-and-inter-annotator-agreement",
    "href": "chapter_ai-nepi_005.html#annotation-methodology-and-inter-annotator-agreement",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Annotation Methodology and Inter-Annotator Agreement",
    "text": "5.19 Annotation Methodology and Inter-Annotator Agreement\n\n\n\nSlide 06\n\n\nFor the annotation process, the authors defined the paragraph as the primary unit. These paragraphs originated from the ABBYY OCR output and were subsequently merged based on consistent font patterns—including type, size, plain text, and italics—within each page. The annotation effort focused on a carefully selected sample from two periodicals: the Swedish “Diabetes” and the German “Diabetiker Journal,” specifically their first and mid-year issues from each publication year.\nSix project members undertook the annotation task, comprising four historians and two computational linguists, all possessing native or proficient fluency in either Swedish or German. Each paragraph received two independent annotations. This rigorous approach yielded an impressive average inter-annotator agreement of 0.95, measured by Krippendorff’s alpha, signifying a remarkably high level of consistency amongst the annotators.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-process-illustration",
    "href": "chapter_ai-nepi_005.html#annotation-process-illustration",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.20 Annotation Process Illustration",
    "text": "5.20 Annotation Process Illustration\n\n\n\nSlide 06\n\n\nTo illustrate the annotation process, the authors presented a sample from the periodical “Der Diabetiker.” This example took the form of a screenshot from a .numbers file, the digital tool employed by the annotators. Whilst the original sentences appeared in German, the illustration provided Google Translate renditions for clarity. The file structure featured columns for metadata such as Year, Volume, Issue Number, Title, and the Paragraph text itself. Following these, a series of binary flag columns corresponded to each defined genre—academic, administrative, advertisement, fiction, guide, nonfiction prose, legal, QA, and news. Annotators were tasked with providing definitive, hard assignments for the genre of each paragraph, a process that, whilst challenging, they successfully completed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "href": "chapter_ai-nepi_005.html#dataset-splitting-and-experimental-configurations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.21 Dataset Splitting and Experimental Configurations",
    "text": "5.21 Dataset Splitting and Experimental Configurations\n\n\n\nSlide 07\n\n\nFor the experimental phase, the authors meticulously split the annotated data into distinct sets. The main division comprised a training set of 1182 paragraphs and a held-out set of 552 paragraphs, constituting approximately 30% of the total annotated data, with stratification applied by label.\nFor few-shot experiments, the training data was further subdivided into six different sizes: 100, 200, 300, 400, 500, and the full 1182 paragraphs. The authors randomly sampled these subsets from the main training set, ensuring balance across labels. The held-out set was then equally partitioned into validation and test sets, also maintaining label balance. Notably, the legal and news genres were excluded from these experiments due to insufficient training data. Conversely, zero-shot experiments utilised the entirety of the test set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-and-language-distribution-in-actdisease-datasets",
    "href": "chapter_ai-nepi_005.html#genre-and-language-distribution-in-actdisease-datasets",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.22 Genre and Language Distribution in ActDisease Datasets",
    "text": "5.22 Genre and Language Distribution in ActDisease Datasets\n\n\n\nSlide 07\n\n\nA visual representation of the ActDisease dataset’s composition reveals the distributions across languages and genres within both the training and held-out samples. The authors’ analysis highlighted a pronounced imbalance in the representation of advertisement and non-fictional prose genres when examined across different languages, indicating potential challenges for models in generalising across these categories.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-genre-classification",
    "href": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.23 External Datasets for Zero-Shot Genre Classification",
    "text": "5.23 External Datasets for Zero-Shot Genre Classification\n\n\n\nSlide 07\n\n\nFor the zero-shot experiments, the authors incorporated several external datasets, drawing primarily from modern collections previously utilised in automatic web genre classification. The Corpus of Online Registers of English (CORE), compiled by Egbert et al. (2015), provided English data, alongside main categories in Swedish, Finnish, and French, with annotations at the document level. Similarly, the Functional Text Dimensions (FTD) dataset of web genres, developed by Sharoff (2018), offered balanced English and Russian content, also annotated at the document level, and had seen prior application in web genre classification by Kuzman et al. (2023). Additionally, a subset of Universal Dependencies, known as UD-MULTIGENRE (de Marneffe et al., 2021), provided genre annotations at the sentence level across 38 languages, with recovered annotations by Danilova and Stymne (2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-mapping",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.24 Cross-Dataset Genre Mapping",
    "text": "5.24 Cross-Dataset Genre Mapping\n\n\n\nSlide 08\n\n\nA critical step in the zero-shot methodology involved mapping the project’s custom genre labels to those present in the external datasets. The authors employed two independent annotators for this genre mapping, with only assignments achieving full agreement selected for the final mapping. The resulting table illustrates how ActDisease genres—including Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction, and QA—correspond to labels within the CORE, UDM, and FTD datasets. Notably, the authors observed that certain ActDisease genres lacked suitable, directly mappable equivalents within the available external datasets, posing a challenge for comprehensive cross-dataset alignment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-creation-pipeline-and-model-fine-tuning",
    "href": "chapter_ai-nepi_005.html#training-data-creation-pipeline-and-model-fine-tuning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.25 Training Data Creation Pipeline and Model Fine-tuning",
    "text": "5.25 Training Data Creation Pipeline and Model Fine-tuning\n\n\n\nSlide 08\n\n\nThe training data creation pipeline commenced with the established genre mapping, followed by crucial preprocessing, chunking, and sampling stages. The authors generated training sets in four distinct configurations: one focusing exclusively on Germanic languages ([G+]), another balancing data according to the ActDisease labels ([B1]), a third incorporating all language families ([G-]), and a final configuration balancing data by both ActDisease and original labels ([B2]).\nThis systematic approach yielded four training samples each from the FTD, CORE, UDM, and a merged dataset. Subsequently, these samples underwent fine-tuning with a selection of multilingual encoder models, specifically XLM-Roberta, mBERT, and hmBERT. This comprehensive process ultimately produced a total of 48 fine-tuned models for evaluation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models-for-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.26 Multilingual Encoder Models for Genre Classification",
    "text": "5.26 Multilingual Encoder Models for Genre Classification\n\n\n\nSlide 09\n\n\nFor the genre classification experiments, the authors employed a selection of robust multilingual encoder models. These included XLM-Roberta, developed by Conneau et al. (2020); mBERT, introduced by Devlin et al. (2019); and historical mBERT, a model from Schweter et al. (2022). The choice of these models was deliberate, resting on their proven efficacy in prior research.\nBERT-like models, in general, have seen extensive application in web register and genre classification, as evidenced by works from Lepekhin and Sharoff (2022), Kuzman and Ljubešić (2023), and Laippala et al. (2023). XLM-RoBERTa, in particular, stands out as a state-of-the-art web genre classifier, according to Kuzman et al. (2023). Historical mBERT held particular interest due to its pretraining on a substantial corpus of multilingual historical newspapers, which notably encompassed the languages relevant to the ActDisease project. Conversely, mBERT served as a crucial comparative baseline against historical mBERT, as direct comparisons with XLM-Roberta were not feasible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#model-training-and-evaluation-metrics",
    "href": "chapter_ai-nepi_005.html#model-training-and-evaluation-metrics",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.27 Model Training and Evaluation Metrics",
    "text": "5.27 Model Training and Evaluation Metrics\n\n\n\nSlide 09\n\n\nFollowing the comprehensive fine-tuning process across all specified configurations, the authors successfully generated 48 distinct models. For evaluation purposes, they computed and presented the performance metrics as averages across these diverse configurations, providing a robust overview of the models’ capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-methodology",
    "href": "chapter_ai-nepi_005.html#zero-shot-learning-evaluation-methodology",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.28 Zero-Shot Learning Evaluation Methodology",
    "text": "5.28 Zero-Shot Learning Evaluation Methodology\n\n\n\nSlide 09\n\n\nEvaluating the zero-shot predictions presented a unique challenge: the imperfect overlap between the project’s custom genre labels and those in the external datasets precluded a direct comparison of overall performance metrics. To circumvent this, the authors meticulously assessed the performance of each genre individually, whilst also analysing confusion matrices to identify and mitigate potential biases.\nThe X-GENRE web genre classifier, a state-of-the-art model developed by Kuzman et al. (2023), served as a crucial baseline. Predictions were strictly limited to the most similar labels that could be directly mapped to the project’s custom categories. Furthermore, the evaluation acknowledged the cross-lingual nature of certain scenarios: the FTD and X-GENRE applications were entirely cross-lingual, lacking German or Swedish training data, whilst the UDM and CORE datasets presented partially cross-lingual contexts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-classification-results-and-model-biases",
    "href": "chapter_ai-nepi_005.html#zero-shot-classification-results-and-model-biases",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.29 Zero-Shot Classification Results and Model Biases",
    "text": "5.29 Zero-Shot Classification Results and Model Biases\n\n\n\nSlide 10\n\n\nOverall, the zero-shot classification experiments revealed that models fine-tuned on the Functional Text Dimensions (FTD) dataset exhibited superior performance when applied with the project’s custom genre mapping. Whilst most configurations avoided significant bias, other datasets demonstrated distinct class-specific tendencies. For instance, models trained on UD-MULTIGENRE (UDM) showed a bias towards news, primarily because the news training data contained the highest proportion of Germanic instances, predominantly German. Conversely, CORE-based models displayed a bias towards the guide genre, as only its guide training data was multilingual.\nIntriguingly, specific models demonstrated notable strengths in particular genres. XLM-Roberta, when fine-tuned on UDM, achieved an average of 32% more correct predictions in the QA category compared to mBERT and hmBERT. Similarly, hmBERT, also on UDM, outperformed XLM-Roberta and mBERT in the Administrative genre by an average of 16% more correct predictions. Furthermore, models trained on CORE consistently performed well in predicting legal texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#visualising-zero-shot-performance-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#visualising-zero-shot-performance-confusion-matrices",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.30 Visualising Zero-Shot Performance: Confusion Matrices",
    "text": "5.30 Visualising Zero-Shot Performance: Confusion Matrices\n\n\n\nSlide 10\n\n\nTo provide a granular understanding of the zero-shot classification performance, the authors presented confusion matrices for several key configurations. These visualisations critically highlight specific classification behaviours, such as instances of particularly strong performance in certain categories or recurring patterns of misclassification, which were explicitly delineated with red frames for emphasis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#detailed-zero-shot-f1-scores-by-category",
    "href": "chapter_ai-nepi_005.html#detailed-zero-shot-f1-scores-by-category",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.31 Detailed Zero-Shot F1 Scores by Category",
    "text": "5.31 Detailed Zero-Shot F1 Scores by Category\n\n\n\nSlide 11\n\n\nThe detailed average F1 scores for each category provide a comprehensive overview of the zero-shot classification performance. Notably, specific values were highlighted, indicating instances where the observed performance did not stem from systematic biases towards those particular categories, thereby suggesting more robust and generalisable classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#overall-average-performance-across-configurations",
    "href": "chapter_ai-nepi_005.html#overall-average-performance-across-configurations",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.32 Overall Average Performance Across Configurations",
    "text": "5.32 Overall Average Performance Across Configurations\n\n\n\nSlide 11\n\n\nThe authors elected to bypass a detailed discussion of the overall average performance across configurations during the presentation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-mlm-fine-tuning",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-and-mlm-fine-tuning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.33 Few-Shot Learning Performance and MLM Fine-tuning",
    "text": "5.33 Few-Shot Learning Performance and MLM Fine-tuning\n\n\n\nSlide 11\n\n\nThe few-shot learning experiments unequivocally demonstrated that further training on the ActDisease dataset, especially when augmented with Masked Language Model (MLM) fine-tuning, confers a distinct advantage. The authors observed a consistent upward trend in the F1 score as the number of training instances increased, although performance remained below 0.8 even with the largest training set of 1182 instances.\nCrucially, hmBERT-MLM emerged as the top performer amongst the models. Prior MLM fine-tuning significantly boosted the performance of this historical model, enabling it to outperform all other models, albeit by a narrow margin. This finding underscores the value of domain-specific pre-training for enhanced classification accuracy in low-resource settings.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#detailed-few-shot-performance-per-category-f1-and-overall-metrics",
    "href": "chapter_ai-nepi_005.html#detailed-few-shot-performance-per-category-f1-and-overall-metrics",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.34 Detailed Few-Shot Performance: Per-Category F1 and Overall Metrics",
    "text": "5.34 Detailed Few-Shot Performance: Per-Category F1 and Overall Metrics\n\n\n\nSlide 12\n\n\nA granular examination of the detailed scores revealed the underlying reason for hmBERT’s superior performance in the few-shot learning paradigm. This model uniquely maintained its ability to distinguish between fiction and nonfiction genres, even when utilising the full dataset size. Conversely, other models, notably XLM-Roberta, exhibited a drastic decline in performance when attempting to differentiate these two categories, highlighting a specific area of weakness in their classification capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#challenges-in-distinguishing-fiction-and-nonfiction-prose",
    "href": "chapter_ai-nepi_005.html#challenges-in-distinguishing-fiction-and-nonfiction-prose",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.35 Challenges in Distinguishing Fiction and Nonfiction Prose",
    "text": "5.35 Challenges in Distinguishing Fiction and Nonfiction Prose\n\n\n\nSlide 12\n\n\nAn analysis of the XLM-Roberta-MLM confusion matrix, particularly with the full-sized training dataset, revealed a persistent challenge: nonfiction prose was frequently overpredicted as fiction. This phenomenon suggests that, with larger datasets, fiction and nonfictional prose may become increasingly similar in their textual characteristics. This convergence is likely exacerbated by the domain-specific nature of the corpus, as all genres are confined to patient organisation magazines primarily focused on diabetes. Consequently, both fictional and (auto)biographical texts frequently revolve around the shared experiences of diabetes patients, leading to an overlap in themes and narrative structures. The authors conclude that more data is likely required to enhance the model’s ability to accurately differentiate between these two closely related genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.36 Few-Shot Prompting Evaluation",
    "text": "5.36 Few-Shot Prompting Evaluation\n\n\n\nSlide 13\n\n\nThe authors also undertook an evaluation of few-shot prompting techniques, exploring their efficacy in genre classification.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-with-llama-3.1-8b-instruct",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.37 Few-Shot Prompting with Llama 3.1 8b Instruct",
    "text": "5.37 Few-Shot Prompting with Llama 3.1 8b Instruct\n\n\n\nSlide 13\n\n\nGiven the current lack of sufficient data for comprehensive instruction tuning, the authors explored few-shot prompting with Llama 3.1 8b Instruct, a widely recognised multilingual generative model with open weights. The prompt structure incorporated clear genre definitions, supplemented by two to three carefully selected examples for each category.\nThe results, measured by F1-score, indicated varied performance across genres: QA achieved 0.62, academic 0.72, administrative 0.60, advertisement 0.73, fiction 0.64, guide 0.61, legal 0.84, news 0.08, and nonfictional prose 0.49. Overall metrics included an accuracy of 0.62, a macro average of 0.59, and a weighted average of 0.63. Whilst the model handled certain labels competently, the limited number of examples proved insufficient to adequately represent the nuances of some genres, particularly nonfictional prose, advertisement, and administrative texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusions-genre-diversity-and-text-mining-challenges",
    "href": "chapter_ai-nepi_005.html#conclusions-genre-diversity-and-text-mining-challenges",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.38 Conclusions: Genre Diversity and Text Mining Challenges",
    "text": "5.38 Conclusions: Genre Diversity and Text Mining Challenges\n\n\n\nSlide 13\n\n\nThe authors conclude that popular magazines, unlike more homogenous scientific journals or books, frequently contain a multitude of genres, which inherently complicates text mining efforts. Nevertheless, these magazines represent a highly promising source for historical research, particularly within the history of science. The rich diversity of genres within these publications directly reflects the deliberate choices of communicative strategies employed by their authors and editors.\nWhilst accounting for these varied genres presents a significant challenge, it remains critically important for ensuring the accurate and detailed interpretation of text mining results. Ultimately, genre classification emerges as a vital methodology, capable of rendering these complex historical sources accessible for advanced textual analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#leveraging-modern-datasets-for-zero-shot-classification",
    "href": "chapter_ai-nepi_005.html#leveraging-modern-datasets-for-zero-shot-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.39 Leveraging Modern Datasets for Zero-Shot Classification",
    "text": "5.39 Leveraging Modern Datasets for Zero-Shot Classification\n\n\n\nSlide 14\n\n\nPopular magazines, characterised by their rich genre diversity, pose a greater challenge for text mining than more uniform scientific journals or books. Despite this, genre classification offers a powerful means to render these sources accessible for advanced textual analysis. Crucially, even in the absence of specific training data, the authors can successfully leverage existing modern datasets for classification, particularly when working with broadly defined, general-purpose genre categories.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#effectiveness-of-open-generative-models",
    "href": "chapter_ai-nepi_005.html#effectiveness-of-open-generative-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.40 Effectiveness of Open Generative Models",
    "text": "5.40 Effectiveness of Open Generative Models\n\n\n\nSlide 14\n\n\nThe inherent genre diversity of popular magazines renders text mining more complex than for scientific journals or books. Nevertheless, genre classification provides a vital pathway to make these historical sources amenable to textual analysis. When confronted with a lack of specific training data, the authors can effectively utilise existing modern datasets. Furthermore, open generative models have demonstrated their capacity to achieve a decent level of classification quality, offering a viable alternative in resource-constrained scenarios.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#superiority-of-few-shot-learning-with-multilingual-encoders",
    "href": "chapter_ai-nepi_005.html#superiority-of-few-shot-learning-with-multilingual-encoders",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.41 Superiority of Few-Shot Learning with Multilingual Encoders",
    "text": "5.41 Superiority of Few-Shot Learning with Multilingual Encoders\n\n\n\nSlide 14\n\n\nPopular magazines, with their rich array of genres, present a more intricate challenge for text mining than the more uniform content of scientific journals or books. Despite this, genre classification proves instrumental in making these historical sources accessible for analysis. In situations where specific training data is scarce, leveraging available modern datasets offers a viable solution. Whilst open generative models can achieve a respectable level of quality, few-shot learning applied to multilingual encoders, particularly when combined with prior Masked Language Model (MLM) fine-tuning, consistently demonstrates superior performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#significant-gains-with-historical-multilingual-bert",
    "href": "chapter_ai-nepi_005.html#significant-gains-with-historical-multilingual-bert",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.42 Significant Gains with Historical Multilingual BERT",
    "text": "5.42 Significant Gains with Historical Multilingual BERT\n\n\n\nSlide 15\n\n\nThe inherent genre diversity of popular magazines renders text mining more complex than for scientific journals or books. Nevertheless, genre classification proves instrumental in making these historical sources accessible for analysis. In situations where specific training data is scarce, leveraging available modern datasets offers a viable solution. Whilst open generative models can achieve a respectable level of quality, few-shot learning applied to multilingual encoders, particularly when combined with prior Masked Language Model (MLM) fine-tuning, consistently demonstrates superior performance. Notably, historical multilingual BERT exhibited particularly strong gains, achieving a 24% improvement, significantly outperforming mBERT-MLM (14.5%) and XLM-RoBERTa (16.9%).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "href": "chapter_ai-nepi_005.html#ongoing-and-future-research-directions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.43 Ongoing and Future Research Directions",
    "text": "5.43 Ongoing and Future Research Directions\n\n\n\nSlide 15\n\n\nThe project continues to advance, currently engaging with specific historical hypotheses to deepen its analytical insights. The authors are actively developing a new annotation scheme designed to capture more fine-grained genre distinctions, an initiative supported by funding from Swe-CLARIN. Furthermore, the team is exploring advanced methodologies such as synthetic data generation and active learning to enhance the quality and efficiency of their classification efforts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#acknowledgements",
    "href": "chapter_ai-nepi_005.html#acknowledgements",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.44 Acknowledgements",
    "text": "5.44 Acknowledgements\n\n\n\nSlide 15\n\n\nThe authors gratefully acknowledge the invaluable contributions of their annotators, comprising the dedicated project team members: Ylva Söderfeldt, Julia Reed, Andrew Burchell, Maria Skeppstedt, and Gijs Aangenendt. Funding for this research was generously provided by the European Research Council under grant ERC-2021-STG 10104099. The Centre for Digital Humanities and Social Sciences offered crucial support, including access to GPUs and data storage facilities. Finally, the authors extend their gratitude to Dr Maria Skeppstedt and the anonymous reviewers for their insightful feedback.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#concluding-remarks",
    "href": "chapter_ai-nepi_005.html#concluding-remarks",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.45 Concluding Remarks",
    "text": "5.45 Concluding Remarks\n\n\n\nSlide 15\n\n\nThe authors concluded their presentation with a reference to the project’s official website, inviting further engagement with the research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant initiative, meticulously charts the profound influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition upon the development of natural philosophy. The project team aims to uncover broad networks of texts, passages, themes, topics, and authors, many of which remain largely unexamined by historians. To achieve this, the authors employ a substantial, diverse multilingual dataset comprising approximately 430,000 printed texts. This corpus, spanning from 1540 to 1728, draws its content from Early English Books Online (EEBO), Gallica, and the Bavarian State Library.\nAddressing significant challenges posed by variable OCR quality, early modern typography, and multilingual semantics, the team harnesses state-of-the-art digital techniques. These include keyword search, text matching, topic modelling, and sentiment analysis. The project integrates Large Language Models (LLMs) in two primary capacities: GPT-based LLMs serve as “judges” for enriching and cleaning bibliographic metadata, whilst BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages, thereby facilitating sophisticated text matching.\nA complex 15-stage data processing pipeline transforms raw XML, HOCR, and HTML files into a structured Elasticsearch database, which underpins the VERITRACE web application. This alpha version of the application offers several functionalities: an “Explore” section for corpus statistics, a “Metadata Explorer” for detailed record examination (including granular language identification and OCR quality assessment), and a “Search” interface supporting complex keyword and field queries. Crucially, the “Match” section enables the identification of textual reuse through both lexical and semantic comparisons, supporting single-document, multi-document, and full-corpus analyses. Although the current BERT-based embedding model (LaBSE) shows promise for semantic matching across languages, its performance with early modern, out-of-domain data, coupled with OCR issues, necessitates further investigation into alternative models or fine-tuning strategies. The project anticipates future challenges related to semantic drift over centuries, scaling computational power for the vast corpus, and optimising performance for user-facing queries.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-project-and-team",
    "href": "chapter_ai-nepi_006.html#the-veritrace-project-and-team",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.1 The VERITRACE Project and Team",
    "text": "6.1 The VERITRACE Project and Team\n\n\n\nSlide 01\n\n\nThe VERITRACE project, formally titled “Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy,” operates as an ERC-STG Project (101076836). A dedicated team of five individuals, primarily based at the Vrije Universiteit Brussel (VUB) in Brussels, drives this initiative. Professor Dr. Cornelis J. Schilt serves as the Principal Investigator, leading a diverse group that includes a class assistant and two historians. The speaker, a historian of science and medicine specialising in the 18th century, fulfils the role of digital humanities specialist for the project. Further information is available on the project’s website, HTTPS://VERITRACE.EU.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-objectives-and-historical-context",
    "href": "chapter_ai-nepi_006.html#project-objectives-and-historical-context",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.2 Project Objectives and Historical Context",
    "text": "6.2 Project Objectives and Historical Context\n\n\n\nSlide 01\n\n\nThis five-year ERC Starting Grant project, active from 2023 to 2028, is firmly rooted at the Vrije Universiteit Brussel. Its central objective involves meticulously charting the profound influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia, upon the nascent field of early modern natural philosophy and science. This tradition manifests in significant historical works such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most famously for scholars of chemistry, the Corpus Hermeticum.\nHistorical evidence already confirms the tradition’s impact; Isaac Newton, for instance, engaged with the Sibylline Oracles, whilst Johannes Kepler demonstrated familiarity with the Corpus Hermeticum. Beyond these well-documented instances, the project team aims to delve deeper. The researchers have assembled a close-reading corpus of 140 works specifically representing this tradition. The broader ambition extends to uncovering a much wider, often overlooked, network of texts, passages, themes, topics, and authors who engaged with this tradition—a collection one scholar aptly terms the ‘great unread’, given the frequent neglect of these numerous works by lesser-known authors in historical scholarship.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-to-history-and-philosophy-of-science-hpss",
    "href": "chapter_ai-nepi_006.html#computational-approaches-to-history-and-philosophy-of-science-hpss",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.3 Computational Approaches to History and Philosophy of Science (HPSS)",
    "text": "6.3 Computational Approaches to History and Philosophy of Science (HPSS)\n\n\n\nSlide 03\n\n\nThe VERITRACE project fundamentally seeks to advance computational approaches within the history and philosophy of science. A primary goal involves applying large-scale multilingual exploration to the central research question. The project team specifically aims to identify textual re-use, distinguishing between direct lexical instances, such as uncited quotations, and more indirect semantic re-use, encompassing paraphrases or conceptually similar content that contemporary readers would have recognised. This functionality effectively serves as an “early modern plagiarism detector.” Beyond this, the initiative strives to uncover potentially ignored networks of texts, passages, themes, topics, and authors. Ultimately, this systematic exploration promises to reveal novel patterns within the intellectual history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-construction-and-digital-analysis-techniques",
    "href": "chapter_ai-nepi_006.html#corpus-construction-and-digital-analysis-techniques",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.4 Corpus Construction and Digital Analysis Techniques",
    "text": "6.4 Corpus Construction and Digital Analysis Techniques\n\n\n\nSlide 04\n\n\nThe project team constructs a substantial, diverse, and multilingual dataset, focusing exclusively on printed works rather than handwritten materials for manageability. This corpus draws from three distinct multilingual sources, encompassing texts in at least six different languages. The selected publication period spans approximately 200 years, commencing in 1540 and concluding in 1728, shortly after Isaac Newton’s passing. Key data sources include Early English Books Online (EEBO), materials downloaded from the French National Library via Gallica, and, as the largest contributor, the Bavarian State Library. Cumulatively, this effort has amassed a corpus of approximately 430,000 books. The researchers plan to analyse this extensive collection using state-of-the-art digital techniques, including keyword search, text matching, topic modelling, and sentiment analysis, amongst others.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-llm-integration",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-llm-integration",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.5 Core Challenges and LLM Integration",
    "text": "6.5 Core Challenges and LLM Integration\n\n\n\nSlide 04\n\n\nThe VERITRACE project confronts several core challenges inherent in processing historical texts at scale. A primary hurdle involves the variable quality of Optical Character Recognition (OCR) text, which libraries provide in raw formats such as XML, HOCR, and HTML, crucially without accompanying ground truth page images. This directly impacts all subsequent data processing. Furthermore, early modern typography and semantics present significant complexities across the project’s six or more languages. The sheer volume of data—hundreds of thousands of texts published across Europe over two centuries—also poses a considerable challenge.\nTo address these issues, the project team integrates Large Language Models (LLMs) in a two-sided approach. On the decoder side, GPT-based LLMs function as “judges” to enrich and clean the project’s metadata. On the encoder side, BERT-based LLMs generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages within the textual corpus, a critical step for enabling the desired text matching functionalities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#case-study-llms-as-judges-for-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#case-study-llms-as-judges-for-metadata-enrichment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.6 Case Study: LLMs as Judges for Metadata Enrichment",
    "text": "6.6 Case Study: LLMs as Judges for Metadata Enrichment\n\n\n\nSlide 06\n\n\nA specific case study explores the application of LLMs as judges for metadata enrichment within the VERITRACE project. The fundamental motivation stems from the Universal Short Title Catalogue (USTC), recognised as a high-quality source of bibliographic metadata. The researchers aim to map VERITRACE records onto USTC records, thereby generating enriched metadata that requires less manual cleaning. Whilst some mapping can be automated, for instance through external identifiers, the majority of records necessitate manual comparison due to the initial uncleaned state of the VERITRACE data. This manual process, involving the comparison of bibliographic metadata pairs to determine if they represent the same underlying printed text, proves exceptionally tedious; each team member was assigned 10,000 such pairs for review. Consequently, the project proposes employing LLMs to automate this laborious task.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llm-bench-for-bibliographic-record-evaluation",
    "href": "chapter_ai-nepi_006.html#llm-bench-for-bibliographic-record-evaluation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.7 LLM Bench for Bibliographic Record Evaluation",
    "text": "6.7 LLM Bench for Bibliographic Record Evaluation\n\n\n\nSlide 06\n\n\nTo evaluate bibliographic record matches, the researchers have devised a system employing a chain of LLMs, conceptualised as a “bench” or panel of judges. This chain comprises a Primary LLM, a Secondary LLM, a Tiebreaker LLM, and an Expert LLM, which handles edge cases and facilitates a circular review process. The LLMs’ task involves assessing pairs of bibliographic records—one originating from a low-quality metadata source and the other from a high-quality source—to ascertain whether they represent the identical underlying text. Crucially, the models must provide not only a judgment (match or no match) but also detailed reasoning and confidence levels for each decision. The project validates these LLM decisions against a ground truth dataset, with the VERITRACE team conducting a final review.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prompt-guidelines-and-output-structure-for-llm-judges",
    "href": "chapter_ai-nepi_006.html#prompt-guidelines-and-output-structure-for-llm-judges",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.8 Prompt Guidelines and Output Structure for LLM Judges",
    "text": "6.8 Prompt Guidelines and Output Structure for LLM Judges\n\n\n\nSlide 06\n\n\nThe LLM judges receive pairs of bibliographic records alongside extensive prompt guidelines. These guidelines delineate field priorities, specific match criteria, and indicators for non-matches, ensuring a structured evaluation process. The expected output from the LLMs adheres to a defined structure, including the ground truth, the decisions made by the Primary, Secondary, and Tiebreaker models, the key factors influencing their judgments, and a detailed reasoning for each match or non-match decision. This comprehensive output facilitates analysis and validation of the LLMs’ performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-in-llm-based-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#challenges-in-llm-based-metadata-enrichment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.9 Challenges in LLM-Based Metadata Enrichment",
    "text": "6.9 Challenges in LLM-Based Metadata Enrichment\n\n\n\nSlide 07\n\n\nThe implementation of LLMs for metadata enrichment remains a work in progress, not yet achieving full functionality. A significant challenge involves the frequent occurrence of hallucinations in the output, where LLMs generate records not present in the original input. This issue is particularly observed with open-source models like Llama. Furthermore, whilst requesting more structured output aims to reduce unhelpful responses, it often results in more generic and less insightful reasoning, especially regarding the justification for decisions. Finding the optimal balance between structured output and the utility of the responses proves difficult, akin to an art rather than a precise science. Despite these hurdles, the project team recognises the substantial potential for time-saving if this approach can be successfully implemented, and actively seeks external advice on these ongoing challenges.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.10 VERITRACE Web Application: Alpha Version",
    "text": "6.10 VERITRACE Web Application: Alpha Version\n\n\n\nSlide 07\n\n\nThe VERITRACE web application currently exists as an alpha version, an extremely new development not yet publicly accessible. This iteration resides on the speaker’s local computer, with functionalities demonstrated through screenshots. It serves as a foundational promise of the project’s future capabilities. Engineers are presently testing a BERT-based LLM, specifically LaBSE, to generate vector embeddings for every passage within the extensive textual corpus. However, preliminary assessments suggest that LaBSE will likely prove insufficient for the project’s comprehensive requirements, despite demonstrating efficacy in certain specific cases.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-processing-pipeline-architecture",
    "href": "chapter_ai-nepi_006.html#data-processing-pipeline-architecture",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.11 Data Processing Pipeline Architecture",
    "text": "6.11 Data Processing Pipeline Architecture\n\n\n\nSlide 08\n\n\nThe project employs a complex 15-stage data processing pipeline to transform raw textual data, received in XML, HOCR, and HTML formats from libraries, into a structured Elasticsearch database that serves as the web application’s backend. This intricate process demands meticulous optimisation at each stage. Key tasks within the pipeline include extracting text into clean text files, generating precise mappings of all character positions, segmenting the text into meaningful units such as sentences and passages, and rigorously assessing the OCR quality. The generation of vector embeddings, crucial for semantic analysis, occurs towards the latter stages of this comprehensive pipeline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#web-application-explore-section",
    "href": "chapter_ai-nepi_006.html#web-application-explore-section",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.12 Web Application: Explore Section",
    "text": "6.12 Web Application: Explore Section\n\n\n\nSlide 08\n\n\nThe VERITRACE web application organises its functionalities across approximately five main sections. The “Explore” section serves as a central hub for users to gain insights into the corpus through various statistics and metadata visualisations. This statistical data is directly sourced from a Mongo database. Currently, the system holds 427,305 metadata records describing the books within the corpus. Users can examine detailed visualisations, including pie charts for language distribution, and bar charts illustrating documents by data source, documents by decade, and publication places.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#elasticsearch-metadata-explorer-and-ocr-quality-assessment",
    "href": "chapter_ai-nepi_006.html#elasticsearch-metadata-explorer-and-ocr-quality-assessment",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.13 Elasticsearch Metadata Explorer and OCR Quality Assessment",
    "text": "6.13 Elasticsearch Metadata Explorer and OCR Quality Assessment\n\n\n\nSlide 09\n\n\nThe Elasticsearch Metadata Explorer provides users with the capability to browse and inspect the rich metadata generated for each text within the corpus. A crucial feature involves granular language identification, performed on every text down to segments of approximately 50 characters. This addresses the common issue of multilingual texts where primary metadata might only indicate a single language; for instance, the system can identify a text as 15% Greek and 85% Latin, correctly classifying it as substantively multilingual. Furthermore, the system undertakes OCR quality assessment. This challenging task is performed solely on the raw text, as ground truth page images are unavailable. The assessment is meticulously applied on a page-by-page basis, rather than assigning a single quality score to an entire book, aiming to provide a detailed quality assessment for every individual page.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-search-functionality",
    "href": "chapter_ai-nepi_006.html#advanced-search-functionality",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.14 Advanced Search Functionality",
    "text": "6.14 Advanced Search Functionality\n\n\n\nSlide 10\n\n\nScholars are anticipated to primarily utilise the “Search” section, which offers robust keyword search capabilities. Users can perform basic keyword searches, such as for “Hermes,” which, even in the current prototype indexing only 132 files, yields 22 documents with 332 total matches, resulting in a 15 GB index. The system supports more sophisticated queries leveraging Elasticsearch functionalities, including field queries (e.g., specifying “author:kepler ‘hermes’” to find one document from 1621), and complex Boolean operations (AND, OR, nested queries). Furthermore, users can execute proximity queries, for instance, locating texts where “Hermes” and “Plato” appear within ten words of each other. The full corpus, comprising over 400,000 texts, will eventually lead to an index size measured in terabytes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#planned-analytical-tools",
    "href": "chapter_ai-nepi_006.html#planned-analytical-tools",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.15 Planned Analytical Tools",
    "text": "6.15 Planned Analytical Tools\n\n\n\nSlide 11\n\n\nThe “Analyse” section of the website, whilst not yet implemented, will host a suite of advanced analytical tools. Planned features include topic modelling, latent semantic analysis (LSA), and various forms of diachronic analysis. These functionalities, drawing upon ongoing developments in the field, will be integrated into the platform to support deeper scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-reading-functionality",
    "href": "chapter_ai-nepi_006.html#text-reading-functionality",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.16 Text Reading Functionality",
    "text": "6.16 Text Reading Functionality\n\n\n\nSlide 11\n\n\nThe “Read” section of the web application provides scholars with the capability to access and read the texts in their original format. Integrating a Mirador viewer, the platform displays digital facsimiles (PDFs) for every text within the corpus. This functionality ensures that users can engage with the historical documents as they would on a library website, whilst also providing access to the associated metadata.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-match-textual-reuse-identification",
    "href": "chapter_ai-nepi_006.html#veritrace-match-textual-reuse-identification",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.17 VERITRACE Match: Textual Reuse Identification",
    "text": "6.17 VERITRACE Match: Textual Reuse Identification\n\n\n\nSlide 12\n\n\nThe “Match” section of the VERITRACE web application is specifically designed to identify textual reuse across different documents. This functionality supports various comparison modes: users can compare a single document against another, conduct multi-document comparisons (for instance, pitting Isaac Newton’s Latin Optics against all of Johannes Kepler’s works within the database), or even perform a full corpus match, comparing one text against the entire collection. The latter, however, presents a significant computational challenge, potentially leading to considerable wait times for users. To address the nuanced nature of text matching, the interface exposes numerous parameters, such as minimum similarity scores, allowing users to fine-tune the comparison settings and obtain tailored results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-types-and-cross-language-sanity-check",
    "href": "chapter_ai-nepi_006.html#text-matching-types-and-cross-language-sanity-check",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.18 Text Matching Types and Cross-Language Sanity Check",
    "text": "6.18 Text Matching Types and Cross-Language Sanity Check\n\n\n\nSlide 13\n\n\nThe text matching functionality offers distinct approaches: lexical matching, which relies on keyword and vocabulary similarity; semantic matching, employing vector embeddings to identify conceptually similar passages, crucially functioning across different languages without requiring shared vocabulary; and hybrid matching, which combines both approaches with adjustable weights. Users can also select from various matching modes: a standard mode, a comprehensive mode that utilises more computational power for exhaustive results, and a faster mode.\nA critical sanity check involves comparing Isaac Newton’s Latin Optics (1719 edition) with its English counterpart (Opticks, 1718 edition). When a lexical match is performed in standard mode, the system correctly yields no significant results, as anticipated for texts in different languages. However, switching to comprehensive mode reveals three matches, indicating the presence of some English text, likely from the preface, within the Latin edition. This outcome validates the system’s ability to differentiate between lexical and conceptual similarities whilst highlighting the nuances of multilingual historical texts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-self-matching-and-result-visualisation",
    "href": "chapter_ai-nepi_006.html#lexical-self-matching-and-result-visualisation",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.19 Lexical Self-Matching and Result Visualisation",
    "text": "6.19 Lexical Self-Matching and Result Visualisation\n\n\n\nSlide 15\n\n\nA second sanity check involves performing a lexical match of a text against itself, yielding expectedly high scores: a Normalized Match Score of 100%, a Coverage Score of 99.7%, and a Quality Score of 100.0%. The system reports that both the query and comparison documents comprise 1,140 passages. This operation, taking 23 seconds, involved 1,299,600 comparisons to identify an estimated 1,137 matches, with 20 of 100 total matches displayed.\nThe results are presented with a clear match summary, providing helpful information about the query’s outcome. A bar chart illustrates the similarity score distribution, showing the majority of matches falling within the 90-100% range. For individual matches, the interface automatically highlights matching terms, presenting the source or query passage on the left, the comparison passage on the right, alongside their respective similarity scores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-matching-across-translations",
    "href": "chapter_ai-nepi_006.html#semantic-matching-across-translations",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.20 Semantic Matching Across Translations",
    "text": "6.20 Semantic Matching Across Translations\n\n\n\nSlide 15\n\n\nA third sanity check investigates semantic matching between a text and its translation, specifically comparing Isaac Newton’s Latin Optics with its English translation, Opticks. The expectation posits that whilst lexically dissimilar, these texts should exhibit strong semantic correspondence. The observed outcomes confirm this, with matches appearing reasonable despite underlying OCR issues. The interface presents the Latin source passage on the left and the English comparison passage on the right, accompanied by high similarity scores (e.g., 91.77%, 91.12%), demonstrating the model’s ability to capture conceptual equivalence across languages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#challenges-and-limitations-of-current-semantic-matching",
    "href": "chapter_ai-nepi_006.html#challenges-and-limitations-of-current-semantic-matching",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.21 Challenges and Limitations of Current Semantic Matching",
    "text": "6.21 Challenges and Limitations of Current Semantic Matching\n\n\n\nSlide 16\n\n\nDespite promising initial results, the current semantic matching implementation faces several challenges. The overall match score calculation requires refinement, and a notably low coverage score, for example 36.9%, significantly impacts this metric. This lower coverage might be partially explained by the Latin edition being considerably longer than its English counterpart, alongside other substantive textual differences. Nevertheless, the primary concern centres on the adequacy of the current embedding model, LaBSE, which is considered “not good enough” for the task. This inadequacy likely stems from an “out-of-domain model collapse,” as the model, trained on modern languages, struggles with the distinct semantics of early modern texts, compounded by poor OCR quality, complex typography, and mixed multilingual content. Whilst the quality score remains high, indicating the precision of the matches identified, the model’s overall performance suggests a need for further development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-research-directions",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-research-directions",
    "title": "6  VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy",
    "section": "6.22 Future Challenges and Research Directions",
    "text": "6.22 Future Challenges and Research Directions\n\n\n\nSlide 16\n\n\nThe project anticipates several significant challenges and outlines key research directions. Regarding embedding models, whilst LaBSE serves as a starting point, the researchers are exploring alternatives such as XLM-Roberta, intfloat multilingual-e5-large, and historical mBERT, each presenting trade-offs between accuracy, storage requirements, and inference time. A crucial consideration involves whether to fine-tune a base model on the unique historical corpus, given its distinct characteristics.\nA fundamental challenge involves addressing semantic drift, the change in meaning over centuries. The project must determine how to handle texts published across a broad temporal span (e.g., 1540 vs. 1700) and in different languages, whilst ensuring their representation within a consistent vector space. Poor OCR quality profoundly impacts all downstream processes, including the crucial segmentation of texts into sentences and passages. Re-OCR of the entire corpus is not feasible; therefore, the team may focus on re-OCR for only the lowest quality texts or invest in locating existing high-quality versions. Finally, scaling and performance present substantial hurdles. The current prototype, indexing merely 132 texts, already incurs query times of 15 seconds. Scaling this to the full corpus of 430,000 texts necessitates significant optimisation to ensure user-acceptable response times. The project actively welcomes expert advice on these complex issues.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>VERITRACE Traces de la Vérité: The reappropriation of ancient wisdom in early modern natural philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nThis presentation explores the dual application of Artificial Intelligence: firstly, in enhancing the interpretability of complex models through Explainable AI (XAI), and secondly, in generating novel scientific insights within the humanities. The authors initially delineate XAI 1.0, focusing on feature attributions via heatmaps for classification models, whilst highlighting the necessity of verifying predictions, identifying biases, and ensuring regulatory compliance.\nThe discussion then transitions to XAI 2.0, which addresses the complexities of Generative AI and Large Language Models (LLMs) by exploring structured interpretability, feature interactions, and mechanistic views. The work demonstrates that models can exhibit surprising errors, such as misclassifying objects based on correlated features or failing at multi-step planning tasks. To overcome the limitations of first-order explanations, the research introduces second-order (pairwise relationships) and higher-order (graph structures, feature walks) attributions, revealing more intricate model behaviours and underlying simplistic strategies in embedding models. Specific examples illustrate how XAI uncovers biases in sentiment prediction and reveals LLMs’ tendency to prioritise recent information in long-context summarisation.\nIn the humanities, the presentation showcases AI’s utility through several case studies. El-Hajj, Eberle, and their colleagues employed heatmap-based approaches to extract visual definitions from corpora of mathematical instruments, identifying fine-grained scales as crucial features. A significant project, led by Valeriani, Eberle, and their team, involved corpus-level analysis of early modern astronomical tables, such as the Sphaera and Sacrobosco Corpora. Here, a bespoke statistical model generating bigram representations proved effective where conventional Foundation Models failed due to heterogeneous, out-of-domain historical data. This led to the concept of the “XAI-Historian,” enabling data-driven hypothesis generation at scale.\nCrucially, cluster entropy analysis, applied to publishing locations, revealed patterns of innovation and control, identifying Frankfurt as a reprinting hub and Wittenberg as a centre where political influence actively shaped the print programme. The presentation concludes by acknowledging the challenges of low-resource data and out-of-domain transfer for LLMs in humanities research, whilst affirming the transformative potential of Machine Learning (ML) and XAI for scaling scholarly inquiry and fostering new research directions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-and-ai-based-scientific-insights",
    "href": "chapter_ai-nepi_007.html#explainable-ai-and-ai-based-scientific-insights",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 Explainable AI and AI-based Scientific Insights",
    "text": "7.1 Explainable AI and AI-based Scientific Insights\n\n\n\nSlide 01\n\n\nThis presentation delineates two principal areas of inquiry. Initially, it explores Explainable Artificial Intelligence (XAI), focusing on developing methodologies to comprehend the intricate operations of complex Large Language Models (LLMs). Subsequently, the discussion shifts to the application of AI for generating scientific insights, particularly within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "href": "chapter_ai-nepi_007.html#explainable-ai-xai-1.0-feature-attributions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Explainable AI (XAI) 1.0: Feature Attributions",
    "text": "7.2 Explainable AI (XAI) 1.0: Feature Attributions\n\n\n\nSlide 01\n\n\nThis section provides a concise introduction to Explainable Artificial Intelligence (XAI), outlining the core concepts that the machine learning community defines as ‘explanation’.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explainable-ai",
    "href": "chapter_ai-nepi_007.html#explainable-ai",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 Explainable AI",
    "text": "7.3 Explainable AI\n\n\n\nSlide 01\n\n\nHistorically, machine learning predominantly focused on visual data, with a more recent surge of interest in language emerging over the past decade. To comprehend the internal workings of ‘black box’ machine learning models, researchers typically examined classification tasks. For instance, an input image, such as a rooster, would yield a prediction like “Rooster”; however, users generally possessed no insight into the underlying basis for this classification. Early work in this area includes contributions from Samek and colleagues (2017).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#post-hoc-explainability",
    "href": "chapter_ai-nepi_007.html#post-hoc-explainability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Post-Hoc Explainability",
    "text": "7.4 Post-Hoc Explainability\n\n\n\nSlide 02\n\n\nThe field of explainable AI has dedicated approximately a decade to tracing the origins of model predictions. Typically, this research yields outputs such as heatmaps, which delineate the specific input features—for example, pixels—responsible for a given prediction, such as the recognition of a rooster. Beyond merely elucidating model behaviour, explainability serves several crucial purposes. It enables the verification of predictions, ensuring that models operate reasonably, and facilitates the identification and correction of errors by illuminating how mistakes occur. Furthermore, explainability offers insights into the underlying problem domain, as models frequently uncover surprising solutions. Increasingly, it also ensures compliance with regulatory frameworks, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#from-classification-to-generative-ai",
    "href": "chapter_ai-nepi_007.html#from-classification-to-generative-ai",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 From Classification to Generative AI",
    "text": "7.5 From Classification to Generative AI\n\n\n\nSlide 03\n\n\nThe scenario of classification models, prevalent until approximately five years ago, has now given way to the era of Generative AI. Contemporary models exhibit remarkable versatility, performing diverse functions such as classification, identifying similar images, generating novel images, and answering a broad spectrum of questions. This expanded capability, however, significantly complicates the task of grounding predictions or answers from Large Language Models (LLMs) to their specific inputs. Consequently, the authors must move beyond conventional heatmap representations, exploring feature interactions and adopting more mechanistic perspectives to achieve deeper understanding. Crucially, today’s foundation models function as both multi-task and ‘world models’, offering profound insights into societal dynamics and the evolution of textual features over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#model-fallibility",
    "href": "chapter_ai-nepi_007.html#model-fallibility",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Model Fallibility",
    "text": "7.6 Model Fallibility\n\n\n\nSlide 04\n\n\nModels frequently exhibit surprising errors, as evidenced by two prominent examples. In object detection, a standard classifier incorrectly identifies a boat by focusing on the surrounding water—a correlated and texturally simpler feature—rather than the boat itself, as demonstrated by Lapuschkin and colleagues (2019). Furthermore, Large Language Models (LLMs) can falter in multi-step planning tasks. For instance, when presented with the Tower of Hanoi puzzle, an LLM might immediately attempt to move the largest, inaccessible disc, thereby failing to comprehend the inherent physical constraints of the problem, a phenomenon observed by Mondal, Webb, and their co-authors (2024).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 XAI 2.0: Structured Interpretability",
    "text": "7.7 XAI 2.0: Structured Interpretability\n\n\n\nSlide 05\n\n\nWhilst more recent reasoning models may exhibit improved performance, the aforementioned Tower of Hanoi example originated from a standard Llama 3.x model. This section now shifts focus to structured interpretability, exploring methodologies that extend beyond conventional heatmap visualisations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#identifying-relevant-features-and-their-interactions-for-interpretability",
    "href": "chapter_ai-nepi_007.html#identifying-relevant-features-and-their-interactions-for-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 Identifying Relevant Features and Their Interactions for Interpretability",
    "text": "7.8 Identifying Relevant Features and Their Interactions for Interpretability\n\n\n\nSlide 05\n\n\nFirst-order explanations, often visualised as heatmaps, prove particularly useful for elucidating classifier behaviour. For instance, the research team employed a table classifier on historical documents, aiming to distinguish specific subgroups of historical tables. After training the classifier, they verified its predictions using heatmaps, confirming that the model accurately focused on numerical content. This numerical focus served as an effective proxy for identifying numerical tables within the corpus.\nThe authors also investigated second-order features, specifically focusing on pairwise relationships such as similarity. Their method involved computing a dot product or similarity score between the embeddings of two entities, for example, two images. To explain these similarity predictions, they found that representing the interaction between features proved highly effective. This approach revealed interactions between specific digits, indicating identical tables and thereby verifying the model’s intended functionality.\n\n\n\nSlide 06\n\n\nIn more recent investigations, the research team explored graph structures, discovering that higher-order interactions offer more meaningful insights. This approach applies to various networks, such as citation networks or networks of books and entities, typically trained on classification tasks. Here, relevant features emerge as feature subgraphs or feature walks, representing sets of features that become significant collectively. This methodology aims to yield more complex insights into models, ultimately progressing towards a circuit-level understanding of their operations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.9 First-Order Attributions in LLMs",
    "text": "7.9 First-Order Attributions in LLMs\n\n\n\nSlide 07\n\n\nThis section presents illustrative examples drawn from the domains of language and the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-1a-biased-sentiment-predictions-in-transformer-llms",
    "href": "chapter_ai-nepi_007.html#ex-1a-biased-sentiment-predictions-in-transformer-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.10 Ex 1a: Biased Sentiment Predictions in Transformer LLMs",
    "text": "7.10 Ex 1a: Biased Sentiment Predictions in Transformer LLMs\n\n\n\nSlide 07\n\n\nAli and colleagues (2022) investigated biased sentiment predictions within Transformer Large Language Models (LLMs) by analysing the feature importance of names in movie reviews. Employing a standard sentiment prediction scenario, common within the language community, they ranked sentences and computed heatmaps using a novel method specifically designed for transformers. Their findings revealed a notable bias: positive sentiment predictions correlated with male Western names such as Lee, Barry, Raphael, or the Cohen Brothers, whilst negative scores were more likely associated with foreign-sounding names like Saddam, Castro, or Chan. This study underscores the utility of Explainable AI in detecting such fine-grained biases within models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-1b-first-order-attributions-for-long-range-dependencies-in-llms",
    "href": "chapter_ai-nepi_007.html#ex-1b-first-order-attributions-for-long-range-dependencies-in-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.11 Ex 1b: First-Order Attributions for Long-Range Dependencies in LLMs",
    "text": "7.11 Ex 1b: First-Order Attributions for Long-Range Dependencies in LLMs\n\n\n\nSlide 08\n\n\nJafari and colleagues (2024) explored first-order attributions for long-range dependencies in Large Language Models (LLMs), specifically examining text summarisation for extensive inputs, up to an 8,000-token context window. In a typical LLM scenario, users provide long texts, such as Wikipedia articles, and request a summary, which the model then generates as free text. The investigation sought to determine the extent to which token dependencies spanned the input and whether models effectively utilised long-range information. Findings indicated that models predominantly focus on the later sections of the context, prioritising information presented closer to the prompt. Although models can draw upon information from the very beginning of the context, this occurs significantly less frequently, as evidenced by a log scale of counts. Consequently, summaries generated by LLMs may not offer a balanced representation of the entire input text, a crucial consideration for users.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "href": "chapter_ai-nepi_007.html#second-higher-order-interactions-in-text",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.12 Second & Higher-Order Interactions in Text",
    "text": "7.12 Second & Higher-Order Interactions in Text\n\n\n\nSlide 09\n\n\nThe research team investigated second and higher-order interactions within textual data, employing a standard embedding scenario involving sentence pairs, such as “a cat I really like, it is a great cat.” Utilising models like the Bird model or a sentence Bird model, they observed that whilst a similarity score was produced, the underlying reasons for its specific value remained opaque. The solution emerged through second-order explanations, which yielded interaction scores between individual tokens. These scores revealed that models primarily relied on simplistic strategies, such as noun matching (including synonyms and identical noun tokens), and to a lesser extent, noun-verb interactions, alongside separator and other token interactions. This reliance on basic strategies stems from the models’ need to compress vast amounts of information. Understanding these mechanisms proves crucial when embedding data and subsequently computing rankings between them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions",
    "href": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.13 Graph Neural Networks for Structured Predictions",
    "text": "7.13 Graph Neural Networks for Structured Predictions\n\n\n\nSlide 11\n\n\nGraph Neural Networks (GNNs) offer a powerful mechanism for structured predictions, providing attributions in terms of ‘walks’ that represent interactions between features. Notably, GNNs, which inherently encode structural information, can be conceptualised as Large Language Models (LLMs) because the attention network within LLMs dictates which tokens can engage in message passing. This conceptual framework facilitates the analysis of language structures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-3-node-interaction-learns-complex-language-structure",
    "href": "chapter_ai-nepi_007.html#ex-3-node-interaction-learns-complex-language-structure",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.14 Ex 3: Node Interaction Learns Complex Language Structure",
    "text": "7.14 Ex 3: Node Interaction Learns Complex Language Structure\n\n\n\nSlide 11\n\n\nSchnake and colleagues (2022) demonstrated how the interaction of nodes in graph structures enables the learning of complex language structures, particularly in sentiment analysis. Recognising that the hierarchical nature of natural language aligns well with graph structures, they trained a Graph Neural Network (or an LLM) on a movie review sentiment task and extracted ‘walks’ to understand its decision-making. For instance, in the sentence “First I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen,” a first-order explanation would fail to capture the complexity, potentially assigning a high score to “like” despite its negation. Conversely, a higher-order explanation accurately assigns a negative score to the initial negative clause and correctly identifies the positive sentiment and hierarchical structure in the latter part of the sentence. This work highlights the superior interpretability offered by higher-order methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-based-scientific-insights-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.15 AI-based Scientific Insights in the Humanities",
    "text": "7.15 AI-based Scientific Insights in the Humanities\n\n\n\nSlide 12\n\n\nThe discussion now transitions to the second principal area of inquiry: the generation of AI-based scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-4-extracting-visual-definitions-from-corpora",
    "href": "chapter_ai-nepi_007.html#ex-4-extracting-visual-definitions-from-corpora",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.16 Ex 4: Extracting Visual Definitions from Corpora",
    "text": "7.16 Ex 4: Extracting Visual Definitions from Corpora\n\n\n\nSlide 12\n\n\nEl-Hajj, Eberle, and their colleagues (2023) initially explored heatmap-based methods for extracting visual definitions from corpora, focusing on a collection of mathematical instruments. Their objective involved classifying these instruments into categories such as ‘machine’ or ‘mathematical instrument’. Collaborating closely with historians, including Matteo Valeriani and Jochen Büttner, they meticulously verified the visual definitions, underscoring the critical role of domain experts in ensuring the meaningfulness of such classifications. A key finding revealed that fine-grained scales present on the mathematical instruments were highly relevant for the model’s decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-5-corpus-level-analysis-of-early-modern-astronomical-tables",
    "href": "chapter_ai-nepi_007.html#ex-5-corpus-level-analysis-of-early-modern-astronomical-tables",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.17 Ex 5: Corpus-Level Analysis of Early Modern Astronomical Tables",
    "text": "7.17 Ex 5: Corpus-Level Analysis of Early Modern Astronomical Tables\n\n\n\nSlide 12\n\n\nIn their most extensive collaborative project, Valeriani and colleagues (2019) and Eberle and colleagues (2024) undertook a corpus-level analysis of early modern astronomical tables, specifically focusing on numerical data. They utilised the Sphaera Corpus, an early modern text collection spanning 1472 to 1650, and the Sacrobosco Table Corpus (1472-1650). Historians expressed keen interest in developing an automated method for matching tables with similar semantics, a task previously unfeasible at scale.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ex-5-historical-insights-at-scale-the-xai-historian",
    "href": "chapter_ai-nepi_007.html#ex-5-historical-insights-at-scale-the-xai-historian",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.18 Ex 5: Historical Insights at Scale: The XAI-Historian",
    "text": "7.18 Ex 5: Historical Insights at Scale: The XAI-Historian\n\n\n\nSlide 13\n\n\nCollaborating with historians, Eberle and colleagues (2024) developed a sophisticated workflow to facilitate historical insights at scale, coining the term “XAI-Historian” to describe a historian leveraging AI and Explainable AI. This approach aims to uncover novel case studies and enable data-driven hypothesis generation. The project focused on historical tables, which serve as crucial carriers of scientific knowledge processes, such as mathematisation, within the Sacrobosco Corpus—a collection of 76,000 pages of university textbooks from 1472 to 1650. A significant machine learning challenge arose from the data’s extreme heterogeneity, limited annotations, and the failure of conventional Optical Character Recognition (OCR) and Foundation Models (FMs). The devised workflow encompassed three key stages: initially, data collection from book images; subsequently, atomisation and recomposition, involving input tables, bigram maps, and histograms; and finally, corpus-level analysis, which included embedding historical tables and assessing data similarity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#verifying-modelling-and-features-using-xai-and-historians",
    "href": "chapter_ai-nepi_007.html#verifying-modelling-and-features-using-xai-and-historians",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.19 Verifying Modelling and Features Using XAI and Historians",
    "text": "7.19 Verifying Modelling and Features Using XAI and Historians\n\n\n\nSlide 13\n\n\nEberle and colleagues (2022, 2024) crafted a statistical model specifically designed to generate bigram representations of historical tables, addressing the challenge posed by foundation models’ inability to process such out-of-domain data effectively. This bespoke model underwent rigorous verification: by detecting identical bigrams—for example, ‘38’ on two distinct inputs—the authors confirmed its reliable operation, thereby establishing trust in its decisions. The methodology involved representing tables as a bag of bigrams, such as ‘01’ or ‘21’, and, given the limited annotations, employed a combination of a learned feature extractor and a hard-coded structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#cluster-entropy-analysis-to-investigate-innovation",
    "href": "chapter_ai-nepi_007.html#cluster-entropy-analysis-to-investigate-innovation",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.20 Cluster Entropy Analysis to Investigate Innovation",
    "text": "7.20 Cluster Entropy Analysis to Investigate Innovation\n\n\n\nSlide 14\n\n\nIn a compelling case study, Eberle and colleagues (2024) applied cluster entropy analysis to investigate the diffusion of innovation across early modern Europe. Their focus centred on the publishing output of specific cities, each producing distinct “programmes” of textual types. Some cities exhibited diverse print programmes, whilst others concentrated on reprinting existing works; critically, this phenomenon had previously defied analysis at scale. The methodology involved calculating the difference between the observed cluster entropy H(p) and the maximum attainable entropy for each print location, drawing upon data from the Sphaera publication EPISD-626.\n\n\n\nSlide 15\n\n\nThe authors devised a clustering approach, leveraging the model’s representations to compute a distance-based clustering and subsequently assess the diversity of print programmes produced by individual cities. They employed entropy as a metric: low entropy indicated a consistent reproduction of identical content, whilst higher entropy signified a more diverse print programme. This analysis identified two particularly compelling cases with the lowest entropy scores: Frankfurt am Main and Wittenberg. Frankfurt am Main was already recognised as a major centre for reprinting editions. More notably, Wittenberg presented a historical anomaly where the political control exerted by Protestant reformers, particularly Melanchthon, actively restricted the print programme, dictating the curriculum to be published. This finding not only revealed a previously unquantifiable historical pattern but also corroborated existing historical intuition and scholarly support.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.21 Conclusion: AI-based Methods for the Humanities",
    "text": "7.21 Conclusion: AI-based Methods for the Humanities\n\n\n\nSlide 15\n\n\nConcluding the discussion, the presenter highlighted several key points regarding AI-based methods in the humanities. Whilst humanities and Digital Humanities (DH) researchers have primarily concentrated on the digitisation of source material, the automated analysis of these corpora presents significant challenges due to data heterogeneity and a scarcity of labels. Multimodality also emerges as a crucial consideration. Nevertheless, the integration of Machine Learning (ML) and Explainable AI (XAI) holds substantial promise for scaling humanities research and fostering novel research directions. Foundation Models and Large Language Models (LLMs), coupled with prompting techniques, can effectively support intermediate tasks such as labelling, data curation, and error correction. However, their utility for addressing more complex research questions remains limited. Significant challenges persist, notably the issue of low-resource data for ML, which impacts scaling laws. Furthermore, out-of-domain transfer, particularly for historical and small-scale datasets, necessitates rigorous evaluation, as current LLMs are predominantly trained and aligned for natural language tasks and code generation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modeling Science",
    "section": "",
    "text": "Overview\nThis chapter critically examines the current capabilities of Large Language Models (LLMs) within the intricate contexts of historical, philosophical, and sociological inquiry, with a particular focus on the history of science. The presenter identifies significant limitations inherent in existing LLM paradigms, such as their susceptibility to hallucination, the misinterpretation of embedding vectors as true meaning, and a fundamental inability to provide justified, evidence-based answers or to formulate coherent plans for scientific investigation. To address these profound deficiencies, the presenter proposes a novel framework centred on “validation” and introduces the nascent concept of “Computational Epistemology.”\nThe proposed solution necessitates the development of a robust research infrastructure, comprising several key components. This includes a Scholarium system, which functions as a meticulously curated, editorially validated database of historical sources and structured content items. This system serves as a direct alternative to conventional embedding-based approaches. The infrastructure seamlessly integrates multimodal LLMs, such as Gemini 2.5, Claude, and Llama, within a Cursor environment, thereby enabling complex historical queries. Furthermore, the project leverages a FAIR (Findable, Accessible, Interoperable, Reusable) data repository, specifically Zenodo, for long-term data preservation and dissemination. Technical support is provided by Open Science Technology, a startup actively developing an MCP API server to standardise global AI access to scholarly knowledge. The overarching aim is to empower AI systems to furnish complete, validated, and historically accurate answers, thus profoundly transforming digital humanities research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#evolution-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#evolution-of-large-language-models",
    "title": "8  Modeling Science",
    "section": "8.1 Evolution of Large Language Models",
    "text": "8.1 Evolution of Large Language Models\n\n\n\nSlide 02\n\n\nLarge Language Models have undergone a remarkably rapid evolution. Initially, the foundational paradigm centred on the principle that “Attention is all you need.” This concept subsequently expanded to encompass “Context is all you need,” exemplified by the integration of Retrieval-Augmented Generation (RAG) systems to provide broader contextual understanding. Presently, the latest models propose that “Thinking is all you need,” signifying a further advancement in their cognitive capabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#critical-deficiencies-in-current-llm-capabilities",
    "href": "chapter_ai-nepi_008.html#critical-deficiencies-in-current-llm-capabilities",
    "title": "8  Modeling Science",
    "section": "8.2 Critical Deficiencies in Current LLM Capabilities",
    "text": "8.2 Critical Deficiencies in Current LLM Capabilities\n\n\n\nSlide 03\n\n\nDespite their rapid evolution, current Large Language Models exhibit several critical deficiencies. Crucially, they lack an inherent mechanism or “opponent” to effectively counter hallucination, a persistent challenge in generating reliable outputs. Furthermore, whilst embedding vectors prove useful for semantic similarity, they do not inherently represent the true meanings of expressions, frequently leading to potential misinterpretations. These models often formulate statements that sound plausible but are factually incorrect, and they merely repeat content from internet media rather than discerning and providing verified knowledge.\nBeyond these issues, LLMs demonstrate a profound inability to seek out the best-justified information or to formulate coherent plans for scientific inquiry. These are fundamental requirements for rigorous academic work. Presently, existing technologies offer no discernible hope for achieving these missing goals, rendering current LLMs largely unsuitable for tasks demanding high levels of validation and epistemic rigour.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#computational-epistemology-and-validation-framework",
    "href": "chapter_ai-nepi_008.html#computational-epistemology-and-validation-framework",
    "title": "8  Modeling Science",
    "section": "8.3 Computational Epistemology and Validation Framework",
    "text": "8.3 Computational Epistemology and Validation Framework\n\n\n\nSlide 04\n\n\nA fundamental requirement for advancing LLM utility in scholarly domains centres on validation—a principle encapsulated by the assertion, “Validation is all you need.” This necessitates systems capable of providing comprehensive reasons, robust arguments, and verifiable evidence both for and against the truth of a given proposition. Moreover, such systems must offer justifications for or against the pursuit of specific actions.\nTo address this critical gap, the presenter proposes a new discipline: Computational Epistemology. This emerging field focuses on the methods and methodologies required to implement the validation framework. Central to this discipline is the concept of epistemic agency, which demands the ability to identify propositions that extend beyond simple sentences, to analyse complex argumentation structures within texts and historical sources, and to discern the intentions, plans, and actions of historical figures as documented in their records.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#interactive-historical-inquiry-environment",
    "href": "chapter_ai-nepi_008.html#interactive-historical-inquiry-environment",
    "title": "8  Modeling Science",
    "section": "8.4 Interactive Historical Inquiry Environment",
    "text": "8.4 Interactive Historical Inquiry Environment\n\n\n\nSlide 05\n\n\nThe research team has developed a sophisticated working environment designed to facilitate historical inquiry. This intuitive interface displays an open historical source, exemplified by a book detailing the construction of Sanssouci castle under Frederick the Great. A dedicated panel, titled “Personen und Aufgaben in der Bibliothek”, allows users to formulate precise queries. For instance, a user might ask, “Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten” (Reconstruct which persons performed which work on the water fountain).\nThe system aims to deliver a validated, qualified, and factually correct answer, relying exclusively on proven evidence rather than anecdotal information. Output is meticulously structured, presenting details such as the names of individuals, descriptions of their work, and the remuneration received—for example, “Nahl (sculptor) created the drawings and models in miniature, receiving 200 Thaler.” This capability proves invaluable for resolving long-standing historical disputes, such as the extent of Leonhard Euler’s involvement in the significant construction failures at Sanssouci during the 18th century. An AI agent, named Bernoulli, supports these complex inquiries.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-source-repository",
    "href": "chapter_ai-nepi_008.html#scholarium-curated-scholarly-source-repository",
    "title": "8  Modeling Science",
    "section": "8.5 Scholarium: Curated Scholarly Source Repository",
    "text": "8.5 Scholarium: Curated Scholarly Source Repository\n\n\n\nSlide 06\n\n\nThe proposed system necessitates five or six core components, commencing with a scholarly curated editorial board responsible for validating source material. A prime example of this rigorous approach is the Opera Omnia Euler, a monumental collection spanning 86 volumes. This work represents 120 years of dedicated scholarly effort by numerous academics, with its comprehensive editing—including all 866 publications and Euler’s complete correspondence—finalised two years prior to this presentation. This foundational resource is further complemented by other significant scholarly editions, such as the Opera Bernoulli Euler, Kepler Gesammelte Werke, and Brahe Opera Omnia, collectively forming a robust evidence base.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-registry-based-content-management",
    "href": "chapter_ai-nepi_008.html#scholarium-registry-based-content-management",
    "title": "8  Modeling Science",
    "section": "8.6 Scholarium: Registry-Based Content Management",
    "text": "8.6 Scholarium: Registry-Based Content Management\n\n\n\nSlide 07\n\n\nA pivotal innovation within this framework is the Scholarium, which serves as a direct substitute for conventional embedding-based approaches. This component functions as a meticulously curated database of content items, systematically organising historical information. It captures a diverse array of data types, including chronologies of personal actions, various communication acts such as letters, publications, and reports, and specific statements. Furthermore, it documents implications, arguments, and inquiries, alongside the evolving use of language, terminology, and concepts by historical figures. The system also tracks the application of models, methods, tools, devices, data, information, evidence, and sources.\nEach entry within this detailed inventory of historically proven activities undergoes rigorous validation against original sources, ensuring unparalleled accuracy and reliability. Technical integration is facilitated through an AI API and a Model Context Protocol (MCP) API, enabling programmatic access to this rich, structured knowledge base.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#multimodal-llm-integration-for-inquiry",
    "href": "chapter_ai-nepi_008.html#multimodal-llm-integration-for-inquiry",
    "title": "8  Modeling Science",
    "section": "8.7 Multimodal LLM Integration for Inquiry",
    "text": "8.7 Multimodal LLM Integration for Inquiry\n\n\n\nSlide 08\n\n\nOnce these meticulously curated records are established, researchers can conduct inquiries using readily accessible multimodal models. Experience indicates that multimodal models, particularly the latest iterations such as Gemini 2.5, prove most effective for this task, given their capacity to seamlessly combine information derived from both text and images. The system also integrates other prominent models, including Claude and Llama, all operating within the Cursor environment, specifically leveraging LettreAI on Cursor.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-data-infrastructure",
    "href": "chapter_ai-nepi_008.html#fair-data-infrastructure",
    "title": "8  Modeling Science",
    "section": "8.8 FAIR Data Infrastructure",
    "text": "8.8 FAIR Data Infrastructure\n\n\n\nSlide 09\n\n\nEstablishing a robust and enduring FAIR (Findable, Accessible, Interoperable, Reusable) data infrastructure constitutes another essential component. For this purpose, the project utilises Zenodo, a repository hosted by CERN in Geneva. Zenodo facilitates the long-term storage and publication of research data, ensuring its sustained availability and discoverability for future scholarly endeavours.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#technical-support-and-standardisation",
    "href": "chapter_ai-nepi_008.html#technical-support-and-standardisation",
    "title": "8  Modeling Science",
    "section": "8.9 Technical Support and Standardisation",
    "text": "8.9 Technical Support and Standardisation\n\n\n\nSlide 10\n\n\nTechnical support for the project is provided by Open Science Technology, a startup specifically founded to manage the operational infrastructure. This includes running the core systems and, crucially, providing an MCP (Model Context Protocol) API server. This server establishes a standardised API, enabling artificial intelligence models worldwide to access the project’s curated data. The initiative aims to standardise AI access APIs to knowledge across the global community, fostering an environment of open collaboration in the pursuit of advanced scholarly inquiry.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nThe authors embarked on an inquiry to assess biases within the classification of publications related to Sustainable Development Goals (SDGs) across prominent bibliometric databases, employing Large Language Models (LLMs) as a core analytical tool. This project critically examines the performative nature of databases such as Web of Science, Scopus, and OpenAlex, which, despite their crucial role in scientific impact assessment, exhibit inconsistencies in SDG classification. Previous studies highlight minimal overlap in SDG labelling across different providers, potentially distorting perceptions of research priorities and influencing resource allocation and policy decisions.\nThe investigation specifically sought to understand the aggregate effects of introducing LLM-based tools on the representation of SDG-related research. It leveraged LLMs both as a detector of inherent data biases and as a proof-of-concept for automating information extraction to inform research policy. Methodologically, the study focused on five selected SDGs pertinent to socio-economic inequalities, analysing a jointly indexed corpus of over 15 million publications from January 2015 to July 2023.\nA key technical approach involved fine-tuning DistilGPT2, a pre-trained, open-source LLM with limited prior knowledge, on subsets of publication titles and abstracts. The research team meticulously derived prompts for the LLM from the specific targets of each SDG, serving as a benchmark for assessing the completeness of information captured by the models. The research employed three distinct decoding strategies—top-k, nucleus, and contrastive search—to generate responses, which then underwent noun phrase extraction and direct content comparison.\nCrucially, the findings reveal a systematic overlook in the data concerning sensitive locations (e.g., African countries, Least Developed Countries), vulnerable demographic groups (e.g., persons with disabilities, indigenous peoples), and critical SDG-specific focuses (e.g., human trafficking, migration). Whilst the United States consistently featured in LLM responses, followed by South Africa and China, these omissions underscore significant biases. Furthermore, the analysis identified distinct methodological leanings amongst the databases: Web of Science demonstrated a more theoretical approach, whereas Scopus and OpenAlex exhibited a more empirical orientation. These insights highlight the sensitivity of LLMs to their training data and the broader implications for research policy and addressing socio-economic inequalities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#initial-research-aim",
    "href": "chapter_ai-nepi_009.html#initial-research-aim",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 Initial Research Aim",
    "text": "9.1 Initial Research Aim\n\n\n\nSlide 01\n\n\nThe research team initially aimed to utilise Large Language Models (LLMs) to assess inherent biases within publications classified across three major bibliometric databases. This foundational objective sought to uncover discrepancies in how these extensive repositories categorise scholarly output, thereby influencing academic and policy landscapes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-bibliometric-databases",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 SDG Classification in Bibliometric Databases",
    "text": "9.2 SDG Classification in Bibliometric Databases\n\n\n\nSlide 01\n\n\nBibliometric databases function as critical digital infrastructures, underpinning bibliometric analyses and impact assessments within the scientific community. These platforms, however, possess a performative nature, shaping perceptions of the science system and attributing value based on specific, inherent understandings, as Whitley (2000) and Winkler (1988) have highlighted. The study specifically considered three major bibliometric databases: Web of Science, Scopus, and OpenAlex. Each of these platforms has implemented bibliometric classifications designed to align publications with the Sustainable Development Goals (SDGs).\nPrevious research, notably by Armitage et al. (2020), revealed that SDG labelling by various providers—including Elsevier, Bergen, and Aurora—produces divergent results, demonstrating minimal overlap in their classifications. Consequently, such disparities in classification can lead to varied perceptions of research priorities, potentially influencing both resource allocation and critical policy decisions. These differences, moreover, often reflect underlying political and commercial interests.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-un-sustainable-development-goals-in-bibliometric-data",
    "href": "chapter_ai-nepi_009.html#case-study-un-sustainable-development-goals-in-bibliometric-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Case Study: UN Sustainable Development Goals in Bibliometric Data",
    "text": "9.3 Case Study: UN Sustainable Development Goals in Bibliometric Data\n\n\n\nSlide 02\n\n\nThis case study, a collaborative effort by Ottaviani and Stahlschmidt (2024), sought to assess the aggregated effects on the representation of SDG-related research within bibliometric databases following the introduction of Large Language Model (LLM)-based tools. The authors employed a method involving the separate training of pre-trained, smaller LLMs, specifically DistilGPT2, on distinct subsets of publication abstracts. They meticulously curated these subsets based on the SDG classifications provided by various bibliometric databases.\nThe project conceptualised LLM technology in two primary roles: firstly, as a sophisticated detector of inherent biases within the data; and secondly, as a proof-of-concept exercise for automating information extraction to directly inform decision-making processes in research. Ultimately, this endeavour aimed to develop a generalisable framework for assessing the potential impact of such technologies on research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#partial-chain-of-dependencies",
    "href": "chapter_ai-nepi_009.html#partial-chain-of-dependencies",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 Partial Chain of Dependencies",
    "text": "9.4 Partial Chain of Dependencies\n\n\n\nSlide 03\n\n\nA conceptual flowchart delineates the intricate chain of dependencies considered within this research. Initially, established SDG classification frameworks directly define the scope and nature of “Research” pertaining to these global goals. Subsequently, a diverse array of stakeholders—including researchers, Small and Medium-sized Enterprises (SMEs), governmental bodies, and various intermediate figures—actively process this “Research” on SDGs. This processed research, in turn, critically informs “Decision-making to align with SDGs,” which ultimately exerts an impact on existing “Socioeconomic inequalities.”\nCrucially, the model integrates the role of LLMs: “LLM as detector of ‘biases’” is positioned to analyse “Research” on SDGs, whilst the “Introduction of LLM in Research Policy” is depicted as a subsequent step, directly influencing “Socioeconomic inequalities.” Consequently, LLMs possess the capacity to alter the underlying metadata, specifically concerning “research on SDGs,” thereby influencing a multitude of advices, policy choices, performance indicators, and practical measures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#methods-and-research-design",
    "href": "chapter_ai-nepi_009.html#methods-and-research-design",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Methods and Research Design",
    "text": "9.5 Methods and Research Design\n\n\n\nSlide 04\n\n\nThe research design carefully selected key bibliometric databases and Sustainable Development Goals for analysis. The investigators chose three prominent databases: the proprietary Web of Science and Scopus, alongside the open-source platform OpenAlex. Furthermore, the study focused on five specific SDGs, strategically chosen to facilitate the modelling of socio-economic inequalities, thereby providing a targeted lens for the inquiry.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data",
    "href": "chapter_ai-nepi_009.html#processed-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Processed Data",
    "text": "9.6 Processed Data\n\n\n\nSlide 04\n\n\nThe researchers meticulously processed a substantial dataset, comprising a jointly indexed subset of 15,471,336 publications. This comprehensive collection specifically included publications shared by all three bibliometric databases—Web of Science, Scopus, and OpenAlex—identified through exact Digital Object Identifier (DOI) matching. The data spanned a period from January 2015 to July 2023.\nThe study then evaluated the performance of three distinct classification standards applied to the five selected SDGs. For each SDG, the team generated three separate subsets of publications, each corresponding to one of the bibliometric databases, all derived from the initial shared corpus. This rigorous approach aimed to establish a robust comparative benchmark for subsequent analyses.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-socio-dimension",
    "href": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-socio-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 Comparing SDG-Classified Papers: Socio Dimension",
    "text": "9.7 Comparing SDG-Classified Papers: Socio Dimension\n\n\n\nSlide 05\n\n\nInitial findings concerning the socio dimension of SDG-classified papers demonstrated a remarkably small overlap in SDG labelling across the databases, a result entirely consistent with the observations of Armitage (2020). The analysis specifically focused on SDG 04 (Quality Education), SDG 05 (Gender Equality), and SDG 10 (Reduced Inequalities).\nNotably, Scopus, for instance, did not classify certain publications as SDG 5, even though these papers were present within its broader database. Conversely, Web of Science classified a significant 10% of its SDG 5 publications as originating from the field of mathematics, including topics such as geometrical differential equations. This particular finding strongly suggests either a misclassification or a notable absence of relevant information within the database’s tagging system.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-economic-dimension-and-llm-approach",
    "href": "chapter_ai-nepi_009.html#comparing-sdg-classified-papers-economic-dimension-and-llm-approach",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 Comparing SDG-Classified Papers: Economic Dimension and LLM Approach",
    "text": "9.8 Comparing SDG-Classified Papers: Economic Dimension and LLM Approach\n\n\n\nSlide 06\n\n\nThe study extended its comparative analysis to the economic dimension, examining SDG 08 (Decent Work and Economic Growth) and SDG 09 (Industry, Innovation, and Infrastructure). A novel approach to LLM modelling underpinned this phase of the research. The core idea involved training a Large Language Model exclusively on specific corpora of publications, each corpus having been classified by a particular bibliometric database for a distinct SDG.\nFor practical implementation, the research team opted to fine-tune DistilGPT2, an existing open-source LLM. This model, characterised by its fundamental architecture and minimal prior knowledge compared to its commercial or larger open-source counterparts, offered a crucial advantage: it ensured the absence of pre-existing semantic knowledge concerning either the publications or the prompts. The fine-tuning process exclusively utilised publication titles and abstracts as training data. Subsequently, a new title served as the prompt input, prompting the LLM to generate a corresponding new abstract.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-targets-and-prompt-generation",
    "href": "chapter_ai-nepi_009.html#sdg-targets-and-prompt-generation",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.9 SDG Targets and Prompt Generation",
    "text": "9.9 SDG Targets and Prompt Generation\n\n\n\nSlide 08\n\n\nEach Sustainable Development Goal is meticulously structured around a series of specific targets; for instance, SDG 4 encompasses targets ranging from 4.1 to 4.6. The SDGs under consideration typically feature between eight and twelve such targets. The researchers meticulously drafted the prompts for the LLM directly from these precise SDG targets.\nThis approach simultaneously served as an attempt at validation and benchmarking. Should the LLM fail to generate relevant responses to prompts derived from the targets, it signals a deficiency in the information it has processed, given that these targets fundamentally articulate the core objectives of each SDG, effectively serving as a “ground zero” for expected content. For analytical simplification, the methodology involved noun phrase extraction, alongside direct textual searching within the actual LLM responses during the comparative phase.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-workflow",
    "href": "chapter_ai-nepi_009.html#research-design-workflow",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.10 Research Design Workflow",
    "text": "9.10 Research Design Workflow\n\n\n\nSlide 10\n\n\nThe research design implemented a precise workflow for processing and analysing data. Initially, a curated “Set of abstracts classified SDG# by DB#” directly informed the “Fine-Tuning” of the DistilGPT-2 model. Concurrently, a “Set of prompts specifically to SDG#” served as input for the “Fine-tuned DistilGPT-2 SDG# DB#”.\nCrucially, the fine-tuned model then employed three distinct decoding strategies—top-k, nucleus, and contrastive search—to generate diverse outputs. These outputs, labelled as “Responses SDG# DB# for top-k”, “Responses SDG# DB# for nucleus”, and “Responses SDG# DB# for contrastive search”, subsequently underwent a rigorous post-processing stage. A “prompts’ words filter” refined these responses, culminating in the extraction of “Noun phrases SDG# DB#”. The analytical phase involved both this noun phrase extraction for simplification and a direct, meticulous search through the actual generated responses during the comparative assessment.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "href": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.11 Illustrative Example: SDG 4 Analysis",
    "text": "9.11 Illustrative Example: SDG 4 Analysis\n\n\n\nSlide 10\n\n\nFor illustrative purposes, the analysis of LLM responses for SDG 4 exemplifies the broader methodology applied across all Sustainable Development Goals. This involved meticulously matching extracted noun phrases with the specific SDG targets. The analysis systematically examined four critical data dimensions: Locations, Actors, Data/Metrics, and Focuses.\nFor each SDG, the investigation assessed two primary aspects: its compliance with the stated targets and the identification of any inherent biases. Notably, distinct differences emerged amongst the bibliometric databases. The findings were structured into a comprehensive table, categorised by “Unique DBs,” “Addressed Targets,” “Not Addressed Targets,” and “Focuses.” Within these dimensions, it became evident that all SDG targets consistently reference locations, whilst actors represent another crucial category. Data and metrics primarily emerged from the LLM’s responses, and focuses were identified as inherently SDG-specific.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-omissions-by-the-llm-illustrative-for-sdg-4",
    "href": "chapter_ai-nepi_009.html#systematic-omissions-by-the-llm-illustrative-for-sdg-4",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.12 Systematic Omissions by the LLM (Illustrative for SDG 4)",
    "text": "9.12 Systematic Omissions by the LLM (Illustrative for SDG 4)\n\n\n\nSlide 11\n\n\nDespite explicit stimulation, the Large Language Model consistently failed to address several critical areas, as exemplified by its performance on SDG 4. The omissions were particularly notable across specific categories. In terms of locations, the LLM neglected to mention African countries (with the exception of South Africa), developing countries (with a question mark over China), least developed countries, and Small Island Developing States.\nRegarding actors, the model systematically overlooked vulnerable populations, including persons with disabilities, indigenous peoples, and children in vulnerable situations. Furthermore, a range of crucial focuses remained unaddressed: vocational training, scholarships, the establishment of safe, non-violent, inclusive, and effective environments, sustainable lifestyles, human rights, the promotion of a culture of peace and non-violence, global citizenship, the appreciation of cultural diversity, free primary and secondary education, and tertiary education. These persistent omissions are highly significant, as they pertain to sensitive categories and focuses explicitly articulated within the SDG targets themselves.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-key-findings",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations-and-key-findings",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.13 Cross-SDG Considerations and Key Findings",
    "text": "9.13 Cross-SDG Considerations and Key Findings\n\n\n\nSlide 12\n\n\nAcross all five Sustainable Development Goals analysed, several recurrent results emerged, highlighting consistent patterns in the LLM’s performance and the underlying data biases. Regarding locations, least developed countries, such as South-Saharan Africa for SDG 8, received minimal attention. Conversely, the United States consistently featured in responses for every SDG, with South Africa and China following as the next most frequently cited locations, ahead of the UK and Australia.\nIn terms of metrics, the LLM frequently recalled various surveys as datasets, including the Demographic and Health Surveys (DHS) and the World Values Survey (WVS). The responses also encompassed a range of research methodologies, spanning theoretical, empirical, thematic analysis, market dynamics, and macroeconomics. However, a significant finding concerned actors: discriminated and vulnerable categories were systematically overlooked across different SDGs, with no overarching response addressing these groups. Similarly, whilst SDG-specific focuses were present, the most sensitive topics, such as human trafficking, human exploitation, and migration, were notably absent.\nFurthermore, the analysis revealed distinct methodological orientations among the bibliometric databases. Web of Science consistently exhibited a highly theoretical approach, often emphasising models and abstract concepts. In stark contrast, both Scopus and OpenAlex demonstrated a more empirical orientation in their classifications. Consequently, the primary finding of this research points to a systematic overlook within the data concerning critical actors, the poorest countries, and underrepresented topics.\nThe study acknowledges several limitations inherent in its general framework. Specific applied cases might yield different outcomes, though the researchers express some uncertainty regarding the extent of such variation. A high sensitivity of LLMs to their training data remains a significant factor, alongside potential variations in model architecture and calibration parameters. Nevertheless, the research endeavoured to account for these aspects by utilising training data from three distinct databases and employing three different decoding strategies drawn from existing literature.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "",
    "text": "Overview\nThis chapter addresses the persistent challenges inherent in parsing footnotes from legal and humanities scholarship, particularly when generating citation graphs for intellectual history. The authors highlight the severe limitations of existing bibliometric databases, such as Web of Science, Scopus, and OpenAlex, which offer inadequate coverage for historical, non-English, and non-STEM literature. Moreover, these platforms struggle with the complex, commentary-rich footnotes prevalent in the humanities.\nTo overcome these obstacles, the project introduces a novel approach leveraging Large Language Models (LLMs) and Vision Language Models (VLMs). A core component of this initiative involves the meticulous compilation of a high-quality, TEI XML-encoded gold standard dataset. This dataset comprises 1,100 foot- and endnotes extracted from 25 articles across 10 Open Access (DOAJ) journals, focusing on legal and historical humanities. It encompasses French, German, Spanish, Italian, and Portuguese texts published between 1958 and 2018. This compilation yields over 1,600 distinct references, with each occurrence encoded separately to preserve contextual information.\nA pivotal tool developed for this endeavour is Llamore, a lightweight Python package. Llamore facilitates the extraction of citation data from raw text or PDFs using LLMs and VLMs, subsequently exporting the results in TEI XML format. Furthermore, it provides a robust framework for evaluating extraction performance through F1-scores, meticulously comparing extracted references against the gold standard. The evaluation methodology employs precision and recall metrics for exact matches, incorporating an unbalanced assignment problem solver (from SciPy) to align extracted and gold references, thereby penalising both missing and hallucinated entries.\nInitial findings demonstrate Llamore’s efficacy. When tested on the PLOS 1000 biomedical dataset, Llamore (utilising Gemini 2.0 Flash) achieved an F1-score of 0.62, closely matching Grobid’s performance of 0.61. Crucially, on the custom-curated humanities dataset, Llamore significantly outperformed Grobid, achieving an F1-score of 0.45 compared to Grobid’s 0.14. This underscores its superior capability in handling complex, footnoted literature. Future work aims to expand the training data, refine evaluation metrics, and enhance the system’s capacity for contextual citation analysis, including the resolution of op cit. and the identification of approving or contracting citations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-research-imperative-unlocking-citation-graphs",
    "href": "chapter_ai-nepi_010.html#the-research-imperative-unlocking-citation-graphs",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.1 The Research Imperative: Unlocking Citation Graphs",
    "text": "10.1 The Research Imperative: Unlocking Citation Graphs\n\n\n\nSlide 01\n\n\nThe authors embark upon a critical investigation into the intricate nature of footnotes within law and humanities scholarship. They specifically address the inherent difficulties Large Language Models (LLMs) and other algorithms encounter when attempting to parse them. The overarching objective involves generating comprehensive citation graphs, which offer profound utility across various academic disciplines.\nThese graphs prove invaluable for scholars engaged in the history of science and intellectual history more broadly. They facilitate the discovery of intricate patterns and relationships within knowledge production, enabling a nuanced analysis of intellectual influences and a precise measurement of how ideas have been received and disseminated over time. A compelling application of this methodology involves identifying the most-cited authors across specific periods, exemplified by an analysis of the Journal of Law and Society between 1994 and 2003.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#problem-one-inadequate-bibliometric-data-coverage",
    "href": "chapter_ai-nepi_010.html#problem-one-inadequate-bibliometric-data-coverage",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.2 Problem One: Inadequate Bibliometric Data Coverage",
    "text": "10.2 Problem One: Inadequate Bibliometric Data Coverage\n\n\n\nSlide 02\n\n\nA significant impediment to scholarly inquiry arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) literature within conventional bibliometric databases. Prominent platforms such as Web of Science, Scopus, and OpenAlex exhibit considerable deficiencies in this domain.\nWeb of Science and Scopus, in particular, impose substantial financial burdens and operate under highly restrictive licensing agreements, fundamentally hindering open research. Whilst OpenAlex offers a more accessible, open-access alternative, it nonetheless falls short of meeting the specific data requirements for this research. Critically, these databases collectively lack comprehensive coverage for journals not classified as “A-journals,” exhibit significant gaps concerning pre-digital publications, and frequently omit content published in languages other than English.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#empirical-evidence-of-data-gaps",
    "href": "chapter_ai-nepi_010.html#empirical-evidence-of-data-gaps",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.3 Empirical Evidence of Data Gaps",
    "text": "10.3 Empirical Evidence of Data Gaps\n\n\n\nSlide 03\n\n\nAn illustrative example underscores the pervasive data gaps within existing bibliometric resources. The authors’ analysis of the Zeitschrift für Rechtssoziologie, a German journal dedicated to law and society established in 1980, reveals stark disparities in citation data availability. A comparative bar chart, contrasting data from Dimensions and OpenAlex across decades, clearly demonstrates that whilst coverage improves considerably after the 2000s, it remains almost non-existent for the journal’s earlier decades. This empirical observation highlights the critical need for alternative data acquisition strategies.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#problem-two-the-intricacies-of-humanities-footnotes",
    "href": "chapter_ai-nepi_010.html#problem-two-the-intricacies-of-humanities-footnotes",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.4 Problem Two: The Intricacies of Humanities Footnotes",
    "text": "10.4 Problem Two: The Intricacies of Humanities Footnotes\n\n\n\nSlide 04\n\n\nThe inadequate coverage within bibliometric databases stems from several fundamental issues. Primarily, humanities scholarship attracts less commercial interest compared to fields such as STEM, medicine, and economics, which typically dominate large bibliometric databases. Furthermore, these databases predominantly focus on “impact factor” for scientific evaluation, a metric largely irrelevant to research in intellectual history.\nCrucially, the literature of interest in the humanities frequently employs highly complex footnotes, colloquially termed “footnotes from hell.” These often incorporate extensive commentary, present in a messy, unstructured format, and are embedded within considerable textual “noise” that is not part of the direct citation. Traditional information extraction instruments, whilst requiring laborious manual annotation processes, consistently demonstrate poor performance. For instance, machine learning tools relying on conditional random forests exhibit low extraction and segmentation accuracies, as evidenced by the ExCite Performance table, where even combined training data yields limited improvement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#large-language-models-a-promising-yet-challenging-solution",
    "href": "chapter_ai-nepi_010.html#large-language-models-a-promising-yet-challenging-solution",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.5 Large Language Models: A Promising Yet Challenging Solution",
    "text": "10.5 Large Language Models: A Promising Yet Challenging Solution\n\n\n\nSlide 05\n\n\nLarge Language Models (LLMs) emerge as a promising avenue for addressing the challenges of footnote parsing. Initial experiments conducted by the authors in 2022, utilising models such as text-davinci-003, unequivocally demonstrated their considerable power in extracting references from highly unstructured and complex textual data. Subsequent advancements in model architecture promise even more refined results, whilst the advent of Vision Language Models (VLMs) now enables direct processing of PDF documents.\nThe authors can employ various methods to leverage these models effectively, including sophisticated prompt engineering, Retrieval Augmented Generation (RAG), and fine-tuning. Nevertheless, an overriding concern persists regarding the trustworthiness of the results. LLMs, whilst powerful, can “hallucinate,” inventing non-existent citations. A lawyer who submitted federal court filings citing fabricated cases generated by ChatGPT starkly illustrates this phenomenon. Consequently, a fundamental principle guides this research: one must refrain from conducting any analysis unless robust validation data confirms the accuracy of the extracted information.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#establishing-a-robust-evaluation-framework",
    "href": "chapter_ai-nepi_010.html#establishing-a-robust-evaluation-framework",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.6 Establishing a Robust Evaluation Framework",
    "text": "10.6 Establishing a Robust Evaluation Framework\n\n\n\nSlide 05\n\n\nEstablishing a robust testing and evaluation solution constitutes a core requirement for advancing reliable information extraction. The authors determined that this solution necessitates three critical components:\n\nA high-quality Gold Standard dataset must be meticulously compiled, serving as the definitive ground truth against which system performance can be measured.\nResearchers require a flexible framework, one capable of readily adapting to the rapidly evolving technological landscape of Large Language Models and their associated methodologies.\nThe framework must incorporate solid testing and evaluation algorithms, ensuring the production of consistent and comparable metrics across different solutions and iterations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.7 Developing a TEI-Annotated Gold Standard",
    "text": "10.7 Developing a TEI-Annotated Gold Standard\n\n\n\nSlide 06\n\n\nThe authors have meticulously compiled a comprehensive dataset for both training and evaluation, employing TEI XML encoding. This choice of standard is deliberate, as TEI XML offers a well-established, precisely specified, and highly comprehensive framework for text interchange. Crucially, it surpasses more limited bibliographical standards, such as CSL or BibTeX, by encompassing a broader spectrum of textual phenomena.\nBeyond mere reference management, TEI XML facilitates the encoding of citations, cross-references, and other contextual markup, enabling the classification of citation intention—a feature of significant interest to bibliometric projects. Furthermore, its widespread adoption in digital editorics projects allows for the integration of existing corpora, thereby enhancing the generalisation and robustness of the developed mechanisms. Despite its advantages, the TEI standard presents certain challenges, including conceptual distinctions between pointers and references, and technical considerations regarding constrained elements versus elliptic material.\nThe dataset’s establishment involves several stages: initially capturing PDF screenshots, then segmenting reference strings from the surrounding non-reference text within footnotes, and finally parsing these into a structured data format. Currently, the dataset comprises 1,100 foot- and endnotes, meticulously extracted from 25 articles published across 10 DOAJ journals. It specifically focuses on legal and historical humanities, incorporating multilingual content in French, German, Spanish, Italian, and Portuguese, spanning the period from 1958 to 2018. This compilation is expected to yield over 1,600 references, with each occurrence of a work encoded separately to preserve its specific context. The dataset remains a work in progress, with a strategic shift mid-way to include PDFs from Open Access journals, ensuring compatibility with Vision Language Models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#leveraging-tei-for-tooling-and-comparison",
    "href": "chapter_ai-nepi_010.html#leveraging-tei-for-tooling-and-comparison",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.8 Leveraging TEI for Tooling and Comparison",
    "text": "10.8 Leveraging TEI for Tooling and Comparison\n\n\n\nSlide 10\n\n\nA significant advantage of adopting the TEI XML standard lies in the extensive availability of compatible tooling. Among these, Grobid stands out as a particularly important resource for reference and information extraction. This widely used tool already employs TEI XML for its own training and evaluation processes.\nConsequently, utilising the same data format offers multiple benefits. It directly enables a rigorous comparison of the developed tool’s performance against Grobid’s established benchmarks. Moreover, the authors can leverage Grobid’s existing training data to enhance their own mechanisms, whilst simultaneously contributing their newly compiled data to the Grobid team, fostering a collaborative environment for mutual advancement in the field.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "href": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package",
    "text": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package\n\n\n\nSlide 11\n\n\nThe authors have developed Llamore, a concise Python package, to address the complexities of reference extraction. The name, an acronym for Large LANguage MOdels for Reference Extraction, precisely encapsulates its core functionality. This lightweight package, comprising fewer than 2000 lines of code, performs two essential tasks: it extracts citation data from either raw text or PDF documents using various Large Language Models, including multimodal variants, and subsequently evaluates the accuracy of these extractions.\nLlamore operates on a clear input-output workflow: it processes textual or PDF inputs to generate references formatted in TEI XML, and it compares these extracted references against a set of gold standard references to compute an F1-score, serving as the primary evaluation metric. During its development, two key objectives guided its design: maintaining a lightweight architecture by functioning as an interface to external models rather than embedding them, and ensuring broad compatibility with both proprietary and open-source LLMs and Vision Language Models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "href": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.10 Llamore: Implementation and Workflow",
    "text": "10.10 Llamore: Implementation and Workflow\n\n\n\nSlide 12\n\n\nLlamore offers a straightforward implementation and workflow, readily accessible via PyPI for installation using pip. For the extraction process, users import specific extractor classes, such as GeminiExtractor or OpenaiExtractor, and then instantiate an extractor, typically providing an API key. The system processes either a PDF file path or a raw input string, subsequently returning the extracted references. These references can then be conveniently exported to a TEI biblStructs XML file. Notably, the OpenaiExtractor ensures broad compatibility, as most open model serving frameworks, including Olama and VLLM, provide an API endpoint compatible with OpenAI’s standard.\nThe evaluation process involves importing the F1 class, which can be instantiated with an optional levenshtein_distance parameter, defaulting to zero for an exact match. Subsequently, the authors compute the macro-average F1-score by supplying both the extracted and the gold standard references to the compute_macro_average method.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#evaluation-metrics-f1-score-for-reference-comparison",
    "href": "chapter_ai-nepi_010.html#evaluation-metrics-f1-score-for-reference-comparison",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.11 Evaluation Metrics: F1-Score for Reference Comparison",
    "text": "10.11 Evaluation Metrics: F1-Score for Reference Comparison\n\n\n\nSlide 13\n\n\nThe F1-score serves as the primary evaluation metric, a well-established standard for comparing structured data. This composite measure integrates both Precision and Recall. Precision quantifies the proportion of correctly identified elements among all predicted elements, whilst Recall measures the proportion of correctly identified elements among all actual gold standard elements. By default, Llamore employs an exact match criterion, though this can be configured to allow for a specified Levenshtein distance, accommodating minor discrepancies.\nConsider an example: an extracted reference might contain an analytic title, a monographic title, author details (forename and surname), and a publication date. A corresponding gold reference might include these fields plus a cited range. If the analytic title, monographic title, surname, and publication date align perfectly, these constitute four matches. However, a slight deviation, such as an extra dot in the forename within the gold reference, would render that specific field a mismatch. The F1-score is then calculated as the harmonic mean of these precision and recall values. An F1-score of one signifies perfect extraction, indicating that the reference has been flawlessly identified, whereas a score of zero denotes a complete absence of matches.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#reference-alignment-the-unbalanced-assignment-problem",
    "href": "chapter_ai-nepi_010.html#reference-alignment-the-unbalanced-assignment-problem",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.12 Reference Alignment: The Unbalanced Assignment Problem",
    "text": "10.12 Reference Alignment: The Unbalanced Assignment Problem\n\n\n\nSlide 15\n\n\nBeyond merely comparing individual references, a significant challenge involves accurately aligning the set of extracted references with their corresponding gold standard counterparts. The authors address this by formulating the problem as an “Unbalanced Assignment Problem,” a well-documented class of optimisation challenges. Llamore employs an internal solver from the SciPy library to resolve this.\nThe process entails computing F1-scores for every conceivable combination of extracted and gold references, subsequently constructing a comprehensive matrix from these scores. The solver then works to maximise the total F1-score across the entire set, whilst rigorously ensuring a unique assignment for each reference. Finally, the system macro-averages the F1-scores of these uniquely assigned pairs. Crucially, the methodology incorporates a stringent penalisation mechanism: both missing references and any hallucinated entries are assigned an F1-score of zero, thereby accurately reflecting their impact on overall performance.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-evaluation-llamore-vs.-grobid",
    "href": "chapter_ai-nepi_010.html#performance-evaluation-llamore-vs.-grobid",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.13 Performance Evaluation: Llamore vs. Grobid",
    "text": "10.13 Performance Evaluation: Llamore vs. Grobid\n\n\n\nSlide 16\n\n\nThe authors rigorously compared Llamore against Grobid across two distinct datasets. On the PLOS 1000 Dataset, comprising 1,000 biomedical PDFs, Llamore, utilising Gemini 2.0 Flash, achieved an F1-score of 0.62 for exact matches, closely mirroring Grobid’s score of 0.61. Whilst their accuracy proved comparable on this dataset, Grobid demonstrated superior efficiency and reduced resource intensity, largely attributable to its prior training on similar journal articles.\nHowever, a stark contrast emerged when testing on the custom-curated humanities dataset, which features complex footnoted literature. Here, Grobid struggled considerably, yielding an F1-score of merely 0.14. In stark contrast, Llamore’s approach achieved a significantly higher F1-score of 0.45, unequivocally demonstrating its superior capability in extracting references from this challenging domain.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "href": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "title": "10  LLMs and Footnotes: Challenges in Humanities Scholarship",
    "section": "10.14 Conclusion and Takeaways",
    "text": "10.14 Conclusion and Takeaways\n\n\n\nSlide 17\n\n\nIn conclusion, Grobid retains its advantage as the preferred tool for literature types upon which it was specifically trained, primarily owing to its superior processing speed and significantly lower resource demands. Nevertheless, for the particularly challenging domain of footnoted literature, experiments employing Llamore in conjunction with Gemini models reveal a compelling three-fold improvement in extraction performance. It is important to note that the current performance metrics pertain exclusively to pure reference extraction, deliberately excluding considerations of contextual information or complex cross-referencing.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LLMs and Footnotes: Challenges in Humanities Scholarship</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview\nThis presentation elucidates the development of an AI-driven solution meticulously crafted to manage the escalating volume of scientific information and to bolster knowledge production. Researchers from DANS (Data Archiving and Networked Services) and GESIS (Leibniz Institute for the Social Sciences) collaborated to confront the pervasive challenge of information overload, a phenomenon that significantly impedes the effective review, evaluation, and selection of scholarly content. Their core objective involved engineering an AI system capable of “chatting with papers” drawn from specific collections, thereby profoundly enhancing both information retrieval and human-machine interaction.\nThe research team conceptualised the system as a “local” or “tailored AI solution,” comprising two principal components: Ghostwriter, which serves as the user interface, and EverythingData, encompassing the intricate back-end processing pipelines. Its foundational methodology employs Retrieval-Augmented Generation (RAG), seamlessly integrating both vector spaces and knowledge graphs. The authors construct vector spaces from data file content, encoding them through embeddings derived from various machine learning algorithms and Large Language Models (LLMs). Concurrently, a graph represents a metadata layer, which the team seamlessly integrates with diverse ontologies and controlled vocabularies, including principles of responsible AI, and expresses via the Croissant ML standard.\nA key vision for this architecture, termed GraphRAG, seeks to unify graphs and vectors within a single model, operating as a distributed AI. Here, the LLM functions as both an interface and a sophisticated reasoning engine, connecting to a “RAG library” (the graph) to navigate datasets and consume embeddings (vectors) as contextual information. The system demonstrably prevents hallucinations by strictly adhering to provided source material, offering precise, factual answers with direct references. It supports iterative query refinement and boasts robust multilingual capabilities, enabling queries in one language whilst processing documents in another.\nThe implementation involves ingesting articles, such as 100 papers from the method-data-analysis (mda) journal, into a vector store (Qdrant) and performing operations like term extraction, embedding construction, and enrichment. Crucially, the authors express selected terms as structured data within a knowledge graph, often enriched with Wikidata, which contextualises embeddings and serves as a “ground truth” for validating LLM outputs. This decoupling of knowledge from the model facilitates benchmarking and ensures the sustainability of knowledge organisation systems for future scientific endeavours. The project ultimately aims to support human thought processes in formulating research questions, offering a controllable and cost-effective alternative to large, cloud-based LLMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#science-dynamics-and-ai-addressing-information-overload",
    "href": "chapter_ai-nepi_011.html#science-dynamics-and-ai-addressing-information-overload",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Science Dynamics and AI: Addressing Information Overload",
    "text": "11.1 Science Dynamics and AI: Addressing Information Overload\n\n\n\nSlide 01\n\n\nThis initiative represents a collaborative endeavour between DANS, the data archive of the Royal Netherlands Academy of Arts and Science, and GESIS, another prominent archive actively engaged in research. The project addresses a fundamental challenge within the evolving landscape of scientific disciplines: the relentless growth and increasing differentiation of knowledge. This expansion invariably complicates the processes of reviewing, evaluating, and selecting pertinent information.\nCrucially, the capacity to find and comprehend information remains a prerequisite for any form of knowledge creation, whether within individual cognitive processes or across broader academic communities. Modern machines, particularly the latest advancements in Artificial Intelligence, have undeniably accelerated this growth. Consequently, a pivotal question emerges: can these technologies also support the intricate knowledge production process, specifically within the domain of Information Retrieval?\nThe impetus for this work stemmed from extensive experimentation conducted by Slava Tikhonov, a senior research engineer at DANS, who has pioneered the construction of various data pipelines. Rather than a straightforward pipeline, the system Slava Tikhonov developed is more accurately characterised as a complex “back of things,” a multifaceted architecture challenging to deconstruct and articulate. Ultimately, the project sought to apply and illustrate AI solutions to effectively manage the overwhelming deluge of information that increasingly submerges contemporary researchers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#project-objectives-and-system-architecture",
    "href": "chapter_ai-nepi_011.html#project-objectives-and-system-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.2 Project Objectives and System Architecture",
    "text": "11.2 Project Objectives and System Architecture\n\n\n\nSlide 02\n\n\nThe central research question guiding this project explored the feasibility of constructing an AI solution capable of facilitating interactive dialogue with scholarly papers drawn from a curated selection. This inquiry necessitated an exploration of several interconnected concepts: information retrieval, the dynamics of human-machine interaction, and the burgeoning field of Retrieval-Augmented Generation (RAG) within generative AI.\nThe researchers selected the method-data-analysis (mda) journal as a specific use case to demonstrate the system’s capabilities. They introduced a workflow underpinning a ‘local’ or ‘tailored AI solution’, distinguishing its primary components with internal project names: Ghostwriter, which functions as the user interface, and EverythingData, a comprehensive term encompassing all underlying back-end processes. The presentation subsequently provided illustrative examples of both front-end and back-end operations, culminating in a summary and outlook on the project’s implications.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-a-novel-information-retrieval-interface",
    "href": "chapter_ai-nepi_011.html#ghostwriter-a-novel-information-retrieval-interface",
    "title": "11  Science dynamics and AI",
    "section": "11.3 Ghostwriter: A Novel Information Retrieval Interface",
    "text": "11.3 Ghostwriter: A Novel Information Retrieval Interface\n\n\n\nSlide 03\n\n\nThe Ghostwriter interface represents a novel approach to information retrieval, designed to facilitate simultaneous interaction with both structured data and natural language inputs. The authors metaphorically describe this dual capability as “chatting with experts and librarians at the same time,” where the “librarian” embodies structured data and knowledge organisation systems, whilst the “expert” represents natural language.\nHistorically, traditional information retrieval, involving a query against a single database representation, necessitated prior knowledge of the database schema and its typical values to yield results. This scenario, likened to “Me and a database,” presented the classic information retrieval problem of formulating the precise query. More advanced information retrieval systems, operating on connected structured data or graphs, offered improvements. Here, the underlying machinery could suggest similar or superior queries based on schema interconnections, subsequently providing lists of potential results. This advancement, conceptualised as “Me and a librarian,” mirrors features found in Google and schema.org, though typically applied to web-scale interactions rather than local ones.\nThe advent of Large Language Models (LLMs) introduced another paradigm: a query against an LLM interprets natural language input and suggests results, also expressed in natural language. This interaction is akin to “Me and a library” or “Me and a round of experts.” Ghostwriter synthesises these approaches. It integrates a local LLM with target data collections and a network of additional data interpretation sources, accessible via Application Programming Interfaces (APIs). This sophisticated architecture enables the system to generate families of terms around a given query, identify related structured information, and ultimately return a comprehensive list of results. Crucially, applying this system iteratively empowers users to reformulate their questions, thereby gaining a deeper understanding of their actual query intent and the scope of the available data space.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-framework",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-framework",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Retrieval-Augmented Generation (RAG) Framework",
    "text": "11.4 Retrieval-Augmented Generation (RAG) Framework\n\n\n\nSlide 04\n\n\nScientifically, this system firmly situates itself within the broader academic discourse surrounding Retrieval-Augmented Generation (RAG). A foundational understanding of this topic is readily available through resources such as Philip Rattliff’s “The GraphRAG Manifesto: Adding Knowledge to GenAI,” published by Neo4j.\nThe system’s efficacy hinges upon two primary ingredients. Firstly, the authors meticulously construct a vector space from the content of data files. This content undergoes encoding into embeddings, which capture both properties and their associated attributes. Various machine learning algorithms, leveraging different Large Language Models, compute these embeddings. Secondly, a robust graph serves as a comprehensive metadata layer. This graph seamlessly integrates diverse ontologies and controlled vocabularies, notably incorporating principles of responsible AI, and adheres to the Croissant ML standard for its expression.\nA key strategic vision, termed GraphRAG, aims to unify both the graph and vector components into a singular, cohesive model. The authors designed this integration for local implementation, thereby fostering a form of Distributed AI. Within this architecture, the LLM assumes a dual role: it acts as the primary interface facilitating human-AI interaction and simultaneously functions as a sophisticated reasoning engine. The practical implementation involves the LLM connecting directly to a “RAG library,” which is essentially the graph, enabling it to navigate through datasets and consume the generated embeddings as contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-operational-workflow",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-operational-workflow",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Ghostwriter and EverythingData: Operational Workflow",
    "text": "11.5 Ghostwriter and EverythingData: Operational Workflow\n\n\n\nSlide 04\n\n\nThe operational workflow commences with an input comprising a collection of articles. Whilst the demonstration specifically utilised a small, scraped collection from the MDA journal, the system readily accommodates any document collection. This input then enters the “EverythingData” component, which orchestrates a series of intricate back-end operations.\nInitially, EverythingData stores the processed information within a vector store, specifically employing Qdrant. Subsequently, it executes a range of processes, including term extraction, the construction of embeddings, and various enrichments. A crucial step involves coupling this data with knowledge graphs: the authors transform selected terms into structured data within a graph and further enrich them, notably through integration with Wikidata. This strategic coupling serves to contextualise the embeddings, thereby imbuing them with enhanced semantic value.\nAll processed data converges into a unified “Vector Space RAG-Graph.” Users then interact with this comprehensive knowledge base via the query interface, formulating their questions in natural language. The system responds by providing two distinct outputs: a list of relevant documents, aligning with conventional information retrieval practices, and a concise explanatory summary text, intelligently generated by the system’s machinery in response to the user’s query. The Ghostwriter interface serves as the primary conduit for this seamless user interaction.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-indexing-and-factual-retrieval",
    "href": "chapter_ai-nepi_011.html#ghostwriter-indexing-and-factual-retrieval",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter: Indexing and Factual Retrieval",
    "text": "11.6 Ghostwriter: Indexing and Factual Retrieval\n\n\n\nSlide 06\n\n\nThe developer’s approach to Large Language Models (LLMs) stems from early engagement, commencing with GPT-2 testing in 2020. Rather than relying on monolithic models, the strategy involves deconstructing the LLM training process into smaller, more manageable components, enabling their targeted application. This modularity grants the system remarkable flexibility in content processing: whilst demonstrated with scholarly papers, it seamlessly handles any web content and even spreadsheets, facilitating precise queries regarding specific data values.\nCrucially, the system rigorously prevents hallucinations. Its responses remain strictly factual and non-hallucinatory because it draws exclusively from the provided source material. Should information be unavailable within the designated source, the system transparently indicates “I don’t know.” This commitment to factual integrity is achieved by employing a relatively simple 1 billion parameter LLM, which, when synergistically combined with knowledge graphs, proves highly effective in answering complex questions.\nFor the presented use case, researchers ingested 100 articles from the MDA (GESIS journal) website directly into Ghostwriter, establishing a dedicated collection. The system deliberately avoids reliance on any pre-ingested LLM knowledge; instead, it directly queries specific papers to extract factual information. The overarching goal remains to provide responses derived solely from the content present within the paper, without introducing extraneous details. This Ghostwriter instance, operating on MDA papers, is publicly accessible at https://gesis.now.museum.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#chatting-with-papers-the-male-breadwinner-model-example",
    "href": "chapter_ai-nepi_011.html#chatting-with-papers-the-male-breadwinner-model-example",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Chatting with Papers: The Male Breadwinner Model Example",
    "text": "11.7 Chatting with Papers: The Male Breadwinner Model Example\n\n\n\nSlide 08\n\n\nA practical demonstration of the Ghostwriter’s capabilities involved the query: “explain male breadwinner model to me.” In response, the system generated a comprehensive explanation of the Male Breadwinner Ideology, elucidating its societal concept, the expectation for men to serve as primary financial providers, and its observed influence on individual attitudes and entrepreneurial activities within Germany.\nCrucially, the system meticulously provides direct references to the original scholarly papers from which the information was extracted, including titles such as “The Past, Present and Future of Factorial Survey Experiments…” and “Gender and Survey Participation…”. This rigorous source referencing ensures transparency and validates the information presented. The system’s design inherently prevents hallucinations, as it precisely identifies and retrieves information directly from the source texts.\nTechnically, this precision is achieved by segmenting each paper into numerous small blocks, with a unique identifier assigned to every block. The system then employs advanced Large Language Model techniques to intelligently connect and retrieve these blocks, applying specific weights to prioritise their relevance. Furthermore, the integration of knowledge graphs significantly enhances this process by accurately predicting which particular text segments will most effectively address a given question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-approach-to-query-refinement-and-factual-integrity",
    "href": "chapter_ai-nepi_011.html#iterative-approach-to-query-refinement-and-factual-integrity",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Iterative Approach to Query Refinement and Factual Integrity",
    "text": "11.8 Iterative Approach to Query Refinement and Factual Integrity\n\n\n\nSlide 09\n\n\nThe system’s commitment to factual integrity extends to its handling of information gaps. When presented with a refined query, such as “explain how data was collected on male breadwinner model,” and the direct information is unavailable within its indexed sources, the system explicitly states “there is no direct information.”\nFor instance, in response to this particular query, the system noted a study that utilised German data and employed a mixed-methods research strategy, including a survey experiment by Hanhmueller et al. (2015). It also referenced another article by Haase et al. (2016) that examined the male breadwinner model, yet it transparently indicated the absence of data collection details for this specific study. Furthermore, the interface incorporates an “Add paper” button, empowering users to contribute new articles to the collection. Crucially, any information added through this feature will subsequently be incorporated into the system’s knowledge base, enriching future query responses.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-mechanics-entity-extraction-knowledge-graphs-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#system-mechanics-entity-extraction-knowledge-graphs-and-multilinguality",
    "title": "11  Science dynamics and AI",
    "section": "11.9 System Mechanics: Entity Extraction, Knowledge Graphs, and Multilinguality",
    "text": "11.9 System Mechanics: Entity Extraction, Knowledge Graphs, and Multilinguality\n\n\n\nSlide 09\n\n\nUnderpinning the system’s functionality lies a sophisticated entity extraction pipeline. This pipeline meticulously annotates terms with semantic meaning by mapping them to controlled vocabularies, thereby transforming raw vector space data into a coherent knowledge graph. Beyond this initial transformation, the system actively links these entities to more extensive knowledge graph representations, notably leveraging Wikidata. This integration with Wikidata serves a crucial purpose: it establishes a “ground truth,” providing a reliable benchmark against which the accuracy of LLM-generated answers can be rigorously validated.\nA significant feature of the system is its immediate and robust multilinguality. This capability proves indispensable for processing scholarly papers published in diverse languages, such as Chinese or German, whilst enabling users to pose questions and receive answers in English. Ultimately, the Large Language Model orchestrates the synthesis of various text segments, culminating in the production of a concise, explanatory summary.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#fact-extraction-and-wikidata-integration-for-semantic-enrichment",
    "href": "chapter_ai-nepi_011.html#fact-extraction-and-wikidata-integration-for-semantic-enrichment",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Fact Extraction and Wikidata Integration for Semantic Enrichment",
    "text": "11.10 Fact Extraction and Wikidata Integration for Semantic Enrichment\n\n\n\nSlide 10\n\n\nThe fact extraction process commences by segmenting user queries into granular pieces. A sophisticated knowledge organisation system then processes these segments, systematically revealing new and deeper levels of associated terms. Crucially, all extracted information undergoes a rigorous linking process with Wikidata, transforming free-form strings into structured, canonical identifiers.\nThese Wikidata identifiers confer substantial benefits. They inherently enable multilingual translations, providing access to a comprehensive array of associated properties. Consequently, the system can comprehend and process questions posed in various languages. The determination of conceptual similarity for these linkages relies upon the precise measurements derived from Large Language Model embeddings.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#multilingual-capabilities-and-the-future-of-knowledge-organisation",
    "href": "chapter_ai-nepi_011.html#multilingual-capabilities-and-the-future-of-knowledge-organisation",
    "title": "11  Science dynamics and AI",
    "section": "11.11 Multilingual Capabilities and the Future of Knowledge Organisation",
    "text": "11.11 Multilingual Capabilities and the Future of Knowledge Organisation\n\n\n\nSlide 11\n\n\nThe system’s robust multilingual capabilities are exemplified by its treatment of core query concepts, such as “bread winner mo,” as abstract entities. The Gemma3 Large Language Model then generates translations for these concepts into hundreds of languages. A pivotal innovation involves establishing a “ground truth” by decoupling knowledge from specific questions and papers. The authors achieve this by storing knowledge as a comprehensive list of Wikidata identifiers, maintained externally to the model itself.\nThis externalisation of knowledge facilitates rigorous benchmarking. Researchers can test various models, including those not yet fully trained, by posing identical questions and comparing the consistency of their generated identifier lists. This comparative analysis effectively identifies models unsuitable for specific tasks. Furthermore, this methodological approach supports the creation of robust benchmarks and champions the utilisation of knowledge organisation systems for future generations of scientists. Collaborations with prominent industry partners, including Google and Meta, underscore a commitment to ensuring the long-term sustainability of this process. Ultimately, the developers firmly believe that knowledge organisation systems represent the future paradigm for information management.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-and-philosophical-implications",
    "href": "chapter_ai-nepi_011.html#live-demonstration-and-philosophical-implications",
    "title": "11  Science dynamics and AI",
    "section": "11.12 Live Demonstration and Philosophical Implications",
    "text": "11.12 Live Demonstration and Philosophical Implications\n\n\n\nSlide 13\n\n\nA live demonstration showcased the system’s capabilities via the GESIS Leibniz-Institut für Sozialwissenschaften website, specifically within its “Ask Questions” section. When queried with “Rational Choice Theory,” the system promptly retrieved pertinent information, synthesised a summary from various papers, and provided direct references to the original sources. A subsequent, more specific query, “explain utility in Rational Choice Theory,” prompted the system to select distinct pieces of information from the indexed papers, yielding varied results whilst consistently referencing the same source documents.\nAn Application Programming Interface (API) further extends the system’s utility, enabling an automatic mode suitable for agentic architectures. This allows for the construction of sophisticated pipelines, facilitating automated result collection and the identification of novel knowledge. Users also possess the ability to augment the collection by adding new pages or content, either via a webpage URL or an RSS feed, which the system seamlessly incorporates. A compelling demonstration of its multilingual prowess involved posing a question in English and successfully retrieving information from a source paper written entirely in German, save for its abstract.\nThe developers emphasised the significant benefits of a local system, which affords greater control over data and mitigates the considerable costs and inherent control limitations associated with large, cloud-based Large Language Models (LLMs). The interaction with scholarly papers is evocatively likened to engaging with an “invisible college.” Crucially, the system’s fundamental purpose transcends merely providing definitive facts or ultimate answers. Instead, it aims to stimulate human thought processes, assist in comprehending complex questions, and support the precise formulation of research questions. Ultimately, these technological possibilities should be perceived as powerful tools designed to augment and support human intellectual activity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "",
    "text": "Overview\nThis chapter explores the application of Retrieval-Augmented Generation (RAG) systems within philosophical research, specifically addressing the unique challenges posed by the discipline’s emphasis on linguistic and semantic accuracy. The authors highlight the limitations of standalone Large Language Models (LLMs) for philosophical inquiry, including their lack of direct access to full texts, restricted context windows, and inability to attribute claims reliably. Consequently, RAG systems emerge as a suitable solution, enabling direct engagement with specific corpora and providing verifiable citations.\nThe authors detail a RAG system prototype developed using the Stanford Encyclopedia of Philosophy as its data source. This system aims to facilitate both didactic engagement with philosophical texts and advanced research tasks, such as fact-checking, corpus exploration, and passage identification for close reading. Crucially, the development process revealed that effective RAG implementation necessitates extensive “tweaking” and rigorous, domain-specific evaluation. The system’s architecture includes a frontend for user interaction, a backend for processing, and a comparative output display that benchmarks RAG performance against LLM-only responses.\nA key optimisation involved determining the optimal chunk size for text retrieval. This process revealed that main sections of the highly systematised Stanford Encyclopedia yielded superior results despite their length. These findings underscore RAG systems’ capacity to integrate verbatim corpora and domain knowledge, thereby reducing hallucinations and enabling citation. Nevertheless, the authors caution that RAGs require domain expertise for effective tweaking and evaluation, as their performance remains highly sensitive to the specific corpus, question types, and evaluation criteria. A notable challenge identified involves RAGs’ tendency to provide less effective responses to broad overview questions, as their focus on local information can obscure broader perspectives. This observation suggests a future direction towards more flexible, agentic RAG systems capable of discerning question types.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-philosophical-research-challenges-with-rag-systems",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.1 Addressing Philosophical Research Challenges with RAG Systems",
    "text": "12.1 Addressing Philosophical Research Challenges with RAG Systems\n\n\n\nSlide 02\n\n\nPhilosophical inquiry frequently necessitates precise engagement with foundational texts. This requirement is exemplified by questions such as Aristotle’s theory of matter in the Physics or the evolution of Einstein’s concept of locality from his early relativity works to his 1948 paper on “Quantenmechanik und Wirklichkeit.” Whilst Large Language Models (LLMs) like ChatGPT can offer decent, differentiated answers to such queries, they present several significant limitations for rigorous philosophical research.\nPrimarily, LLMs contend with an inherent problem of access. Although their training data may include full texts, they cannot directly quote specific passages or chapters verbatim without an online search, which itself is constrained by copyright. Crucially, LLM training mechanisms actively prevent verbatim memorisation, instead focusing on learning generalisable statistical rules for text production. Consequently, they cannot provide the deep, fine-grained textual analysis essential for philosophical work. Furthermore, LLMs face a limited context window; even models like ChatGPT 4o, with 128,000 tokens, quickly exhaust their capacity when processing extensive philosophical corpora. Finally, LLMs inherently struggle with attribution, failing to provide verifiable sources or citations for their claims, which can lead to unverified information or outright hallucinations.\nRetrieval-Augmented Generation (RAG) systems offer a robust solution to these challenges. A typical RAG setup integrates a specific data source—such as Aristotle’s or Einstein’s complete works—from which documents are retrieved, often via semantic search, though hybrid or classic search options also exist. The system then augments user prompts with relevant text chunks, directly addressing the limitations of standalone LLMs. This approach resolves the problem of access by providing the LLM with the original text sources, mitigates context window constraints by supplying only relevant segments, and crucially, enables the attribution of claims, offering verifiable citations akin to those found in platforms like Perplexity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "href": "chapter_ai-nepi_012.html#potential-applications-of-rag-systems-in-philosophy",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.2 Potential Applications of RAG Systems in Philosophy",
    "text": "12.2 Potential Applications of RAG Systems in Philosophy\n\n\n\nSlide 06\n\n\nRAG systems hold significant promise for philosophical applications, fundamentally transforming how scholars and students interact with complex texts. The overarching concept involves enabling users to “chat” with philosophical corpora, such as Locke’s complete works, in a manner akin to ChatGPT, yet with the crucial advantage of providing much more detailed domain knowledge and a verbatim text basis.\nDidactically, these systems prove invaluable. They facilitate student engagement with challenging texts; for instance, they allow students to initiate their exploration of Locke’s Essay by querying its general ideas before progressively delving into specific concepts like his epistemology or theory of matter. This iterative questioning fosters a deeper, more instructive understanding of the texts.\nBeyond pedagogy, RAG systems offer compelling applications for research. They enable reliable fact lookup in handbooks, effectively modernising the traditional process of manually consulting physical books for footnotes and remarks. Furthermore, scholars can utilise RAGs to explore previously unexamined or newly digitised corpora, gaining comprehensive overviews of their contents. The systems also assist in identifying specific passages relevant for close reading, directly supporting focused scholarly analysis. Ultimately, the ambition extends to generating detailed answers for at least portions of complex research questions, painting a picture of future capabilities in philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#developing-an-example-rag-system-the-stanford-encyclopedia-of-philosophy",
    "href": "chapter_ai-nepi_012.html#developing-an-example-rag-system-the-stanford-encyclopedia-of-philosophy",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.3 Developing an Example RAG System: The Stanford Encyclopedia of Philosophy",
    "text": "12.3 Developing an Example RAG System: The Stanford Encyclopedia of Philosophy\n\n\n\nSlide 08\n\n\nTo explore the practical utility of RAG systems in philosophy, the authors developed an example implementation utilising the Stanford Encyclopedia of Philosophy (SEP), a widely recognised online handbook, as its primary data source. They meticulously scraped and converted the SEP content into Markdown format, forming the basis for the system.\nInitially, the project aimed simply to create a beneficial tool for the philosophical community. However, during the development and testing phases, a significant challenge emerged: the authors found the RAG system’s initial answers surprisingly poor, often performing worse than direct queries to a standalone ChatGPT instance. This outcome necessitated a shift in the project’s focus, evolving into a qualitative study on the optimal setup of RAG systems for philosophical applications.\nThe authors found that improving the system’s performance demanded extensive “tweaking,” involving adjustments to the underlying language models, optimisation of various hyperparameters, and the implementation of more complex algorithms, such as reranking. For this iterative improvement, they adopted a methodology of theoretically grounded trial and error, systematically identifying which measures enhanced answer quality. This process underscored the critical importance of sound evaluation standards, particularly given the nature of philosophical inquiry. Unlike historical research that might seek atomic facts, philosophical questions often elicit complex, unstructured textual propositions. Evaluating the factual accuracy and nuanced understanding within these propositions presents a distinct challenge, demanding robust and sophisticated evaluation criteria.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-overview-and-frontend-details",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-overview-and-frontend-details",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.4 SEP RAG System Overview and Frontend Details",
    "text": "12.4 SEP RAG System Overview and Frontend Details\n\n\n\nSlide 11\n\n\nThe authors developed the SEP RAG system, which comprises a user-friendly frontend and a robust backend, implemented in several thousand lines of Python code. The frontend provides a clear interface for user interaction, allowing for the configuration of various parameters.\nSpecifically, the frontend’s “SEP RAG Details 1” section presents input fields for critical hyperparameters. These include the choice of generative model, the prompt token limit (both model-defined and user-defined), and the number of texts to retrieve. Users can also define a “Persona” for the system and, crucially, input their “Philosophical Question,” exemplified by a query such as “What is priority monism?” A “Generate answer” button initiates the processing.\nFor comparative analysis, the “SEP RAG Details 2” section displays the output in two distinct columns: an “Answer with LLM alone” column serves as a benchmark, whilst the “Answer with RAG” column showcases the system’s enhanced response. This side-by-side presentation facilitates a direct and immediate comparison of performance. Finally, the “SEP RAG Details 3” section offers a comprehensive “Retrieved Texts Overview.” This detailed list includes file names, section headings, text lengths in tokens, total tokens, and an explicit indication of which texts were successfully included in the prompt and which were truncated due to token limitations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunk-size-a-philosophical-perspective",
    "href": "chapter_ai-nepi_012.html#optimising-chunk-size-a-philosophical-perspective",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.5 Optimising Chunk Size: A Philosophical Perspective",
    "text": "12.5 Optimising Chunk Size: A Philosophical Perspective\n\n\n\nSlide 12\n\n\nOptimising the chunk size represents a critical hyperparameter adjustment in RAG system development. The authors explored three primary chunking options: a fixed number of words, typically around 500 tokens, a common and straightforward criterion in computer science; chunking by paragraphs; and chunking by sections, whether at a low or high level.\nRemarkably, the most effective results emerged from utilising the main sections of the Stanford Encyclopedia of Philosophy as the primary retrieval documents. This outcome surprised the authors, particularly given that the average section length, at approximately 3,000 words, substantially exceeded the embedding model’s typical cutoff of just over 500 words. The authors hypothesise that this success stems from the highly systematised nature of the Stanford Encyclopedia; its main sections are so well-structured that their initial 500 words often convey the core ideas effectively.\nNevertheless, this finding carries a crucial generalisability caveat. This section-based chunking strategy might not prove effective for more heterogeneous texts that lack such rigorous internal organisation. Philosophical arguments, in particular, frequently “spread” across multiple pages, requiring extensive explanation beyond the confines of a single paragraph. Consequently, chunking by paragraphs can artificially fragment semantically coherent arguments, diminishing the quality of the retrieved information.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#results-cautions-and-open-challenges-in-rag-implementation",
    "href": "chapter_ai-nepi_012.html#results-cautions-and-open-challenges-in-rag-implementation",
    "title": "12  Possible applications of RAG systems in philosophy",
    "section": "12.6 Results, Cautions, and Open Challenges in RAG Implementation",
    "text": "12.6 Results, Cautions, and Open Challenges in RAG Implementation\n\n\n\nSlide 14\n\n\nRAG systems offer compelling advantages for scholarly applications. They seamlessly integrate verbatim corpora and highly specific domain knowledge, which dramatically reduces the occurrence of hallucinations whilst enabling the citation of relevant documents for generated answers. Consequently, RAG setups are inherently well-suited to assist with a diverse array of scientific tasks.\nNevertheless, the authors highlight crucial cautions. Firstly, RAG systems fundamentally necessitate extensive “tweaking” to achieve optimal performance. Secondly, rigorous evaluation proves indispensable, demanding a representative set of questions and corresponding expected answers. This evaluation process, moreover, critically requires the involvement of domain experts, as no single RAG setup can universally claim superiority across all domains, corpus types, or question types. Each implementation demands specific tailoring.\nSeveral open challenges persist. A significant issue arises when the system fails to retrieve relevant documents, leading to a noticeable decrease in answer quality and necessitating prompt adjustments. Intriguingly, RAGs often yield inferior results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This counterintuitive outcome stems from RAGs’ inherent focus on the local information contained within retrieved chunks; this granular focus can inadvertently distract from the broader perspective required for comprehensive overview responses. Addressing this limitation, the authors suggest developing more flexible systems capable of discerning different question types, thereby paving the way for the emergence of more sophisticated, agentic RAG systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Possible applications of RAG systems in philosophy</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "",
    "text": "Overview\nThis chapter details a novel methodology for addressing fundamental questions in the philosophy of science, specifically concerning the structure of research fields. The authors integrate advanced computational methods, including linguistic analysis and social network analysis, to reconstruct the intellectual and social landscape of scientific inquiry. Their work focuses on quantum gravity as a compelling case study, a field characterised by a plurality of theoretical approaches.\nThe authors’ methodology involves a two-pronged, bottom-up reconstruction. Firstly, they conduct a linguistic analysis of a large corpus of theoretical physics abstracts and titles, employing the Bertopic pipeline to identify intellectual topics. Secondly, they perform a social network analysis of co-authorship data to delineate scientific communities. Recognising the scale-dependent nature of both topics and communities, the authors employ hierarchical clustering for both structures, utilising Ward agglomerative clustering for topics and a hierarchical stochastic block model for communities. Crucially, an adaptive topic coarse-graining strategy, based on the Minimum Description Length (MDL) criterion, refines the linguistic partition by prioritising information relevant to the social structure.\nThe authors’ findings reveal that their bottom-up reconstruction can either confirm or re-evaluate physicists’ intuitive understanding of their field’s structure. For instance, whilst some approaches align well with emergent topics, others, particularly those less conceptually autonomous, do not. Notably, their analysis suggests that string theory and supergravity, despite historical distinctions, coalesce into a single intellectual cluster when linguistic nuances without social consequences are removed. This convergence underscores the utility of computational methods in challenging long-standing philosophical intuitions, positing computation as a continuation of philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#investigating-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#investigating-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.1 Investigating Plural Pursuit in Quantum Gravity",
    "text": "13.1 Investigating Plural Pursuit in Quantum Gravity\n\n\n\nSlide 01\n\n\nThe authors’ research endeavours to address fundamental questions within the philosophy of science, specifically by integrating social network analysis with established computational methodologies. Their investigation centres on quantum gravity, a complex field serving as a compelling case study.\nThe authors’ approach unfolds in three distinct stages. Initially, they introduce the quantum gravity case study, establishing its philosophical context. Subsequently, they propose a bottom-up reconstruction of the quantum gravity research landscape, meticulously detailing its intellectual and social contours. Finally, they rigorously compare this empirically derived reconstruction with the prevailing intuitions of physicists regarding their field’s inherent structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-plurality-of-approaches-to-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#the-plurality-of-approaches-to-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.2 The Plurality of Approaches to Quantum Gravity",
    "text": "13.2 The Plurality of Approaches to Quantum Gravity\n\n\n\nSlide 01\n\n\nThis inquiry focuses on quantum gravity and the concept of plural pursuit within scientific research. This area of fundamental physics presents a unique landscape for exploring how diverse approaches co-exist and evolve.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#problem-and-attempted-solutions-in-fundamental-physics",
    "href": "chapter_ai-nepi_015.html#problem-and-attempted-solutions-in-fundamental-physics",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.3 Problem and Attempted Solutions in Fundamental Physics",
    "text": "13.3 Problem and Attempted Solutions in Fundamental Physics\n\n\n\nSlide 01\n\n\nA long-standing contemporary issue in fundamental physics concerns the formulation of a quantum theory of gravity. This formidable challenge necessitates reconciling our understanding of phenomena at minuscule scales with observations at vast cosmological dimensions.\nPhysicists have proposed numerous attempted solutions to this problem. String theory stands as the most prominent amongst these, whilst other notable approaches include Supergravity, Loop Quantum Gravity, spin foams, Causal Set Theory, and Asymptotic Safety. To comprehensively account for this diverse landscape of concurrent research efforts, the authors contend that the concept of “plural pursuit” becomes indispensable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit-and-empirical-inquiry",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit-and-empirical-inquiry",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.4 Defining Plural Pursuit and Empirical Inquiry",
    "text": "13.4 Defining Plural Pursuit and Empirical Inquiry\n\n\n\nSlide 02\n\n\nPlural pursuit designates situations where distinct yet concurrent instances of normal science converge on a singular, common problem-solving objective. In the context of fundamental physics, this objective involves reconciling quantum mechanics with gravitation.\nEach instance of normal science, as conceptualised by the authors, finds articulation through a specific social community inextricably linked to an intellectual disciplinary matrix. This framework resonates with established accounts of research programmes, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes. Consequently, the authors pose a pivotal empirical question: does quantum gravity research exemplify plural pursuit, manifesting as independent communities concurrently advancing distinct paradigms?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.5 Bottom-Up Reconstruction of Research Landscape",
    "text": "13.5 Bottom-Up Reconstruction of Research Landscape\n\n\n\nSlide 03\n\n\nTo address the empirical question concerning plural pursuit, the authors first undertake a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This process meticulously delineates both the linguistic and intellectual architecture of the field, alongside its intricate social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#clustering-pipeline-data-acquisition-and-linguistic-analysis",
    "href": "chapter_ai-nepi_015.html#clustering-pipeline-data-acquisition-and-linguistic-analysis",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.6 Clustering Pipeline: Data Acquisition and Linguistic Analysis",
    "text": "13.6 Clustering Pipeline: Data Acquisition and Linguistic Analysis\n\n\n\nSlide 03\n\n\nThe authors initiated their analysis by gathering a substantial dataset comprising approximately 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP. Their subsequent analytical pipeline unfolds in two principal stages: a linguistic analysis to reconstruct the intellectual framework, followed by a social network analysis to map the field’s social architecture.\nThe linguistic analysis, powered by the Bertopic pipeline, commences by spatialising the collected documents into an embedding space. Subsequently, an unsupervised clustering algorithm operates on this space, identifying 611 distinct, fine-grained topics. This granular resolution proves essential for capturing niche approaches within quantum gravity, some of which may encompass as few as one hundred papers. Finally, the authors’ system assigns a “specialty” to each physicist, determined by the predominant topic across their published works. This process ultimately yields a comprehensive partition of authors into topics, thereby illuminating the field’s underlying linguistic and intellectual structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#clustering-pipeline-social-network-analysis",
    "href": "chapter_ai-nepi_015.html#clustering-pipeline-social-network-analysis",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.7 Clustering Pipeline: Social Network Analysis",
    "text": "13.7 Clustering Pipeline: Social Network Analysis\n\n\n\nSlide 04\n\n\nComplementing the linguistic analysis, the authors conducted a comprehensive social network analysis. They commenced this by constructing a co-authorship graph, wherein individual physicists represent the nodes, whilst co-authorship relationships form the edges connecting them.\nApplying a community detection method to this extensive network of 30,000 physicists, the authors’ analysis successfully identified approximately 800 distinct communities. This process ultimately yields a robust partition of authors into these communities, thereby providing a clear reflection of the field’s intricate social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#operationalising-plural-pursuit-community-topic-mapping",
    "href": "chapter_ai-nepi_015.html#operationalising-plural-pursuit-community-topic-mapping",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.8 Operationalising Plural Pursuit: Community-Topic Mapping",
    "text": "13.8 Operationalising Plural Pursuit: Community-Topic Mapping\n\n\n\nSlide 05\n\n\nOperationally, plural pursuit manifests as a direct, one-to-one correspondence between identified communities and their associated intellectual topics. Conceptually, this ideal scenario would appear as a block-diagonal correlation matrix, where communities align perfectly with distinct topics. Such a configuration would signify that each community dedicates itself entirely to a singular topic, thereby establishing a clear and unambiguous division of labour across the research landscape.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#challenges-in-interpreting-fine-grained-partitions",
    "href": "chapter_ai-nepi_015.html#challenges-in-interpreting-fine-grained-partitions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.9 Challenges in Interpreting Fine-Grained Partitions",
    "text": "13.9 Challenges in Interpreting Fine-Grained Partitions\n\n\n\nSlide 05\n\n\nThe authors’ initial attempts to directly apply the fine-grained partitions, comprising 611 topics and 800 communities, yielded a highly complex and largely indecipherable correlation matrix. This complexity stems from several inherent challenges.\nFirstly, the level of fine-graining for topics proves somewhat arbitrary; for instance, a major research programme like string theory might inadvertently scatter across numerous granular topics. Secondly, large-scale research programmes often involve parallel efforts undertaken by multiple distinct communities. Thirdly, and more fundamentally, the computational definitions of both “topic” and “community” exhibit inherent scale-dependency, meaning their delineation varies with the chosen level of granularity. Beyond these technical considerations, a deeper conceptual issue arises: research programmes are themselves hierarchically nested. String theory, for example, encompasses Superstring Theory, which further branches into Type II and Heterotic varieties, amongst others. Consequently, this inherent ambiguity across different scales mandates a strategic approach to effectively identify genuine instances of plural pursuit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-research-landscape",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-research-landscape",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.10 Hierarchical Reconstruction of Research Landscape",
    "text": "13.10 Hierarchical Reconstruction of Research Landscape\n\n\n\nSlide 07\n\n\nTo overcome the challenges posed by scale-dependency, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, they employ Ward agglomerative clustering, systematically merging the initial 600 fine-grained topics one by one according to a defined objective function.\nConcurrently, for the community structure, the authors implement a hierarchical stochastic block model from the outset. This model intrinsically learns a multi-level partition, progressively grouping physicists into coarser communities. These resultant hierarchical structures collectively introduce a crucial notion of scale, enabling the observation and analysis of the research system at various levels of granularity. For example, the authors can visualise a co-authorship network where each physicist’s specialty, indicated by colour, reflects the linguistic structure at different levels of coarse-graining.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-scale-selection-for-optimal-interpretation",
    "href": "chapter_ai-nepi_015.html#adaptive-scale-selection-for-optimal-interpretation",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.11 Adaptive Scale Selection for Optimal Interpretation",
    "text": "13.11 Adaptive Scale Selection for Optimal Interpretation\n\n\n\nSlide 08\n\n\nA significant challenge arises from the arbitrary nature of selecting an appropriate observation scale for both topic and community structures. To address this, the authors propose an adaptive topic coarse-graining strategy. This approach acknowledges that whilst linguistic topics capture subtle nuances, some of these distinctions hold no practical consequence for the collaborative dynamics amongst scientists.\nTheir methodology systematically removes degrees of freedom from the fine-grained partition, but only when such removal does not diminish useful information pertinent to understanding the social structure. This process is governed by the Minimum Description Length (MDL) criterion, which seeks to minimise the quantity [- log P(G|sigma) - log P(sigma)]. This criterion effectively balances the model’s fit—the linguistic partition’s explanatory power regarding the social structure—with its complexity, ensuring the partition remains parsimonious. The process involves iteratively refining the topic dendrogram, halting when the increase in complexity no longer yields a commensurate gain in information about the social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#resulting-topics-and-community-topic-correlations",
    "href": "chapter_ai-nepi_015.html#resulting-topics-and-community-topic-correlations",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.12 Resulting Topics and Community-Topic Correlations",
    "text": "13.12 Resulting Topics and Community-Topic Correlations\n\n\n\nSlide 10\n\n\nThe adaptive coarse-graining strategy ultimately yielded 50 distinct topics, each clearly labelled by representative N-grams. To analyse their relationship with social structures, the authors constructed a correlation matrix, mapping these coarse-grained topics against community structures across various scales.\nTheir analysis revealed nuanced patterns. Some topics, such as a large purple cluster, demonstrated universal relevance, remaining untethered to specific communities. Conversely, other topics, notably string theory, exhibited strong correspondence with community structures at particular hierarchical levels, specifically the third level in this instance. Intriguingly, certain research programmes, such as loop quantum gravity, aligned with communities situated at much lower, more fine-grained levels within the hierarchy. Overall, the findings indicate the presence of nested structures and entangled scales, rather than a clear, simple division of labour. For example, a smaller community embedded within the broader string theory community showed a distinct connection to the intellectual topic of holography, thereby exemplifying these complex, nested relationships and the absence of a straightforward plural pursuit configuration.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuitions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.13 Top-Down Approach: Physicists’ Intuitions",
    "text": "13.13 Top-Down Approach: Physicists’ Intuitions\n\n\n\nSlide 12\n\n\nTo validate the bottom-up reconstruction, the authors systematically confronted it with the prevailing intuitions of physicists themselves. They surveyed the founding members of the International Society for Quantum Gravity, requesting them to enumerate the quantum gravity approaches they perceived as structuring the overall research landscape.\nThe collected responses yielded a detailed list of approaches, including asymptotic safety, causal sets, dynamical triangulations, group field theory, Loop Quantum Gravity (LQG), spin foams, noncommutative geometry, swampland conjectures, modified dispersion relations (DSR), quantum modified black holes, shape dynamics, tensor models, string theory, supergravity, and holography. A particular focus emerged on string theory, supergravity, and holography, primarily because some physicists expressed disagreement regarding whether these should be considered distinct entities, despite their historical and conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#comparing-top-down-and-bottom-up-perspectives",
    "href": "chapter_ai-nepi_015.html#comparing-top-down-and-bottom-up-perspectives",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.14 Comparing Top-Down and Bottom-Up Perspectives",
    "text": "13.14 Comparing Top-Down and Bottom-Up Perspectives\n\n\n\nSlide 13\n\n\nThe authors trained a Support Vector Machine (SVM) classifier to predict the specific approach of individual papers, leveraging text embeddings derived from titles and abstracts (using all-MiniLM-L6-v2) and hand-coded labels for training. The authors then compared this supervised, top-down classification with the bottom-up reconstruction.\nTheir results demonstrated varied alignment. Certain top-down approaches exhibited strong correspondence with the emergent bottom-up topics. Conversely, other approaches, particularly those of a phenomenological nature or lacking a comprehensive conceptual framework, did not align effectively. The method proved most robust for well-defined, conceptually autonomous frameworks. Notably, the authors’ bottom-up analysis identified a substantial string theory cluster that encompassed both supergravity and string theory. This finding resonates with physicists’ intuitions, as articulated in a survey response: “I suppose there are a few people still interested in supergravity as a theory in its own right, […but] I don’t think this is a large community […] the overlap of people working on”supergravity” and “string theory” is so large that I’m not sure the communities can be separated in a meaningful way.” This convergence suggests that when linguistic nuances without direct social consequences are disregarded, conceptually distinct areas may merge, a finding supported by both empirical data and expert perception.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#key-conclusions-and-philosophical-implications",
    "href": "chapter_ai-nepi_015.html#key-conclusions-and-philosophical-implications",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.15 Key Conclusions and Philosophical Implications",
    "text": "13.15 Key Conclusions and Philosophical Implications\n\n\n\nSlide 15\n\n\nThe authors’ research yields several significant conclusions. Firstly, socio-epistemic systems demonstrably operate across multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Secondly, effectively identifying configurations of plural pursuit—characterised by a one-to-one mapping between communities and their intellectual substrate—demands a meticulous alignment of these structures across their respective scales. Thirdly, a bottom-up reconstruction of the quantum gravity research landscape offers a robust mechanism to either confirm or critically re-assess the prevailing intuitions of physicists.\nFinally, and perhaps most profoundly, the increasing power of computational methods empowers scholars to revisit and challenge long-held philosophical insights and intuitions, particularly those concerning the nature of paradigms or communities within specific scientific contexts like quantum gravity. This work compellingly argues that “Computation is the continuation of philosophy by other means!”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThis chapter details a comparative study assessing the efficacy of Latent Dirichlet Allocation (LDA) and BERTopic models when applied to distinct textual levels: titles, abstracts, and full texts within a scientific corpus. The authors aimed to ascertain whether topic modelling on titles or abstracts suffices, or if full-text analysis remains indispensable, particularly given the substantial resources required for its processing.\nTheir methodology involved constituting a corpus of scientific articles, segmenting these into titles, abstracts, and full texts, and subsequently applying both LDA and BERTopic approaches. A comprehensive analysis, encompassing both qualitative and quantitative methods, facilitated the comparison of the resulting topic models. Key quantitative metrics included the Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.\nThe authors’ findings indicate that title-based models generally exhibit poor performance, whilst abstract models consistently demonstrate robust and meaningful topic extraction, often aligning well with full-text models. Full-text models, whilst offering comprehensive coverage, can present challenges such as loosely defined topics or class-size imbalances, particularly with BERTopic. Ultimately, the study recommends employing topic modelling on abstracts or full texts with either LDA or BERTopic, provided such approaches do not lead to misclassification of relevant documents.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "href": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Methodological Approach",
    "text": "14.1 Research Question and Methodological Approach\n\n\n\nSlide 02\n\n\nThis research addresses a critical inquiry: whether applying topic modelling solely to titles or abstracts suffices, or if full-text analysis remains an indispensable requirement. This question gains particular urgency given the substantial resources demanded for the acquisition, preprocessing, and subsequent analysis of extensive full-text corpora.\nTo investigate this, the authors meticulously constituted a corpus of scientific articles. They then systematically identified and segmented the title, abstract, and full-text sections from each article. Subsequently, they applied two prominent topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. The resulting topic models underwent rigorous analysis and comparison, employing both qualitative and quantitative methodologies.\nThe overall workflow involved segmenting the scientific corpus into titles, abstracts, and full texts. Each segment then served as input for both LDA and BERTopic models. The outputs from these models were then subjected to both qualitative and quantitative scrutiny. This comprehensive approach aimed to provide robust insights into the comparative performance of these models across different textual granularities.\nBeyond this specific investigation, topic modelling itself stands as a vital analytical instrument for processing vast scientific literature, especially within the history, philosophy, and sociology of science. Historically, researchers have deployed topic modelling for diverse tasks, including discerning research trends and paradigm shifts, identifying thematic substructures and interrelationships, and charting the evolution of scientific vocabulary. These prior applications have consistently involved various textual structures, ranging from titles and abstracts to complete full texts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Material: Astrobiology Corpus for Qualitative Comparison",
    "text": "14.2 Material: Astrobiology Corpus for Qualitative Comparison\n\n\n\nSlide 05\n\n\nThe authors grounded this study in an extensive topic analysis of an astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a thorough evaluation process, they selected a full-text LDA model comprising 25 distinct topics as the primary material for comparison.\nTheir analysis of these 25 topics involved a meticulous examination of their most representative words and documents, enabling the authors to assign a descriptive name to each topic based on its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric derived from the topics’ presence within the documents. A community detection algorithm then identified four thematic clusters, designated by letters A, B, C, and D, and visually distinguished by red, green, yellow, and blue colours, respectively.\nA graphical representation visually conveys these findings, illustrating the correlations amongst the 25 topics. This graph incorporates topic labels and the colour variations corresponding to their thematic clusters. Crucially, the thickness of the lines connecting topics denotes the strength of their correlation, whilst the size of each circle reflects the topic’s overall prevalence across all documents. This comprehensive analytical framework enables a robust qualitative comparison of the six distinct topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Methodology: Quantitative Analysis Metrics",
    "text": "14.3 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 06\n\n\nFor the quantitative dimension of this study, the authors employed four distinct metrics to compare the various topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between two document clusterings, with a correction applied for chance agreement. This metric precisely quantifies the extent to which documents cluster together, or diverge, across different models.\nSecondly, Topic Diversity assessed the proportion of distinct top words, thereby determining whether individual topics within a given model were characterised by unique vocabulary. Thirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words, evaluating how effectively these top words collectively represented the documents assigned to each topic. Finally, Coherence CV, calculated as the average cosine relative distance between top words within topics, provided an assessment of whether these top words formed a semantically meaningful grouping. Each of these metrics is underpinned by specific mathematical formulations, ensuring rigorous quantitative comparison.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Results: Adjusted Rand Index Between Topic Models",
    "text": "14.4 Results: Adjusted Rand Index Between Topic Models\n\n\n\nSlide 07\n\n\nThe Adjusted Rand Index (ARI) provides crucial insights into the similarities amongst the six topic models. A value of zero for this metric indicates a clustering equivalent to random assignment. Analysis of the heatmap reveals that the LDA model applied to titles stands out as the most distinct, consistently demonstrating poor similarity with all other models, as evidenced by ARI values below 0.20, depicted by yellow hues in the visualisation.\nConversely, the remaining models generally exhibit a superior overall match, with ARI values consistently exceeding 0.20. Notably, BERTopic models display a stronger internal correspondence, with their inter-model ARI values typically surpassing 0.35. The BERTopic abstract model emerges as particularly central within this network of similarities, demonstrating robust correspondence with every other model, apart from the outlier LDA title model, with values consistently above 0.30. The heatmap visually encapsulates these relationships, where warmer colours signify higher degrees of similarity between the compared topic models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: LDA Full-text Versus Abstracts and Titles",
    "text": "14.5 Results: LDA Full-text Versus Abstracts and Titles\n\n\n\nSlide 08\n\n\nA more granular analysis of the LDA models provides detailed insights into their interrelationships. Table A, which compares the LDA full-text model with the LDA abstract model, indicates a generally good overall fit. This strong correspondence is evident from the reddish diagonal in the table, signifying that each topic from one model typically aligns with a topic from the other, sharing a high proportion of common documents.\nHowever, this alignment is not without dynamic shifts. Three full-text LDA topics effectively disappear, represented by long horizontal dark grey lines. Conversely, three full-text topics fragment into multiple topics within the abstract model, visible as short horizontal dark grey lines. The abstract model also sees the emergence of three entirely new topics, marked by long vertical dark grey lines, whilst three topics arise from mergers, again indicated by short horizontal dark grey lines. Furthermore, one small class, comprising fewer than 50 documents, is discernible within the abstract topics.\nIn stark contrast, Table B, comparing the LDA full-text model with the LDA title model, reveals a poor overall fit. This disparity necessitates substantial reorganisation, manifested by a proliferation of vertical and horizontal dark lines across the table. This indicates that numerous full-text topics vanish, whilst a considerable number of new abstract topics emerge, highlighting a significant divergence in thematic representation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles",
    "text": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles\n\n\n\nSlide 09\n\n\nAnalysis of the BERTopic models, when compared against the LDA full-text baseline, reveals varied performance. Table C, which juxtaposes LDA full-text with BERTopic full-text, indicates an average overall fit. Within this comparison, eight LDA topics vanish along the horizontal axis, whilst six LDA topics fragment into the BERTopic model. Conversely, the vertical axis shows the emergence of five new BERTopic topics, with one topic resulting from mergers. A notable observation from the total document count is the presence of four small classes alongside one exceptionally large class.\nMoving to Table D, the comparison between LDA full-text and BERTopic abstract demonstrates a relatively good overall fit. Here, four LDA topics disappear, whilst six topics undergo splitting. The vertical axis reveals two new BERTopic topics appearing and four topics resulting from mergers. Crucially, this model maintains balanced class sizes.\nFinally, Table E, comparing LDA full-text with BERTopic title, again indicates an average overall fit. In this instance, seven LDA topics disappear, and one topic splits. The vertical axis shows seven new BERTopic topics emerging, with one topic resulting from a merger. The total document count for this model highlights three small classes and one large class. These heatmaps collectively illustrate the proportions of shared documents between topics across these diverse model comparisons.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: LDA - Comparing Top-words",
    "text": "14.7 Results: LDA - Comparing Top-words\n\n\n\nSlide 11\n\n\nAn examination of the top words within the LDA models revealed that topics were generally well-formed across all iterations. The authors observed several robust topics that maintained strong correspondence across the full-text, abstract, and title models. The topic “A-Radiation-spore” serves as a prime example of this consistency.\nConversely, certain topics from the full-text model fragmented into multiple, more granular topics within both the abstract and title models. For instance, the splitting of “A-Life-civilization” proved semantically coherent, yielding a broader topic encompassing research in astrobiology. However, the fragmentation of “B-Chemistry” presented a more ambiguous case, necessitating further analysis for clear interpretation.\nFurthermore, the study identified instances where topics from the full-text model coalesced into new, merged topics within the abstract and title models. A notable example is the merger of “B-Amino-acid” and “B-Protein-gene-rna” in the LDA abstract model. This particular consolidation formed a more general topic, which aligns logically with the underlying subject matter. The visual representation provides side-by-side tables illustrating the top words for selected topics across these LDA models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: BERTopic - Comparing Top-words",
    "text": "14.8 Results: BERTopic - Comparing Top-words\n\n\n\nSlide 12\n\n\nContinuing the assessment of top words, the three BERTopic models also yielded relatively well-formed topics. The robustness of “A-Radiation-spore” persisted across all models, including LDA Full-text, BERTopic Full-text, BERTopic Abstract, and BERTopic Title, underscoring its consistent thematic representation.\nWhilst “A-Life-civilization” generally maintained its stability across the BERTopic models, it exhibited some instances of splitting. This fragmentation led to the emergence of more narrowly defined topics specifically pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic also underwent splitting across the BERTopic models, resulting in a series of more focused thematic areas. The visual data provides comparative tables of top words from selected topics across these models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results of Quantitative Analysis: Coherence",
    "text": "14.9 Results of Quantitative Analysis: Coherence\n\n\n\nSlide 12\n\n\nThe coherence metric, specifically Coherence CV, provides a quantitative assessment of the semantic meaningfulness of the top words within each topic. Across a range of topics from 5 to 50, distinct patterns emerged. Models based on titles consistently exhibited the poorest coherence. Conversely, abstract models demonstrably outperformed full-text models in this regard.\nFurthermore, BERTopic models generally achieved superior coherence compared to LDA, particularly for abstract and title-based analyses. However, this performance differential tended to diminish as the number of topics increased, indicating a convergence in coherence scores at higher topic counts. Ultimately, the BERTopic abstract model unequivocally emerged as the leading performer in terms of topic coherence. A line graph visually represents these trends, plotting the Coherence CV for each of the six models against varying numbers of topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results of Quantitative Analysis: Diversity",
    "text": "14.10 Results of Quantitative Analysis: Diversity\n\n\n\nSlide 13\n\n\nRegarding the diversity of top words characterising the topics, a clear trend emerged: diversity generally diminishes as the number of topics increases. Within this context, models derived from titles consistently offered the highest diversity, surpassing their abstract or full-text counterparts.\nMoreover, BERTopic models demonstrated superior diversity compared to LDA across the board. The BERTopic title model ultimately emerged as the top performer in terms of diversity, with the BERTopic full-text model closely trailing. A line graph visually illustrates these diversity trends for each of the six models across varying topic counts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results of Quantitative Analysis: Joint Recall",
    "text": "14.11 Results of Quantitative Analysis: Joint Recall\n\n\n\nSlide 13\n\n\nThe Joint Recall metric assesses the efficacy with which the top words collectively represent every document assigned to a given topic. Analysis revealed that models based on titles consistently yielded the poorest recall. Conversely, full-text models demonstrated superior performance compared to their abstract and title counterparts.\nIn terms of algorithmic performance, LDA models generally exhibited better Joint Recall than BERTopic. The LDA full-text and BERTopic full-text models emerged as the leading performers in this category, with the BERTopic abstract model following very closely behind. A line graph visually depicts the micro Joint Recall for each of the six models across a range of topic numbers.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Summary of Model Performance",
    "text": "14.12 Summary of Model Performance\n\n\n\nSlide 14\n\n\nThe authors compiled the individual assessment results to offer a holistic perspective on the models’ performance. For each criterion—overall fit, top-words, coherence, diversity, and joint recall—a circular representation indicates performance: a black circle denotes the highest score, a white circle signifies a lesser score, and a half-black, half-white circle indicates intermediate performance. Crucially, the study underscores that no single model emerges as universally superior; rather, diverse research objectives inherently dictate varying needs and, consequently, different model choices.\nFor instance, if the primary objective involves the discovery of main topics without stringent requirements for precise document classification, then issues such as poor recall or large class sizes might be acceptable. In such scenarios, the BERTopic Full-text model performed commendably, albeit with some observed class imbalance. Similarly, whilst far from optimal, the BERTopic Title model did yield certain robust topics that were consistently identified across other models.\nConversely, if the aim is to achieve maximum document coverage across all topics, then neither BERTopic Full-text nor BERTopic Title is recommended, as both approaches lead to large document classes and, in the case of BERTopic Title, poor recall. Furthermore, the LDA Title model receives a general non-recommendation due to its consistently poor performance across nearly all assessments. In essence, the study advocates for conducting topic modelling on either abstracts or full texts, employing either LDA or BERTopic, provided that such applications do not result in the misclassification of documents pertinent to specific topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Discussion and Conclusion",
    "text": "14.13 Discussion and Conclusion\n\n\n\nSlide 16\n\n\nThis research yields several pivotal findings. Firstly, title models consistently exhibit poor performance, primarily attributable to the inherent lack of information within titles, which can consequently lead to erroneous document classification. Nevertheless, the BERTopic title model, surprisingly, generated numerous meaningful topics, suggesting that future efforts might focus on striking a balance between precisely defined topics and comprehensive document coverage.\nSecondly, full-text models occasionally encounter difficulties in processing vast quantities of information. With LDA, topics can become more broadly defined and encompass wider coverage, potentially including secondary or transverse themes such as methodologies. Conversely, BERTopic, when applied to full texts, can produce overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently demonstrate strong performance with summary information. Their results align remarkably well with the LDA full-text model, as well as with both LDA and BERTopic abstract models. This consistency underscores their utility in capturing core thematic content.\nFourthly, the study highlights the notable robustness of topics. Across the board, the authors identified highly similar topics, a finding that facilitates the application of meta-analytic methods to pinpoint the most enduring and robust themes. Moreover, this consistency suggests the potential for employing relative distance metrics across models to identify an optimal solution; in this study, the BERTopic abstract model emerged as such an optimum, performing exceptionally well across all other metrics.\nFinally, the findings prompt a consideration of new model development. It appears feasible and potentially beneficial to leverage the structural information inherent in documents—specifically, full text, abstract, and title—to extract more semantically rich sets of top words or topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "",
    "text": "Overview\nThis report details a novel approach for imbuing large language models (LLMs) with explicit temporal awareness, thereby addressing a fundamental limitation inherent in current architectures. Presently, LLMs derive their understanding of time implicitly from statistical patterns within training texts; however, this method often proves insufficient for tasks demanding precise temporal context. The authors propose the Time Transformer, an architectural modification that integrates a dedicated temporal dimension directly into token embeddings. This innovation enables models to learn and reproduce evolving linguistic patterns as a function of time, thereby resolving ambiguities that arise from temporally contradictory information within training data.\nTo validate this concept, the engineers developed a modest Transformer model, training it on a bespoke dataset of UK Met Office weather reports spanning 2018 to 2024. Characterised by its restricted vocabulary and repetitive language, this dataset provided an ideal testbed for demonstrating the Time Transformer’s efficacy. The experiments involved injecting synthetic temporal drifts—encompassing both synonymic succession (e.g., replacing ‘rain’ with ‘liquid sunshine’) and co-occurrence changes (e.g., ‘rain’ becoming ‘rain and snow’)—into the training data. The Time Transformer consistently reproduced these time-dependent patterns with high fidelity, confirming its ability to efficiently learn and reflect temporal variations in underlying data distributions.\nBeyond this initial proof of concept, the Time Transformer holds significant implications for historical analysis, offering a robust foundation for downstream tasks on historical data and enabling instruction-tuned models to ‘talk to a specific time’. Whilst this architectural modification necessitates training from scratch—posing considerable computational challenges for large-scale applications and introducing intricate data curation complexities—the potential for enhanced training efficiency and more precise temporal reasoning remains compelling. Further research explores benchmarking the Time Transformer against explicit time-token approaches and investigating the utility of a modest, targeted encoder model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "href": "chapter_ai-nepi_017.html#addressing-implicit-temporal-understanding",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.1 Addressing Implicit Temporal Understanding",
    "text": "15.1 Addressing Implicit Temporal Understanding\n\n\n\nSlide 01\n\n\nCurrent large language models (LLMs) fundamentally derive their understanding of time implicitly, through statistical analysis of patterns within their extensive training corpora. Whilst these models exhibit a remarkable grasp of temporal concepts, their reliance on implicit cues presents inherent limitations. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns in training data as a function of time, would profoundly enhance their utility, particularly within historical analysis and beyond.\nA critical challenge arises when training data contains temporally contradictory information. For instance, consider two sentences: ‘The primary architectures for processing text through NNs are LSTMs’ (true in 2017) and ‘The primary architectures for processing text through NNs are Transformers’ (true in 2025). Without explicit temporal context, an LLM treats these as competing statements, unable to perfectly fulfil its objective without making an error. Consequently, models often exhibit a ‘recency bias’, favouring more recent information in next-token prediction. Current workarounds, such as prompt engineering—inserting explicit temporal cues like ‘In 2017’—offer only limited guarantees and represent an imprecise method for eliciting time-specific knowledge. A more robust solution necessitates an architecture that enables LLMs to explicitly learn and reproduce these changing patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probabilities-the-time-transformer",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer",
    "text": "15.2 Formalising Time-Dependent Probabilities: The Time Transformer\n\n\n\nSlide 05\n\n\nFormalising the challenge, current large language models estimate the probability distribution over their vocabulary for the next token, x_n, given a sequence of preceding tokens, x_1, ..., x_{n-1}. Crucially, in real-world contexts, this probability is not static; it inherently depends on time, rendering the true distribution as p(x_n | x_1, ..., x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a specific time t is expressed as the product of these time-dependent conditional probabilities. Despite this temporal dependency, existing LLMs largely treat these underlying distributions as static during their training process, reflecting temporal drift solely through in-context learning during inference.\nTo overcome this limitation, a direct approach involves explicitly modelling the time-dependent probability distribution p(x_n | x_1, ..., x_{n-1}, t). Whilst time slicing—training separate models for distinct temporal periods—offers a conceptual solution, it proves prohibitively data-inefficient. A more elegant and efficient method, termed the Time Transformer, introduces a simple yet profound modification: an additional dimension, φ(t), is appended to the latent semantic feature vector of each token. This creates a time-aware embedding, E(x, t), which then serves as input to the Transformer architecture. Consequently, the Transformer processes a sequence of time-dependent token embeddings, enabling it to estimate the time-conditioned probability distribution p_θ(x_n | x_1, ..., x_{n-1}, t). The training objective remains the standard maximisation of log likelihood across all sequences. This direct injection of time into each token’s representation empowers the model to precisely learn how strongly or weakly the temporal dimension influences individual tokens, thereby capturing dynamic linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "href": "chapter_ai-nepi_017.html#empirical-validation-data-and-architecture",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.3 Empirical Validation: Data and Architecture",
    "text": "15.3 Empirical Validation: Data and Architecture\n\n\n\nSlide 09\n\n\nTo empirically validate the Time Transformer concept, the authors required a text dataset characterised by a restricted vocabulary and simple, repetitive language patterns. UK Met Office weather reports, sourced from the National Meteorological Service’s digital archive, proved an ideal choice. The team scraped daily reports for the years 2018 to 2024, yielding approximately 2,500 documents, each comprising 150 to 200 words. The tokenisation process was intentionally simplistic, neglecting sub-word tokenisation, case, and interpunctuation, which resulted in a compact vocabulary of only 3,395 unique words across the entire seven-year period. The authors also considered TinyStories, an alternative dataset, for its similar characteristics.\nA modest Transformer architecture, termed the ‘Vanilla model’, underpinned the experimental setup. This model incorporated an Embedding Layer, Positional Encoding, Dropout, Multi-Head Attention, Add & Norm layers, a Feed-Forward Network, and multiple Decoder Layers, culminating in a Final Dense Layer for output. Specifically, the architecture featured four multi-head attention decoder blocks, each employing eight attention heads. With a mere 39 million parameters, equating to 150 MB, this model stands in stark contrast to colossal architectures such as GPT-4, which boasts 1.8 trillion parameters distributed across 120 layers. Training occurred on an HPC cluster in Munich, utilising two H100 GPUs; remarkably, each epoch completed in just 11 seconds—a testament to the dataset’s small scale and the model’s compact design. The authors have made the code for this implementation publicly available on GitHub, though they developed it primarily for foundational understanding rather than optimal performance. Crucially, the trained model demonstrated a perfect ability to reproduce the language of weather reports; generated texts, initiated from a seed sequence such as ‘During the night, a band …’, proved indistinguishable from authentic reports, confirming the model’s proficiency in capturing the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "href": "chapter_ai-nepi_017.html#time-transformer-implementation-and-results",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.4 Time Transformer Implementation and Results",
    "text": "15.4 Time Transformer Implementation and Results\n\n\n\nSlide 12\n\n\nImplementing the Time Transformer required only a minimal architectural adjustment to the previously described ‘Vanilla model’. The engineers reserved a single dimension within the 512-dimensional latent semantic space to encode temporal information. This time dimension is non-trainable and employs a min-max normalised representation of the day of the year, calculated as (day of year - 1) / (365 - 1). The authors chose this specific encoding to leverage the natural seasonal variations inherent in weather data, such as the prevalence of snow in winter or heat in summer, though alternative temporal embeddings remain feasible.\nThe first experiment aimed to demonstrate the model’s capacity for learning synthetic temporal drift through synonymic succession. The research team injected a time-dependent replacement rule into the training data: ‘rain’ was replaced by ‘liquid sunshine’ according to a sigmoid probability function, transitioning from zero replacement at the year’s beginning to full replacement by its end. Validation involved generating a weather prediction for each day of the year and subsequently counting the monthly frequencies of ‘rain’ versus ‘liquid sunshine’. The Time Transformer flawlessly reproduced the injected sigmoid pattern, exhibiting ‘rain’ predominantly early in the year and ‘liquid sunshine’ towards the end, with the transition occurring precisely mid-year.\nThe second experiment explored the model’s ability to learn a more complex temporal pattern: a change in co-occurrence, or the ‘fixation of a collocation’. Here, instances of ‘rain’ not immediately followed by ‘and’ were synthetically replaced with ‘rain and snow’. This modification effectively transformed a variable collocation into an obligatory one. Validation involved generating daily predictions and comparing the monthly occurrences of ‘rain and snow’ against ‘rain only’. The model successfully acquired this pattern, generating ‘rain and snow’ almost exclusively in the latter part of the year, whilst early-year occurrences of ‘rain’ (sometimes accompanied by ‘snow’) reflected natural January weather patterns. Furthermore, introspection into the model’s attention heads revealed specialised learning of these temporal patterns, with specific heads conditioning early-year ‘rain and snow’ on the presence of a ‘cold system’, underscoring the model’s capacity for intricate pattern recognition even in this modest experimental setup.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-applications-and-challenges",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.5 Proof of Concept, Applications, and Challenges",
    "text": "15.5 Proof of Concept, Applications, and Challenges\n\n\n\nSlide 16\n\n\nThis research unequivocally establishes a proof of concept: Transformer-based large language models can indeed achieve explicit time awareness through the simple addition of a temporal dimension to their token embeddings. This fundamental capability opens a spectrum of fascinating applications. A foundation Time Transformer, for instance, could provide an unparalleled basis for a myriad of downstream tasks involving historical data. Furthermore, an instruction-tuned variant would empower users to ‘talk to a specific time’, potentially yielding superior results even in conventional usage scenarios by providing more precise temporal context. Beyond this, the methodology extends to modelling the dependence of token sequence distributions on other crucial metadata dimensions, such as country or genre.\nSeveral promising avenues for future research emerge. Benchmarking the Time Transformer against explicit time-token approaches will quantify its performance advantages. Crucially, investigating whether the temporal dimension enhances training efficiency, by aiding the model in deciphering inherent patterns, represents a significant next step.\nNevertheless, substantial challenges persist concerning practical application. The architectural modification fundamentally alters the model, rendering it unclear whether fine-tuning existing, pre-trained LLMs remains feasible or efficient; this often necessitates training models from scratch, which demands prohibitive computational resources for any serious application beyond the presented toy case. Moreover, the elegant simplicity of metadata-free self-supervised learning is lost, plunging the process into the intricate complexities of data curation. Assigning accurate dates to historical token sequences presents a formidable task for historians, fraught with ambiguities concerning original utterance dates versus reprint dates. Despite these hurdles, a compelling future direction involves exploring a modest, targeted encoder model, akin to BERT, built upon the same temporal principle. Such an encoder would focus on learning only the patterns relevant to a specific task, circumventing the need to capture all intricate linguistic variations and offering a more scalable path forward.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers within the Cascade project, a Marie Curie doctoral network, meticulously explore the computational analysis of semantic change. PhD student Sophia Aguilar leads this investigation, focusing on modelling diverse contextual factors and their intricate interplay. Building upon previous work that modelled distinct context types in isolation, the current objective is to integrate these approaches, thereby illuminating their complex interactions.\nThe chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a pivotal pilot study. Linguists engaged in this endeavour examine how language adapts to real-world transformations, drawing upon register theory and principles of rational communication. The study aims to detect periods of significant linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately comprehend the linguistic mechanisms and communicative drivers underpinning these transformations. To this end, the authors propose a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics, positioning context as a central signal and seeking to overcome limitations of existing methods in capturing the interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot\nWithin the Cascade project, a Marie Curie doctoral network, researchers meticulously investigate the computational analysis of semantic change. PhD student Sophia Aguilar spearheads efforts to model context comprehensively, examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing upon the Royal Society Corpus (RSC). This historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors:\n\nSituational (where)\nTemporal (when)\nExperiential (what)\nInterpersonal (who)\nTextual (how)\nCausal (why)\n\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal higher divergence due to substantial linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time—for example, from 1725 to 1845—reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models\nBeyond temporal detection, the investigation delves into paradigmatic context and the dynamics of conceptual change, referencing work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand who spearheads and propagates these linguistic and conceptual shifts, Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, the research team examines shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, visualised through graphs plotting surprisal against year, supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community using that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. Aguilar’s proposed framework positions context as a central signal for modelling language dynamics, identifying Graph Convolutional Networks (GCNs) as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four stages:\n\nData Sampling: This stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: The process begins by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the authors refine network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research acknowledges several profound questions that delineate its current limitations and chart future directions. A primary concern involves the nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview\nThe research team investigates the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the investigators reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]