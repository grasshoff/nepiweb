[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held April 2-4, 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "",
    "text": "Overview\nThis workshop emerges from the confluence of two primary initiatives. The first is the ‘Network Epistemology in Practice’ (NEPI) project, within which Arno Simons has pioneered the training of large language models on physics texts, whilst Michael Zichert has explored their application to conceptual problems in physics. The second originates with Gerd Graßhoff, a key cooperation partner who has long championed the use of artificial intelligence in the history and philosophy of science, particularly for analysing processes of scientific discovery.\nRepresenting a fusion of these intellectual currents, the workshop is supported by an ERC grant awarded to the NEPI project. The project’s central objective is to investigate the internal communication of the ATLAS collaboration at CERN. To this end, the team employs network analysis to map the collaboration’s communication structures and uses advanced semantic tools, including large language models, to trace the flow of ideas across these networks.\nThe organisation of this event was expertly managed by Svenja Goetz, Lea Stengel, and Julia Kim. Essential technical support was provided by Oliver Ziegler and his Unicam team.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-and-dissemination",
    "href": "chapter_ai-nepi_001.html#recording-and-dissemination",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.1 Recording and Dissemination",
    "text": "2.1 Recording and Dissemination\n\n\n\nSlide 02\n\n\nA comprehensive recording protocol is in place for all workshop sessions to ensure a high-quality record of the proceedings. The technical arrangement, managed by Oliver Ziegler and the Unicam team, comprises a primary camera focused on the active speaker, four microphones to capture audience contributions, and an iPhone as a backup audio recorder. This configuration also facilitates a seamless experience for remote attendees joining via Zoom.\nFollowing the workshop, and with the explicit consent of the presenters, recordings of the talks and subsequent discussions will be made publicly available on the NEPI project’s YouTube channel. Participants consented to this policy upon registration and are welcome to contact the organisers with any queries.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#participant-interaction",
    "href": "chapter_ai-nepi_001.html#participant-interaction",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.2 Participant Interaction",
    "text": "2.2 Participant Interaction\n\n\n\nSlide 04\n\n\nTo foster productive dialogue, the workshop follows a structured protocol for interaction. During question-and-answer sessions, attendees are requested to formulate their questions concisely. The session chair will gather a small group of questions before inviting the presenter to offer a collective response, a method that streamlines the exchange of ideas.\nFor discussions that extend beyond the live sessions, a shared Etherpad (or CryptPad), accessible via a QR code, provides an asynchronous forum. This platform enables participants to post comments and questions in dedicated sections for each presentation. The Zoom chat also remains available for commentary throughout the event. Furthermore, the programme features a dedicated discussion session on the second day to explore common themes, complemented by generous breaks and social events designed to encourage informal networking.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-semantic-change-and-data-science",
    "href": "chapter_ai-nepi_001.html#keynote-semantic-change-and-data-science",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.3 Keynote: Semantic Change and Data Science",
    "text": "2.3 Keynote: Semantic Change and Data Science\n\n\n\nSlide 05\n\n\nThe first keynote address, delivered by Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg, will explore large-scale text analysis for the study of cultural and societal change. Nina Tahmasebi leads the ‘Change is Key!’ research programme, to which Pierluigi Cassotti contributes as a researcher. Their collective work has made significant contributions to the field of semantic change detection.\nTheir research is notable for its dual focus, spanning not only technical innovations such as the development of evaluation benchmarks but also the broader application of data science methodologies to address complex questions within the humanities. This perspective resonates strongly with the workshop’s interdisciplinary aims. For logistical arrangements, coffee breaks will be held in the main venue, whilst lunch and the evening reception will take place in room H2051.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-cross-document-nlp",
    "href": "chapter_ai-nepi_001.html#keynote-cross-document-nlp",
    "title": "2  AI-assisted Methods for History and Philosophy of Science Workshop: Introduction",
    "section": "2.4 Keynote: Cross-Document NLP",
    "text": "2.4 Keynote: Cross-Document NLP\n\n\n\nSlide 06\n\n\nThe second keynote will be delivered by Professor Iryna Gurevych, who leads the Ubiquitous Knowledge Processing Lab at the Technical University of Darmstadt. Her presentation, titled How to InterText? Elevating NLP to the cross-document level, will delve into advanced natural language processing techniques.\nProfessor Gurevych’s research concentrates on information extraction, semantic text processing, and machine learning. A defining feature of her work is the application of these computational methods to generate new insights within the social sciences and humanities, an approach that aligns closely with the interdisciplinary objectives of this workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI-assisted Methods for History and Philosophy of Science Workshop: Introduction</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter details the foundational architecture of Transformer-based Large Language Models (LLMs) and their specialised adaptation for research in the History, Philosophy, and Sociology of Science (HPSS). The authors begin by deconstructing the Transformer model into its core encoder and decoder components. They elucidate the process flow from input words to numerical representations through embedding, positional encoding, multi-head attention, and feed-forward networks.\nThe analysis then demonstrates how this core architecture gives rise to distinct model families. BERT, for instance, leverages the encoder for bidirectional language understanding, whilst GPT employs the decoder for unidirectional text generation. For application within the HPSS domain, the authors outline several adaptation strategies. These include four distinct training-based methods for domain and task specialisation, alongside the increasingly prevalent Retrieval Augmented Generation (RAG) technique, which combines external document retrieval with text generation.\nFurthermore, the presentation provides a taxonomy for classifying LLMs based on their architecture, fine-tuning methods, embedding types, and deployment abstraction levels. Finally, the authors map these computational methods to specific applications in HPSS, covering tasks related to data and source management, the analysis of knowledge structures, the investigation of knowledge dynamics, and the examination of knowledge practices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#presentation-agenda",
    "href": "chapter_ai-nepi_003.html#presentation-agenda",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.1 Presentation Agenda",
    "text": "3.1 Presentation Agenda\n\n\n\nSlide 02\n\n\nThe presentation commences by outlining its structure, framed metaphorically as ‘Today’s Menu’. This agenda introduces the audience to the main topics and the logical progression of the discussion, thereby setting clear expectations for the content to be covered.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.2 The Transformer Architecture",
    "text": "3.2 The Transformer Architecture\n\n\n\nSlide 03\n\n\nAt the centre of modern large language models lies the Transformer architecture, which operates on a sophisticated encoder-decoder structure. This design governs the flow of information, systematically converting a sequence of input words into a set of output probabilities. An accompanying architectural diagram visualises this entire process, clarifying the pathway from initial input to final predictive output and illustrating the model’s core mechanics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-encoder-component",
    "href": "chapter_ai-nepi_003.html#the-encoder-component",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.3 The Encoder Component",
    "text": "3.3 The Encoder Component\n\n\n\nSlide 04\n\n\nThe encoder component of the Transformer architecture performs the critical function of converting input words into rich, context-aware numerical representations. This conversion unfolds through a multi-stage process. Initially, an embedding layer maps each word to a vector. Subsequently, the authors integrate positional encoding to provide the model with crucial information about the sequence and order of the words.\nThe resulting vectors are then processed by a multi-head attention mechanism, which dynamically weighs the significance of different words in relation to one another. Finally, these refined representations pass through feed-forward networks, completing their transformation into a format suitable for downstream tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#specialised-architectures-bert-and-gpt",
    "href": "chapter_ai-nepi_003.html#specialised-architectures-bert-and-gpt",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.4 Specialised Architectures: BERT and GPT",
    "text": "3.4 Specialised Architectures: BERT and GPT\n\n\n\nSlide 08\n\n\nEngineers have adapted the foundational Transformer architecture to create specialised models tailored for distinct tasks, giving rise to two prominent model families: BERT and GPT. BERT, which stands for Bidirectional Encoder Representations from Transformers, leverages the encoder stack to develop a deep, bidirectional understanding of language context. In contrast, GPT, or Generative Pre-trained Transformer, primarily employs the decoder stack to excel at unidirectional, generative tasks such as text creation. Both of these architectural specialisations have found significant application within the HPSS domain, enabling new forms of computational analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#divergent-processing-capabilities",
    "href": "chapter_ai-nepi_003.html#divergent-processing-capabilities",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.5 Divergent Processing Capabilities",
    "text": "3.5 Divergent Processing Capabilities\n\n\n\nSlide 09\n\n\nThe distinct architectural choices in BERT and GPT result in fundamentally different processing capabilities, even though both originate from the same core Transformer model. BERT’s reliance on the encoder allows it to analyse entire sequences at once, making it exceptionally proficient at tasks requiring nuanced language understanding. Conversely, GPT’s decoder-focused design enables it to generate coherent and contextually relevant text in a sequential, unidirectional manner. This clear division of capabilities—understanding versus generation—dictates their respective applications and strengths.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#unanalysed-slide-content",
    "href": "chapter_ai-nepi_003.html#unanalysed-slide-content",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.6 Unanalysed Slide Content",
    "text": "3.6 Unanalysed Slide Content\n\n\n\nSlide 10\n\n\nThe information corresponding to this segment of the presentation was not available for extraction. An analysis of the source slide content could not be completed; consequently, no factual details can be reported for this section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#adaptation-through-training",
    "href": "chapter_ai-nepi_003.html#adaptation-through-training",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.7 Adaptation Through Training",
    "text": "3.7 Adaptation Through Training\n\n\n\nSlide 11\n\n\nTo harness the power of Large Language Models for specialised fields, researchers must adapt them for specific domains and tasks. For applications within the History, Philosophy, and Sociology of Science (HPSS), the authors present four distinct strategies centred on model training. These methods allow for the fine-tuning of a general-purpose LLM, imbuing it with the specific knowledge and capabilities required for scholarly inquiry in the HPSS domain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#adaptation-via-retrieval-augmented-generation",
    "href": "chapter_ai-nepi_003.html#adaptation-via-retrieval-augmented-generation",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.8 Adaptation via Retrieval Augmented Generation",
    "text": "3.8 Adaptation via Retrieval Augmented Generation\n\n\n\nSlide 12\n\n\nBeyond direct training, Retrieval Augmented Generation (RAG) offers a powerful alternative for domain and task adaptation. This technique enhances an LLM’s capabilities by integrating an external knowledge source. The RAG process operates through a two-stage mechanism: first, a retrieval component searches a corpus of documents to find information relevant to a given query. Subsequently, a text generation component uses this retrieved information as context to produce a more accurate, detailed, and factually grounded response.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#a-taxonomy-of-large-language-models",
    "href": "chapter_ai-nepi_003.html#a-taxonomy-of-large-language-models",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.9 A Taxonomy of Large Language Models",
    "text": "3.9 A Taxonomy of Large Language Models\n\n\n\nSlide 13\n\n\nThe field of large language models can be systematically organised using a taxonomy based on four key characteristics. This classification framework helps to distinguish models by their underlying architecture, such as whether they are encoder-only, decoder-only, or a combination of both. Moreover, models are differentiated by the fine-tuning strategies applied to them and the specific types of embeddings they utilise. Finally, the level of abstraction in their deployment—ranging from simple API access to a fully self-hosted model—provides another critical axis for categorisation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#computational-methods-in-hpss",
    "href": "chapter_ai-nepi_003.html#computational-methods-in-hpss",
    "title": "3  Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.10 Computational Methods in HPSS",
    "text": "3.10 Computational Methods in HPSS\n\n\n\nSlide 14\n\n\nComputational methods, especially Large Language Models, provide powerful tools for research within the History, Philosophy, and Sociology of Science. Their applications are diverse and can be organised into four primary areas of scholarly work. Firstly, they assist in managing and processing data and sources. Secondly, these models enable the analysis of complex knowledge structures within large corpora. Thirdly, researchers use them to investigate knowledge dynamics, such as how concepts and theories evolve over time. Finally, LLMs facilitate the examination of knowledge practices by analysing patterns in scientific communication and argumentation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "",
    "text": "Overview\nMax Neuchel, Andrea Loetgers, and Taya Knulla of Utrecht University have developed OpenAlex Mapper, a system designed to visualise and query scientific discourse. Their approach processes large datasets of scholarly articles from OpenAlex, employing advanced dimensionality reduction techniques to map publications into a two-dimensional conceptual space.\nThe methodology begins by sampling 300,000 English-language articles with well-formed abstracts. The team then embeds these articles using a language model before projecting them via a pre-trained UMAP model. A crucial feature of the tool is its ability to accept arbitrary queries to OpenAlex, download the results, and seamlessly integrate these new articles into the existing conceptual map. This function allows for the dynamic exploration of research trends and the conceptual distribution of scientific knowledge, despite a minor technical issue encountered during a live demonstration.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#project-origins-and-data-acquisition",
    "href": "chapter_ai-nepi_004.html#project-origins-and-data-acquisition",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.1 Project Origins and Data Acquisition",
    "text": "4.1 Project Origins and Data Acquisition\n\n\n\nSlide 02\n\n\nOriginating from the theoretical philosophy department at Utrecht University, the OpenAlex Mapper project is a collaborative endeavour by Max Neuchel, Andrea Loetgers, and Taya Knulla. The team initiated this work by acquiring a substantial dataset from the OpenAlex database, comprising 300,000 randomly selected articles.\nTo ensure data quality, the authors applied stringent selection criteria. Each article was required to possess a reasonably well-formed abstract and be published in English, thereby establishing a robust foundation for all subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#dimensionality-reduction-and-functionality",
    "href": "chapter_ai-nepi_004.html#dimensionality-reduction-and-functionality",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.2 Dimensionality Reduction and Functionality",
    "text": "4.2 Dimensionality Reduction and Functionality\n\n\n\nSlide 03\n\n\nTo process the extensive article dataset, the team employed Uniform Manifold Approximation and Projection (UMAP), which effectively reduces the data’s dimensionality to two dimensions. This trained UMAP model is meticulously retained for consistent application across all operations.\nThe OpenAlex Mapper tool leverages this foundational model whilst enabling users to submit arbitrary queries directly to the OpenAlex database. This architecture facilitates dynamic data retrieval and sophisticated analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#embedding-and-projection-workflow",
    "href": "chapter_ai-nepi_004.html#embedding-and-projection-workflow",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.3 Embedding and Projection Workflow",
    "text": "4.3 Embedding and Projection Workflow\n\n\n\nSlide 04\n\n\nThe workflow for integrating new data into the OpenAlex Mapper is meticulously structured. Initially, the tool downloads query results directly from the OpenAlex database. It then embeds these newly acquired articles using the identical language model employed during the initial dataset processing.\nFinally, the system projects these embedded representations through the pre-trained UMAP model. This procedure ensures a consistent and accurate projection into the established two-dimensional conceptual space.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#conceptual-mapping-and-integration",
    "href": "chapter_ai-nepi_004.html#conceptual-mapping-and-integration",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.4 Conceptual Mapping and Integration",
    "text": "4.4 Conceptual Mapping and Integration\n\n\n\nSlide 05\n\n\nThis systematic process assigns each newly processed article a precise position on the two-dimensional conceptual map. The underlying principle ensures that these positions are determined as if the articles had been an integral part of the original layout.\nSuch seamless integration allows for a coherent and continuously evolving visualisation of scholarly discourse, thereby maintaining the integrity of the conceptual distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#demonstration-and-umap-features",
    "href": "chapter_ai-nepi_004.html#demonstration-and-umap-features",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.5 Demonstration and UMAP Features",
    "text": "4.5 Demonstration and UMAP Features\n\n\n\nSlide 06\n\n\nThe inherent features of UMAP considerably simplify the integration of new data points into an existing map, streamlining the process significantly. Following the technical explanation, the presenter proceeded with a live demonstration of the OpenAlex Mapper tool, aiming to illustrate its functionality in real-time.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#technical-challenges",
    "href": "chapter_ai-nepi_004.html#technical-challenges",
    "title": "4  Investigating the transdiciplinary application of model templates through projective methods",
    "section": "4.6 Technical Challenges",
    "text": "4.6 Technical Challenges\n\n\n\nSlide 07\n\n\nUnfortunately, an unforeseen technical issue emerged during the live demonstration, which prevented the tool from operating as intended. This anomaly, which had not been observed during extensive prior testing, was unexpected. Nevertheless, the presenter promptly initiated a second attempt to run the demonstration and resolve the challenge.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Investigating the transdiciplinary application of model templates through projective methods</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThe authors present a comprehensive study on genre classification within historical patient periodicals, conducted as part of the ERC-funded ActDisease programme. This programme investigates how patient organisations in 20th-century Europe shaped concepts of disease and medical practices, using their magazines as a primary data source. A substantial corpus, the ActDisease Dataset, underpins this work, comprising 96,186 digitised pages from patient magazines across Germany, Sweden, France, and the United Kingdom.\nThe core technical challenge the team addresses is the classification of diverse and often ambiguous textual genres found within these historical documents. This task is complicated by issues in Optical Character Recognition (OCR) and the limitations of traditional methods such as topic modelling. To overcome this, the authors developed a sophisticated, expert-driven genre-labelling scheme and conducted extensive experiments using both zero-shot and few-shot learning paradigms.\nTheir study evaluates a range of multilingual encoder models, including XLM-Roberta, mBERT, and a specialised historical mBERT (hmBERT), alongside generative models like Llama-3.1 8b. Key findings demonstrate that few-shot learning, particularly with Masked Language Model (MLM) fine-tuning, significantly improves performance. The hmBERT-MLM model emerges as the most effective, highlighting the value of domain-specific pre-training for historical texts. The work concludes that the rich generic diversity of popular magazines makes them a more complex text mining target than scientific journals, underscoring the necessity of robust genre classification for fine-grained historical analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#presentation-outline",
    "href": "chapter_ai-nepi_005.html#presentation-outline",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 Presentation Outline",
    "text": "5.1 Presentation Outline\n\n\n\nSlide 02\n\n\nThe research programme unfolds across three principal sections. It begins with an introduction to the ActDisease project, followed by a detailed examination of the genre classification experiments. The presentation culminates in a summary of the key conclusions drawn from the research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-aims-and-scope",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-aims-and-scope",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Project: Aims and Scope",
    "text": "5.2 The ActDisease Project: Aims and Scope\n\n\n\nSlide 03\n\n\nThe authors initiated the ‘ActDisease’ project, an ERC-funded research programme designed to explore the influence of patient organisations on medicine and society. The project’s central aim is to analyse how these groups, active throughout 20th-century Europe, shaped concepts of disease, the personal experience of illness, and prevailing medical practices.\nTo achieve this, the investigators use patient-published periodicals as the primary source material for their historical analysis. An image of Heligoland, Germany, provides visual context for the historical settings under investigation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-dataset-composition",
    "href": "chapter_ai-nepi_005.html#the-actdisease-dataset-composition",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 The ActDisease Dataset Composition",
    "text": "5.3 The ActDisease Dataset Composition\n\n\n\nSlide 04\n\n\nThe project team meticulously compiled the ActDisease Dataset, a specialised corpus of digitised magazines published by patient organisations. This collection spans materials from Germany, Sweden, France, and the UK, amounting to a total of 96,186 pages. A summary table provides a detailed breakdown of the dataset, specifying the number of unique magazine titles, total page counts, and the range of publication years covered for different diseases within each country.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-and-post-ocr-correction",
    "href": "chapter_ai-nepi_005.html#digitisation-and-post-ocr-correction",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Digitisation and Post-OCR Correction",
    "text": "5.4 Digitisation and Post-OCR Correction\n\n\n\nSlide 05\n\n\nA significant technical hurdle in the project involved the digitisation of historical documents. The process of Optical Character Recognition (OCR) often yields suboptimal results when applied to older, varied source materials. This challenge necessitates further research into effective post-OCR correction methods, which are crucial for enhancing the accuracy and utility of the digitised text for subsequent analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#challenges-in-genre-classification",
    "href": "chapter_ai-nepi_005.html#challenges-in-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 Challenges in Genre Classification",
    "text": "5.5 Challenges in Genre Classification\n\n\n\nSlide 06\n\n\nThe analysis of patient periodicals presents a distinct challenge in genre classification. These source materials contain a wide diversity of text types, ranging from medical advice to personal stories and advertisements. Consequently, conventional methods such as topic modelling or basic term-counting prove inadequate for accurately differentiating these nuanced genres, which often share overlapping vocabularies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#motivation-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#motivation-for-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Motivation for Genre Classification",
    "text": "5.6 Motivation for Genre Classification\n\n\n\nSlide 07\n\n\nThe team’s focus on genre classification stems from its analytical utility. From a language technology standpoint, genre provides a framework for understanding the communicative purpose of a text. By classifying content into distinct genres, the authors can conduct more rigorous historical investigations, separating, for instance, official announcements from personal testimonials. This capability ultimately enables a more fine-grained and context-aware analysis of the entire dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrating-genre-diversity-in-actdisease",
    "href": "chapter_ai-nepi_005.html#illustrating-genre-diversity-in-actdisease",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Illustrating Genre Diversity in ‘ActDisease’",
    "text": "5.7 Illustrating Genre Diversity in ‘ActDisease’\n\n\n\nSlide 08\n\n\nTo illustrate the project’s central challenge, the authors present a collage of documents showcasing the diverse textual genres related to a specific disease, likely diabetes. This visual representation effectively communicates the variety of formats and styles—from scientific articles to personal letters and advertisements—that the team must categorise. It underscores the complexity of performing automated analysis on such heterogeneous source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#textual-examples-of-genre-variation",
    "href": "chapter_ai-nepi_005.html#textual-examples-of-genre-variation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Textual Examples of Genre Variation",
    "text": "5.8 Textual Examples of Genre Variation\n\n\n\nSlide 09\n\n\nFurther reinforcing the concept of genre diversity, the presentation offers several concrete textual examples from the ‘ActDisease’ domain, specifically concerning diabetes. These snippets highlight the distinct linguistic and structural features of different genres found within the patient magazines. By presenting these varied examples, the authors clarify the practical difficulties and the importance of developing a robust classification system.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels-for-analysis",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels-for-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Defining Genre Labels for Analysis",
    "text": "5.9 Defining Genre Labels for Analysis\n\n\n\nSlide 11\n\n\nThe authors established a formal set of genre labels to structure their classification task. These labels were not algorithmically derived but were instead defined by subject-matter experts to ensure historical and contextual relevance. The primary function of this schema is to enable the systematic separation of content according to its type, which is essential for nuanced historical analysis. Moreover, the team designed the labels with a view towards general-purpose applicability, aiming for a system that could be adapted for other historical text analysis projects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-genre-classification-schema",
    "href": "chapter_ai-nepi_005.html#the-genre-classification-schema",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 The Genre Classification Schema",
    "text": "5.10 The Genre Classification Schema\n\n\n\nSlide 12\n\n\nA detailed classification schema formally defines the nine distinct text genres used in the project. The categories include Academic, Administrative, Advertisement, Guide, Fiction, Legal, News, Nonfiction Prose, and QA (Question & Answer). For clarity and consistency in annotation, a comprehensive table outlines the specific characteristics of each genre and provides representative examples drawn from the source material.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-methodology",
    "href": "chapter_ai-nepi_005.html#annotation-methodology",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Annotation Methodology",
    "text": "5.11 Annotation Methodology\n\n\n\nSlide 13\n\n\nThe team developed a rigorous methodology for creating the ground-truth dataset, establishing the paragraph as the fundamental unit for genre annotation. Two student annotators, working with German patient magazines focused on diabetes, applied the predefined genre labels to the text. To ensure the reliability of this process, the authors calculated the inter-annotator agreement, achieving a Cohen’s Kappa score of 0.77, which signifies a substantial level of consistency.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-in-practice-an-example",
    "href": "chapter_ai-nepi_005.html#annotation-in-practice-an-example",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Annotation in Practice: An Example",
    "text": "5.12 Annotation in Practice: An Example\n\n\n\nSlide 14\n\n\nA practical example demonstrates the annotation process in action. Three sample paragraphs extracted from the German magazine Der Diabetiker are presented in a table. Each paragraph is paired with its assigned genre label, clearly illustrating how the classification schema is applied to actual source text. This example serves to clarify the task for both training and evaluation purposes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splits-for-learning-experiments",
    "href": "chapter_ai-nepi_005.html#dataset-splits-for-learning-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Dataset Splits for Learning Experiments",
    "text": "5.13 Dataset Splits for Learning Experiments\n\n\n\nSlide 15\n\n\nFor their machine learning experiments, the authors partitioned the annotated ActDisease data into specific training and held-out sets. They carefully designed this division to evaluate model performance in both few-shot and zero-shot learning scenarios. The training set consists exclusively of annotated German texts. In contrast, the held-out (test) set comprises texts in German, French, and Swedish, and crucially, it includes genres that are deliberately absent from the training data to test zero-shot generalisation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-and-language-distribution",
    "href": "chapter_ai-nepi_005.html#genre-and-language-distribution",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Genre and Language Distribution",
    "text": "5.14 Genre and Language Distribution\n\n\n\nSlide 16\n\n\nAn analysis of the dataset reveals the distribution of text instances across different languages and genres. A comparison between the training and held-out sets highlights two important characteristics. First, there are significant imbalances in the prevalence of certain genres, a common feature of real-world data. Second, the held-out set intentionally includes novel genres absent from the training set. This design is critical for rigorously assessing the zero-shot classification capabilities of the models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-experiments",
    "href": "chapter_ai-nepi_005.html#external-datasets-for-zero-shot-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 External Datasets for Zero-Shot Experiments",
    "text": "5.15 External Datasets for Zero-Shot Experiments\n\n\n\nSlide 17\n\n\nTo enhance their zero-shot learning experiments, the team incorporated several publicly available, multilingual datasets for genre classification. These external resources, which include CORE, UDM, and FTD, provide a mix of document-level and sentence-level annotations. Leveraging these datasets allows for more robust training and evaluation of the models’ ability to generalise to unseen labels and data distributions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#cross-dataset-genre-label-mapping",
    "href": "chapter_ai-nepi_005.html#cross-dataset-genre-label-mapping",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Cross-Dataset Genre Label Mapping",
    "text": "5.16 Cross-Dataset Genre Label Mapping\n\n\n\nSlide 18\n\n\nA comparative table illustrates the mapping of genre labels across the four datasets used in the study: ActDisease, CORE, UDM, and FTD. This visualisation reveals considerable variation in the classification schemas, with different datasets using distinct and sometimes conflicting genre definitions. Such heterogeneity poses a significant challenge for zero-shot learning, as models trained on one schema must adapt to another.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-creation-pipeline",
    "href": "chapter_ai-nepi_005.html#training-data-creation-pipeline",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Training Data Creation Pipeline",
    "text": "5.17 Training Data Creation Pipeline\n\n\n\nSlide 19\n\n\nThe authors designed a comprehensive and flexible pipeline for generating training data. This process integrates data from all four sources—ActDisease, CORE, UDM, and FTD—and subjects them to a series of pre-processing steps. Crucially, the pipeline incorporates configurable sampling strategies, allowing the team to systematically create various training set compositions to test different hypotheses about model performance and data balancing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Multilingual Encoder Models",
    "text": "5.18 Multilingual Encoder Models\n\n\n\nSlide 20\n\n\nThe experiments leverage several powerful multilingual encoder models as the basis for classification. The selected models include the widely used XLM-Roberta (xlmr) and multilingual BERT (mBERT). In addition, the team evaluates a specialised historical mBERT (hmbert), which has been pre-trained on a large corpus of historical texts. This selection allows for a comparison between general-purpose multilingual models and a model adapted for the specific domain of historical language.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#experimental-setup-for-fine-tuning",
    "href": "chapter_ai-nepi_005.html#experimental-setup-for-fine-tuning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Experimental Setup for Fine-Tuning",
    "text": "5.19 Experimental Setup for Fine-Tuning\n\n\n\nSlide 21\n\n\nThe experimental design for fine-tuning involved a systematic and large-scale approach. The authors created 16 unique training set configurations by varying the data sources and sampling strategies. They then fine-tuned each of the three base language models (XLM-R, mBERT, and hmBERT) on every one of these 16 configurations. This comprehensive methodology resulted in a total of 48 distinct fine-tuned models, enabling a thorough analysis of how training data composition affects performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-zero-shot-learning",
    "href": "chapter_ai-nepi_005.html#evaluating-zero-shot-learning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.20 Evaluating Zero-Shot Learning",
    "text": "5.20 Evaluating Zero-Shot Learning\n\n\n\nSlide 22\n\n\nThe investigation now shifts its focus towards evaluating the models’ performance in a zero-shot learning context. This phase assesses the ability of the fine-tuned models to classify genres that they have not encountered during their training phase.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-prediction-evaluation",
    "href": "chapter_ai-nepi_005.html#zero-shot-prediction-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.21 Zero-Shot Prediction Evaluation",
    "text": "5.21 Zero-Shot Prediction Evaluation\n\n\n\nSlide 23\n\n\nEvaluating zero-shot predictions requires a specialised methodology to handle inherent complexities. The primary challenges include managing the partial overlap between genre label sets from different source datasets and accounting for cross-lingual scenarios. The evaluation protocol outlined by the authors is designed specifically to navigate these issues, ensuring a robust and fair assessment of model generalisation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-experimental-results",
    "href": "chapter_ai-nepi_005.html#zero-shot-experimental-results",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.22 Zero-Shot Experimental Results",
    "text": "5.22 Zero-Shot Experimental Results\n\n\n\nSlide 24\n\n\nAn overview of the zero-shot experiment results reveals several key patterns. For the FTD dataset, employing a specific label mapping strategy yields a noticeable improvement in model performance. Across other datasets, however, results indicate the presence of class-specific and language-related biases. On the UDM dataset, the investigators observed intriguing performance variations between the different models. Notably, models fine-tuned using the CORE dataset demonstrate a particular aptitude for correctly identifying texts belonging to the ‘Legal’ genre.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#comparative-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#comparative-confusion-matrices",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.23 Comparative Confusion Matrices",
    "text": "5.23 Comparative Confusion Matrices\n\n\n\nSlide 25\n\n\nFour confusion matrices provide a visual comparison of the performance of different genre classification models. The matrices detail the results for models such as hmbert_UDM, xlmr_CORE, xlmr_UDM, and xlmr_FTD, each representing a unique combination of base architecture and training data. These visualisations allow for a direct comparison of error patterns and classification accuracy under various experimental conditions when evaluated on the held-out ActDisease data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-per-category-f1-scores",
    "href": "chapter_ai-nepi_005.html#zero-shot-per-category-f1-scores",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.24 Zero-Shot Per-Category F1 Scores",
    "text": "5.24 Zero-Shot Per-Category F1 Scores\n\n\n\nSlide 26\n\n\nA summary table presents the zero-shot performance using the per-category F1 score as the primary metric. These scores, which measure the balance between precision and recall for each genre, are averaged across the various training data configurations. This allows for a consolidated view of how well each language model performs on individual genre categories in a zero-shot setting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#impact-of-data-configuration-on-performance",
    "href": "chapter_ai-nepi_005.html#impact-of-data-configuration-on-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.25 Impact of Data Configuration on Performance",
    "text": "5.25 Impact of Data Configuration on Performance\n\n\n\nSlide 27\n\n\nThe authors analysed the average F1 performance of the classification models across three distinct target tasks, corresponding to the FTD, CORE, and UDM datasets. This analysis specifically investigates how performance is affected by two key factors in the training data construction: the application of data balancing techniques and the inclusion or exclusion of certain language families. The results illuminate the sensitivity of model performance to the composition of the training corpus.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-few-shot-learning",
    "href": "chapter_ai-nepi_005.html#evaluating-few-shot-learning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.26 Evaluating Few-Shot Learning",
    "text": "5.26 Evaluating Few-Shot Learning\n\n\n\nSlide 28\n\n\nThe report now transitions to an evaluation of the models under a few-shot learning paradigm. This section assesses how effectively the models can learn to classify genres when provided with only a small number of training examples from the target ActDisease dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-learning-performance-trends",
    "href": "chapter_ai-nepi_005.html#few-shot-learning-performance-trends",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.27 Few-Shot Learning Performance Trends",
    "text": "5.27 Few-Shot Learning Performance Trends\n\n\n\nSlide 29\n\n\nThe evaluation of few-shot learning reveals clear performance trends. As expected, F1 scores for all models generally improve as the number of available training examples increases. A crucial finding is that an intermediate step of Masked Language Model (MLM) fine-tuning on the target domain’s text confers a distinct advantage, consistently boosting classification accuracy. Amongst all configurations, the hmBERT-MLM model, which combines historical pre-training with domain-specific MLM fine-tuning, achieves the highest performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-per-category-f1-score-details",
    "href": "chapter_ai-nepi_005.html#few-shot-per-category-f1-score-details",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.28 Few-Shot Per-Category F1 Score Details",
    "text": "5.28 Few-Shot Per-Category F1 Score Details\n\n\n\nSlide 30\n\n\nA detailed table presents the per-category F1 scores for the few-shot learning experiments. The results are broken down for each language model and are shown at two distinct levels of data availability (for example, with 16 and 32 training examples per class). In addition to the granular, per-category scores, the table also includes overall performance metrics, allowing for a comprehensive comparison of the models’ effectiveness in a data-scarce environment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#full-dataset-performance-of-xlm-roberta-mlm",
    "href": "chapter_ai-nepi_005.html#full-dataset-performance-of-xlm-roberta-mlm",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.29 Full-Dataset Performance of XLM-Roberta-MLM",
    "text": "5.29 Full-Dataset Performance of XLM-Roberta-MLM\n\n\n\nSlide 31\n\n\nWhen trained on the full dataset, the XLM-Roberta-MLM model demonstrates strong classification capabilities, as illustrated by a confusion matrix. The matrix reveals specific patterns of misclassification, which in turn provide insights into the nature of the data. For instance, it highlights the thematic and stylistic similarities between the ‘Guide’, ‘Nonfiction Prose’, and ‘QA’ genres as they appear within the context of diabetes patient magazines, explaining why the model sometimes confuses them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-few-shot-prompting",
    "href": "chapter_ai-nepi_005.html#evaluating-few-shot-prompting",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.30 Evaluating Few-Shot Prompting",
    "text": "5.30 Evaluating Few-Shot Prompting\n\n\n\nSlide 32\n\n\nThe final experimental section shifts to an evaluation of few-shot prompting. This approach assesses the ability of large generative language models to perform genre classification based on instructions and a small number of examples provided directly in the input prompt.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#instruction-based-genre-classification",
    "href": "chapter_ai-nepi_005.html#instruction-based-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.31 Instruction-Based Genre Classification",
    "text": "5.31 Instruction-Based Genre Classification\n\n\n\nSlide 33\n\n\nFor the prompting experiment, the authors formulated a detailed set of instructions for a text genre classification task. The prompt explicitly defines the nine target genres, complete with illustrative examples for each, and specifies the required input and output format. The chosen language model for executing this instruction-based, few-shot task is Llama-3.1 8b.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-classification-performance",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-classification-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.32 Llama-3.1 8b Classification Performance",
    "text": "5.32 Llama-3.1 8b Classification Performance\n\n\n\nSlide 34\n\n\nThe performance of the Llama-3.1 8b Instruct model on the few-shot prompting task is presented. The results include F1-scores for each genre, quantifying the model’s accuracy. A detailed confusion matrix is also provided, offering a granular view of the model’s classification decisions and revealing which genres were most frequently confused with one another.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusion-on-text-mining-complexity",
    "href": "chapter_ai-nepi_005.html#conclusion-on-text-mining-complexity",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.33 Conclusion on Text Mining Complexity",
    "text": "5.33 Conclusion on Text Mining Complexity\n\n\n\nSlide 35\n\n\nA primary conclusion from this work is that applying text mining techniques to popular magazines is an inherently more complex task than analysing more uniform corpora like scientific journals or books. The authors attribute this increased difficulty directly to the rich and varied multitude of genres that coexist within a single magazine issue, demanding more sophisticated analytical approaches.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#summary-of-key-conclusions",
    "href": "chapter_ai-nepi_005.html#summary-of-key-conclusions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.34 Summary of Key Conclusions",
    "text": "5.34 Summary of Key Conclusions\n\n\n\nSlide 37\n\n\nThe research yields several key conclusions. The authors demonstrate that the generic diversity of historical magazines poses a substantial challenge for text mining, and that genre classification is an indispensable tool for enabling fine-grained historical inquiry. Their work also shows that modern, large-scale datasets can be successfully leveraged to improve the analysis of historical texts, and that contemporary generative models exhibit promising quality on these classification tasks. Finally, the experiments confirm that few-shot learning with multilingual encoders is a highly effective strategy, with performance being particularly strong when using models specifically adapted for historical language.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant at Vrije Universiteit Brussel, investigates the profound influence of the ‘ancient wisdom’ tradition upon early modern natural philosophy. The team analyses a vast, multilingual corpus of 430,000 printed books from 1540 to 1728, sourced from Early English Books Online (EEBO), Gallica, and the Bavarian State Library. This ambitious undertaking applies a computational approach to the History, Philosophy, and Sociology of Science (HPSS), aiming to uncover previously overlooked intellectual networks.\nCentral to this endeavour is the identification of both direct lexical reuse and more subtle, indirect semantic reuse across texts. The project’s alpha-stage web application, VERITRACE, built upon an Elasticsearch backend, facilitates corpus exploration, advanced keyword searching, and sophisticated text matching. The team confronts significant challenges, including the variable quality of Optical Character Recognition (OCR), the complexities of early modern typography across six languages, and the sheer scale of the data.\nTo address these obstacles, the authors employ Large Language Models (LLMs) for two distinct purposes. GPT-based models function as ‘judges’ to enrich and clean bibliographic metadata, whilst BERT-based models such as LaBSE generate the vector embeddings that encode semantic meaning for passage comparison. Ultimately, the project seeks to provide scholars with a powerful tool to discover new patterns in intellectual history, effectively creating an ‘early modern plagiarism detector’ that illuminates the ‘great unread’ texts of the period.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-context-and-research-objectives",
    "href": "chapter_ai-nepi_006.html#project-context-and-research-objectives",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.1 Project Context and Research Objectives",
    "text": "6.1 Project Context and Research Objectives\n\n\n\nSlide 03\n\n\nLed by Professor Cornelis J. Schilt at Vrije Universiteit Brussel, the VERITRACE project is a five-year initiative funded by an ERC Starting Grant. Its central objective is to trace the influence of the ‘ancient wisdom’ tradition on the evolution of early modern natural philosophy and science. The investigation centres on a core collection of 140 works representing this tradition, including such texts as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the historically significant Corpus Hermeticum.\nThe team’s inquiry, however, extends far beyond established connections, such as Isaac Newton’s documented engagement with the Sibylline Oracles or Johannes Kepler’s knowledge of the Corpus Hermeticum. A primary goal is to delve deeper, uncovering a much broader and often neglected network of texts and authors. By focusing on what the authors term the ‘great unread’—a vast body of work by often lesser-known figures—the project aims to reveal a more comprehensive picture of this tradition’s impact.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-methodology-for-hpss",
    "href": "chapter_ai-nepi_006.html#computational-methodology-for-hpss",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.2 Computational Methodology for HPSS",
    "text": "6.2 Computational Methodology for HPSS\n\n\n\nSlide 04\n\n\nThe project’s authors adopt a computational framework for the History, Philosophy, and Sociology of Science (HPSS), applying large-scale, multilingual exploration to their core research questions. A central feature of this methodology is the identification of textual reuse across the extensive corpus. The team has developed systems to detect both direct lexical reuse, where wording is identical or highly similar, and indirect semantic reuse, where concepts are shared without verbatim overlap.\nThis capability, which functions as a form of ‘early modern plagiarism detector’, is not merely for identifying copied text. Rather, the approach seeks to uncover networks of texts, passages, themes, and authors that traditional scholarship may have overlooked. By systematically mapping these connections, the team hopes to reveal new, large-scale patterns in intellectual history and the philosophy of science, offering fresh perspectives on the period’s intellectual landscape.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#dataset-composition-and-scope",
    "href": "chapter_ai-nepi_006.html#dataset-composition-and-scope",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.3 Dataset Composition and Scope",
    "text": "6.3 Dataset Composition and Scope\n\n\n\nSlide 05\n\n\nTo facilitate this investigation, the team has assembled a large and diverse multilingual dataset focused exclusively on printed books and texts; handwritten materials are deliberately excluded to maintain a manageable scope. The corpus comprises approximately 430,000 books in six different languages, covering a period of nearly 200 years from 1540 to 1728. This timeframe was chosen to begin at a significant point in printing history and to conclude shortly after the death of Isaac Newton.\nThe data originates from three principal sources. These include the Early English Books Online (EEBO) collection, digitised materials from the French National Library accessed via Gallica, and the project’s largest contributor, the Bavarian State Library. This vast collection of texts is analysed using a suite of state-of-the-art digital techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-llm-applications",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-llm-applications",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.4 Core Challenges and LLM Applications",
    "text": "6.4 Core Challenges and LLM Applications\n\n\n\nSlide 06\n\n\nThe project team confronts several core challenges inherent in working with historical texts at scale. A primary issue stems from the variable quality of Optical Character Recognition (OCR), as the team receives raw text directly from libraries in formats like XML, HOCR, and HTML, without access to the ground truth page images for verification. This poor OCR quality significantly affects all downstream processing. Furthermore, the work must navigate the complexities of early modern typography and semantics across at least six languages, alongside the sheer scale of managing hundreds of thousands of texts.\nTo address these issues, the authors apply Large Language Models (LLMs) in two distinct ways. On the decoder side, GPT-based models function as ‘judges’ to help enrich and clean the vast collection of bibliographic metadata. This report, however, focuses on the encoder-side application, where the team uses BERT-based LLMs to generate vector embeddings. These embeddings encode the semantic meaning of sentences and short passages, forming the foundation for the project’s sophisticated text-matching capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.5 LLMs as Judges for Metadata Enrichment",
    "text": "6.5 LLMs as Judges for Metadata Enrichment\n\n\n\nSlide 09\n\n\nThe team is exploring the use of LLMs to automate the highly tedious task of enriching and verifying bibliographic metadata. The manual process required each team member to compare 10,000 pairs of records to determine if they referred to the same underlying printed text. To streamline this, the authors developed The LLM Bench, a system that employs a panel of open-source models like Llama to act as ‘judges’ in evaluating these potential matches.\nThe models receive extensive prompt guidelines and are tasked with producing a decision, a confidence level, and detailed reasoning. A significant hurdle, however, complicates this approach: the models frequently hallucinate information. Whilst forcing more structured output can eliminate these hallucinations, it comes at a cost. The models then tend to provide generic, less useful responses, diminishing the value of their reasoning. Consequently, this application remains a work-in-progress as the team seeks to balance structured output with insightful analysis to create a genuinely useful automation tool.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-web-application-and-pipeline",
    "href": "chapter_ai-nepi_006.html#the-veritrace-web-application-and-pipeline",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.6 The VERITRACE Web Application and Pipeline",
    "text": "6.6 The VERITRACE Web Application and Pipeline\n\n\n\nSlide 12\n\n\nThe project’s central output is the VERITRACE web application, currently an alpha-stage proof-of-concept that is not yet publicly available. For its semantic analysis features, the application is currently testing a BERT-based model, LaBSE, to generate vector embeddings, although the team anticipates that this model may not be sufficient for the final product.\nUnderpinning the application is a complex data processing pipeline required to prepare the raw texts for the Elasticsearch database that serves as the backend. This multi-stage pipeline involves numerous steps, including text extraction, the creation of positional mappings, text segmentation into passages, and OCR quality assessment. Each of these steps demands careful optimisation to handle the data’s complexity and ensure the quality of the final indexed content.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-exploration-and-metadata-analysis",
    "href": "chapter_ai-nepi_006.html#corpus-exploration-and-metadata-analysis",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.7 Corpus Exploration and Metadata Analysis",
    "text": "6.7 Corpus Exploration and Metadata Analysis\n\n\n\nSlide 14\n\n\nThe VERITRACE web application features an ‘Explore’ section that provides users with high-level statistics about the corpus, which currently contains 427,305 metadata records in its prototype stage. This data is served directly from a MongoDB database. A ‘Metadata Explorer’ allows for deeper investigation, where users can inspect the rich metadata generated for each text.\nA crucial feature is the detailed language identification, which is performed on text segments as small as 50 characters. Such granularity proves essential for accurately cataloguing the multilingual texts common throughout the corpus. For instance, the system can identify a book as being 85% Latin and 15% Greek, information not typically available in standard library metadata. Furthermore, the system attempts to assess OCR quality on a page-by-page basis, providing a nuanced quality metric despite the challenge of not having ground truth images for comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-keyword-search-functionality",
    "href": "chapter_ai-nepi_006.html#advanced-keyword-search-functionality",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.8 Advanced Keyword Search Functionality",
    "text": "6.8 Advanced Keyword Search Functionality\n\n\n\nSlide 15\n\n\nA central feature for scholars is the ‘Search’ section, which leverages the power of Elasticsearch to offer both basic and advanced querying. Even in its prototype stage with only 132 texts, the search index is a substantial 15 gigabytes, indicating that the full 430,000-text corpus will scale into the terabytes. Users can perform simple keyword searches, but the system’s strength lies in its advanced capabilities.\nScholars can construct fielded queries, for example, to find the keyword ‘Hermes’ only within works authored by ‘Kepler’. The system also supports complex nested queries with Boolean operators and, notably, proximity queries. This allows a user to search for instances where two terms, such as ‘Hermes’ and ‘Plato’, are mentioned within a specified word distance of each other, enabling more nuanced textual investigations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#planned-analysis-and-reading-interfaces",
    "href": "chapter_ai-nepi_006.html#planned-analysis-and-reading-interfaces",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.9 Planned Analysis and Reading Interfaces",
    "text": "6.9 Planned Analysis and Reading Interfaces\n\n\n\nSlide 17\n\n\nThe application’s roadmap includes a planned ‘Analyse’ section, which will offer users a suite of advanced analytical tools. These will include topic modelling, Latent Semantic Analysis (LSA), and diachronic analysis to track changes in language and concepts over time.\nIn contrast, the ‘Read’ section is already functional. It provides scholars with direct access to the source material, recognising their need to consult original texts beyond the OCR data. This section integrates a Mirador viewer, which displays high-quality PDF facsimiles of every text in the corpus. This feature allows for a seamless reading experience, similar to using a modern digital library portal, with all relevant bibliographic metadata displayed alongside the text.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-text-matching-tool-functionality-and-parameters",
    "href": "chapter_ai-nepi_006.html#the-text-matching-tool-functionality-and-parameters",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.10 The Text Matching Tool: Functionality and Parameters",
    "text": "6.10 The Text Matching Tool: Functionality and Parameters\n\n\n\nSlide 19\n\n\nThe application’s most innovative component is the ‘Match’ section, designed to identify textual reuse. The tool is highly flexible, allowing users to compare a single text against another, or to compare a collection of documents—such as all of Kepler’s works—against another text. The ultimate ambition is to allow a single text to be compared against the entire corpus, a feature that poses immense computational challenges regarding user wait times.\nCrucially, the interface exposes technical parameters to the user, allowing them to tweak settings like the minimum similarity score. The tool offers two primary match types: lexical matching, which identifies shared vocabulary, and semantic matching, which uses vector embeddings to find conceptually similar passages even if they share no keywords. This latter capability is vital for cross-lingual comparisons. Users can also select from different performance modes, including a standard mode, a computationally intensive ‘comprehensive’ mode, and a ‘faster’ mode for quick checks.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#case-study-lexical-and-semantic-matching-of-newtons-opticks",
    "href": "chapter_ai-nepi_006.html#case-study-lexical-and-semantic-matching-of-newtons-opticks",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.11 Case Study: Lexical and Semantic Matching of Newton’s Opticks",
    "text": "6.11 Case Study: Lexical and Semantic Matching of Newton’s Opticks\n\n\n\nSlide 20\n\n\nTo validate the matching tool, the team performed a series of sanity checks using the Latin (1719) and English (1718) editions of Isaac Newton’s Opticks. A lexical match between the two different-language texts correctly returned no results in standard mode, as expected. However, switching to the more intensive ‘comprehensive’ mode revealed three matches, correctly identifying small sections of English text present within the predominantly Latin volume. The interface presents these results with a quality score, a coverage score, and details on the millions of comparisons performed.\nConversely, a semantic match between the two editions produced reasonable results. The system successfully identified conceptually similar passages, such as discussions of colours, demonstrating its ability to work across translations. Nevertheless, some metrics, like the coverage score, appeared inaccurate, although this might reflect genuine differences between the two editions, as the Latin version is considerably longer. Despite these partial successes, the authors conclude that the current embedding model is not yet sufficiently robust for the project’s demanding requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-strategic-considerations",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-strategic-considerations",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.12 Future Challenges and Strategic Considerations",
    "text": "6.12 Future Challenges and Strategic Considerations\n\n\n\nSlide 26\n\n\nLooking ahead, the project team must navigate several significant challenges. The choice of an embedding model is paramount; the current model is likely insufficient, forcing a decision between adopting a more accurate but resource-intensive pre-trained model or undertaking the complex task of fine-tuning a base model specifically on the unique historical corpus. A fundamental conceptual challenge arises from semantic drift: how can a model effectively map the meaning of words that changed over two centuries and across multiple languages into a single, coherent vector space?\nPractically, the persistent problem of poor OCR quality cascades through the entire system, hindering fundamental tasks like sentence segmentation. As re-OCRing the entire 430,000-book corpus is infeasible, the team is considering targeted re-OCRing of the worst-performing texts or supplementing the corpus with high-quality versions from other sources. Finally, scaling and performance remain a major concern. With queries on a tiny 132-text prototype already taking 15 seconds, ensuring acceptable performance on the full corpus will require substantial optimisation and computational resources.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "",
    "text": "Overview\nThis chapter presents a dual-pronged investigation into artificial intelligence. The authors first explore the evolution of explainable AI (XAI), before demonstrating its application in generating novel scientific insights within the humanities.\nThe initial part charts the progression from first-generation XAI, which relied upon heatmap-based feature attributions for simple classification models, towards a more sophisticated paradigm the authors term ‘XAI 2.0’. This advanced approach focuses on structured interpretability, analysing second-order (pairwise) and higher-order (graph-based) feature interactions. Such methods are essential for understanding the complex mechanisms of modern foundation models, including Large Language Models (LLMs). The authors demonstrate how these techniques can uncover biases in sentiment prediction, analyse how LLMs handle long-range dependencies, and reveal the surprisingly simple heuristics, such as noun matching, that models employ for tasks like sentence similarity.\nThe second part transitions to a series of case studies applying these AI techniques to historical research. One project involves classifying a corpus of early modern mathematical instruments, using heatmaps to derive visual definitions based on features like fine-grained scales. A more extensive project, the ‘XAI-Historian’, analyses the Sphera corpus of numerical tables from 1472–1650. By developing a specialised model for bigram detection and using XAI to verify its logic, the team could analyse historical publishing patterns at scale. A key finding emerged from a cluster entropy analysis, which identified Wittenberg as a centre of low print diversity—a data-driven discovery that corroborated historical knowledge about the political control exerted over its publishing curriculum by Protestant reformers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#presentation-structure",
    "href": "chapter_ai-nepi_007.html#presentation-structure",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.1 Presentation Structure",
    "text": "7.1 Presentation Structure\n\n\n\nSlide 02\n\n\nThe discourse is structured into two principal sections. The first part addresses the field of Explainable AI and its role in understanding the inner workings of Large Language Models. Subsequently, the second part demonstrates how these AI-driven methods can yield new scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#foundations-of-explainable-ai",
    "href": "chapter_ai-nepi_007.html#foundations-of-explainable-ai",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.2 Foundations of Explainable AI",
    "text": "7.2 Foundations of Explainable AI\n\n\n\nSlide 03\n\n\nThe initial exploration into explainable AI, which may be termed XAI 1.0, centres on the concept of feature attribution. This approach seeks to establish a clear definition of what constitutes a model explanation within the machine learning community, providing a foundation for the more complex methods of interpretability that followed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-black-box-problem",
    "href": "chapter_ai-nepi_007.html#the-black-box-problem",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.3 The ‘Black Box’ Problem",
    "text": "7.3 The ‘Black Box’ Problem\n\n\n\nSlide 04\n\n\nHistorically, machine learning research focused predominantly on visual data, creating powerful but opaque ‘black box’ systems. These models could, for instance, correctly classify an object in an image but offered no insight into the basis for their decision. The field of explainable AI emerged to address this opacity, dedicating a decade of research to methods that trace a model’s predictions back to its inputs. A foundational technique is the heatmap, which visually highlights the input pixels most influential in a classification, thereby showing why a model recognised a rooster.\nThe imperative for such explainability is fourfold. It enables scholars and engineers to:\n\nVerify that a model is functioning reasonably.\nDiagnose and correct its errors.\nLearn from the surprising or novel solutions that models can uncover.\nEnsure compliance with emerging regulatory frameworks, including the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-advent-of-generative-ai",
    "href": "chapter_ai-nepi_007.html#the-advent-of-generative-ai",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.4 The Advent of Generative AI",
    "text": "7.4 The Advent of Generative AI\n\n\n\nSlide 06\n\n\nThe landscape of AI has shifted dramatically from the standard classification models prevalent five years ago to the current era of Generative AI. Unlike their predecessors, today’s foundation models are multi-task systems. They can classify content, retrieve similar images, generate entirely new images, and answer questions across a vast range of topics.\nThis expanded capability introduces a significant challenge: grounding a model’s output, such as a generated answer, in specific input data becomes far more complex. Consequently, research must now advance beyond simple heatmap representations. The focus is shifting towards analysing feature interactions and adopting more mechanistic perspectives to understand these models, which effectively act as ‘world models’ that encode societal knowledge and patterns of textual evolution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "href": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.5 Illustrative Model Failures",
    "text": "7.5 Illustrative Model Failures\n\n\n\nSlide 07\n\n\nAI models are prone to making surprising and revealing errors. One well-known example involves an object classifier that incorrectly bases its identification of a boat on the surrounding water; the model learns this correlation because the water’s texture is a simpler feature to detect than the boat itself.\nAnother example highlights failures in multi-step planning. When a standard LLM, such as a Llama 3 model, is prompted to solve the Tower of Hanoi puzzle, it immediately violates the game’s rules by attempting to move the largest, inaccessible disk. This demonstrates a fundamental misunderstanding of the problem’s physical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#a-new-paradigm-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#a-new-paradigm-structured-interpretability",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.6 A New Paradigm: Structured Interpretability",
    "text": "7.6 A New Paradigm: Structured Interpretability\n\n\n\nSlide 08\n\n\nTo address the limitations of earlier methods, the authors’ work introduces a new paradigm termed ‘XAI 2.0’, which champions the concept of structured interpretability. This approach aims to move beyond simple heatmap visualisations to uncover more complex, relational patterns within a model’s decision-making process. The need for such methods is underscored by failures in standard models, like the Llama 3 variant that struggled with the Tower of Hanoi, although more recent reasoning models may show improvement.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-and-second-order-explanations",
    "href": "chapter_ai-nepi_007.html#first-and-second-order-explanations",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.7 First and Second-Order Explanations",
    "text": "7.7 First and Second-Order Explanations\n\n\n\nSlide 10\n\n\nStructured interpretability distinguishes between different orders of explanation. First-order explanations, akin to heatmaps, are useful for simple classifiers. For instance, when the authors trained a model to classify historical tables, these explanations verified that the model correctly focused on numerical content to make its predictions.\nSecond-order explanations, however, analyse pairwise relationships between features. This becomes crucial for understanding tasks like similarity measurement. When explaining the similarity score between two images, an interaction-based method reveals the specific features that correspond. In an example with two identical tables, this approach correctly highlights the interactions between matching digits, confirming the model’s logic.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-in-graphs",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-in-graphs",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.8 Higher-Order Interactions in Graphs",
    "text": "7.8 Higher-Order Interactions in Graphs\n\n\n\nSlide 11\n\n\nMore recent work by the team extends this analysis to graph structures, such as citation networks or relationships between entities like books. In these contexts, higher-order interactions provide more meaningful explanations than simpler methods.\nFor models trained on graph classification tasks, explanations manifest as ‘feature walks’ or subgraphs—sets of interconnected features that become relevant only when considered collectively. The ultimate goal of this research is to derive more complex insights into model behaviour and progress towards a circuit-level understanding of their internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#detecting-bias-in-language-models",
    "href": "chapter_ai-nepi_007.html#detecting-bias-in-language-models",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.9 Detecting Bias in Language Models",
    "text": "7.9 Detecting Bias in Language Models\n\n\n\nSlide 13\n\n\nApplying first-order attributions to language models reveals their underlying biases. In a standard sentiment prediction task using a movie review dataset, the authors employed a heatmap-style method adapted for Transformers. The analysis showed that the model’s predictions were skewed by the names present in the text.\nA review was more likely to receive a positive classification if it contained male, Western names like ‘Lee’ or ‘Raphael’. Conversely, the presence of foreign-sounding names like ‘Saddam’ or ‘Chan’ correlated with a negative score. This work demonstrates that explainable AI is a powerful tool for detecting such fine-grained, and often unintended, biases within language models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#long-range-dependencies-in-summarisation",
    "href": "chapter_ai-nepi_007.html#long-range-dependencies-in-summarisation",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.10 Long-Range Dependencies in Summarisation",
    "text": "7.10 Long-Range Dependencies in Summarisation\n\n\n\nSlide 14\n\n\nThe authors investigated how LLMs handle long-range dependencies when summarising extensive texts, such as Wikipedia articles, within an 8,000-token context window. By tracing the generated summary back to its sources in the input, they sought to determine if the models effectively use information from the entire document.\nTheir analysis revealed a strong recency bias: the model predominantly focuses on information from the latter parts of the context. Although it can access information from the beginning of the text, it is significantly less likely to do so, as shown by a logarithmic scale of attribution counts. This finding implies that LLM-generated summaries are not balanced representations of the source material but are skewed towards content presented closer to the end of the prompt.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explaining-sentence-similarity",
    "href": "chapter_ai-nepi_007.html#explaining-sentence-similarity",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.11 Explaining Sentence Similarity",
    "text": "7.11 Explaining Sentence Similarity\n\n\n\nSlide 17\n\n\nTo understand how models compute sentence similarity, the team applied second-order explanations to a standard pretrained model like Sentence-BERT. Given two sentences, the method generates interaction scores between their tokens to reveal the basis for the calculated similarity score.\nThe analysis of a toy example (‘A cat I really like’ and ‘It is a great cat’) and other pairs revealed that the models do not employ complex semantic reasoning. Instead, they rely on surprisingly simplistic heuristics, operating like a ‘bag-of-tokens’ system. The primary strategy is simple noun matching, supplemented by noun-verb pairings and interactions with separator tokens. This suggests that in the process of compressing vast amounts of information, these models default to simple, and perhaps not immediately obvious, decision-making strategies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#modelling-complex-language",
    "href": "chapter_ai-nepi_007.html#modelling-complex-language",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.12 Modelling Complex Language",
    "text": "7.12 Modelling Complex Language\n\n\n\nSlide 19\n\n\nGraph Neural Networks (GNNs) can be conceptually framed as LLMs, as their message-passing mechanism is analogous to an LLM’s attention network. This perspective allows for the application of higher-order explanation methods to complex language phenomena.\nStandard first-order, or bag-of-words, explanations often fail in this regard. For instance, in the sentence ‘First, I didn’t like the boring pictures’, such a method would incorrectly assign a positive sentiment due to the word ‘like’, completely missing the negation. In contrast, a higher-order explanation method successfully captures the complex structure. It correctly assigns a negative value to the entire negated phrase and properly interprets the sentence’s overall sentiment hierarchy, demonstrating a more nuanced understanding of language.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-defining-historical-instruments",
    "href": "chapter_ai-nepi_007.html#case-study-defining-historical-instruments",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.13 Case Study: Defining Historical Instruments",
    "text": "7.13 Case Study: Defining Historical Instruments\n\n\n\nSlide 24\n\n\nIn a collaborative project with historians Matteo Valeriani and Jochen Büttner, the authors applied AI to a corpus of historical mathematical instruments. Their goal was to build a classifier that could distinguish between categories such as ‘machine’ and ‘mathematical instrument’.\nBy employing heatmap-based explanations, the team sought to extract objective ‘visual definitions’ that the model used for its classifications. This process necessitated close interaction with the domain experts to validate the meaningfulness of the AI-derived criteria. A key finding was that the model correctly identified fine-grained scales as a highly relevant and defining feature for the ‘mathematical instrument’ class.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-the-sphera-corpus",
    "href": "chapter_ai-nepi_007.html#case-study-the-sphera-corpus",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.14 Case Study: The Sphera Corpus",
    "text": "7.14 Case Study: The Sphera Corpus\n\n\n\nSlide 25\n\n\nThe team’s largest collaborative project with historians from the Bifold institute involved the Sphera corpus, a collection of early modern texts published between 1472 and 1650. The central challenge, brought forward by Matteo Valeriani and Jochen Büttner, was to analyse the corpus’s vast collection of numerical tables.\nDespite initial assessments that the data would be extremely difficult to process computationally, the research goal was to develop an automated method for matching tables based on semantic similarity—a task that had been impossible to conduct at scale using traditional methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-xai-historian-workflow",
    "href": "chapter_ai-nepi_007.html#the-xai-historian-workflow",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.15 The ‘XAI-Historian’ Workflow",
    "text": "7.15 The ‘XAI-Historian’ Workflow\n\n\n\nSlide 26\n\n\nThe authors developed a workflow to support what they term the ‘XAI-Historian’—a scholar who leverages AI and its explanations for data-driven hypothesis generation. Rather than applying a large, general foundation model, which performs poorly on such specialised, out-of-domain data, the team engineered a small, custom model.\nThis model was trained specifically to detect numerical bigrams within the historical tables. Crucially, they used explainable AI to verify that the model functioned as intended. By confirming that it correctly identified identical bigrams across different tables, they could trust its outputs and proceed with large-scale analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#case-study-print-diversity-and-innovation",
    "href": "chapter_ai-nepi_007.html#case-study-print-diversity-and-innovation",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.16 Case Study: Print Diversity and Innovation",
    "text": "7.16 Case Study: Print Diversity and Innovation\n\n\n\nSlide 28\n\n\nWith a trusted model in place, the team conducted case studies, including an analysis of innovation diffusion using cluster entropy. They analysed the publishing output of various European cities by clustering the representations of their printed tables. By calculating the entropy of each city’s output, they could quantify its diversity; low entropy signified a programme focused on reprinting, whilst high entropy indicated a more varied and innovative output.\nThe analysis yielded two notable low-entropy cases. It confirmed Frankfurt am Main’s known status as a reprinting hub. More significantly, it uncovered a historical anomaly in Wittenberg. The model detected an unusually low diversity in its print programme, a finding that perfectly matched historical knowledge about the active political control exerted by Protestant reformers, who strictly managed the city’s curriculum.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion",
    "href": "chapter_ai-nepi_007.html#conclusion",
    "title": "7  Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities",
    "section": "7.17 Conclusion",
    "text": "7.17 Conclusion\n\n\n\nSlide 33\n\n\nIn conclusion, this research demonstrates the successful application of bespoke AI and explainability methods to complex historical data. The work yields verifiable insights that augment traditional scholarship. Whilst the presentation concluded before detailing future challenges related to data scarcity and model capabilities, the authors’ work establishes a strong foundation for a new, computationally-assisted approach to humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Modeling Scientific Reasoning",
    "section": "",
    "text": "Overview\nIn this chapter, the authors chart a course for integrating Artificial Intelligence into scholarly research. Their work progresses from the foundational concepts of Large Language Model (LLM) evolution towards a robust framework grounded in validation and critical thinking. The authors identify key deficiencies in current AI, particularly its lack of information verification, and propose a solution centred on computational epistemology and epistemic agency.\nTo realise this vision, the team introduces a suite of purpose-built tools and platforms. The Scholarium initiative, for instance, is governed by a curated editorial board and provides access to validated historical sources, such as the collected works of Euler. A digital academic workspace, featuring an AI Cockpit, demonstrates how scholars can interact with LLMs to analyse historical documents with greater efficacy. The entire technical infrastructure rests upon the principles of Open Science Technology—embracing open source, open access, open data, and open collaboration—and exemplifies FAIR data principles through its use of platforms like Zenodo.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-evolution-of-llm-competence",
    "href": "chapter_ai-nepi_008.html#the-evolution-of-llm-competence",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.1 The Evolution of LLM Competence",
    "text": "8.1 The Evolution of LLM Competence\n\n\n\nSlide 02\n\n\nThe authors trace the evolution of competence in Large Language Models (LLMs) through three distinct conceptual stages. This progression begins with the foundational mechanism of ‘attention’, which allows models to weigh the significance of different words in a sequence. Development subsequently advanced to incorporate ‘context’, enabling LLMs to understand and process information within a broader frame of reference. The most recent stage in this evolution, as outlined by the team, introduces the capacity for ‘thinking’, signifying a move towards more sophisticated reasoning and problem-solving capabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#deficiencies-in-current-ai-systems",
    "href": "chapter_ai-nepi_008.html#deficiencies-in-current-ai-systems",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.2 Deficiencies in Current AI Systems",
    "text": "8.2 Deficiencies in Current AI Systems\n\n\n\nSlide 03\n\n\nA critical examination of current AI capabilities, conducted by the authors, reveals fundamental principles largely absent from contemporary systems. These deficiencies centre primarily on the need for robust critical thinking, a capacity that must extend beyond mere pattern recognition. Furthermore, the authors identify a significant gap in information verification, as models often generate content without a reliable mechanism for confirming its accuracy. Their analysis also highlights the inherent limitations of AI representations, underscoring that they are not infallible and demand careful scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#validation-and-computational-epistemology",
    "href": "chapter_ai-nepi_008.html#validation-and-computational-epistemology",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.3 Validation and Computational Epistemology",
    "text": "8.3 Validation and Computational Epistemology\n\n\n\nSlide 04\n\n\nThe authors posit validation as a central theme for advancing trustworthy AI. This principle, they argue, must apply not only to the propositions an AI generates but also to the actions it recommends or undertakes, ensuring both are sound and justifiable. By situating their work within the field of Computational Epistemology, the team establishes a formal framework for analysing knowledge in computational systems. This approach connects directly to the nature of Epistemic Agency, exploring how systems can responsibly and accurately acquire, assess, and use information.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#ai-assisted-document-analysis",
    "href": "chapter_ai-nepi_008.html#ai-assisted-document-analysis",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.4 AI-Assisted Document Analysis",
    "text": "8.4 AI-Assisted Document Analysis\n\n\n\nSlide 05\n\n\nThe practical application of this AI-driven approach is demonstrated within a digital academic workspace designed for scholarly research, particularly in the humanities. The user interface presents a historical document alongside an AI-powered analysis panel. In a specific use case presented by the authors, their system processes a document concerning a historical art commission. The AI successfully extracts and organises key information, identifying all individuals involved and detailing their respective roles, thereby significantly accelerating the research process.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-scholarium-platform",
    "href": "chapter_ai-nepi_008.html#the-scholarium-platform",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.5 The Scholarium Platform",
    "text": "8.5 The Scholarium Platform\n\n\n\nSlide 06\n\n\nThe Scholarium initiative, developed by the research team, provides a dedicated platform for high-integrity academic work, distinguished by its rigorous oversight. A Curated Scholarly Editorial Board governs the platform, ensuring all included sources meet stringent academic standards. Its collection encompasses the major collected works of influential historical scientists. To illustrate its function, the authors showcase a digital viewer for navigating the comprehensive works of Leonhard Euler, offering scholars direct access to this foundational scientific corpus.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-ai-cockpit-interface",
    "href": "chapter_ai-nepi_008.html#the-ai-cockpit-interface",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.6 The AI Cockpit Interface",
    "text": "8.6 The AI Cockpit Interface\n\n\n\nSlide 08\n\n\nTo streamline interaction with Large Language Models, the authors have engineered the AI Cockpit, a specialised user interface. This tool provides a focused environment for conducting complex, AI-assisted tasks. Its capabilities are demonstrated through its effective application to historical documents, where it automatically extracts salient information and generates concise summaries. This function enables scholars to grasp the core content of archival materials with remarkable speed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-principles-and-the-zenodo-repository",
    "href": "chapter_ai-nepi_008.html#fair-principles-and-the-zenodo-repository",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.7 FAIR Principles and the Zenodo Repository",
    "text": "8.7 FAIR Principles and the Zenodo Repository\n\n\n\nSlide 09\n\n\nThe authors’ commitment to open scholarship is exemplified by their use of infrastructures built upon FAIR principles, such as the Zenodo research data repository. By design, Zenodo ensures that research outputs are Findable, Accessible, Interoperable, and Reusable, thereby promoting transparency and collaboration across academic communities. As a general-purpose repository, it supports a wide range of disciplines and data types, and its active use is evident from the dynamic list of recent uploads from scholars worldwide.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#principles-of-open-science-technology",
    "href": "chapter_ai-nepi_008.html#principles-of-open-science-technology",
    "title": "8  Modeling Scientific Reasoning",
    "section": "8.8 Principles of Open Science Technology",
    "text": "8.8 Principles of Open Science Technology\n\n\n\nSlide 10\n\n\nThe technical framework supporting the authors’ scholarly ecosystem rests upon the core principles of Open Science Technology. This commitment manifests in four key areas:\n\nOpen-source software, ensuring the underlying tools are transparent and customisable.\nOpen access to publications, removing barriers to knowledge.\nOpen data, allowing for the verification of results and the reuse of datasets.\nOpen collaboration, fostering an environment where scholars can work together effectively.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Scientific Reasoning</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nIn this case study, the authors investigate the representation of the United Nations Sustainable Development Goals (SDGs) within major bibliometric databases. They employ a fine-tuned Large Language Model (LLM) to analyse and unveil systematic biases in how these databases classify scientific publications. The investigation centres on three key databases—Web of Science, Scopus, and OpenAlex—and examines the significant inconsistencies in their SDG labelling methodologies.\nTo conduct this analysis, the research team developed a specialised workflow, fine-tuning the DistilGPT2 model on a shared corpus of publications to act as an analytical probe. This approach was designed to assess the aggregate effects of classification choices on research policy and perception.\nTheir findings demonstrate that the resulting body of SDG-classified literature systematically overlooks the most disadvantaged populations, the poorest countries, and sensitive topics explicitly mentioned in SDG targets. Conversely, the literature shows a strong focus on economic superpowers and highly developed nations. The study thus highlights the performative nature of bibliometric classifications and underscores the critical impact of these seemingly objective, science-informed practices.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#the-performative-nature-of-bibliometrics",
    "href": "chapter_ai-nepi_009.html#the-performative-nature-of-bibliometrics",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 The Performative Nature of Bibliometrics",
    "text": "9.1 The Performative Nature of Bibliometrics\n\n\n\nSlide 03\n\n\nBibliometric databases occupy a critical position within the sociology of science, exerting considerable influence over the entire academic ecosystem. Their classification systems and metrics actively shape the behaviour of researchers, academics, funding bodies, and policymakers.\nThis influence, however, is not neutral. The databases themselves respond to various political and commercial interests, which imbues them with a performative nature. Rather than passively reflecting scientific activity, they actively construct and influence it.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#rationale-and-dependency-chain",
    "href": "chapter_ai-nepi_009.html#rationale-and-dependency-chain",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 Rationale and Dependency Chain",
    "text": "9.2 Rationale and Dependency Chain\n\n\n\nSlide 05\n\n\nThis case study centres on three principal bibliometric databases: Web of Science, Scopus, and OpenAlex. The authors build upon previous research that had already revealed a minimal overlap amongst publications labelled with SDGs, a discrepancy attributed to how each service drafts its search queries.\nTheir investigation examines the chain of dependencies that links the processing of metadata at the database level to its eventual impact on end-users such as researchers, consultants, and policymakers. Consequently, the authors’ primary objective was to deploy an LLM as an analytical tool. This model serves to conduct a generalised exercise, assessing the aggregate effects of these classification systems on research policy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#initial-publication-overlap",
    "href": "chapter_ai-nepi_009.html#initial-publication-overlap",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Initial Publication Overlap",
    "text": "9.3 Initial Publication Overlap\n\n\n\nSlide 08\n\n\nTo establish a comparative baseline, the authors performed a classification analysis on a shared corpus of publications jointly indexed across Web of Science, Scopus, and OpenAlex. The initial results were entirely consistent with the findings of Armitage (2020), revealing a remarkably small overlap in publications that the different databases classified under the same SDG.\nFor instance, a publication indexed in Scopus may not be tagged as relevant to SDG 5 (Gender Equality) by that platform, even whilst other databases classify it as such. Furthermore, the analysis uncovered classification anomalies. Web of Science, for example, categorises a substantial portion—around 10%—of publications under SDG 5 that originate from the field of mathematics, including topics like geometrical differential equations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning",
    "href": "chapter_ai-nepi_009.html#llm-selection-and-fine-tuning",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 LLM Selection and Fine-Tuning",
    "text": "9.4 LLM Selection and Fine-Tuning\n\n\n\nSlide 11\n\n\nThe authors developed a specific strategy for leveraging LLM technology. Their initial concept involved training a bespoke LLM exclusively on the corpus of publications classified under a given SDG by a specific database. Recognising the prohibitive resource intensity of this approach, the team adopted a practical compromise: fine-tuning an existing, open-source model.\nFor this purpose, the team selected DistilGPT2. Its basic architecture, limited parameters, and minimal pre-existing knowledge made it an ideal candidate, ensuring it held no significant prior understanding of the publication or prompt semantics. This quality contrasts sharply with larger commercial or open-source models. The fine-tuning process was designed for similarity; the model was trained using only publication titles and abstracts, where a new title serves as a prompt to generate a new abstract.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompt-generation-and-analysis",
    "href": "chapter_ai-nepi_009.html#prompt-generation-and-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Prompt Generation and Analysis",
    "text": "9.5 Prompt Generation and Analysis\n\n\n\nSlide 15\n\n\nThe authors’ research design incorporated a systematic method for generating prompts to benchmark the LLM’s performance. Recognising that each SDG comprises between eight and twelve distinct targets, the team crafted ten diverse prompts for every single target. This process yielded a specific set of 80 to 120 prompts for each SDG, designed to probe different facets of the goals.\nThe fine-tuned DistilGPT-2 model then generated responses to these prompts based on the publication sets from each bibliometric database. To ensure a robust analysis, the team employed three distinct decoding strategies for text generation: top-k, nucleus, and contrastive search. Subsequently, they applied a word filter to the generated text to extract key noun phrases for the final discussion of the results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#analysis-of-sdg-4-quality-education",
    "href": "chapter_ai-nepi_009.html#analysis-of-sdg-4-quality-education",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Analysis of SDG 4: Quality Education",
    "text": "9.6 Analysis of SDG 4: Quality Education\n\n\n\nSlide 17\n\n\nAn illustrative analysis of SDG 4 (Quality Education) reveals the system’s inherent biases. The investigation, structured across four dimensions—Locations, Actors, Data/Metrics, and Focuses—shows a clear pattern of inclusion and exclusion. The generated content frequently addresses locations such as South Africa, the U.S., Australia, and China, and actors like teachers, youth, and students in classrooms. It also references specific metrics including PISA, socioeconomic status (SES), and thematic analysis, with focuses on curriculum, performance, and English language.\nConversely, the LLM’s output systematically fails to address most other African nations, developing countries, and small island states. Critically, it overlooks vulnerable actors explicitly named in SDG targets, such as persons with disabilities, indigenous peoples, and children in vulnerable situations. Key educational focuses like vocational training, scholarships, and the promotion of a culture of peace and global citizenship are also conspicuously absent.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-omissions-across-sdgs",
    "href": "chapter_ai-nepi_009.html#systematic-omissions-across-sdgs",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 Systematic Omissions Across SDGs",
    "text": "9.7 Systematic Omissions Across SDGs\n\n\n\nSlide 19\n\n\nGeneralising their findings across the five SDGs studied, the authors reveal consistent and systematic omissions in the scientific literature as classified by the databases. A pronounced geographic bias exists; least developed countries receive scant attention, whilst the United States commands a near-monopoly of focus, followed by China, South Africa, the UK, and Australia.\nFurthermore, discriminated and vulnerable populations are systematically overlooked, a failing that persists across all analysed goals. The highlighted research methodologies tend to be general, such as thematic analysis or macroeconomic modelling. Most critically, the analysis shows that the most sensitive and challenging topics central to the SDGs—including human trafficking, exploitation, and migration—are largely absent from the discourse.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#findings-and-limitations",
    "href": "chapter_ai-nepi_009.html#findings-and-limitations",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 Findings and Limitations",
    "text": "9.8 Findings and Limitations\n\n\n\nSlide 20\n\n\nThe authors’ central finding is that using an LLM as an intermediate analytical tool starkly reveals systematic oversights within the body of scientific publications classified under the SDGs. This curated literature consistently neglects the most disadvantaged individuals, the poorest nations, and specific underrepresented topics that the SDG targets explicitly prioritise. In stark contrast, the research corpus pays full attention to economic superpowers and highly developed countries.\nThese results clearly demonstrate the decisive, shaping power of a supposedly objective practice like bibliometric classification. Nevertheless, the authors acknowledge certain limitations. Their methodology exhibits high sensitivity to the LLM’s architecture, its training data, the chosen hyperparameters, and the decoding strategy, though these factors were partly accounted for in the experimental design.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "",
    "text": "Overview\nThis chapter addresses the profound challenge of extracting accurate citation data from scholarly publications in the humanities, a domain poorly served by existing commercial bibliometric databases. Presenters from the Max Planck Institute for Legal History and Legal Theory and the Max Planck Computing & Data Facility articulate a core research problem: the inadequacy of platforms like Web of Science and Scopus for intellectual history. These databases struggle with non-English, pre-digital, and footnote-heavy literature, rendering them unsuitable for the field.\nTo surmount these obstacles, the authors propose a new methodology centred on Large Language Models (LLMs) and a robust evaluation framework. Their team developed a high-quality, TEI-annotated Gold Standard dataset from open-access humanities journals, which serves as the foundation for testing a new Python package. This package, Llamore, is a lightweight interface for LLMs to perform reference extraction. The chapter details Llamore’s architecture, its evaluation methodology, and its comparative performance. On a standard biomedical dataset (PLOS 1000), Llamore’s performance is comparable to the established tool Grobid; however, on the custom humanities dataset, it significantly outperforms Grobid, demonstrating the value of LLM-based approaches for complex, domain-specific extraction tasks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-utility-and-challenge-of-citation-graphs",
    "href": "chapter_ai-nepi_010.html#the-utility-and-challenge-of-citation-graphs",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.1 The Utility and Challenge of Citation Graphs",
    "text": "10.1 The Utility and Challenge of Citation Graphs\n\n\n\nSlide 03\n\n\nThe authors’ central objective is to construct comprehensive citation graphs that illuminate trends in intellectual history. Such graphs are powerful analytical tools, enabling scholars to uncover latent patterns in knowledge production, trace the influence of specific authors and ideas, and quantitatively measure their reception within a scholarly community. An analysis of the most cited authors over a given period, for instance, provides a clear window into a field’s intellectual evolution.\nA key challenge, however, lies in the initial data production. The team notes that a significant detour is required to generate the specialised data necessary to construct these graphs before their core research questions can ultimately be addressed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-existing-bibliometric-databases",
    "href": "chapter_ai-nepi_010.html#limitations-of-existing-bibliometric-databases",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.2 Limitations of Existing Bibliometric Databases",
    "text": "10.2 Limitations of Existing Bibliometric Databases\n\n\n\nSlide 05\n\n\nA fundamental impediment to this line of enquiry is the profound inadequacy of established bibliometric databases. For the specific field of intellectual history, the authors find that major platforms such as Web of Science and Scopus are effectively unusable due to critical data deficiencies. Their coverage of essential academic journals is incomplete, they largely neglect scholarship from the pre-digital age, and they exhibit a strong bias towards English-language content.\nBeyond these content-related failings, these commercial services are also prohibitively expensive and impose restrictive licensing terms. This fosters an undesirable dependency for the academic community, further hindering independent and comprehensive research.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#systemic-biases-and-technical-hurdles",
    "href": "chapter_ai-nepi_010.html#systemic-biases-and-technical-hurdles",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.3 Systemic Biases and Technical Hurdles",
    "text": "10.3 Systemic Biases and Technical Hurdles\n\n\n\nSlide 06\n\n\nThe authors’ analysis of the German journal Zeitschrift für Rechtssoziologie exemplifies this poor coverage, with major databases showing almost no citation data prior to the 2000s. This gap, they argue, stems from systemic biases. Commercial database providers focus on STEM, medicine, and economics—fields perceived as more profitable than the humanities. Consequently, their infrastructure is optimised for metrics like the impact factor, which holds little relevance for tracking intellectual history.\nFurthermore, a significant technical hurdle arises from the very nature of humanities scholarship. Texts are frequently replete with dense, discursive footnotes that embed citations within extensive commentary and other textual noise. The authors describe these complex structures, which frustrate conventional extraction tools, as a ‘footnote from hell’.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#from-failing-tools-to-language-models",
    "href": "chapter_ai-nepi_010.html#from-failing-tools-to-language-models",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.4 From Failing Tools to Language Models",
    "text": "10.4 From Failing Tools to Language Models\n\n\n\nSlide 09\n\n\nTraditional approaches to citation extraction have proven insufficient for these complex documents. The team found that creating the necessary training data involves a laborious manual annotation process, yet even with this investment, the performance of tools relying on established machine learning techniques like Conditional Random Forests is poor.\nIn contrast, the authors’ early experiments with Large Language Models (LLMs) yielded surprisingly effective results, with performance improving further in subsequent model generations. The advent of Visual Language Models (VLMs) now enables the direct processing of PDF documents, introducing a new frontier of methods that the research team is beginning to investigate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#the-imperative-for-a-robust-evaluation-framework",
    "href": "chapter_ai-nepi_010.html#the-imperative-for-a-robust-evaluation-framework",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.5 The Imperative for a Robust Evaluation Framework",
    "text": "10.5 The Imperative for a Robust Evaluation Framework\n\n\n\nSlide 13\n\n\nTo harness the power of LLMs responsibly, the authors contend that a robust testing and evaluation regime is non-negotiable. Their work operates on a core principle: one must not attempt to solve problems without first securing the data needed for validation.\nThis philosophy necessitates the development of a high-quality Gold Standard dataset to serve as a benchmark for accuracy. Moreover, the evaluation framework itself must be flexible enough to accommodate the fast-moving technological landscape, whilst the testing tools must generate comparable metrics. This ensures rigorous and transparent performance comparisons against other existing or future solutions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "href": "chapter_ai-nepi_010.html#developing-a-tei-annotated-gold-standard",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.6 Developing a TEI-Annotated Gold Standard",
    "text": "10.6 Developing a TEI-Annotated Gold Standard\n\n\n\nSlide 16\n\n\nThe authors selected the Text Encoding Initiative (TEI) XML format to build their Gold Standard dataset, a decision grounded in its status as the pre-eminent standard in the digital humanities. Unlike purely bibliographic formats such as CSL or BibTeX, TEI offers a far more comprehensive and well-specified framework. Its richness allows for the encoding of crucial contextual phenomena, such as the author’s intention behind a citation, which goes beyond simple reference management.\nThis choice also enables the team to tap into a wealth of existing digital editions and text corpora already encoded in TEI. Crucially, using this interoperable standard ensures compatibility with key information extraction tools like Grobid. This alignment allows for direct performance comparisons, the use of Grobid’s training data, and the contribution of their new dataset back to the Grobid project.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#constructing-the-humanities-citation-dataset",
    "href": "chapter_ai-nepi_010.html#constructing-the-humanities-citation-dataset",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.7 Constructing the Humanities Citation Dataset",
    "text": "10.7 Constructing the Humanities Citation Dataset\n\n\n\nSlide 17\n\n\nThe team is actively constructing a new dataset comprising over 1,000 footnotes sourced from 20 different articles, which are expected to yield more than 1,500 reference instances. Midway through the project, their strategy shifted towards using Open Access journals; this change ensures that the entire pipeline—from the original PDFs to the final parsed data—can be made publicly available.\nThe selected articles provide linguistic and temporal diversity, spanning multiple languages and a broad historical period. The encoding workflow proceeds in stages: first, the reference string is segmented and isolated from non-bibliographic text within the footnote, and second, this string is parsed into a structured format. Notably, the authors encode every single reference as a unique occurrence, thereby preserving the context of each citation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-a-lightweight-python-package",
    "href": "chapter_ai-nepi_010.html#llamore-a-lightweight-python-package",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.8 Llamore: A Lightweight Python Package",
    "text": "10.8 Llamore: A Lightweight Python Package\n\n\n\nSlide 20\n\n\nTo implement their LLM-based approach, the authors developed Llamore, a Python package for Large Language Models for Reference Extraction. This tool performs two primary functions: it extracts references from supplied text or PDF files, and it evaluates the performance of this extraction against a gold standard.\nTwo key objectives guided its design. First, it is lightweight, containing no models itself but rather serving as a flexible interface to a user-selected model. Second, it ensures broad compatibility with both open and closed-source LLMs and VLMs. Available via pip, Llamore’s workflow involves defining an extractor, which can connect to any OpenAI-compatible API, thereby supporting a wide range of model-serving frameworks like Olama. The extracted references can then be exported as TEI XML, and their accuracy assessed using the package’s built-in F1 evaluation class.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#evaluation-methodology-and-comparative-results",
    "href": "chapter_ai-nepi_010.html#evaluation-methodology-and-comparative-results",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.9 Evaluation Methodology and Comparative Results",
    "text": "10.9 Evaluation Methodology and Comparative Results\n\n\n\nSlide 24\n\n\nThe authors employ a rigorous evaluation methodology centred on the F1 score, a well-established metric for structured data comparison. This score is calculated from precision and recall, which are determined by counting the number of exact matches for each field within a reference. To solve the complex problem of aligning the list of extracted references with the gold standard list, the team frames it as an unbalanced assignment problem. They use a solver from the SciPy library to find the optimal one-to-one mapping that maximises the overall F1 score, penalising both missing and hallucinated references with a score of zero.\nComparative tests reveal a telling divergence in performance. On the PLOS 1000 biomedical dataset, Llamore performs on par with the highly optimised Grobid tool, albeit at a much higher computational expense. On the custom humanities dataset, however, Grobid’s performance falters, whereas Llamore demonstrates significantly superior extraction accuracy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "",
    "text": "Overview\nTo address the profound challenge of managing and comprehending vast quantities of scientific information, the authors have engineered an artificial intelligence solution comprising the Ghostwriter interface and the EverythingData backend. This system enables users to interact with a curated collection of academic papers through natural language queries, functioning as a sophisticated information retrieval tool. Its core methodology extends Retrieval Augmented Generation (RAG) by integrating knowledge graphs, a technique the team has termed ‘GraphRAG’, to deliver more contextual and accurate results.\nThe workflow commences by ingesting documents, such as a test collection of 100 articles scraped from the mda journal, into a processing pipeline. This pipeline deconstructs the full-text articles, splits them into identifiable blocks, and performs both term extraction and embedding construction. A key innovation involves linking extracted entities to the Wikidata knowledge graph, which enriches the semantic context and furnishes stable, multilingual identifiers for concepts.\nAll processed information populates a vector store managed by Quadrant, creating a queryable vector space. When a user poses a question, the system retrieves the most relevant text segments, generates a coherent summary, and crucially, provides direct references to the source documents. This design choice effectively prevents AI ‘hallucination’. The architecture supports multilingual queries and is designed to run locally on standard hardware using a compact one-billion-parameter LLM, ensuring complete user control and data privacy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-challenge-of-information-overload",
    "href": "chapter_ai-nepi_011.html#the-challenge-of-information-overload",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.1 The Challenge of Information Overload",
    "text": "11.1 The Challenge of Information Overload\n\n\n\nSlide 02\n\n\nScientific progress confronts a significant obstacle in the escalating volume of published information, a deluge that threatens to overwhelm the modern researcher. Effectively navigating this information landscape through advanced retrieval techniques is a fundamental prerequisite for generating new knowledge. This project explores how artificial intelligence can assist in this process, stemming from experimental work on complex data pipelines conducted at Dans. The authors sought to apply these technical frameworks to a practical use case, aiming to demonstrate their capabilities in a manner that is both powerful and comprehensible to a broader audience.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#research-question-and-system-design",
    "href": "chapter_ai-nepi_011.html#research-question-and-system-design",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.2 Research Question and System Design",
    "text": "11.2 Research Question and System Design\n\n\n\nSlide 03\n\n\nThe investigation centres on a precise research question: is it feasible to construct an AI solution that enables a conversational interaction with a selected corpus of academic papers? To explore this, the authors engineered a system with two principal components. Ghostwriter functions as the user interface, providing the conversational layer for posing queries. Supporting this is EverythingData, a term encompassing the entire backend architecture responsible for data ingestion, processing, and retrieval.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-approach",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-approach",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.3 The Ghostwriter Approach",
    "text": "11.3 The Ghostwriter Approach\n\n\n\nSlide 04\n\n\nThe Ghostwriter interface embodies a new paradigm for information retrieval, directly addressing the twin challenges of formulating precise questions and identifying relevant sources. The system’s design rests on a powerful metaphor that simplifies its operation: it allows a user to converse simultaneously with two conceptual entities. The ‘librarian’ persona represents the world of structured data, including formal knowledge organisation systems and established classifications. In contrast, the ‘expert’ persona embodies the domain of pure natural language. Ghostwriter’s central proposition is its ability to unify these two modes of interaction, providing a single, coherent conversational experience.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#graph-augmented-generation",
    "href": "chapter_ai-nepi_011.html#graph-augmented-generation",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.4 Graph-Augmented Generation",
    "text": "11.4 Graph-Augmented Generation\n\n\n\nSlide 05\n\n\nThe authors position their work within the scientific discourse of Retrieval Augmented Generation (RAG), a prominent technique for enhancing language models with external knowledge. For a thorough introduction to this topic, a paper by Philip Rustle from Neo4j is recommended. The system’s architecture combines two fundamental ingredients: a vector space for semantic similarity searches and a graph for representing structured relationships. This fusion results in what the team terms ‘GraphRAG’, an implementation that functions as a sophisticated reasoning engine.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-architecture",
    "href": "chapter_ai-nepi_011.html#system-architecture",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.5 System Architecture",
    "text": "11.5 System Architecture\n\n\n\nSlide 07\n\n\nThe system’s architecture processes document collections into a highly structured, queryable resource. It begins by ingesting a set of articles, for instance from the mda journal, into the EverythingData backend. This pipeline first employs the Quadrant engine to populate a vector store, then executes operations such as term extraction and the construction of semantic embeddings. A crucial innovation lies in coupling this vector store with knowledge graphs. This integration enriches the embeddings with structured relationships, adding a deeper layer of context. The final output is a sophisticated vector space, augmented with a graph structure, that users can query with natural language questions. In response, the system delivers both a generated summary and a precise list of the source documents used to formulate the answer.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#implementation-and-local-deployment",
    "href": "chapter_ai-nepi_011.html#implementation-and-local-deployment",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.6 Implementation and Local Deployment",
    "text": "11.6 Implementation and Local Deployment\n\n\n\nSlide 08\n\n\nThe authors implemented the system to be versatile, capable of processing diverse web content including academic papers and spreadsheets. It operates using a lean one-billion-parameter Large Language Model, prioritising efficiency and local deployment. A core design principle is its commitment to factual grounding; the system exclusively uses the provided source material to generate answers, thereby preventing hallucination. If the required information is absent from its knowledge base, it explicitly states its inability to answer. For demonstration, a curated collection of 100 articles from the GESIS mda journal was created via web scraping. This highlights another key feature: the system begins with no knowledge and builds its expertise only from the documents users add to its collection.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#source-referenced-answer-generation",
    "href": "chapter_ai-nepi_011.html#source-referenced-answer-generation",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.7 Source-Referenced Answer Generation",
    "text": "11.7 Source-Referenced Answer Generation\n\n\n\nSlide 09\n\n\nThe system’s ability to generate reliable, source-backed answers is central to its design. When presented with a query, such as a request to explain the ‘male breadwinner model’, it produces a synthesised response accompanied by direct references to the source documents. This mechanism effectively prevents hallucination by tying every piece of generated text to a specific origin. Technically, the authors achieve this by partitioning each paper into small, uniquely identified blocks. The retrieval process then employs a combination of LLM techniques and knowledge graph analysis to intelligently predict and retrieve the most relevant text blocks for answering the user’s question, applying weights to prioritise the best-fitting information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-knowledge-building",
    "href": "chapter_ai-nepi_011.html#iterative-knowledge-building",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.8 Iterative Knowledge Building",
    "text": "11.8 Iterative Knowledge Building\n\n\n\nSlide 10\n\n\nRather than guessing, the system explicitly communicates its limitations by stating when no direct information is available to answer a query. This transparency is complemented by a feature for iterative knowledge building. An ‘add paper’ function empowers users to expand the system’s corpus directly. Consequently, if a user finds an answer in a document not yet in the collection, they can add it. The system will then successfully answer the same question on subsequent attempts, creating a dynamic and collaborative learning loop.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#knowledge-graph-construction",
    "href": "chapter_ai-nepi_011.html#knowledge-graph-construction",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.9 Knowledge Graph Construction",
    "text": "11.9 Knowledge Graph Construction\n\n\n\nSlide 11\n\n\nThe system’s technical backbone is a multi-stage pipeline designed for robust knowledge construction. It begins with an entity extraction process that annotates key concepts within the source texts. Crucially, these entities are then linked to external knowledge graphs, which establish a ‘ground truth’ for validating information. This structure underpins the system’s powerful multilingual support, enabling a user to query documents in one language, such as German, using a different language, like English. In the final stage, the Large Language Model synthesises the various retrieved text fragments into a single, coherent summary for the user.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#hierarchical-knowledge-organisation",
    "href": "chapter_ai-nepi_011.html#hierarchical-knowledge-organisation",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.10 Hierarchical Knowledge Organisation",
    "text": "11.10 Hierarchical Knowledge Organisation\n\n\n\nSlide 12\n\n\nFact extraction begins by deconstructing a user’s query into its constituent conceptual pieces. This procedure is part of a repeatable framework the authors describe as a knowledge organisation system. By applying this process iteratively, the system can generate progressively deeper and more detailed layers of information beneath a given term. This creates a rich, hierarchical representation of concepts, allowing for nuanced exploration of the subject matter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "href": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.11 Entity Linking with Wikidata",
    "text": "11.11 Entity Linking with Wikidata\n\n\n\nSlide 13\n\n\nTo ensure consistency and enable cross-lingual capabilities, the system links extracted entities to the Wikidata knowledge base. This critical step converts potentially ambiguous text strings into stable, language-agnostic identifiers. Because each Wikidata identifier is connected to a network of multilingual translations and structured properties, the system can correctly interpret queries across different languages. The authors describe this mechanism as being analogous to a Transformer model, but one that is grounded in the structured knowledge of Wikidata.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#cross-lingual-query-expansion",
    "href": "chapter_ai-nepi_011.html#cross-lingual-query-expansion",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.12 Cross-Lingual Query Expansion",
    "text": "11.12 Cross-Lingual Query Expansion\n\n\n\nSlide 14\n\n\nThe system’s multilingual functionality is powered by a comprehensive query expansion process. For a given concept, such as the ‘bread winner model’, the system generates translations across hundreds of languages. This expanded set of multilingual terms, rather than just the original English phrase, constitutes the full query submitted to the Large Language Model. This ensures that relevant information is retrieved regardless of the language in which it was written.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#a-decoupled-knowledge-system",
    "href": "chapter_ai-nepi_011.html#a-decoupled-knowledge-system",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.13 A Decoupled Knowledge System",
    "text": "11.13 A Decoupled Knowledge System\n\n\n\nSlide 15\n\n\nA key architectural decision was to decouple the extracted knowledge from the language models that process it. By converting queries into lists of stable Wikidata identifiers, the system creates an external, model-agnostic knowledge base. This separation provides a powerful method for benchmarking; one can evaluate any LLM, including those developed in the future, by assessing its ability to generate the correct set of identifiers for a specific query. The authors propose this knowledge organisation system as a sustainable foundation for the next generation of scientific tools, an effort being pursued in collaboration with industry partners like Google and Meta.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#demonstration-and-concluding-vision",
    "href": "chapter_ai-nepi_011.html#demonstration-and-concluding-vision",
    "title": "11  Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics",
    "section": "11.14 Demonstration and Concluding Vision",
    "text": "11.14 Demonstration and Concluding Vision\n\n\n\nSlide 16\n\n\nA live demonstration showcased the system’s capabilities. A query for ‘rational choice theory’ returned a broad summary with references, whilst a more specific query about ‘utility in rational choice theory’ generated a focused answer using different information from the same sources. The system’s multilingual prowess was proven by successfully answering a question in English about a paper written almost entirely in German.\nBeyond manual use, an API enables the construction of automated agentic workflows. The overarching vision is for the tool to serve as a local, controllable resource that facilitates a dialogue with scientific literature—akin to chatting with an ‘invisible college’. Its ultimate purpose is not to replace human intellect but to provoke and support the researcher’s own thought process.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chatting with Papers - the mixed use of LLM's and semantic artifacts to support the understanding of science dynamics</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "",
    "text": "Overview\nThis chapter presents a methodological exploration into applying Retrieval-Augmented Generation (RAG) systems to enhance philosophical research. It confronts the inherent limitations of standard Large Language Models (LLMs), including their inability to access full texts, their propensity for hallucination, their constrained context windows, and their lack of source attribution. The authors propose the RAG architecture as a robust solution, integrating a specific corpus—such as the works of Aristotle or Einstein—with an LLM to furnish verifiable, context-rich answers.\nA practical case study documents the development of a RAG system that uses the Stanford Encyclopedia of Philosophy (SEP) as its knowledge base. This project reveals that a naive implementation yields poor results, necessitating significant refinement of models, hyperparameters, and algorithms like reranking. The investigation highlights the critical role of hyperparameter optimisation, particularly chunk size. For the highly structured SEP corpus, chunking by main section surprisingly outperformed more granular methods.\nThe chapter concludes that whilst RAG systems offer profound advantages for scholarly tasks by reducing hallucinations and enabling citations, their effectiveness is highly contingent on the corpus, the nature of the research questions, and rigorous evaluation by domain experts. Future work points towards developing more flexible, agentic RAG systems capable of discerning between different query types.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#scholarly-challenges-of-large-language-models",
    "href": "chapter_ai-nepi_012.html#scholarly-challenges-of-large-language-models",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.1 Scholarly Challenges of Large Language Models",
    "text": "12.1 Scholarly Challenges of Large Language Models\n\n\n\nSlide 05\n\n\nStandard Large Language Models can produce differentiated answers to complex philosophical questions, such as analysing Aristotle’s theory of matter or tracing the evolution of Einstein’s concept of locality. Their utility for deep scholarly research, however, is constrained by several fundamental problems. A Retrieval-Augmented Generation system offers a structured approach to overcome these limitations.\nThe core challenge with LLMs is their lack of direct access to full-text sources. Although trained on vast datasets, their training mechanism is explicitly designed to learn generalisable patterns of text production, not to memorise and recall texts verbatim. Consequently, when prompted for a specific quotation, an LLM might either refuse or, more problematically, hallucinate a response. Such an outcome is antithetical to philosophical research, which demands precise engagement with the fine-grained formulations of original texts.\nFurthermore, the context window of LLMs, whilst growing, remains a significant constraint. A model like ChatGPT 4.0 offers a 128,000-token context, yet this is quickly exhausted when dealing with the large corpora typical of scholarly inquiry. RAG systems directly address these issues. The architecture retrieves relevant documents from a specified data source—for instance, the complete works of a philosopher—using methods like semantic or hybrid search. These retrieved text chunks then augment the user’s prompt, feeding the LLM with the exact textual evidence needed to formulate a grounded response. This process not only provides access to the verbatim text but also solves the critical problem of attribution, as the system can cite the sources for its claims, much like the functionality seen in tools such as Perplexity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#a-case-study-in-philosophical-rag",
    "href": "chapter_ai-nepi_012.html#a-case-study-in-philosophical-rag",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.2 A Case Study in Philosophical RAG",
    "text": "12.2 A Case Study in Philosophical RAG\n\n\n\nSlide 09\n\n\nThe central ambition in applying RAG to philosophy is to facilitate a deep, interactive dialogue with scholarly corpora, such as the complete works of John Locke. This approach promises significant benefits for both didactics and advanced research. For students, it offers an intuitive pathway into complex texts, allowing them to move from broad inquiries to specific epistemological questions. For researchers, RAG systems can function as reliable tools for looking up facts, exploring unexamined corpora, and identifying key passages for close reading. The ultimate goal is for these systems to assist in answering detailed research questions directly.\nTo explore this potential, the authors developed a prototype RAG system using the Stanford Encyclopedia of Philosophy (SEP) as its knowledge base, with the initial aim of creating a useful tool for the philosophical community. The team scraped the SEP content into Markdown to serve as the data source. However, a straightforward, textbook implementation of the RAG architecture yielded surprisingly poor results; the answers were often inferior to those generated by a standalone LLM like ChatGPT. This discovery shifted the project’s focus towards rigorous optimisation.\nImproving the system’s performance necessitated a process of extensive adjustment, involving modifications to the models, their hyperparameters, and the introduction of more complex algorithms such as reranking. This optimisation work is largely a matter of trial and error, guided by theoretical principles but ultimately validated by empirical results. A significant challenge emerged in this phase: the evaluation of output quality. Unlike historical queries for atomic facts, philosophical questions elicit complex, unstructured propositions. Developing robust standards to evaluate the factual correctness of these nuanced answers is a non-trivial task that remains a key hurdle.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#system-interface-and-comparative-analysis",
    "href": "chapter_ai-nepi_012.html#system-interface-and-comparative-analysis",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.3 System Interface and Comparative Analysis",
    "text": "12.3 System Interface and Comparative Analysis\n\n\n\nSlide 25\n\n\nThe project team developed a custom frontend to facilitate interaction with and evaluation of the Stanford Encyclopedia of Philosophy RAG system. The interface, comprising a few thousand lines of code, provides users with controls to configure key hyperparameters, including prompt length and the number of documents to retrieve, alongside a field for submitting their query.\nA central feature of the design is its comparative output display, which is structured to aid analysis. On the left, the system shows a benchmark answer generated by the chosen LLM operating in isolation. On the right, it presents the corresponding answer from the RAG-augmented system. This side-by-side layout enables a direct and immediate comparison, making it easier to assess the value added by the retrieval process.\nTo ensure transparency and aid in debugging, the system’s output concludes with a comprehensive list of the texts found during the retrieval stage. This list details the source article names and the specific section headings that the system identified as relevant. Crucially, it also indicates which of these text chunks were successfully included in the final augmented prompt and which were ultimately excluded due to the LLM’s prompt length limitations, providing clear insight into the retrieval and augmentation process.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-hyperparameters-and-chunk-size",
    "href": "chapter_ai-nepi_012.html#optimising-hyperparameters-and-chunk-size",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.4 Optimising Hyperparameters and Chunk Size",
    "text": "12.4 Optimising Hyperparameters and Chunk Size\n\n\n\nSlide 32\n\n\nThe authors conducted experiments to optimise key hyperparameters, focusing specifically on the ‘chunk size’ used for text retrieval. They considered three primary chunking strategies: using a fixed number of words, a common practice in computer science; splitting the text by paragraphs; or dividing it by semantic sections.\nTo some surprise, the investigation revealed that using the main sections of the Stanford Encyclopedia of Philosophy articles as the retrieval units yielded the best results. This outcome was unexpected because the embedding model’s context limit was just over 500 words, whereas the average length of a main section in the corpus was around 3,000 words.\nThe team’s hypothesis for this result centres on the unique structure of the data source. The SEP is a highly systematised and well-ordered work. It is likely that the initial 500 words of each major section effectively summarise its core arguments, providing the embedding model with enough information to assess its relevance to a query accurately. This finding underscores a critical lesson: the optimal chunking strategy is not universal but is instead highly contingent on the specific nature of the corpus and the research questions posed. Consequently, this approach might not transfer well to more heterogeneous texts. Future plans involve experimenting with embedding models that possess longer context windows to explore potential further improvements.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#conclusion-and-future-directions",
    "href": "chapter_ai-nepi_012.html#conclusion-and-future-directions",
    "title": "12  Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights",
    "section": "12.5 Conclusion and Future Directions",
    "text": "12.5 Conclusion and Future Directions\n\n\n\nSlide 40\n\n\nRetrieval-Augmented Generation systems present clear advantages for scientific and scholarly work. They successfully integrate verbatim corpora and specialised domain knowledge, a capability that dramatically reduces the incidence of LLM hallucinations. Moreover, their ability to cite relevant source documents makes them, in principle, exceptionally well-suited to assisting with academic tasks.\nTheir practical application, however, requires caution. RAG systems are not off-the-shelf solutions; they demand careful configuration, as the appropriate settings for retrieval and generation depend heavily on the specific corpus and the kinds of questions being asked. The evaluation of their output is therefore crucial, demanding a representative set of test questions and expected answers—a task for which domain experts are indispensable.\nSeveral open challenges remain. The quality of a RAG system’s answer degrades significantly if no relevant documents are found in the retrieval phase, a scenario that necessitates careful prompt engineering. A notable, counter-intuitive finding is that these systems often perform poorly on broad, overview questions. The likely reason is that the RAG process forces the LLM to focus on the local information contained within the retrieved text chunks. This local focus can distract the model from synthesising information and adopting the wide perspective needed to answer a general query effectively.\nTo address this, future development must move towards more flexible, agentic RAG systems. Such systems would be capable of discerning between different types of questions and dynamically adjusting their information processing strategy, paving the way for more sophisticated scholarly assistants.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "",
    "text": "Overview\nThis research introduces a novel computational framework to analyse the structure of scientific fields, employing quantum gravity as its primary case study. The authors developed a dual-pronged approach to map the research landscape, contrasting a bottom-up, data-driven reconstruction with a top-down model derived from the intuition of expert physicists.\nThe bottom-up analysis draws upon a corpus of approximately 200,000 abstracts and titles from the literature of fundamental physics. It combines linguistic analysis, using the Bertopic pipeline to identify fine-grained intellectual topics, with a social network analysis of a co-authorship graph of 30,000 physicists to detect research communities. A central challenge the authors address is the inherent scale-dependency of concepts like ‘topics’ and ‘communities’. To resolve this, they implemented a hierarchical clustering method for both structures and an adaptive coarse-graining strategy guided by the Minimum Description Length (MDL) criterion. This technique optimises the topic structure by balancing its descriptive complexity against its power to explain the social network.\nFor the top-down approach, the team surveyed founding members of the International Society for Quantum Gravity to compile a list of recognised research programmes. A supervised classifier, trained on hand-coded labels, then categorised papers according to this expert-defined structure. By confronting these two perspectives, the study reveals a complex, multi-scale, and nested research landscape, rather than a simple model of ‘plural pursuit’. It confirms expert intuitions, for instance, regarding the tight integration of the string theory and supergravity communities, whilst providing a robust, scalable methodology for exploring the socio-epistemic dynamics of science.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#quantum-gravity-and-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#quantum-gravity-and-plural-pursuit",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.1 Quantum Gravity and Plural Pursuit",
    "text": "13.1 Quantum Gravity and Plural Pursuit\n\n\n\nSlide 03\n\n\nA long-standing challenge in fundamental physics centres on formulating a quantum theory of gravity, a unified framework intended to reconcile our understanding of phenomena at both the smallest and the largest scales. Physicists have proposed numerous solutions to this problem, amongst which string theory is the most prominent. To analyse a situation where many different research avenues are explored simultaneously, the authors introduce the conceptual framework of ‘plural pursuit’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.2 Defining Plural Pursuit",
    "text": "13.2 Defining Plural Pursuit\n\n\n\nSlide 04\n\n\nThe project formally defines ‘plural pursuit’ as a situation characterised by distinct yet concurrent instances of ‘normal science’ that share a common problem-solving objective; in this case, the reconciliation of quantum mechanics and gravitation. Each instance of this normal science, the authors propose, should be articulated by a distinct social community that adheres to a particular intellectual disciplinary matrix.\nThis concept synthesises established philosophical frameworks, including Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’s research programmes. Consequently, the investigation frames a central empirical question: does the field of quantum gravity research actually constitute an instance of plural pursuit, comprising independent communities that pursue different paradigms in parallel?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#a-dual-methodology-pipeline",
    "href": "chapter_ai-nepi_015.html#a-dual-methodology-pipeline",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.3 A Dual-Methodology Pipeline",
    "text": "13.3 A Dual-Methodology Pipeline\n\n\n\nSlide 05\n\n\nTo conduct their analysis, the authors gathered a substantial dataset of around 200,000 abstracts and titles from the literature of fundamental physics. They then proceeded with a two-step methodology.\nThe first step involved a linguistic analysis of the field’s intellectual structure, for which they relied on the Bertopic pipeline. This process begins by spatialising the documents into an embedding space, upon which the team performs unsupervised clustering at a very fine-grained level. Such granularity, which yields 600 distinct topics, proves essential for identifying niche research approaches that may only encompass around 100 papers. Based on this classification, each physicist can be assigned a specialty corresponding to the most frequent topic in their publications, creating a partition of authors according to the intellectual structure.\nIn parallel, the second step performs a social network analysis. The authors constructed a co-authorship graph where nodes represent 30,000 physicists and edges signify co-authorship. Applying a community detection method to this network, they identified approximately 800 distinct communities. This provides an alternative partition of the authors, this time reflecting the social structure of the field.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.4 Conceptualising Plural Pursuit",
    "text": "13.4 Conceptualising Plural Pursuit\n\n\n\nSlide 06\n\n\nWithin the analytical framework constructed by the authors, the concept of plural pursuit translates into an intuitive, idealised model: a one-to-one mapping between social communities and intellectual topics.\nIf this relationship were visualised in a correlation matrix, with communities on one axis and topics on the other, a perfect instance of plural pursuit would manifest as a clean, diagonal pattern. Such a structure would signify a clear division of labour, where each community dedicates its efforts exclusively to a single, distinct topic.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-challenge-of-scale",
    "href": "chapter_ai-nepi_015.html#the-challenge-of-scale",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.5 The Challenge of Scale",
    "text": "13.5 The Challenge of Scale\n\n\n\nSlide 07\n\n\nWhen applied directly to the fine-grained partitions of the field, the correlation matrix reveals a convoluted and complex structure that is difficult to interpret. This complexity arises from several underlying issues.\nFirstly, the level of fine-graining in the topic partition is somewhat arbitrary; a broad research area like string theory, for instance, might be scattered across numerous smaller topics. Secondly, community formation is influenced by many micro-social processes, which can result in large research programmes being pursued by several distinct communities simultaneously. These challenges point towards a more fundamental problem: the computational definitions of ‘topic’ and ‘community’ are inherently scale-dependent. Moreover, research programmes are often nested conceptually, with families and subfamilies of inquiry, further complicating any attempt at a simple, flat classification.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.6 Hierarchical Reconstruction",
    "text": "13.6 Hierarchical Reconstruction\n\n\n\nSlide 08\n\n\nTo address the issue of scale, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For the intellectual structure, they begin with the 600 fine-grained topics and progressively merge them using an agglomerative clustering technique to build a topic hierarchy.\nFor the social structure, they employ a hierarchical stochastic block model from the start, a method that learns a multi-level partition of the co-authorship network into increasingly coarse communities. These hierarchical models effectively introduce a notion of scale, enabling the system to be observed at various levels of granularity. One can, for instance, colour the co-authorship network according to topic specialties at different levels of the linguistic hierarchy. Nevertheless, the problem of arbitrariness persists, as it is not yet clear which scale should be chosen for analysing either structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-arbitrariness-of-scale",
    "href": "chapter_ai-nepi_015.html#the-arbitrariness-of-scale",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.7 The Arbitrariness of Scale",
    "text": "13.7 The Arbitrariness of Scale\n\n\n\nSlide 10\n\n\nThe freedom to select an observational scale for the topic and community hierarchies presents a significant challenge. Depending on the level of granularity chosen for each structure, the resulting correlation matrix will look markedly different. Consequently, this arbitrary choice can lead to very different narratives and conclusions about the organisation of the quantum gravity field.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#an-adaptive-coarse-graining-strategy",
    "href": "chapter_ai-nepi_015.html#an-adaptive-coarse-graining-strategy",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.8 An Adaptive Coarse-Graining Strategy",
    "text": "13.8 An Adaptive Coarse-Graining Strategy\n\n\n\nSlide 11\n\n\nTo select an appropriate analytical scale, the authors developed an adaptive topic coarse-graining strategy. The core idea is to systematically remove degrees of freedom from the fine-grained topic partition by merging topics whose linguistic distinctions have no discernible impact on scientists’ collaborative behaviour.\nThis process is guided by the Minimum Description Length (MDL) criterion, an information-theoretic principle. The MDL criterion seeks a partition that optimally balances two competing factors: its power to explain the social structure of the field and its own simplicity. In practice, the algorithm navigates the hierarchical tree of 600 topics, progressively coarse-graining the structure. It stops when adding further complexity—that is, maintaining finer topic distinctions—no longer provides a worthwhile gain in information about the social network.\nThis procedure effectively reduces the initial 600 topics to a more meaningful set of 50. Crucially, some small, niche topics are preserved because they correspond to genuine social divisions, whilst many others are consolidated into larger intellectual domains.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#labelling-the-topic-landscape",
    "href": "chapter_ai-nepi_015.html#labelling-the-topic-landscape",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.9 Labelling the Topic Landscape",
    "text": "13.9 Labelling the Topic Landscape\n\n\n\nSlide 12\n\n\nFollowing the coarse-graining procedure, the resulting 50 topics are assigned descriptive labels by retrieving representative n-grams from their constituent papers. This step renders the computationally derived clusters interpretable. With this manageable and meaningful topic landscape, the analysis can then be narrowed to focus specifically on those topics clearly related to quantum gravity research.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#mapping-topics-to-communities",
    "href": "chapter_ai-nepi_015.html#mapping-topics-to-communities",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.10 Mapping Topics to Communities",
    "text": "13.10 Mapping Topics to Communities\n\n\n\nSlide 13\n\n\nWith a refined set of topics, the analysis returns to the correlation matrix to match these intellectual structures with social communities across different scales. For each of the 50 topics, the authors identify the community level in the social hierarchy that best explains it. The results are varied.\nSome topics, such as string theory, map very well to a specific community structure, in this case at the third level of the hierarchy. In contrast, other research programmes like loop quantum gravity appear to be tied to communities at a much more fine-grained level. Furthermore, some very large topics are not associated with any single community, suggesting they represent concepts of general interest across the field.\nUltimately, the landscape does not reflect a clear case of plural pursuit. Instead, the analysis reveals nested structures—for example, a small community focused on holography that is part of the larger string theory community—and entangled scales, demonstrating a complex interplay rather than a clean division of labour.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#a-top-down-expert-led-approach",
    "href": "chapter_ai-nepi_015.html#a-top-down-expert-led-approach",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.11 A Top-Down, Expert-Led Approach",
    "text": "13.11 A Top-Down, Expert-Led Approach\n\n\n\nSlide 14\n\n\nTo complement their bottom-up analysis, the authors implemented a top-down approach grounded in expert knowledge. They surveyed the founding members of the International Society for Quantum Gravity, asking them to list the research approaches they believe structure the field.\nAlthough the experts did not all agree, their feedback was synthesised into a detailed list of programmes that partition the landscape. For the subsequent analysis, the investigation focuses on three of these: string theory, supergravity, and holography. This particular trio was selected because of an interesting disagreement amongst the physicists themselves about whether these should be treated as separate approaches. Some contend that supergravity and holography are fundamentally aspects of string theory, despite their distinct historical and conceptual origins.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-the-models",
    "href": "chapter_ai-nepi_015.html#confronting-the-models",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.12 Confronting the Models",
    "text": "13.12 Confronting the Models\n\n\n\nSlide 15\n\n\nBased on the expert-derived list, the authors trained a classifier to automatically assign papers to these top-down categories. Using the all-MiniLM-L6-v2 model on text embeddings of titles and abstracts, and training it with hand-coded labels, they could predict which papers belong to each approach.\nThe output of this supervised, top-down classification was then confronted with the results of the unsupervised, bottom-up reconstruction. This comparison yielded a key insight: the two models align well for approaches that are conceptually autonomous and well-defined. Conversely, the correspondence is poor for approaches that are more phenomenological in nature or do not represent fully-fledged conceptual frameworks. Notably, the bottom-up analysis generated a single, large string theory cluster that appears to encompass what experts separately labelled as ‘supergravity’ and ‘string theory’, thereby computationally reflecting the very ambiguity the physicists had expressed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#convergence-of-views",
    "href": "chapter_ai-nepi_015.html#convergence-of-views",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.13 Convergence of Views",
    "text": "13.13 Convergence of Views\n\n\n\nSlide 16\n\n\nThe computational findings converge with direct expert testimony. One physicist commented that the community of researchers working on supergravity as a standalone theory is likely very small. The practical overlap of personnel working on both supergravity and string theory is so large that, in their view, the two communities cannot be separated in any meaningful way.\nThis assessment perfectly mirrors the outcome of the bottom-up model. After the MDL-based procedure strips away linguistic nuances that lack social consequences, the model consolidates these two areas. This occurs despite the fact that the initial, fine-grained linguistic analysis correctly identified them as conceptually distinct clusters.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusion-computation-as-an-extension-of-philosophy",
    "href": "chapter_ai-nepi_015.html#conclusion-computation-as-an-extension-of-philosophy",
    "title": "13  Plural Pursuit: The Case of Quantum Gravity",
    "section": "13.14 Conclusion: Computation as an Extension of Philosophy",
    "text": "13.14 Conclusion: Computation as an Extension of Philosophy\n\n\n\nSlide 17\n\n\nThe research yields several key conclusions. Firstly, it demonstrates that socio-epistemic systems must be observed at multiple scales, as core concepts like ‘community’ and ‘disciplinary matrix’ are inherently scale-dependent. Secondly, identifying configurations of plural pursuit—the one-to-one mapping of communities to their intellectual foundations—necessitates methods that can match these structures across different scales.\nFor the specific case of quantum gravity, the bottom-up reconstruction of the research landscape serves to either confirm or challenge the intuitions of physicists about their own field. More broadly, this work shows how powerful computational methods can enable us to revisit and test philosophical insights, such as the nature of a paradigm, that have long relied on intuition alone. In this spirit, the presentation concludes with a paraphrase of Clausewitz: computation is the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Plural Pursuit: The Case of Quantum Gravity</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "",
    "text": "Overview\nThis research confronts a central question in computational text analysis: does effective topic modelling necessitate full-text documents, or can titles and abstracts alone provide sufficient data? The authors undertake a rigorous comparative analysis of two prominent techniques, Latent Dirichlet Allocation (LDA) and BERTopic. Applying these models to a specialised corpus on Astrobiology, the team systematically segmented the data into three distinct types: full-text documents, abstracts, and titles.\nTheir evaluation framework is twofold. A qualitative analysis explores thematic clustering and the coherence of top-words, whilst a comprehensive quantitative analysis employs four key metrics. The Adjusted Rand Index (ARI) measures model similarity, Topic Diversity assesses the uniqueness of topics, Joint Recall evaluates content coverage, and Coherence CV gauges the interpretability of the resulting themes.\nThe findings reveal a nuanced trade-off between the models and data types. BERTopic, for instance, excels in generating diverse topics, particularly from titles. Conversely, LDA models trained on full-text achieve the highest joint recall, indicating superior content coverage. The results suggest that the optimal choice of model and input data depends entirely on the specific analytical goals of the researcher, whether they prioritise thematic diversity, content coverage, or topic coherence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "href": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.1 Topic Modelling in HPSS",
    "text": "14.1 Topic Modelling in HPSS\n\n\n\nSlide 02\n\n\nTopic modelling has established itself as a significant analytical tool within the domains of History, Philosophy, and Sociology of Science (HPSS). Its utility is demonstrated across a range of applications that enhance scholarly inquiry. Scholars in these fields employ this technique to identify influential authors and papers, trace the conceptual evolution of scientific ideas over time, and map the intellectual structure of entire disciplines.\nFurthermore, topic modelling enables the discovery of previously hidden thematic connections in large corpora, the analysis of long-term trends in scientific discourse, and the comparison of distinct research programmes. This capacity for large-scale analysis also makes it an invaluable resource for conducting comprehensive literature reviews.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-methodology",
    "href": "chapter_ai-nepi_016.html#research-question-and-methodology",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.2 Research Question and Methodology",
    "text": "14.2 Research Question and Methodology\n\n\n\nSlide 03\n\n\nThe investigation centres on a fundamental question: does robust topic modelling depend on the analysis of full-text documents, or can researchers achieve comparable results using only titles or abstracts? To address this, the authors implement a formal comparative methodology.\n\n\n\nSlide 04\n\n\nThis framework systematically evaluates two distinct topic modelling approaches—the probabilistic Latent Dirichlet Allocation (LDA) and the transformer-based BERTopic. Each model is applied to three different granularities of text data: complete full-text documents, abstracts, and titles. Subsequently, the outputs from these combinations are assessed through both qualitative and quantitative analysis, providing a multi-faceted evaluation of their performance.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.3 Comparing LDA and BERTopic",
    "text": "14.3 Comparing LDA and BERTopic\n\n\n\nSlide 05\n\n\nAt their core, both Latent Dirichlet Allocation and BERTopic share common postulates; they generally rely on a bag-of-words representation and conceptualise topics as distinct distributions over a vocabulary. Nevertheless, their underlying mechanisms differ significantly. LDA is a generative probabilistic model that assumes each document is a mixture of various topics.\nIn contrast, BERTopic leverages modern transformer models to create contextual word and sentence embeddings. It then applies clustering algorithms to these rich semantic representations to identify topics. This allows it to capture nuances of meaning that frequency-based models like LDA may miss.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#qualitative-comparison-framework",
    "href": "chapter_ai-nepi_016.html#qualitative-comparison-framework",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.4 Qualitative Comparison Framework",
    "text": "14.4 Qualitative Comparison Framework\n\n\n\nSlide 06\n\n\nThe authors established a clear framework for the qualitative comparison of the models, using a specialised Astrobiology corpus as the primary dataset. Within this framework, they configured an LDA model to generate 25 distinct topics from the corpus.\nFollowing this initial modelling, these 25 topics were further organised through a clustering process into four high-level thematic groups. To visualise the interplay and connections between these themes, the team created a correlation graph, mapping the relationships between the identified topic clusters.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.5 Quantitative Metrics",
    "text": "14.5 Quantitative Metrics\n\n\n\nSlide 07\n\n\nThe quantitative evaluation relies on four distinct metrics, each chosen to assess a specific aspect of model performance. First, the Adjusted Rand Index (ARI) measures the similarity between the clustering structures produced by different topic models. Second, Topic Diversity calculates the percentage of unique words present in the top terms across all topics, providing a measure of model redundancy.\nThird, Joint Recall is used to evaluate how effectively a model trained on a text subset, such as abstracts, can retrieve the topics found in the corresponding full-text model. Finally, Coherence CV assesses the human interpretability of a topic by computing the semantic similarity of its most prominent words.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results",
    "href": "chapter_ai-nepi_016.html#results",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.6 Results",
    "text": "14.6 Results\n\n\n\nSlide 08\n\n\nThe subsequent sections present the empirical results derived from the comprehensive qualitative and quantitative analyses.\n\n14.6.1 Model Similarity via Adjusted Rand Index\n\n\n\nSlide 09\n\n\nThe authors used the Adjusted Rand Index (ARI) to quantify the similarity between the outputs of different models. The results, presented in a matrix, show that models of the same family—such as LDA models trained on abstracts versus titles—exhibit greater similarity to one another than they do to models from the other family, like BERTopic.\nThis finding indicates that the choice of algorithm (LDA versus BERTopic) has a more profound impact on the resulting topic structure than the choice of input text. Furthermore, the analysis reveals that models trained on abstracts more closely resemble their full-text counterparts than models trained on titles do, suggesting abstracts retain more of the core thematic structure.\n\n\n14.6.2 LDA Performance Across Text Types\n\n\n\nSlide 10\n\n\nAn analysis of Latent Dirichlet Allocation (LDA) performance across different text granularities reveals notable variations. Using heatmaps to visualise topic distributions, the authors compared models trained on full-text documents against those trained on only abstracts or titles. The results indicate that whilst some thematic correspondence exists, the topic structures generated from abstracts and titles frequently diverge from those derived from the full text. This suggests that relying on shorter text segments can lead to a different, and potentially less complete, thematic representation of the corpus when using LDA.\n\n\n14.6.3 BERTopic Performance Across Text Types\n\n\n\nSlide 11\n\n\nThe performance of BERTopic also varies significantly depending on the input text. Visualised through three distinct matrices, the analysis shows that the BERTopic model trained on full-text documents tends to produce a high number of small and highly specific topics. In contrast, when trained on abstracts, the model yields topics that are more stable and clearly defined. Training on titles, however, results in the formation of broader and more generalised thematic categories, demonstrating how the input data’s scope directly influences the granularity of the output.\n\n\n14.6.4 Qualitative Analysis of LDA Top-Words\n\n\n\nSlide 12\n\n\nA qualitative comparison of the top-words generated by LDA models highlights the impact of text granularity on topic interpretability. By examining the lists of top-words from models trained on full-text, abstracts, and titles, the team observed clear divergences in topic coherence and thematic focus. For instance, a distinct topic related to ‘life detection’ might appear clearly in both the full-text and abstract-based models. However, in the model trained solely on titles, the same theme could become less coherent, potentially merging with other, more general topics and losing its specific meaning.\n\n\n14.6.5 Topic Formation: LDA versus BERTopic\n\n\n\nSlide 13\n\n\nContrasting the behaviour of LDA and BERTopic reveals fundamental differences in how they construct topics from text. The analysis shows that a single, broad topic identified by an LDA model can often be resolved into several more specific and nuanced topics by BERTopic, a phenomenon known as topic splitting. Conversely, BERTopic’s ability to discern fine-grained semantic distinctions may result in multiple related topics that LDA, with its focus on word co-occurrence, merges into a single, more generalised category. These patterns of splitting and merging underscore the distinct operational logics of the two algorithms.\n\n\n14.6.6 Quantitative Results: Coherence\n\n\n\nSlide 14\n\n\nThe quantitative analysis of topic coherence, measured using the CV score, produced nuanced results. When comparing BERTopic and LDA across titles, abstracts, and full-text data, no single model or text type demonstrated consistent superiority. Instead, topic coherence appears to be highly dependent on the specific model configuration. The number of topics a user specifies is a particularly influential variable, with coherence scores for both LDA and BERTopic fluctuating significantly as this parameter changes.\n\n\n14.6.7 Quantitative Results: Diversity\n\n\n\nSlide 15\n\n\nIn the evaluation of topic diversity, a clear pattern emerged. BERTopic models consistently outperform their LDA counterparts, generating topic sets with less word overlap. Notably, the peak diversity scores were achieved when BERTopic was trained on titles alone. This finding suggests that for research goals where maximising the variety of distinct themes is paramount, the combination of the BERTopic algorithm and title-only data provides a highly effective strategy.\n\n\n14.6.8 Quantitative Results: Joint Recall\n\n\n\nSlide 16\n\n\nThe analysis of joint recall, which measures how well a model captures the themes of the entire document, yields an unambiguous result. Models trained on full-text data consistently achieve the highest recall scores. Specifically, the LDA model applied to full-text documents registered the top performance. This outcome demonstrates that for applications where comprehensive thematic coverage is the primary objective, there is no substitute for analysing the complete text of the documents.\n\n\n14.6.9 Summary of Model Performance\n\n\n\nSlide 17\n\n\nA summary matrix provides a consolidated overview of the comparative analysis. It systematically contrasts the performance of LDA and BERTopic when applied to full-text, abstract, and title data. Using a range of evaluation metrics, the matrix employs a simple visual key—filled circles—to indicate which combination of model and data input excels for each specific measure. This allows for a quick, at-a-glance assessment of the relative strengths of each approach.\n\n\n14.6.10 Performance Summary and Trade-offs\n\n\n\nSlide 18\n\n\nThe final performance summary synthesises the findings across all evaluation criteria, including overall fit, top-word quality, coherence, diversity, and joint recall. This overview uses filled circles to denote strong performance and red crosses to flag identified weaknesses. The LDA model trained on full-text, for example, is highlighted for its excellent joint recall and overall fit but shows limitations in topic diversity. Conversely, the BERTopic model trained on titles excels in producing diverse topics but at the cost of lower content coverage. This clearly illustrates a fundamental trade-off: methods that maximise topic diversity often do so at the expense of comprehensive recall, and vice versa.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusions",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusions",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels",
    "section": "14.7 Discussion and Conclusions",
    "text": "14.7 Discussion and Conclusions\n\n\n\nSlide 19\n\n\nIn conclusion, this study synthesises the distinct performance characteristics of LDA and BERTopic when applied to different sections of scholarly documents. As the similarity matrix demonstrated, the choice of algorithm is a more powerful determinant of the final topic structure than the granularity of the input text.\nResearchers seeking the most comprehensive thematic coverage should favour full-text analysis, particularly with LDA, which excels in joint recall. However, for projects prioritising the discovery of a wide and diverse range of topics, BERTopic applied to titles proves to be a superior strategy. Ultimately, the authors confirm that there is no single best approach; the optimal combination of model and data is entirely contingent on the specific objectives of the research.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "",
    "text": "Overview\nThis chapter details a novel approach for integrating explicit time awareness into Transformer-based Large Language Models (LLMs). The authors identify a core limitation in current models: they possess only an implicit, statistical understanding of time derived from their training data. This deficiency leads to an inability to resolve time-dependent contradictory information and contributes to a pronounced recency bias.\nTo address this, the team proposes the Time Transformer, an architecture that incorporates an explicit temporal dimension directly into the token embedding space. This minimal adjustment allows the model to learn the influence of time on language patterns without altering the fundamental training objective of maximising log likelihood.\nA proof of concept was developed using a small, decoder-only Transformer model built from scratch, featuring 39 million parameters. For training, the authors curated a specialised dataset of UK Met Office daily weather reports from 2018 to 2024, selected for its restricted vocabulary and repetitive linguistic structures.\nTwo experiments involving synthetically injected temporal drift demonstrated the model’s efficacy. The first involved a ‘synonymic succession’ where one word was progressively replaced by another over a year, a pattern the model successfully learned and reproduced. The second, a more complex ‘collocation fixation’, altered word co-occurrence probabilities over time, which the model also learned, as verified by analysing its internal attention mechanisms.\nWhilst the proof of concept is successful, the authors acknowledge significant challenges for broader application. These include the necessity of training new models from scratch and the extensive data curation required to assign accurate timestamps to all training sequences.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-problem-of-implicit-time",
    "href": "chapter_ai-nepi_017.html#the-problem-of-implicit-time",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.1 The Problem of Implicit Time",
    "text": "15.1 The Problem of Implicit Time\n\n\n\nSlide 03\n\n\nCurrent Large Language Models derive their understanding of time statistically, extracting implicit cues from vast training corpora. Whilst remarkably capable, this approach has inherent limitations. Models cannot easily resolve information that is contradictory without temporal context; for example, two statements identifying different dominant neural network architectures are both valid, but at different points in time. During training, these sentences compete directly for attention, forcing the model into a state where it cannot perfectly fulfil its objective because validating one statement necessitates penalising the other.\nConsequently, during inference, these models often exhibit a recency bias, favouring the most recently prevalent information. An input sequence about neural architectures will likely elicit the completion ‘Transformers’, even though ‘long short-term memories’ (LSTMs) is also present within its learned knowledge. To retrieve this older information, users must resort to prompt engineering—adjusting the input by adding a year or changing a verb’s tense. This process is imprecise and unreliable, as it depends on exploiting how the model has happened to learn temporal cues.\nA more robust solution requires models that are explicitly time-aware, capable of learning and reproducing evolving patterns as a direct function of time. To this end, the authors have developed a proof of concept that achieves this for generative language models.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "href": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.2 Formalising Temporal Dependence",
    "text": "15.2 Formalising Temporal Dependence\n\n\n\nSlide 05\n\n\nTo formalise the problem, it is necessary to consider the core function of a language model. Based on its training data, an LLM learns to estimate the probability distribution over its entire vocabulary for the next token, given a preceding sequence of tokens. It then outputs the most probable continuation.\nA critical factor, however, is that these probabilities are not static in the real world; they are inherently a function of time. For instance, the probability of the token ‘Transformers’ completing a specific sentence about neural architectures was effectively zero in 2017.\nDespite this reality, the training process typically treats the probability distributions for token sequences as static. This simplification means that when the model is later used for inference, it can only reflect the temporal drift in language patterns via in-context learning. The model’s ability to generate time-appropriate text is therefore contingent on the specific cues provided in the immediate prompt, rather than on a fundamental, built-in understanding of temporal dynamics.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-time-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#the-time-transformer-architecture",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.3 The Time Transformer Architecture",
    "text": "15.3 The Time Transformer Architecture\n\n\n\nSlide 06\n\n\nTo model time-dependent probability distributions effectively, the authors propose a new architecture, the Time Transformer, as a more efficient alternative to data-intensive methods like time-slicing. The core idea is elegant in its simplicity. Every natural language processing task begins by converting tokens into a vectorial representation, or embedding, within a latent space that is learned during training. The Time Transformer augments this process by adding a single, additional dimension to the embedding that explicitly encodes the token’s time of utterance.\nIn this model, every token in an input sequence is assigned a specific time value, meaning its vector representation will differ slightly based on when it was recorded. When these time-aware embeddings are processed by the Transformer, the resulting output probability distributions for subsequent tokens are inherently time-dependent.\nCrucially, the training objective remains the standard maximisation of log likelihood. The beauty of this approach lies in the Transformer’s innate ability to process statistical relationships; it learns precisely how much influence the temporal dimension should have on each token, allowing some words to remain stable over time whilst others change significantly.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-curation-and-pre-processing",
    "href": "chapter_ai-nepi_017.html#dataset-curation-and-pre-processing",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.4 Dataset Curation and Pre-processing",
    "text": "15.4 Dataset Curation and Pre-processing\n\n\n\nSlide 07\n\n\nTo test the Time Transformer concept, the authors required a dataset with a restricted language and a small vocabulary, which would allow a modest model to learn its linguistic patterns. They identified UK Met Office daily weather reports as an ideal candidate. These reports, available online as monthly PDFs from the UK’s national meteorological service, feature highly repetitive language. As a potential alternative, the TinyStories dataset was also noted.\nThe team scraped all daily reports from 2018 to 2024, creating a corpus of roughly 2,500 documents. The text was then chunked and tokenised using Keras TextVectorization. A simple pre-processing scheme was employed, which ignored case and punctuation and did not use subword tokenisation. This straightforward approach was sufficient for the simple language of the reports, which, across seven years of data, comprised a vocabulary of only 3,400 words.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#baseline-model-and-training",
    "href": "chapter_ai-nepi_017.html#baseline-model-and-training",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.5 Baseline Model and Training",
    "text": "15.5 Baseline Model and Training\n\n\n\nSlide 08\n\n\nBefore implementing the temporal component, the authors constructed a baseline ‘vanilla’ Transformer from scratch to confirm that a small model could learn the language patterns of the weather report dataset. This modest, decoder-only model features four decoder layers. Each layer is composed of a multi-head attention block with eight heads, followed by a normalisation layer, a non-linear feed-forward layer, and a final normalisation layer. A concluding dense layer performs the classification task of assigning probabilities across the vocabulary.\nThe resulting model is very small by modern standards, with only 39 million parameters and a file size of 150 MB. It was trained on an HPC cluster in Munich using two A100 GPUs, with each training epoch completing in just 11 seconds. After training, the model demonstrated its ability to autoregressively generate coherent and realistic weather reports from a simple seed phrase, proving it had successfully captured the linguistic patterns of the source data. The code for this model and the subsequent experiments is publicly available.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#implementing-the-time-transformer",
    "href": "chapter_ai-nepi_017.html#implementing-the-time-transformer",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.6 Implementing the Time Transformer",
    "text": "15.6 Implementing the Time Transformer\n\n\n\nSlide 10\n\n\nAdapting the baseline model into the Time Transformer requires a remarkably minimal architectural change. The implementation reserves just one dimension within the model’s 512-dimensional latent semantic space to carry temporal information. For this proof of concept, every token is assigned a non-trainable, min-max normalised value corresponding to the day of the year on which its source report was published.\nThis specific encoding—the day of the year—was deliberately chosen to exploit the seasonal variations naturally present in the weather data, such as the higher frequency of ‘snow’ in winter and ‘hot’ in summer. This design choice enables the model to directly associate linguistic patterns with cyclical time. Nevertheless, the framework is flexible, and any other form of time embedding could be integrated as needed.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-one-synonymic-succession",
    "href": "chapter_ai-nepi_017.html#experiment-one-synonymic-succession",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.7 Experiment One: Synonymic Succession",
    "text": "15.7 Experiment One: Synonymic Succession\n\n\n\nSlide 11\n\n\nThe first experiment sought to determine if the new architecture could efficiently learn a temporal drift injected into its training data. The authors designed a ‘synonymic succession’ by systematically replacing the word ‘rain’ with ‘liquid sunshine’. This replacement was governed by a sigmoid function tied to the day of the year, where the probability of replacement was zero at the beginning of the year and gradually increased to one by the end.\nAfter training on this modified dataset, the model was tasked with generating one weather prediction for every day of the year. By counting the monthly frequencies of ‘rain’ and ‘liquid sunshine’ in the output, the team confirmed the experiment’s success. The generated text, with some expected statistical variation, faithfully reproduced the engineered pattern: ‘rain’ appeared almost exclusively in the early months, ‘liquid sunshine’ dominated the later months, and the transition occurred precisely in the middle of the year.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-two-collocation-fixation",
    "href": "chapter_ai-nepi_017.html#experiment-two-collocation-fixation",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.8 Experiment Two: Collocation Fixation",
    "text": "15.8 Experiment Two: Collocation Fixation\n\n\n\nSlide 13\n\n\nSeeking a more complex challenge than simple word replacement, a second experiment was designed to test if the model could learn a change in word co-occurrence. This ‘collocation fixation’ synthetically altered the weather language over the course of the year. Specifically, instances of the word ‘rain’ not already followed by ‘and’ were replaced with the phrase ‘rain and snow’, with the probability of this replacement increasing throughout the year. From the model’s perspective, this changes the language such that by the year’s end, the appearance of ‘snow’ becomes almost entirely conditioned on the preceding token being ‘rain’.\nOnce again, the model successfully learned the injected pattern. Generated forecasts for the end of the year almost always featured ‘rain and snow’ together, whereas forecasts for the beginning of the year showed more varied patterns. Deeper analysis into the model’s internals provided further proof. The authors found that specific attention heads had specialised to detect this relationship, paying significantly more attention to the token ‘rain’ late in the year when deciding whether to generate ‘snow’.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#conclusions-and-future-challenges",
    "href": "chapter_ai-nepi_017.html#conclusions-and-future-challenges",
    "title": "15  Making Transformer-Based LLMs Time-Aware: A Proof of Concept",
    "section": "15.9 Conclusions and Future Challenges",
    "text": "15.9 Conclusions and Future Challenges\n\n\n\nSlide 14\n\n\nThe research successfully demonstrates as a proof of concept that Transformer-based LLMs can be made explicitly time-aware. This is achieved simply by adding a temporal dimension to the initial token embeddings. Whilst this opens up fascinating application possibilities, several potential next steps and significant challenges remain. One promising avenue for future work is to investigate whether the explicit time signal could actually make training more efficient, as it provides a clear signal for patterns the model would otherwise have to decipher from implicit clues.\nHowever, major hurdles exist for widespread application. Firstly, because this is a novel architecture, one cannot simply fine-tune an existing pre-trained model; it requires training from scratch, a computationally prohibitive task for large-scale models. Secondly, the approach sacrifices the simplicity of metadata-free learning. It necessitates a rigorous data curation process to assign an accurate date to every text sequence, a complex and often ambiguous task, especially for historians.\nAs a final thought, a more practical application might be to use this principle not for a full generative model, but to build a targeted, BERT-like embedder for specific, time-sensitive analytical tasks.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "",
    "text": "Overview\nThis research programme investigates the interplay between different forms of context in shaping language variation and semantic change. It operates within the Cascade project, a Marie Curie Doctoral Network dedicated to the computational analysis of semantic change across diverse environments. As a pilot study, the project examines the chemical revolution—a pivotal paradigm shift from the phlogiston theory to modern oxygen theory—using the Royal Society Corpus as its primary data source.\nTo understand how language adapts to scientific evolution, the investigation integrates principles from linguistics and information theory, including language variation, register theory, and rational communication. The team employs a multi-faceted computational toolkit to dissect this process. Previous work demonstrated the utility of Kullback-Leibler divergence for detecting periods of significant linguistic change, and cascade models, based on Hawkes processes, for identifying key innovators like Priestley and influential spreaders like Pearson.\nFurthermore, the authors used word2vec to visualise the displacement of the ‘phlogiston’ semantic space by ‘oxygen’ terminology. They also applied the concept of surprisal to model the cognitive effort associated with new terms, demonstrating how structural compression fosters more efficient terminology as concepts become established. Building on this foundation, the current project, led by PhD student Sofía Aguilar, aims to synthesise these approaches. Aguilar proposes a novel framework combining BERT embeddings with a Transformer-Graph Convolutional Network (GCN) to model the latent interactions between semantic content and contextual metadata, thereby creating a more holistic understanding of conceptual change.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-framework-for-scientific-discourse",
    "href": "chapter_ai-nepi_019.html#a-framework-for-scientific-discourse",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.1 A Framework for Scientific Discourse",
    "text": "17.1 A Framework for Scientific Discourse\n\n\n\nSlide 19\n\n\nThe authors have developed a robust theoretical framework to analyse transformations in scientific discourse, situating their work within the Cascade project. Their central objective is to model how different forms of context interact to drive semantic change. For this purpose, the project uses the chemical revolution as a pilot case study, drawing on texts from the Royal Society Corpus.\nThe investigation rests upon two key linguistic principles. First, language variation and register theory posit that situational context fundamentally shapes language use. This theory also explains how the linguistic system’s inherent flexibility permits various encodings for a concept, such as the evolution from ‘dephlogisticated air’ to the term ‘oxygen’.\nSecond, the framework incorporates principles of rational communication and information theory. These principles suggest that such variation is not arbitrary; instead, it serves as a mechanism for modulating information content. This allows speakers and writers to achieve efficient communication whilst keeping cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-change-with-kl-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-change-with-kl-divergence",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.2 Detecting Change with KL Divergence",
    "text": "17.2 Detecting Change with KL Divergence\nTo pinpoint moments of linguistic transformation, the team employs Kullback-Leibler (KL) divergence, a method that moves beyond the static comparisons of predefined periods common in traditional corpus linguistics. Instead, the authors model change as a continuous process. Their underlying assumption is that linguistic divergence increases with temporal distance; language from periods far apart should differ more than language from adjacent periods.\nIn practice, the team creates a continuous timeline of language use by calculating KL divergence repeatedly within sliding temporal bins. This dynamic process yields a diachronic comparison that highlights moments of significant transformation, visualised as peaks of divergence, and periods of stability, represented by troughs of convergence.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#tracking-lexical-and-grammatical-evolution",
    "href": "chapter_ai-nepi_019.html#tracking-lexical-and-grammatical-evolution",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.3 Tracking Lexical and Grammatical Evolution",
    "text": "17.3 Tracking Lexical and Grammatical Evolution\nThe team’s analysis of linguistic evolution extends across both lexical and grammatical levels to construct a comprehensive picture of change. At the lexical level, their diachronic comparison reveals that peaks in divergence are primarily driven by the emergence of new terminology. During the chemical revolution, for instance, scientists first wrote about experimenting with ‘air’ that was ‘dephlogisticated’ long before the concept and term ‘oxygen’ became established.\nSimultaneously, the authors track grammatical shifts by retaining function words that are often discarded in such analyses. By analysing part-of-speech (POS) trigrams, they identify grammatical patterns that co-occur with and support lexical innovation. These structures, such as adjectival patterns like ‘dephlogisticated air’ or nominal phrases like ‘oxide of iron’, also generate divergence peaks, which demonstrates that grammar plays an active role in expressing new scientific concepts.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#visualising-paradigmatic-shifts",
    "href": "chapter_ai-nepi_019.html#visualising-paradigmatic-shifts",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.4 Visualising Paradigmatic Shifts",
    "text": "17.4 Visualising Paradigmatic Shifts\nTo visualise the profound conceptual shifts occurring during the chemical revolution, the authors modelled the semantic space using the word2vec method. This approach maps the relationships between terms, offering a visual representation of the paradigmatic context.\nThe results of this analysis proved decisive. Their model showed how the established ‘phlogiston’ semantic space was systematically replaced by terminology associated with the new oxygen theory. Over time, the term ‘phlogiston’ and its related concepts occupied a progressively smaller area within the semantic map, eventually disappearing from the dominant scientific discourse.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#identifying-innovators-and-spreaders",
    "href": "chapter_ai-nepi_019.html#identifying-innovators-and-spreaders",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.5 Identifying Innovators and Spreaders",
    "text": "17.5 Identifying Innovators and Spreaders\nBeyond tracking what changes, the authors sought to identify who drives these transformations. To determine the key actors leading and disseminating new scientific terminology, the team employed cascade models, a methodology derived from Hawkes processes. This technique, originating in fields like seismology and now applied to social media analysis, models how events trigger subsequent events in a network.\nBy applying these models to the usage of chemical terms in the Royal Society Corpus, the authors could distinguish innovators from their followers. Their analysis identified distinct roles within the scientific community: Priestley emerged as a central innovator, who in turn heavily influenced George Pearson, a critical early adopter and spreader responsible for disseminating the new terminology. The models also successfully categorised other figures, such as late adopters, providing a clear map of the innovation’s diffusion.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#surprisal-effort-and-terminological-change",
    "href": "chapter_ai-nepi_019.html#surprisal-effort-and-terminological-change",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.6 Surprisal, Effort, and Terminological Change",
    "text": "17.6 Surprisal, Effort, and Terminological Change\nTo understand the underlying rationale for terminological evolution, the team’s analysis adopts a communicative perspective centred on the concept of surprisal. Surprisal quantifies the predictability of a word in a given context and is proportional to the cognitive effort required to process it; a highly surprising word is informative but difficult to process. The first mention of ‘oxygen’ by Priestley, for example, carried a high surprisal value for its contemporary readers.\nThe authors observed a distinct evolutionary pattern they term structural compression. Initially, a new technical term exhibits high surprisal. As it gains traction within the community, its usage becomes more common and its surprisal value steadily declines, eventually reaching a saturation point. This reduction in collective cognitive effort paves the way for a more compact and efficient linguistic form to emerge. Crucially, the authors note that this mechanism appears to be a specific feature of terminological development, as it does not apply to the evolution of general-purpose phrases.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-unified-framework-for-contextual-interactions",
    "href": "chapter_ai-nepi_019.html#a-unified-framework-for-contextual-interactions",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.7 A Unified Framework for Contextual Interactions",
    "text": "17.7 A Unified Framework for Contextual Interactions\nBuilding on this foundation, a new project led by Sofía Aguilar aims to create a unified framework for modelling the interaction between diverse contextual factors. Her multi-stage methodology seeks to provide a more holistic view of conceptual change. The process begins with data sampling, where Aguilar uses Kullback-Leibler divergence to identify the key terms and historical periods that define the phlogiston-oxygen debate.\nNext, during network construction, she represents the interactions that fostered the new concept. She uses BERT to create term embeddings and combines them with contextual metadata—such as authors and journals—into comprehensive node feature matrices for every 20-year period. To manage the high computational cost of this dense graph, Aguilar’s framework proposes using community detection to simplify the network structure.\nThe third stage focuses on predicting latent relationships, for which Aguilar employs a hybrid Transformer-Graph Convolutional Network (GCN). The GCN learns structural patterns from the node profiles to predict new links, whilst the Transformer’s attention mechanism identifies the most influential nodes driving interactions.\nFinally, for validation, her framework performs entity alignment. This step assesses whether the predicted relationships are meaningful by searching for isomorphic graphs or recurring network motifs over time. By identifying stable structures, Aguilar can confirm the persistence and significance of the discovered interaction patterns.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#limitations-and-future-perspectives",
    "href": "chapter_ai-nepi_019.html#limitations-and-future-perspectives",
    "title": "17  Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing",
    "section": "17.8 Limitations and Future Perspectives",
    "text": "17.8 Limitations and Future Perspectives\nThe authors conclude by outlining several critical limitations and forward-looking questions that define the future of computational conceptual history. A primary challenge is distinguishing genuine epistemic shifts from mere linguistic drift; can models truly trace the evolution of thought itself? Another open question concerns the role of context: how exactly do language models incorporate metadata, and should it be treated as a core signal of meaning or as external noise? This project proceeds on the assumption that context is a core signal.\nFurthermore, the team continues to explore the fundamental unit of language change. Are shifts best observed at the level of words, concepts, grammar, or broader discourse patterns? This leads to the question of whether it is possible to identify recurring linguistic pathways for the emergence of new concepts across different domains. Finally, as the models used to investigate these phenomena grow in complexity, the limits of their interpretability become a central concern, demanding new methods for understanding and validating their outputs.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "18.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "18.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "18.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "18.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "18.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "18  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "18.6 Conclusion and Future Trajectories",
    "text": "18.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]