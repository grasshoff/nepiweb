[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held April 2-4, 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  NEPI Workshop Content Overview",
    "section": "",
    "text": "Overview\nThis chapter documents the operational and academic architecture of the workshop, detailing essential logistics alongside the intellectual focus of its keynote sessions. The discussion begins with the recording policy for all sessions, including the specific equipment employed and the consent-based protocol for disseminating talks via NEPI’s YouTube channel.\nFollowing this, the chapter provides practical guidance on the social programme, outlining the designated venues for coffee breaks, lunch, and the main reception. The academic core of the workshop is then introduced through two keynote addresses.\nThe first, from Pierluigi Cassotti and Nina Tahmasebi of the University of Gothenburg, explores large-scale text analysis for studying cultural and societal change. Their work centres on semantic change detection and the application of data science within the humanities. The second keynote, delivered by Iryna Gurevych from the Technical University of Darmstadt, investigates the challenge of elevating Natural Language Processing to a cross-document level. Her presentation covers information extraction, semantic processing, and machine learning, with a particular focus on their utility in the social sciences and humanities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NEPI Workshop Content Overview</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols-and-dissemination",
    "href": "chapter_ai-nepi_001.html#recording-protocols-and-dissemination",
    "title": "2  NEPI Workshop Content Overview",
    "section": "2.1 Recording Protocols and Dissemination",
    "text": "2.1 Recording Protocols and Dissemination\n\n\n\nSlide 02\n\n\nThe organisers established a transparent recording protocol, clearly communicated to participants through on-screen notifications and during the registration process. The technical configuration is precise: a single camera remains fixed on the presenter, whilst four microphones capture high-quality audio. An iPhone serves as a supplementary backup device to ensure a complete record.\nA cornerstone of this policy is presenter autonomy. NEPI will only upload recordings of talks and their subsequent discussions to its YouTube channel after securing explicit consent from each speaker. Attendees with questions or concerns regarding consent were encouraged to speak directly with the organising team.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NEPI Workshop Content Overview</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#social-programme-and-venues",
    "href": "chapter_ai-nepi_001.html#social-programme-and-venues",
    "title": "2  NEPI Workshop Content Overview",
    "section": "2.2 Social Programme and Venues",
    "text": "2.2 Social Programme and Venues\n\n\n\nSlide 04\n\n\nThe workshop’s social programme is designed to foster community and collegial exchange. A vibrant, comic-style illustration, evoking the communal feasts of the Asterix series, sets a convivial tone for these gatherings.\nTo facilitate networking and refreshment, the organisers have arranged specific venues for all social events. Coffee and other refreshments will be served in H 3005. The adjacent H 2051 will host both the lunch breaks and the evening reception, providing a central hub for informal discussion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NEPI Workshop Content Overview</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-analysing-cultural-change-with-large-scale-text",
    "href": "chapter_ai-nepi_001.html#keynote-analysing-cultural-change-with-large-scale-text",
    "title": "2  NEPI Workshop Content Overview",
    "section": "2.3 Keynote: Analysing Cultural Change with Large-Scale Text",
    "text": "2.3 Keynote: Analysing Cultural Change with Large-Scale Text\n\n\n\nSlide 05\n\n\nPierluigi Cassotti and Nina Tahmasebi of the University of Gothenburg deliver the first keynote, titled Large-scale text analysis for the study of cultural and societal change. Their presentation articulates a central theme: that ‘Change is Key!’ to understanding our world through computational methods.\nTheir research delves into the technical domain of semantic change detection, where they employ computational techniques to trace shifts in word meanings across vast textual archives. This work involves establishing robust benchmarks to standardise the evaluation of such analytical models. Ultimately, Cassotti and Tahmasebi demonstrate how data science methodologies can be powerfully applied within the humanities, forging a vital link between computational precision and humanistic inquiry.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NEPI Workshop Content Overview</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-elevating-nlp-to-the-cross-document-level",
    "title": "2  NEPI Workshop Content Overview",
    "section": "2.4 Keynote: Elevating NLP to the Cross-Document Level",
    "text": "2.4 Keynote: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 06\n\n\nIn the second keynote, Iryna Gurevych addresses the complex challenge articulated in her title: How to InterText? Elevating NLP to the cross-document level. Representing the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University of Darmstadt, she brings deep expertise to this field.\nProfessor Gurevych’s research programme integrates information extraction, semantic text processing, and machine learning. These pillars form the foundation for developing more sophisticated Natural Language Processing systems capable of analysing information across multiple documents. Her work underscores the profound potential of these advanced computational methods, particularly through their practical application in the social sciences and humanities, thereby highlighting their interdisciplinary value.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NEPI Workshop Content Overview</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter provides a comprehensive primer on Large Language Models (LLMs), charting their foundational architectures, adaptation for scientific domains, and specific applications within the History, Philosophy, and Sociology of Science (HPSS). It begins by introducing the Transformer, a core neural network architecture that underpins modern LLMs. The discussion then differentiates between encoder-based models like BERT, which excel at bidirectional contextual understanding, and decoder-based models such as GPT, renowned for their generative capabilities.\nThe chapter traces the rapid evolution of scientific LLMs, highlighting a proliferation of domain-specific models since 2018. It meticulously outlines key adaptation methods, including pre-training, continued pre-training, and prompt-based techniques. A significant focus is placed on Retrieval-Augmented Generation (RAG), a multi-model pipeline that enhances LLM responses by integrating retrieved information.\nFurthermore, the analysis categorises LLM applications in HPSS into four distinct areas: managing data and sources, analysing knowledge structures, understanding knowledge dynamics, and examining knowledge practices. Finally, the chapter addresses HPSS-specific challenges, such as the historical evolution of language and the need for critical, reconstructive reading. It advocates for greater LLM literacy and shared datasets, whilst championing the integration of computational methods without compromising core HPSS methodologies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#agenda-and-scope",
    "href": "chapter_ai-nepi_003.html#agenda-and-scope",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.1 Agenda and Scope",
    "text": "3.1 Agenda and Scope\n\n\n\nSlide 02\n\n\nThis chapter outlines an agenda to deliver a foundational primer on Large Language Models and their adaptation to scientific domains. It further intends to summarise current applications within the History, Philosophy, and Sociology of Science (HPSS) and to share critical reflections for wider discussion. Recognising a heterogeneous audience, the text prioritises accessibility whilst delivering necessary technical insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.2 The Transformer Architecture",
    "text": "3.2 The Transformer Architecture\n\n\n\nSlide 03\n\n\nThe foundational Transformer architecture, which underpins all contemporary Large Language Models, was introduced by Vaswani and colleagues in their 2017 paper, Attention is all you need. Initially conceived for language translation, this model operates through two interconnected streams: an encoder and a decoder. The encoder processes an entire input sentence simultaneously, allowing each word to interact bidirectionally with every other, thereby constructing a full contextual representation.\nConversely, the decoder generates the output sentence sequentially, with each new word able to access only its predecessors. This unidirectional flow is essential for coherent text generation. Each word produced by the decoder is then fed back into the input stream, creating a loop that continues until the target sentence is complete. Following the Transformer’s introduction, its individual encoder and decoder streams were re-engineered to develop distinct families of pre-trained language models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#encoder-only-models",
    "href": "chapter_ai-nepi_003.html#encoder-only-models",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.3 Encoder-Only Models",
    "text": "3.3 Encoder-Only Models\n\n\n\nSlide 04\n\n\nFocusing on the encoder component of the Transformer architecture reveals its function in transforming linguistic input into numerical representations. The process begins as words undergo input embedding, converting discrete tokens into continuous vectors. Positional encoding is then appended to provide vital information about the tokens’ sequential order, a crucial addition given the Transformer’s non-sequential processing nature.\nThis combined input then feeds into a stack of identical encoder layers. Each layer comprises two primary sub-layers: a multi-head attention mechanism, which enables the model to attend to information from diverse representational subspaces concurrently, and a simple feed-forward network. Both sub-layers are followed by an ‘add and norm’ step, incorporating residual connections and layer normalisation essential for training deep networks. This architecture aims to develop models capable of profound language understanding, which can then be applied to various Natural Language Processing tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#bert-bidirectional-understanding",
    "href": "chapter_ai-nepi_003.html#bert-bidirectional-understanding",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.4 BERT: Bidirectional Understanding",
    "text": "3.4 BERT: Bidirectional Understanding\n\n\n\nSlide 05\n\n\nThe encoder side of the Transformer architecture gave rise to the BERT family of models, which remain highly prevalent. BERT, an acronym for Bidirectional Encoder Representations from Transformers, operates on the principle that each word within an input stream can interact with every other word. This bidirectional interaction enables the model to construct a comprehensive, full-context understanding of the entire input sequence simultaneously.\nThe term ‘bidirectional’ signifies that words can consider context from both preceding and succeeding tokens, whilst ‘encoder-based’ denotes its derivation from the original Transformer encoder. Devlin and colleagues pioneered this model in 2018. Its architecture holds significant promise for applications as LLMs within HPSS, particularly when engaging with specialised vocabularies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#gpt-unidirectional-generation",
    "href": "chapter_ai-nepi_003.html#gpt-unidirectional-generation",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.5 GPT: Unidirectional Generation",
    "text": "3.5 GPT: Unidirectional Generation\n\n\n\nSlide 06\n\n\nConversely, the decoder side of the Transformer architecture forms the basis for Generative Pre-trained Transformer (GPT) models, which power contemporary systems such as ChatGPT. These models are inherently unidirectional, capable of examining only preceding tokens. This structural characteristic, however, enables them to generate novel text, a capability largely absent in BERT-like models.\nConsequently, GPT and BERT models serve fundamentally different purposes: GPT excels at language generation, whilst BERT specialises in coherent sentence understanding. Beyond these two primary distinctions, the field also encompasses models combining encoder-decoder functionalities, alongside sophisticated methods for employing decoders in an encoder-like fashion, exemplified by architectures such as XLM and XLNet. Radford and colleagues introduced the foundational GPT model in 2018.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-evolution-of-scientific-llms",
    "href": "chapter_ai-nepi_003.html#the-evolution-of-scientific-llms",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.6 The Evolution of Scientific LLMs",
    "text": "3.6 The Evolution of Scientific LLMs\n\n\n\nSlide 07\n\n\nA comprehensive overview reveals the substantial evolution of Large Language Models, particularly those tailored for scientific domains. The landscape exhibits a greater proliferation of encoder-based (BERT-type) models compared to decoders, indicating considerable developmental activity on the encoder side. Early, popular models in this scientific context included BioBERT, Specter, and Cyber.\nThe field has since expanded to encompass a diverse array of domain-specific models, catering to areas such as biomedicine, chemistry, material science, climate science, and social science. For researchers in HPSS, this trend suggests a growing opportunity either to leverage existing models or to develop bespoke architectures pertinent to their specific research needs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#domain-and-task-adaptation",
    "href": "chapter_ai-nepi_003.html#domain-and-task-adaptation",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.7 Domain and Task Adaptation",
    "text": "3.7 Domain and Task Adaptation\n\n\n\nSlide 08\n\n\nAdapting Large Language Models to specific scientific language necessitates several distinct approaches. Initial pre-training involves the model learning language by either predicting the next token, as seen in GPT models, or by predicting randomly masked words, characteristic of BERT models. This foundational stage, however, demands prohibitive computational resources.\nA more accessible strategy involves continued pre-training, where an existing model is further trained on domain-specific language. Alternatively, researchers can add extra layers atop pre-trained models, fine-tuning them to function as classifiers for specific tasks. Contrastive learning also emerges as a crucial technique for generating sentence or document embeddings from existing word embeddings, thereby facilitating similarity comparisons. Sentence BERT, a widely adopted model, exemplifies this approach.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.8 Retrieval-Augmented Generation (RAG)",
    "text": "3.8 Retrieval-Augmented Generation (RAG)\n\n\n\nSlide 09\n\n\nRetrieval-Augmented Generation (RAG) represents a sophisticated pipeline rather than a singular model, orchestrating the action of at least two distinct models. The process commences when a user query is encoded into a sentence embedding by a BERT-type model. This model then searches a document database, retrieving the most semantically similar passages.\nThese retrieved passages are then integrated into the prompt of a generative model, which formulates a comprehensive answer based on this newly augmented context. The RAG framework is now commonplace in contemporary LLM applications, exemplified by ChatGPT’s ability to search the internet. More broadly, advanced reasoning agents are not merely single LLMs but intricate systems that combine LLMs with a diverse array of other computational tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#a-timeline-of-scientific-llms",
    "href": "chapter_ai-nepi_003.html#a-timeline-of-scientific-llms",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.9 A Timeline of Scientific LLMs",
    "text": "3.9 A Timeline of Scientific LLMs\n\n\n\nSlide 10\n\n\nA comprehensive timeline documented by Ho and colleagues in 2024, spanning from 2018 to 2024, illustrates the rapid proliferation of pre-trained LLMs engineered for scientific texts. The timeline commences in 2018 with the seminal BERT model. Subsequent years witnessed a significant expansion: 2019 introduced FLAIR; 2020 saw BioFLAIR, G-BERT, BioELMo, and RoBERTa. By 2021, T5, GPT-2, RadBERT, SciBERT, and ClinicalBERT had emerged.\nThe year 2022 marked a substantial increase across all categories, with models such as SciFive, BioBART, MedGPT, SciGPT2, and a multitude of encoders including CovidBERT, SPECTER, NukeBERT, and BioBERT. Development accelerated in 2023, introducing BioReader, DRAGON, BioMedGPT, Clinical-T5, Galactica, BioGPT, and an extensive array of encoders like SciEdBERT, MatSciBERT, ChemBERT, PubMedBERT, MathBERT, ClimateBERT, and ProteinBERT. The timeline extends into 2024 with GIT-Mol, Patton, and MOTOR, underscoring the dynamic and expanding landscape of scientific LLMs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#adaptation-approaches-in-detail",
    "href": "chapter_ai-nepi_003.html#adaptation-approaches-in-detail",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.10 Adaptation Approaches in Detail",
    "text": "3.10 Adaptation Approaches in Detail\n\n\n\nSlide 11\n\n\nAdapting Large Language Models to specific domains and tasks primarily involves four distinct approaches. These methods allow for the customisation of general-purpose models to meet the nuanced demands of specialised fields like HPSS.\nThe main approaches are:\n\nPre-training, which focuses on refining the model’s vocabulary for domain-specific queries, often through continuous training on relevant textual corpora.\nExtra Parameters, which facilitates task-specific adaptation by adding dedicated layers to the model for functions like sentiment analysis or named entity recognition.\nPrompt-Based methods, which guide the LLM’s behaviour through carefully constructed prompts for tasks like masked language modelling or term definition.\nContrastive learning, which generates robust representations by comparing inputs, proving invaluable for information retrieval and semantic search.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#llm-applications-in-hpss",
    "href": "chapter_ai-nepi_003.html#llm-applications-in-hpss",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.11 LLM Applications in HPSS",
    "text": "3.11 LLM Applications in HPSS\n\n\n\nSlide 12\n\n\nA recent survey identifies four principal categories for the application of Large Language Models within HPSS research.\n\nDealing with data and sources: This includes parsing and extracting information like publication types or citations, alongside interactive engagement with sources through summarisation or RAG-type conversational interfaces.\nKnowledge structures: This focuses on identifying and mapping elements within knowledge domains, such as scientific instruments or chemicals, and delineating relationships between disciplines.\nKnowledge dynamics: This addresses the evolution of knowledge over time, facilitating the study of conceptual histories and the identification of novelty, such as breakthrough papers.\nKnowledge practices: This examines the social dimensions of knowledge production, involving argument reconstruction, citation context analysis, and discourse analysis.\n\nNotably, there is an accelerating interest in LLMs within HPSS, with findings increasingly appearing in journals traditionally less amenable to computational methods. This trend is attributed to the semantic power of these models, which appeals to qualitative researchers and philosophers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#key-distinctions-and-challenges",
    "href": "chapter_ai-nepi_003.html#key-distinctions-and-challenges",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.12 Key Distinctions and Challenges",
    "text": "3.12 Key Distinctions and Challenges\n\n\n\nSlide 13\n\n\nThe application of LLMs exhibits a spectrum of customisation, from off-the-shelf usage of tools like ChatGPT to the development of novel architectures. Despite this versatility, several recurring concerns persist, including the demand for significant computational resources, the opaqueness of complex models, and a lack of sufficient training data. Researchers consistently encounter trade-offs between model types, underscoring the principle that no single model serves all purposes; selecting the adequate model for a specific objective remains paramount.\nEncouragingly, a discernible trend towards increased accessibility is evident, exemplified by tools like BERTTopic for topic modelling. Fundamentally, understanding key distinctions proves vital: differentiating between encoder, decoder, and encoder-decoder architectures; recognising the various fine-tuning strategies; and appreciating the profound difference between word and sentence embeddings.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#hpss-challenges-and-future-directions",
    "href": "chapter_ai-nepi_003.html#hpss-challenges-and-future-directions",
    "title": "3  Large Language Models in History, Philosophy, and Sociology of Science",
    "section": "3.13 HPSS Challenges and Future Directions",
    "text": "3.13 HPSS Challenges and Future Directions\n\n\n\nSlide 14\n\n\nApplying LLMs within HPSS presents unique challenges. The historical evolution of concepts and language poses a significant hurdle, as models trained on modern data may introduce anachronistic biases. Furthermore, HPSS adopts a reconstructive and critically reflective perspective, necessitating an ability to read between the lines and discern subtle discursive strategies for which current LLMs are not inherently equipped.\nTo address these issues, several recommendations emerge. Cultivating LLM literacy is paramount to prevent the uncritical adoption of off-the-shelf tools. The HPSS community should also actively develop shared datasets and benchmarks tailored to its specific needs. Whilst translating HPSS problems into Natural Language Processing tasks, researchers must remain steadfast in their methodological focus.\nNevertheless, these models present novel opportunities for bridging qualitative and quantitative research. Reflecting upon HPSS’s own pre-history, particularly methods like co-word analysis pioneered by scholars such as Callon and Rip in the 1980s, offers valuable insights into theoretically informed tool development for contemporary computational endeavours.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models in History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila have developed OpenAlex Mapper, an innovative tool designed to mitigate critical generalisation and validation challenges pervasive in History, Philosophy, and Sociology of Science (HPSS) research. This presentation introduces the tool, clarifies its technical underpinnings, demonstrates its interactive capabilities, and examines its manifold applications within transdisciplinary contexts.\nThe methodology centres on fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, the team sampled 300,000 English abstracts from the OpenAlex database, a comprehensive and openly accessible repository of scholarly material. Engineers embedded these abstracts and reduced their dimensionality to two dimensions using Uniform Manifold Approximation and Projection (UMAP), thereby creating a foundational 2D base map. OpenAlex Mapper then allows users to submit arbitrary queries, downloading and embedding the corresponding records before projecting them onto this pre-trained UMAP model.\nThe interactive map facilitates in-depth investigation of specific terms, authors, temporal distributions, and citation networks. Significantly, the tool provides a rigorous quantitative framework that grounds qualitative, heuristic investigations, enabling researchers to trace the diffusion of models, map the distribution of concepts, and analyse method usage patterns across vast interdisciplinary samples. Examples include tracking the Hopfield model’s adoption, visualising model templates like Ising and Sherrington-Kirkpatrick, and contrasting the spatial distribution of concepts such as “phase transition” and “emergence,” alongside methods like Random Forest and Logistic Regression.\nDespite its utility, the system acknowledges several qualifications. It relies on the OpenAlex database, which, whilst robust, is not without imperfections, particularly concerning disciplinary representation. The current language model processes English-only sources, and the embedding step necessitates the presence of abstracts or well-formed titles. Furthermore, the UMAP algorithm, a stochastic process, introduces inherent trade-offs in dimensionality reduction, meaning the 768 dimensions of the Specter model cannot be perfectly represented in two, leading to potential misalignments. A working paper, Philosophy at Scale: Introducing OpenAlex Mapper, offers more detailed technical insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper Architecture and Workflow",
    "text": "4.1 OpenAlex Mapper Architecture and Workflow\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila crafted OpenAlex Mapper, a novel tool funded by an ERC grant focused on “possible life.” This innovation aims to introduce the tool, clarify its high-level technical operations, demonstrate its practical application, and ultimately discuss its utility for research within the History, Philosophy, and Sociology of Science (HPSS).\nThe core workflow of OpenAlex Mapper comprises several distinct stages. Initially, the team fine-tuned the Specter 2 embedding model, specifically to enhance its recognition of disciplinary boundaries. This process involved training the model on a dataset of articles originating from highly similar disciplinary backgrounds, with UMAP dimensionality reduction providing a visualisation of this training. Notably, these adjustments constituted minor modifications to the language model, rather than a comprehensive retraining effort.\nSubsequently, for base-map preparation, the researchers leveraged the OpenAlex database, a vast and inclusive repository of scholarly material that surpasses the scale of Web of Science or Scopus. OpenAlex offers fully open data, facilitating easy batch querying and free accessibility, which distinguishes it from many proprietary alternatives. From this extensive database, the team sampled 300,000 random articles, imposing minimal restrictions beyond requiring reasonably well-formed English abstracts. These abstracts then underwent embedding using the previously fine-tuned Specter 2 model. Engineers further reduced these embeddings to two dimensions through Uniform Manifold Approximation and Projection (UMAP), yielding both a 2D base map and a trained UMAP model.\nFor individual user queries, OpenAlex Mapper allows submission of arbitrary searches to the OpenAlex database. The tool downloads the relevant records—for instance, the first 1,000 for demonstration purposes—and embeds their abstracts using the identical fine-tuned language model. These new embeddings are then projected through the pre-trained UMAP model, ensuring that the queried articles acquire positions on the two-dimensional map consistent with their hypothetical presence during the original layout process. The resulting interactive map is accessible online and available for download via data mappers, offering features such as temporal distributions and citation graph overlays. Users can access the slides and interactive tool via maxnoichl.eu/talk, whilst a version with a higher latency GPU setup is also available for processing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration of OpenAlex Mapper",
    "text": "4.2 Interactive Demonstration of OpenAlex Mapper\nThe OpenAlex Mapper tool, accessible via https://m7n-openalex-mapper.hf.space, offers a straightforward user experience. Users initiate their investigation by searching the OpenAlex database directly through its comprehensive search interface, for example, by entering a query such as “scale-free network models.”\nIn the backend, the system efficiently downloads the initial 1,000 records pertinent to the search query, a limit imposed to optimise processing time. Subsequently, it embeds all abstracts from these downloaded records. If the user enables the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as a projection of the search results onto a pre-existing grey base map, visually representing the disciplinary landscape.\nCrucially, the map is fully interactive, empowering users to delve into specific data points. For instance, one can investigate the presence of a term like “coriander” within unexpected fields such as epidemiology or public health, gaining nuanced insights into interdisciplinary connections. The demonstration showcased queries for both “coriander,” a standard OpenAlex example, and “scale-free network models.” Furthermore, the developers have made an alternative setup available, featuring a higher latency GPU, which accommodates larger and more complex queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper primarily addresses the persistent challenges of generalisation and validation that arise from the reliance on small samples and case studies within History, Philosophy, and Sociology of Science (HPSS). Whilst traditional HPSS methods—such as the close reading of scholarly papers, direct interaction with scientists, and studies conducted by researchers with scientific training—offer invaluable detailed, close-up views of scientific processes, they often struggle to scale. Generalising these granular insights to the vast, global, and rapidly evolving landscape of contemporary science presents a significant hurdle.\nOpenAlex Mapper contributes by providing rigorous quantitative methods that effectively ground qualitative, heuristic investigations. A key feature of the tool is its capacity to trace all analytical results directly back to their original textual sources, ensuring transparency and scholarly rigour.\nThe tool supports several specific applications, offering compelling examples of its utility. Researchers can trace the diffusion of particular models, such as the Hopfield model, to ascertain where it genuinely “stuck” or achieved widespread adoption and sustained reference across diverse scientific domains. Furthermore, the system facilitates the investigation of “model templates”—conceptual frameworks defining models of similar structure that emerge in disparate scientific fields, potentially structuring science in ways orthogonal to established disciplines. Examples like the Ising, Hopfield, and Sherrington-Kirkpatrick models often appear at specific, non-continuous locations on the base map, providing crucial insights for ongoing debates concerning model transfer in science.\nBeyond models, OpenAlex Mapper enables the mapping of concept distribution. For instance, it can visually contrast the spread of “phase transition” (depicted in blue) with “emergence” (in orange), broadening such analyses into interdisciplinary contexts and circumventing common problems associated with acquiring specific datasets. Finally, the tool proves invaluable for analysing method usage. It reveals distinguishable patterns of specific methods within interdisciplinary contexts; for example, neuroscientists frequently employ Random Forest algorithms, whilst researchers in psychiatry or mental health often utilise Logistic Regression. This observation prompts profound philosophical questions regarding the underlying reasons for these patterns and their implications for debates on machine learning in science versus classical statistics, and the concept of “theory-free science.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Considerations",
    "text": "4.4 Limitations and Future Considerations\nWhilst OpenAlex Mapper offers significant analytical capabilities, its application is subject to several important qualifications. The system’s efficacy inherently depends on the OpenAlex database, which, despite its overall reasonable data quality compared to other available sources, is not without imperfections. Notably, certain disciplines, such as law, may exhibit underrepresentation within the database, potentially skewing comprehensive analyses.\nThe current language model processes English-only sources, which somewhat limits the tool’s global scope. Nevertheless, this constraint poses less of a problem for investigations focused on the more recent history of science. In principle, the integration of multilingual models could remedy this limitation, although the availability of high-quality, science-trained multilingual models remains scarce. Furthermore, the embedding step of the methodology necessitates that sources include either abstracts or well-formed titles, thereby restricting the range of processable data.\nCrucially, the method relies heavily on the Uniform Manifold Approximation and Projection (UMAP) algorithm, which presents its own set of imperfections. As a stochastic algorithm, UMAP generates one specific output amongst many possible configurations. Moreover, the algorithm must make inherent trade-offs during dimensionality reduction; the 768 dimensions of the Specter language model cannot be perfectly compressed into two, inevitably leading to some degree of “pushing and pulling and misaligning” of data points.\nFor those seeking further information, the presentation slides are available online at maxnoichl.eu/talk. Additionally, a working paper, titled Philosophy at Scale: Introducing OpenAlex Mapper, provides more exhaustive technical details regarding the tool’s development and operation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "",
    "text": "Overview\nThe ActDisease project at Uppsala University represents a focused initiative to advance genre classification within historical medical periodicals. The project team has developed automated methods for categorising diverse textual content, thereby enhancing the accessibility of these archives for digital humanities research. Their work involves the meticulous collection and digitisation of patient organisation magazines, a process that confronts significant challenges such as Optical Character Recognition (OCR) errors and the complex layouts inherent in historical documents.\nEmploying a range of machine learning techniques, including zero-shot and few-shot learning with multilingual encoders, the team explored the efficacy of different models. Their experiments reveal that historical mBERT, in particular, accurately classifies genres such as academic reports, advertisements, and personal narratives. Ultimately, this research underscores the complexity of text mining popular magazines due to their multifaceted genre composition, whilst demonstrating that robust genre classification can render these rich historical sources amenable to advanced textual analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#presentation-outline",
    "href": "chapter_ai-nepi_005.html#presentation-outline",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.1 Presentation Outline",
    "text": "5.1 Presentation Outline\n\n\n\nSlide 02\n\n\nA postdoctoral researcher from the ActDisease project at Uppsala University presents a procedural report detailing experiments in genre classification for historical medical periodicals. This presentation systematically covers the project’s background, the methodology employed, the experiments conducted, and the resultant findings. The structured outline guides the audience through three principal sections: an introduction to ActDisease, a comprehensive review of genre classification experiments, and a concluding summary of the research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.2 The ActDisease Project",
    "text": "5.2 The ActDisease Project\n\n\n\nSlide 03\n\n\nThe ActDisease project, formally titled ActDisease (Acting out Disease) - How Patient Organisations Shaped Modern Medicine, is an ERC-funded research endeavour. This initiative meticulously investigates the history of patient organisations across 20th-century Europe. Its primary objective is to scrutinise how these organisations influenced the evolution of disease concepts, patient illness experiences, and medical practices.\nThe project’s scope encompasses ten distinct European patient organisations from Sweden, Germany, France, and Great Britain, spanning the period from approximately 1890 to 1990. Periodicals from these organisations, predominantly magazines, serve as the main source material for this extensive historical inquiry. A pertinent historical example includes the Hay Fever Association of Heligoland, founded in 1897, which exemplifies the project’s focus on the origins and impact of such organisations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-dataset",
    "href": "chapter_ai-nepi_005.html#the-actdisease-dataset",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.3 The ActDisease Dataset",
    "text": "5.3 The ActDisease Dataset\n\n\n\nSlide 04\n\n\nThe authors begin by delineating the background and motivation underpinning their study. Historical medical periodicals contain a rich repository of information, offering profound insights into the evolution of medical knowledge and practices. The ActDisease dataset itself comprises a private, recently digitised collection of patient organisation magazines, totalling an impressive 96,186 pages. This extensive collection is systematically organised by country and disease, providing a comprehensive overview of its composition.\n\nGermany: Two magazines on Allergy/Asthma (10,926 pages, 1901-1985), one on Diabetes (19,324 pages, 1931-1990), and one on Multiple Sclerosis (5,646 pages, 1954-1990).\nSweden: One magazine each on Allergy/Asthma (4,054 pages, 1957-1990), Diabetes (7,150 pages, 1949-1990), and Lung Diseases (16,790 pages, 1938-1991).\nFrance: One Diabetes magazine (6,206 pages, 1947-1990) and three on Rheumatism/Paralysis (9,317 pages, 1935-1990).\nUnited Kingdom: One magazine on Diabetes (11,127 pages, 1935-1990) and one on Rheumatism (5,646 pages, 1950-1990).\n\nIllustrative examples from the dataset include the BRA Review, the quarterly journal of the British Rheumatic Association, and Allergia, a German publication from 1981.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-and-ocr-challenges",
    "href": "chapter_ai-nepi_005.html#digitisation-and-ocr-challenges",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.4 Digitisation and OCR Challenges",
    "text": "5.4 Digitisation and OCR Challenges\n\n\n\nSlide 05\n\n\nThe research focuses on developing automated methods for classifying diverse genres within these historical medical texts, a crucial endeavour for advancing digital humanities research. Whilst Optical Character Recognition (OCR) tools, such as ABBYY FineReader Server 14, generally perform competently on common layouts and fonts, significant challenges persist. These include difficulties with complex layouts, slanted text, rare fonts, and inconsistent scan quality.\nConsequently, the digitised texts often exhibit remaining issues, particularly OCR errors prevalent in German and French materials, alongside problems with disrupted reading order. To mitigate these inaccuracies, the authors conducted experiments on post-OCR correction of German texts using instruction-tuned generative models, work documented in a forthcoming publication by Danilova and Aangenendt. Notably, OCR errors frequently manifest in ‘creative texts’, such as advertisements and poems, which often feature non-standard typography.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-case-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#the-case-for-genre-classification",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.5 The Case for Genre Classification",
    "text": "5.5 The Case for Genre Classification\n\n\n\nSlide 06\n\n\nThe methodology employed in this research encompasses several critical stages: data collection, preprocessing, feature extraction, and machine learning classification. A primary motivation for genre classification stems from the inherent textual challenges within these materials. The magazines exhibit an extreme diversity of text types, often presenting different genres—such as administrative reports, advertisements, and humour sections—side by side on the same page.\nCurrent analytical approaches, including yearly and decade-based topic models, fail to account for this intricate textual heterogeneity. This oversight introduces a significant bias into topic models and term counts, underscoring the necessity for a more granular classification approach.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre",
    "href": "chapter_ai-nepi_005.html#defining-genre",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.6 Defining Genre",
    "text": "5.6 Defining Genre\n\n\n\nSlide 07\n\n\nThe preprocessing phase of this research encountered several challenges inherent to historical texts, including pervasive OCR errors, varying typography, and the natural evolution of language over time. Within this context, the concept of genre has emerged as a particularly useful analytical framework. In language technology, genre is precisely defined as ‘a class of documents sharing a communicative purpose’, a definition supported by the foundational work of Petrenz (2004) and Kessler (1997).\nGenre classification serves several key objectives:\n\nIt facilitates the exploration of historical material from diverse perspectives.\nIt enables the systematic study of communicative strategies over extended periods and across different countries.\nIt allows for a fine-grained analysis of term distributions and topic models within identified genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrating-genre-diversity",
    "href": "chapter_ai-nepi_005.html#illustrating-genre-diversity",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.7 Illustrating Genre Diversity",
    "text": "5.7 Illustrating Genre Diversity\n\n\n\nSlide 08\n\n\nThe authors experimented with various feature extraction techniques, meticulously comparing traditional linguistic features against modern deep learning approaches. This exploration aimed to understand the nuanced textual characteristics that define different genres within the ActDisease dataset. The project visually demonstrates this diversity through a collection of distinct text excerpts.\nThese examples span a wide array of categories: poetry, such as Arne Nyman’s Swedish piece Människoson; scientific prose discussing concepts like the pancreas and insulin; commercial information, exemplified by advertisements for diabetic chocolate; and medical conference reports. Furthermore, the dataset contains legal texts, such as a ‘Deed of Covenant’, alongside practical genres like recipes and patient education materials.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#diverse-textual-forms",
    "href": "chapter_ai-nepi_005.html#diverse-textual-forms",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.8 Diverse Textual Forms",
    "text": "5.8 Diverse Textual Forms\n\n\n\nSlide 09\n\n\nThe ActDisease project illustrates the broad spectrum of textual forms encountered within its domain. A central image, labelled ‘Patient Experiences’, symbolises the personal dimension of illness. Surrounding this, various text snippets exemplify distinct genres. These include historical scientific texts, commercial advertisements, and personal narratives capturing first-person accounts of events like diabetic comas.\nInstructional texts provide practical guidance on food preparation, whilst legal and financial documents appear alongside medical advice on balancing insulin doses. Collectively, these examples underscore the rich and varied textual landscape of the ActDisease corpus.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#defining-genre-labels",
    "href": "chapter_ai-nepi_005.html#defining-genre-labels",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.9 Defining Genre Labels",
    "text": "5.9 Defining Genre Labels\n\n\n\nSlide 11\n\n\nThe genre labels employed in this research were meticulously defined under the supervision of the project’s principal historian, an expert specialising in patient organisations. This ensures that the categorisation system aligns precisely with the nuances of historical inquiry. The labels serve a crucial practical purpose: they facilitate the systematic separation of content, thereby enabling more focused historical analysis. A guiding principle throughout their development was to ensure they remained as general-purpose as possible, maximising their utility across diverse analytical contexts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-definitions",
    "href": "chapter_ai-nepi_005.html#genre-definitions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.10 Genre Definitions",
    "text": "5.10 Genre Definitions\n\n\n\nSlide 12\n\n\nThe team established a systematic framework defining nine distinct text genres, each with clear objectives.\n\nAcademic: Research-based reports or explanations of scientific ideas.\nAdministrative: Documents pertaining to organisational activities, like meeting minutes.\nAdvertisement: Content promoting products or services for commercial purposes.\nGuide: Step-by-step instructions, from health tips to recipes.\nFiction: Creative works intended to entertain, such as stories and poems.\nLegal: Texts explaining terms and conditions, such as contracts.\nNews: Reports on recent events and developments.\nNonfiction Prose: Narratives of real events or cultural topics, including memoirs.\nQA (Question/Answer): Content structured as questions paired with expert answers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-methodology",
    "href": "chapter_ai-nepi_005.html#annotation-methodology",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.11 Annotation Methodology",
    "text": "5.11 Annotation Methodology\n\n\n\nSlide 13\n\n\nThe data annotation process defined the ‘paragraph’ as its fundamental unit, specifically utilising ABBYY paragraphs merged based on consistent font patterns. Source material comprised two key periodicals: the Swedish journal Diabetes and the German Diabetiker Journal. A dedicated team of four historians and two computational linguists, all with native or advanced proficiency in both Swedish and German, undertook the annotation task.\nTo ensure robust quality control, two independent annotations were collected for every paragraph. This rigorous approach yielded an exceptionally high average inter-annotator agreement of 0.95, as measured by Krippendorff’s alpha, affirming the strong consistency of the annotated dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#annotation-example",
    "href": "chapter_ai-nepi_005.html#annotation-example",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.12 Annotation Example",
    "text": "5.12 Annotation Example\n\n\n\nSlide 14\n\n\nAn illustrative annotation example derives from Der Diabetiker (1958), under the title ‘THE ISLAND (Diabetes was not an obstacle)’. This sample showcases three distinct paragraphs, each classified as ‘non-fiction prose’. The first describes a family’s decision to travel, the second conveys confidence in medical care, and the third articulates apprehension about a first flight. The original German sentences were translated for presentation purposes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#dataset-splitting",
    "href": "chapter_ai-nepi_005.html#dataset-splitting",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.13 Dataset Splitting",
    "text": "5.13 Dataset Splitting\n\n\n\nSlide 15\n\n\nThe ActDisease annotations underwent a meticulous dataset splitting procedure. The main split allocated 1182 paragraphs to the training set and 552 to the held-out set, rigorously stratified by label. For few-shot experiments, the team prepared six training set sizes, ranging from 100 to 1182 paragraphs, randomly sampled and balanced by label.\nThe held-out set was subsequently divided into equal, balanced parts for validation and testing. Notably, the ‘legal’ and ‘news’ categories were excluded from few-shot experiments due to insufficient training data. For zero-shot experiments, the entire test set was utilised to evaluate performance on unseen classes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-distribution",
    "href": "chapter_ai-nepi_005.html#genre-distribution",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.14 Genre Distribution",
    "text": "5.14 Genre Distribution\n\n\n\nSlide 16\n\n\nAn analysis of genre distribution reveals distinct patterns across the training and held-out sets. In the German training data, ‘non-fiction’ is the most prevalent genre, followed by ‘academic’ and ‘fiction’. Conversely, the Swedish training data is overwhelmingly dominated by ‘advertisement’ content, with ‘administrative’ and ‘QA’ also well-represented.\nThe held-out dataset exhibits similar trends. For German, ‘nonfiction_prose’ remains most frequent, whilst for Swedish, ‘advertisement’ continues to dominate. Significantly, the ‘legal’ and ‘news’ genres, largely absent from the training set, appear sparsely in the held-out set, posing a specific challenge for classification experiments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#public-datasets-for-zero-shot-experiments",
    "href": "chapter_ai-nepi_005.html#public-datasets-for-zero-shot-experiments",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.15 Public Datasets for Zero-Shot Experiments",
    "text": "5.15 Public Datasets for Zero-Shot Experiments\n\n\n\nSlide 17\n\n\nFor the zero-shot experiments, the authors leveraged several publicly available datasets. They utilised two primary document-level datasets: the Corpus of Online Registers of English (CORE), published by Egbert et al. (2015), and the Functional Text Dimensions (FTD) dataset of web genres, developed by Sharoff (2018).\nAdditionally, the team used the UD-MULTIGENRE (UDM) dataset, a subset of Universal Dependencies offering sentence-level genre annotations across 38 languages, with recovered annotations by Danilova and Stymne (2023). This distinction between document-level and sentence-level annotation proved crucial for the experimental design.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#genre-mapping-across-datasets",
    "href": "chapter_ai-nepi_005.html#genre-mapping-across-datasets",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.16 Genre Mapping Across Datasets",
    "text": "5.16 Genre Mapping Across Datasets\n\n\n\nSlide 18\n\n\nA comprehensive mapping of genre classifications across the ActDisease, CORE, UDM, and FTD datasets reveals both consistencies and divergences. The ‘Academic’ genre, for instance, aligns across all datasets. ‘Advertisement’ maps to ‘advertisement’ in CORE and ‘commercial’ in FTD, but is absent from UDM.\n‘Guide’ and ‘Fiction’ are consistently represented, though with varying granularities. ‘Legal’ and ‘News’ show strong consistency across most datasets. ‘Nonfiction’, however, demonstrates greater divergence in its sub-categories. This comparative analysis underscores the heterogeneity of genre definitions across different text corpora, a critical consideration for cross-dataset classification.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#training-data-pipeline",
    "href": "chapter_ai-nepi_005.html#training-data-pipeline",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.17 Training Data Pipeline",
    "text": "5.17 Training Data Pipeline\n\n\n\nSlide 19\n\n\nThe genre classification experiments employed a meticulously designed training data pipeline. The process commences with three data sources—CORE, FTD, and UDM—which feed into a mapping stage that standardises genre labels. A pre-processing step then cleans the data by removing web addresses, emails, and other artefacts.\nFollowing this, a sampling stage selects data based on specific configuration options. These options define the language scope (Germanic languages only, or all language families) and the balancing strategy (balancing by project labels only, or by both project and original labels). This structured approach produces distinct training samples for the different experimental conditions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "href": "chapter_ai-nepi_005.html#multilingual-encoder-models",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.18 Multilingual Encoder Models",
    "text": "5.18 Multilingual Encoder Models\n\n\n\nSlide 20\n\n\nThe authors employed three prominent multilingual encoder models in their experiments: XLM-Roberta, mBERT, and historical mBERT (hmBERT). BERT-like models have seen extensive application in previous work on genre classification. XLM-RoBERTa, in particular, has established itself as a state-of-the-art web genre classifier.\nNotably, hmBERT distinguishes itself through its pretraining on a substantial corpus of multilingual historical newspapers, offering a specialised capability for processing historical texts. The team included mBERT primarily for comparative analysis with hmBERT.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#fine-tuning-pipeline",
    "href": "chapter_ai-nepi_005.html#fine-tuning-pipeline",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.19 Fine-Tuning Pipeline",
    "text": "5.19 Fine-Tuning Pipeline\n\n\n\nSlide 21\n\n\nThe fine-tuning process for the language models involved a structured pipeline. Training data was organised into four primary configurations: FTD, CORE, UDM, and a Merged set. Each of these configurations, in turn, comprised four distinct variations, resulting in a total of sixteen unique training datasets. These diverse datasets then served as input for fine-tuning the selected pre-trained language models: XLM-Roberta, mBERT, and hmBERT. This systematic approach ultimately yielded a comprehensive collection of forty-eight fine-tuned models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-evaluation-framework",
    "href": "chapter_ai-nepi_005.html#zero-shot-evaluation-framework",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.20 Zero-Shot Evaluation Framework",
    "text": "5.20 Zero-Shot Evaluation Framework\n\n\n\nSlide 22\n\n\nThe evaluation framework for zero-shot learning models centres on assessing their performance in novel classification tasks. Zero-shot learning is a paradigm where a model, trained on known classes, can recognise data for classes it has not encountered during training. This capability typically relies on auxiliary information, such as semantic descriptions of the unseen classes. The evaluation therefore rigorously focuses on pertinent metrics and experimental design to analyse how effectively these models perform under such challenging conditions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#evaluating-zero-shot-predictions",
    "href": "chapter_ai-nepi_005.html#evaluating-zero-shot-predictions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.21 Evaluating Zero-Shot Predictions",
    "text": "5.21 Evaluating Zero-Shot Predictions\n\n\n\nSlide 23\n\n\nEvaluating zero-shot predictions presents a significant challenge due to the imperfect overlap of label sets across different datasets. To circumvent this limitation, the authors’ evaluation strategy meticulously analyses the performance of each genre independently and scrutinises confusion matrices. This provides a more granular understanding of classification accuracy.\nThe X-GENRE web genre classifier, developed by Kuzman et al. (2023), served as the established baseline for comparative analysis. The team generated predictions on the most similar labels that could be directly mapped to the project’s own categories, ensuring a fair comparison.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-results",
    "href": "chapter_ai-nepi_005.html#zero-shot-results",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.22 Zero-Shot Results",
    "text": "5.22 Zero-Shot Results\n\n\n\nSlide 24\n\n\nAn overview of the zero-shot classification results reveals several key findings. Firstly, models fine-tuned on the FTD dataset demonstrate enhanced performance when employing the project’s specific mapping strategy. Secondly, other datasets exhibit distinct class-specific biases. The UDM dataset shows a notable bias towards ‘news’, whilst the CORE dataset shows a bias towards the ‘guide’ genre.\nInterestingly, specific models showed varying strengths. XLM-Roberta, when applied to the UDM dataset, achieved, on average, 32% more correct predictions in Question Answering (QA) tasks compared to mBERT and hmBERT. Conversely, hmBERT demonstrated superior performance in classifying ‘Administrative’ content on the same dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-confusion-matrices",
    "href": "chapter_ai-nepi_005.html#zero-shot-confusion-matrices",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.23 Zero-Shot Confusion Matrices",
    "text": "5.23 Zero-Shot Confusion Matrices\n\n\n\nSlide 25\n\n\nFour confusion matrices visually represent the performance of various zero-shot genre classification models. Each matrix plots ‘True Genre’ against ‘Predicted Genre’. The hmbert_UDM_True_True matrix, for example, shows 30 instances of the ‘administrative’ genre correctly predicted, yet 8 ‘academic’ instances were erroneously classified as ‘administrative’. The xlmr_CORE_True_False matrix demonstrates robust performance for several genres, with 36 ‘academic’ and 33 ‘legal’ documents correctly classified. Conversely, the xlmr_UDM_False_False matrix highlights areas of confusion, notably 23 ‘QA’ instances misclassified as ‘administrative’.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-f1-scores",
    "href": "chapter_ai-nepi_005.html#zero-shot-f1-scores",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.24 Zero-Shot F1 Scores",
    "text": "5.24 Zero-Shot F1 Scores\n\n\n\nSlide 26\n\n\nThe zero-shot per-category F1 scores, averaged across dataset configurations, provide a detailed performance assessment. The evaluation encompassed four main dataset configurations—FTD, CORE, UDM, and merged—with three language models: hmBERT, mBERT, and XLM-RoBERTa. Within the FTD configuration, the ‘legal’ category consistently demonstrated high performance, with hmBERT achieving the highest score of 0.9. In the CORE configuration, ‘legal’ continued to be a robust category, with XLM-RoBERTa achieving 0.84. The UDM configuration highlighted XLM-RoBERTa’s strength in ‘QA’ (0.53) and hmBERT’s lead in ‘administrative’ (0.43).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#zero-shot-configuration-performance",
    "href": "chapter_ai-nepi_005.html#zero-shot-configuration-performance",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.25 Zero-Shot Configuration Performance",
    "text": "5.25 Zero-Shot Configuration Performance\n\n\n\nSlide 27\n\n\nAn analysis of average performance across zero-shot configurations reveals the nuanced impacts of data balancing and language scope. For the FTD task, F1 scores were highest when including all language families. In the CORE task, performance remained largely consistent across configurations. Conversely, for the UDM task, the presence of other language families and balancing generally improved macro F1 performance; including all language families achieved the highest F1 score of 0.22, significantly outperforming the Germanic-only configuration at 0.14.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-evaluation-framework",
    "href": "chapter_ai-nepi_005.html#few-shot-evaluation-framework",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.26 Few-Shot Evaluation Framework",
    "text": "5.26 Few-Shot Evaluation Framework\n\n\n\nSlide 28\n\n\nThe evaluation framework for few-shot learning centres on the rigorous assessment of few-shot prompting methods. This technique involves providing large language models with a minimal number of examples to guide task performance without necessitating extensive fine-tuning. The evaluation comprehensively discusses the methodologies employed, the metrics utilised, and the results obtained.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-performance-with-mlm-fine-tuning",
    "href": "chapter_ai-nepi_005.html#few-shot-performance-with-mlm-fine-tuning",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.27 Few-Shot Performance with MLM Fine-Tuning",
    "text": "5.27 Few-Shot Performance with MLM Fine-Tuning\n\n\n\nSlide 29\n\n\nA line graph illustrates the performance of various models in a few-shot learning setting, comparing their efficacy with and without Masked Language Model (MLM) fine-tuning. The F1 score for all models consistently increases with larger dataset sizes. Crucially, models incorporating MLM fine-tuning (hmbert-mlm, mbert-mlm, xlmr-mlm) consistently achieve higher F1 scores than their non-MLM counterparts. The hmbert-mlm model demonstrates the highest performance, attaining the peak F1 score amongst all tested models, particularly at larger dataset sizes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-f1-scores-and-metrics",
    "href": "chapter_ai-nepi_005.html#few-shot-f1-scores-and-metrics",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.28 Few-Shot F1 Scores and Metrics",
    "text": "5.28 Few-Shot F1 Scores and Metrics\n\n\n\nSlide 30\n\n\nA comprehensive table details few-shot learning performance, presenting F1 scores for seven text categories alongside overall metrics. The evaluation encompassed six models, each tested with 500 and 1182 training examples. For the ‘QA’ category, XLMR-MLM achieved the highest F1 score of 0.84 with 1182 examples. The ‘advertisement’ category consistently demonstrated high F1 scores, frequently reaching 0.93. In terms of overall performance, hmBERT-MLM achieved the highest accuracy of 0.82 and the highest macro F1 of 0.77, both with 1182 examples, establishing it as the top-performing model in this setting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-confusion-analysis",
    "href": "chapter_ai-nepi_005.html#few-shot-confusion-analysis",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.29 Few-Shot Confusion Analysis",
    "text": "5.29 Few-Shot Confusion Analysis\n\n\n\nSlide 31\n\n\nAn analysis of the confusion matrix for the XLM-Roberta-MLM model provides insights into few-shot classification performance. The matrix displays correct classifications along its diagonal for most genres. However, notable misclassifications occurred; for instance, ‘fiction’ texts were frequently misclassified as ‘non-fictional prose’.\nThese patterns are explicable. Fictional and non-fictional prose may exhibit increasing similarity within this specific domain. Crucially, all genres are confined to patient organisation magazines focused on diabetes. Consequently, both fictional and (auto)biographical narratives frequently revolve around the shared experiences of diabetes patients, contributing to the difficulty in distinguishing between these genres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "href": "chapter_ai-nepi_005.html#few-shot-prompting-evaluation",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.30 Few-Shot Prompting Evaluation",
    "text": "5.30 Few-Shot Prompting Evaluation\n\n\n\nSlide 32\n\n\nThe evaluation of few-shot prompting methods constitutes a critical focus of this research. This sophisticated technique guides a large language model’s behaviour with a minimal number of examples, avoiding the need for extensive fine-tuning. The following section details the specific instructions and results from applying this method with the Llama-3.1 8b Instruct model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-labelling-instructions",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-labelling-instructions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.31 Llama-3.1 8b Labelling Instructions",
    "text": "5.31 Llama-3.1 8b Labelling Instructions\n\n\n\nSlide 33\n\n\nThe Llama-3.1 8b Instruct model received a precise instruction for genre labelling: ‘Label the text with one of the following genres based on their purpose and content.’ This was accompanied by a comprehensive list of nine genres with examples.\n\nAcademic: academic articles, reports, popular science.\nAdministrative: meeting minutes, financial reports, annual reports.\nAdvertisement: promotions, invitations.\nGuide: dietary advice, recipes, procedural guidelines.\nFiction: poems, short stories, humour, novels.\nLegal: contracts, terms and conditions.\nNews: daily news reports.\nNonfiction_prose: (auto)biographies, memoirs, personal letters, essays.\nQA: text in a question-answer format.\n\nAn example input format, [INST] Input: test[i] [\"text\"] [/INST] Genre:, illustrates how the model processes text for classification in a few-shot setup.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#llama-3.1-8b-prediction-results",
    "href": "chapter_ai-nepi_005.html#llama-3.1-8b-prediction-results",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.32 Llama-3.1 8b Prediction Results",
    "text": "5.32 Llama-3.1 8b Prediction Results\n\n\n\nSlide 34\n\n\nThe Llama-3.1 8b Instruct model’s performance on a genre classification task, using few-shot prediction, yielded detailed results. The ‘legal’ genre demonstrated the highest F1 score at 0.84, whilst ‘news’ exhibited a notably low F1 score of 0.08. Overall performance metrics recorded an accuracy of 0.62 and a macro average F1-score of 0.59. A confusion matrix detailed these outcomes, highlighting correct classifications and misclassifications, such as 14 instances of ‘QA’ being erroneously classified as ‘non-fictional prose’.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusion-the-challenge-of-popular-magazines",
    "href": "chapter_ai-nepi_005.html#conclusion-the-challenge-of-popular-magazines",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.33 Conclusion: The Challenge of Popular Magazines",
    "text": "5.33 Conclusion: The Challenge of Popular Magazines\n\n\n\nSlide 35\n\n\nA key conclusion from this research is that popular magazines contain a multitude of genres, a characteristic that inherently complicates text mining compared to the analysis of scientific journals and books. This rich diversity of content introduces significant complexities for automated text analysis. In stark contrast, scientific texts typically exhibit a more specialised and standardised structure, making them considerably more amenable to current text mining techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusion-genre-classification-as-a-solution",
    "href": "chapter_ai-nepi_005.html#conclusion-genre-classification-as-a-solution",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.34 Conclusion: Genre Classification as a Solution",
    "text": "5.34 Conclusion: Genre Classification as a Solution\n\n\n\nSlide 36\n\n\nThe research culminates in two pivotal findings. Firstly, the multitude of genres within popular magazines complicates text mining efforts. Secondly, and crucially, genre classification emerges as a viable solution. By systematically categorising the varied content, the authors can effectively manage this inherent diversity, thereby rendering these rich sources accessible for comprehensive analysis through advanced text mining techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#overall-conclusions",
    "href": "chapter_ai-nepi_005.html#overall-conclusions",
    "title": "5  Genre Classification for Historical Medical Periodicals",
    "section": "5.35 Overall Conclusions",
    "text": "5.35 Overall Conclusions\n\n\n\nSlide 37\n\n\nThis research culminates in several significant conclusions. The inherent multitude of genres within popular magazines complicates text mining, yet genre classification offers a robust methodology for rendering these sources accessible. The study demonstrates that modern datasets can be successfully leveraged even without specific training data, and that open generative models can achieve a decent quality of classification.\nCrucially, few-shot learning applied to multilingual encoders, particularly when preceded by Masked Language Model (MLM) fine-tuning, consistently yields superior performance. Quantitative evidence strongly supports this, revealing particularly substantial gains for historical multilingual BERT, which achieved a 24% improvement. This underscores that domain-specific fine-tuning for historical multilingual data significantly enhances classification performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  The VERITRACE Project",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant (2023-2028), operates from the Vrije Universiteit Brussel (VUB) under the distinguished leadership of Research Professor Cornelis J. Schilt. This ambitious initiative meticulously investigates the profound influence of early modern ‘ancient wisdom’ traditions upon natural philosophy. Professor Schilt and his team aim to trace the enduring impact of seminal texts such as the Chaldean Oracles, Sibylline Oracles, Orphic Hymns, and the Corpus Hermeticum on intellectual giants like Isaac Newton and Johannes Kepler. Concurrently, they endeavour to uncover a broader, frequently overlooked network of related works, aptly termed the ‘great Unread’.\nThe VERITRACE team employs sophisticated computational methods to facilitate large-scale multilingual exploration. Their approach identifies textual reuse, discerning both direct lexical overlaps and subtle semantic similarities, whilst also revealing hidden networks of texts, passages, themes, topics, and authors. This rigorous investigation further seeks to illuminate novel patterns within the intellectual history and philosophy of science. Utilising a diverse multilingual dataset of approximately 430,000 printed texts from 1540 to 1728, meticulously sourced from Early English Books Online (EEBO), Gallica, and the Bavarian State Library, the team applies state-of-the-art digital techniques. These include advanced keyword search, precise text matching, nuanced topic modelling, and insightful sentiment analysis.\nCore challenges for the project encompass the variable quality of Optical Character Recognition (OCR), the complexities of early modern typography and semantics across at least six languages, and the sheer volume of data. To address these hurdles, the team leverages Large Language Models (LLMs) for both metadata enrichment, employing GPT-based LLMs as discerning ‘judges’, and semantic encoding, utilising BERT-based LLMs for robust vector embeddings. A newly developed web application, currently in its alpha version, provides essential functionalities for corpus exploration, advanced search, and text matching. Future plans for this application include the integration of powerful analytical tools such as topic modelling and diachronic analysis. Comprising five members, including Professor Schilt as PI, a classicist, and historians, the VERITRACE team adeptly navigates significant computational and methodological complexities in its pursuit of comprehensive historical analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#project-overview-and-core-objectives",
    "href": "chapter_ai-nepi_006.html#project-overview-and-core-objectives",
    "title": "6  The VERITRACE Project",
    "section": "6.1 Project Overview and Core Objectives",
    "text": "6.1 Project Overview and Core Objectives\n\n\n\nSlide 03\n\n\nThe VERITRACE project, a five-year ERC Starting Grant awarded from 2023 to 2028, operates from the Vrije Universiteit Brussel (VUB) under the leadership of Research Professor Cornelis J. Schilt. This initiative comprises a five-member team, including a classicist, historians, and a digital humanities specialist, who collectively aim to trace the profound influence of an early modern ‘ancient wisdom’ tradition on the evolution of natural philosophy and science.\nProfessor Schilt and his colleagues specifically focus on a close-reading corpus of 140 works, encompassing seminal texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence confirms the significant impact of these works; for instance, Isaac Newton engaged with the Sibylline Oracles, whilst Johannes Kepler demonstrated familiarity with the Corpus Hermeticum. Beyond these well-known connections, the project endeavours to uncover a much broader, often overlooked network of texts and intellectual relationships within this tradition, collectively termed the ‘great Unread’. These works, frequently authored by lesser-known figures, typically remain outside the primary focus of historical scholarship, yet they offer crucial insights into the intellectual landscape of the early modern period.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-approaches-to-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_006.html#computational-approaches-to-history-philosophy-and-sociology-of-science-hpss",
    "title": "6  The VERITRACE Project",
    "section": "6.2 Computational Approaches to History, Philosophy, and Sociology of Science (HPSS)",
    "text": "6.2 Computational Approaches to History, Philosophy, and Sociology of Science (HPSS)\n\n\n\nSlide 04\n\n\nThe VERITRACE project fundamentally aims to advance the field of History, Philosophy, and Sociology of Science (HPSS) through the application of sophisticated computational methodologies. Professor Schilt and his team are developing tools for large-scale multilingual exploration, primarily through advanced keyword search capabilities. Crucially, the initiative seeks to identify textual re-use within an extensive, multilingual corpus, distinguishing between direct lexical overlaps and more subtle indirect semantic similarities. This capability effectively functions as an ‘Early Modern Plagiarism Detector’, enabling the detection of unacknowledged textual appropriation.\nFurthermore, the project strives to uncover previously ignored networks of texts, passages, themes, topics, and authors, thereby illuminating hidden intellectual connections. Ultimately, these computational investigations are poised to reveal novel patterns and insights into the intellectual history and philosophy of science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-set-characteristics-and-analytical-techniques",
    "href": "chapter_ai-nepi_006.html#data-set-characteristics-and-analytical-techniques",
    "title": "6  The VERITRACE Project",
    "section": "6.3 Data Set Characteristics and Analytical Techniques",
    "text": "6.3 Data Set Characteristics and Analytical Techniques\n\n\n\nSlide 05\n\n\nTo achieve its ambitious objectives, the VERITRACE team has assembled a substantial and diverse multilingual dataset. They focus exclusively on printed works from approximately 1540 to 1728, concluding shortly after Isaac Newton’s death. This extensive corpus comprises around 430,000 texts in at least six different languages.\nThe researchers meticulously gathered these digital texts from three primary sources: Early English Books Online (EEBO), Gallica (the digital library of the French National Library), and the Bavarian State Library, which constitutes the largest single contributor to the dataset. Leveraging this rich collection, the team applies state-of-the-art digital techniques, including advanced keyword search functionalities, sophisticated text matching algorithms, topic modelling for thematic discovery, and sentiment analysis to discern emotional tones, amongst other analytical methods.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#core-challenges-and-initial-llm-applications",
    "href": "chapter_ai-nepi_006.html#core-challenges-and-initial-llm-applications",
    "title": "6  The VERITRACE Project",
    "section": "6.4 Core Challenges and Initial LLM Applications",
    "text": "6.4 Core Challenges and Initial LLM Applications\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several formidable challenges inherent in processing historical texts at scale. Firstly, the team contends with highly variable Optical Character Recognition (OCR) quality, as libraries provide texts in raw formats—including XML, HOCR, and HTML files—without corresponding ground truth page images. This initial data quality significantly impacts all subsequent processing stages.\nSecondly, the project navigates the complexities of early modern typography and the evolving semantics of at least six distinct languages, presenting considerable linguistic hurdles. Thirdly, the sheer volume of data—hundreds of thousands of texts printed across Europe over approximately two centuries—necessitates robust computational strategies. To address these challenges, the team currently employs Large Language Models (LLMs) in two principal capacities. On the decoder side, GPT-based LLMs assist in enriching and cleaning metadata, effectively acting as ‘judges’ to refine bibliographic information. Concurrently, on the encoder side, BERT-based LLMs generate vector embeddings, encoding the semantic meaning of sentences and short passages within the textual corpus to facilitate precise text matching.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment-motivation-and-challenges",
    "href": "chapter_ai-nepi_006.html#llms-as-judges-for-metadata-enrichment-motivation-and-challenges",
    "title": "6  The VERITRACE Project",
    "section": "6.5 LLMs as Judges for Metadata Enrichment: Motivation and Challenges",
    "text": "6.5 LLMs as Judges for Metadata Enrichment: Motivation and Challenges\n\n\n\nSlide 07\n\n\nA fundamental motivation for the VERITRACE project involves leveraging the Universal Short Title Catalogue (USTC) as a high-quality source for metadata enrichment. The primary objective is to map VERITRACE’s internal records onto USTC’s comprehensive entries, thereby generating ‘enriched’ metadata that demands significantly less manual cleaning.\nHowever, this process presents a considerable challenge: whilst some mapping can be automated using existing external identifiers, the vast majority of VERITRACE records currently lack such identifiers and remain uncleaned. Consequently, the project faces the complex task of matching these records at scale, a process that proves exceedingly tedious for human review. The Universal Short Title Catalogue, accessible at https://www.ustc.ac.uk, serves as the authoritative reference for this enrichment endeavour.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#automating-bibliographic-record-matching",
    "href": "chapter_ai-nepi_006.html#automating-bibliographic-record-matching",
    "title": "6  The VERITRACE Project",
    "section": "6.6 Automating Bibliographic Record Matching",
    "text": "6.6 Automating Bibliographic Record Matching\n\n\n\nSlide 08\n\n\nThe VERITRACE team seeks to automate the arduous task of comparing bibliographic metadata pairs to ascertain whether they represent the same underlying printed text. Previously, each team member undertook the extremely tedious manual review of 10,000 such pairs. To mitigate this, the researchers initially generate potential matches using a fuzzy matching algorithm, which assigns a match score to each pair.\nThe ultimate aim is for Large Language Models (LLMs) to perform these yes/no decisions at scale, critically providing detailed reasoning for each determination. However, this LLM-based approach has not yet achieved full functionality. The models frequently produce hallucinations, and whilst requesting more structured output can reduce these, it often results in generic, less helpful reasoning, indicating an ongoing challenge in balancing precision with informative responses.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#llm-bench-for-match-evaluation",
    "href": "chapter_ai-nepi_006.html#llm-bench-for-match-evaluation",
    "title": "6  The VERITRACE Project",
    "section": "6.7 LLM Bench for Match Evaluation",
    "text": "6.7 LLM Bench for Match Evaluation\n\n\n\nSlide 09\n\n\nTo address the challenge of automated bibliographic record matching, the VERITRACE project proposes ‘The LLM Bench’, a sophisticated panel of Large Language Models designed to evaluate potential matches. This system employs a tiered model configuration: llama3:8b serves as the primary model, prized for its power and accuracy; qwen2:5.7b acts as a secondary model, offering architectural diversity; mixtral:8x7b functions as a tiebreaker, possessing greater power than the initial two; and llama3.3:latest is designated as an expert model, reserved exclusively for complex edge cases requiring human review.\nThe process involves feeding pairs of bibliographic records—one from a low-quality metadata source and the other from a high-quality source—through this chain of LLMs. Each model provides a judgment (match or non-match), accompanied by detailed reasoning and confidence levels. Subsequently, the team validates these LLM decisions against ground truth data, with the VERITRACE team conducting a final review to ensure accuracy and refine the system.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#prompt-guidelines-and-llm-output-analysis",
    "href": "chapter_ai-nepi_006.html#prompt-guidelines-and-llm-output-analysis",
    "title": "6  The VERITRACE Project",
    "section": "6.8 Prompt Guidelines and LLM Output Analysis",
    "text": "6.8 Prompt Guidelines and LLM Output Analysis\n\n\n\nSlide 10\n\n\nThe VERITRACE team has established comprehensive prompt guidelines, termed ‘MATCHING_GUIDELINES’, to direct the LLMs in determining bibliographic record matches. These guidelines prioritise title content, whilst accounting for minor formatting differences, and emphasise author alignment, publication dates within a one-year window, and corroborating place/printer information.\nSpecific match criteria mandate identical core work in titles, precise or near-precise dates, matching or equivalent publication places, recognisable printer variations, and substantial author overlap. Conversely, non-match indicators include significantly divergent titles, dates exceeding a one-year difference, unexplained discrepancies in places or printers, and distinct authors or edition variations. An illustrative example demonstrates a ‘Ground Truth’ of ‘NON-MATCH’ yet a ‘Final Decision’ of ‘MATCH’ with high confidence (87.7%), driven by the tiebreaker model. The reasoning provided highlights factors such as title similarity, identical authors and dates, and equivalent publication places, even when language discrepancies exist. A significant ongoing challenge, however, stems from hallucinations in the output of the open-source LLMs employed. Whilst requesting more structured output mitigates these hallucinations, it often results in generic, less informative reasoning, presenting a delicate balance for the researchers to refine.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-technical-approach",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-alpha-version-and-technical-approach",
    "title": "6  The VERITRACE Project",
    "section": "6.9 VERITRACE Web Application: Alpha Version and Technical Approach",
    "text": "6.9 VERITRACE Web Application: Alpha Version and Technical Approach\n\n\n\nSlide 11\n\n\nThe VERITRACE project has developed an ‘alpha’ version of its web application, which remains in its nascent stages and is not yet publicly accessible, currently residing on the presenter’s local machine. This application represents a future promise of the project’s capabilities, serving primarily as a testing and development platform.\nThe engineers are currently evaluating a BERT-based Large Language Model, specifically LaBSE, for generating vector embeddings that represent every passage within the extensive textual corpus. However, initial assessments suggest that whilst LaBSE functions in certain scenarios, it will likely prove insufficient for the project’s comprehensive requirements. Due to its early developmental stage, the application is demonstrated through static screenshots rather than a live, interactive website.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#data-processing-pipeline-for-web-application",
    "href": "chapter_ai-nepi_006.html#data-processing-pipeline-for-web-application",
    "title": "6  The VERITRACE Project",
    "section": "6.10 Data Processing Pipeline for Web Application",
    "text": "6.10 Data Processing Pipeline for Web Application\n\n\n\nSlide 12\n\n\nThe VERITRACE web application relies on an Elasticsearch database as its backend, necessitating a complex and extensive data processing pipeline to transform raw text into a usable format. This pipeline involves numerous critical steps, each demanding significant optimisation.\nInitially, the system extracts text into standardised text files, subsequently generating precise mappings of all character positions. Further stages include segmenting documents into manageable units and rigorously assessing the Optical Character Recognition (OCR) quality, a particularly challenging task given the raw nature of the input. Each of these stages, whilst seemingly straightforward, requires considerable time and effort—potentially a week per stage—to optimise fully, underscoring the intricate background work underpinning the application’s functionality.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-data-processing-pipeline-details",
    "href": "chapter_ai-nepi_006.html#veritrace-data-processing-pipeline-details",
    "title": "6  The VERITRACE Project",
    "section": "6.11 VERITRACE Data Processing Pipeline Details",
    "text": "6.11 VERITRACE Data Processing Pipeline Details\n\n\n\nSlide 13\n\n\nThe VERITRACE project employs a sophisticated 15-stage data processing pipeline, currently running with 40% overall progress, indicating six completed stages. A detailed status summary reveals eight pending stages, one actively running, and no failures or skips. The pipeline’s configuration specifies parameters such as a working directory, file and error policies, input directory, dashboard port (8080), and settings for dependency verification and browser interaction.\nCompleted stages include batch processing (1.3s), character position generation (5.3s), page extraction (7.3s), language analysis (4m 35s), language map generation (0.7s), and OCR quality assessment (12.7s). The ‘segment documents’ stage is presently active, whilst subsequent stages, such as filtering segments, tracking relationships, unifying JSON, enriching MongoDB, and enriching sequences, remain pending. This complex pipeline transforms raw XML, HOCR, and HTML files into a format suitable for Elasticsearch, with vector embeddings generated towards its conclusion, each stage demanding meticulous optimisation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-web-application-explore-section-and-metadata",
    "href": "chapter_ai-nepi_006.html#veritrace-web-application-explore-section-and-metadata",
    "title": "6  The VERITRACE Project",
    "section": "6.12 VERITRACE Web Application: Explore Section and Metadata",
    "text": "6.12 VERITRACE Web Application: Explore Section and Metadata\n\n\n\nSlide 14\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Search, Match, Analyse, and Read. The ‘Explore’ section serves as a comprehensive hub for corpus statistics and metadata exploration, currently presenting 427,305 metadata records directly from a Mongo database.\nThis section offers various visualisations, including pie charts for language distribution and data sources, a bar chart detailing documents by decade, and a donut chart illustrating top publication places. Furthermore, the ‘Metadata Explorer’ enables users to browse and filter documents based on their metadata attributes. Crucially, the system performs granular language identification on every text, down to approximately 50 characters, to accurately account for multilingual content. For instance, a text might be identified as 15% Greek and 85% Latin, classifying it as substantively multilingual. The application also attempts to assess OCR quality on a page-by-page basis, a challenging endeavour given the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-search-functionality",
    "href": "chapter_ai-nepi_006.html#veritrace-search-functionality",
    "title": "6  The VERITRACE Project",
    "section": "6.13 VERITRACE Search Functionality",
    "text": "6.13 VERITRACE Search Functionality\n\n\n\nSlide 15\n\n\nFor most scholars, the primary point of engagement with the VERITRACE web application will be its robust search functionality. Whilst the current prototype operates on a limited corpus of 132 files, rather than the full 400,000-plus, its index already occupies 15 gigabytes, indicating that the complete online corpus will span many terabytes.\nA basic keyword search, for example, for ‘Hermes’, rapidly retrieves 22 documents with 332 total matches in 107 milliseconds, including works like ‘Hermes Trismegisti Erkännuß…’ by Hermes (1706) and ‘Hermes theologus…’ by Wodenote (1649). Conversely, a more refined, structured query such as ‘author:kepler ’hermes’’ significantly narrows the results, yielding just one document with two total matches in a mere 17 milliseconds. This specific result points to Kepler’s ‘Prodromus Dissertationum Cosmographicarum…’ from 1621, which contains the phrase ‘Hermes tuus.’. This comparison effectively demonstrates the enhanced precision attainable through the use of structured queries within the digital humanities search environment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-search-interface-and-query-specificity",
    "href": "chapter_ai-nepi_006.html#veritrace-search-interface-and-query-specificity",
    "title": "6  The VERITRACE Project",
    "section": "6.14 VERITRACE Search Interface and Query Specificity",
    "text": "6.14 VERITRACE Search Interface and Query Specificity\n\n\n\nSlide 16\n\n\nThe VERITRACE search interface consistently displays corpus statistics, indicating 132 unique files, over 16.9 million total segments, and an index size of 15.37 gigabytes for the ‘veritrace_2025_a2’ dataset. A general keyword search for ‘hermes’, for example, rapidly retrieves 22 documents with 332 total matches in 107 milliseconds, including works like ‘Hermes Trismegisti Erkännuß…’ by Hermes (1706) and ‘Hermes theologus…’ by Wodenote (1649).\nConversely, a more refined, structured query such as ‘author:kepler ’hermes’’ significantly narrows the results, yielding just one document with two total matches in a mere 17 milliseconds. This specific result points to Kepler’s ‘Prodromus Dissertationum Cosmographicarum…’ from 1621, which contains the phrase ‘Hermes tuus.’. This comparison effectively demonstrates the enhanced precision attainable through the use of structured queries within the digital humanities search environment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-analytical-capabilities-the-analyse-module",
    "href": "chapter_ai-nepi_006.html#future-analytical-capabilities-the-analyse-module",
    "title": "6  The VERITRACE Project",
    "section": "6.15 Future Analytical Capabilities: The ‘Analyse’ Module",
    "text": "6.15 Future Analytical Capabilities: The ‘Analyse’ Module\n\n\n\nSlide 17\n\n\nWhilst currently unimplemented, the ‘Analyse’ section of the VERITRACE website is slated to host a suite of advanced analytical tools. Future functionalities will include Topic Modelling, enabling users to discover prevalent themes across the entire corpus or within selected documents. Additionally, the platform will integrate Latent Semantic Analysis (LSA) for assessing document similarity and Diachronic Analysis, designed to visualise linguistic and conceptual shifts over extended historical periods. The development team actively incorporates insights from contemporary research and community feedback to inform the meticulous implementation of these sophisticated analytical capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-read-section-digital-facsimiles-and-metadata",
    "href": "chapter_ai-nepi_006.html#veritrace-read-section-digital-facsimiles-and-metadata",
    "title": "6  The VERITRACE Project",
    "section": "6.16 VERITRACE Read Section: Digital Facsimiles and Metadata",
    "text": "6.16 VERITRACE Read Section: Digital Facsimiles and Metadata\n\n\n\nSlide 18\n\n\nThe VERITRACE platform incorporates a dedicated ‘Read’ section, designed to provide scholars with access to high-quality digital facsimiles of historical texts, moving beyond raw OCR output. This section integrates a Mirador viewer, enabling users to read PDFs of every text within the corpus, much like a conventional library website.\nAlongside the visual facsimile, comprehensive metadata is readily available. For instance, a document such as ‘Mercurii Trismegisti Pymander…’ by Hermes, Trismegistus, published in 1534, is presented with extensive bibliographic details including its creator, full title, printer, publication place, and date. Furthermore, granular language information, OCR quality assessments, and detailed document statistics enhance the scholarly utility of each entry.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#veritrace-match-tool-textual-similarity-and-customisation",
    "href": "chapter_ai-nepi_006.html#veritrace-match-tool-textual-similarity-and-customisation",
    "title": "6  The VERITRACE Project",
    "section": "6.17 VERITRACE Match Tool: Textual Similarity and Customisation",
    "text": "6.17 VERITRACE Match Tool: Textual Similarity and Customisation\n\n\n\nSlide 19\n\n\nThe VERITRACE Match Tool is meticulously designed to identify textual reuse and similarities between documents, primarily leveraging vector embeddings. This versatile tool supports various comparison modes: users can compare a single document against another, conduct multi-document comparisons (e.g., Newton’s Latin Opticks against all of Kepler’s works), or even attempt a full corpus comparison, though the latter presents significant computational challenges regarding processing power and user waiting times.\nCrucially, the tool offers extensive customisation through a multitude of parameters, allowing users to fine-tune the matching algorithms. Lexical matching, for instance, includes adjustable parameters such as minimum similarity (0.65), maximum results (100), and Jaccard/BM25 weights. Semantic matching, conversely, features settings for minimum similarity (0.85), short passage thresholds, and vector normalisation. A hybrid matching option combines both approaches with adjustable weighting. Additional parameters govern corpus matching batch sizes and query settings. A key case study involves comparing Newton’s Latin and English Opticks to evaluate both lexical and semantic matches, demonstrating the tool’s capacity for cross-lingual analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-matching-methodologies-lexical-semantic-and-hybrid",
    "href": "chapter_ai-nepi_006.html#text-matching-methodologies-lexical-semantic-and-hybrid",
    "title": "6  The VERITRACE Project",
    "section": "6.18 Text Matching Methodologies: Lexical, Semantic, and Hybrid",
    "text": "6.18 Text Matching Methodologies: Lexical, Semantic, and Hybrid\n\n\n\nSlide 20\n\n\nThe VERITRACE Match Tool employs two fundamental types of text matching: lexical and semantic, with a hybrid option combining both. Lexical matching identifies textually similar passages based on keyword overlap and vocabulary similarity. However, this approach proves ineffective across different languages, as direct lexical matches are unlikely. Consequently, for the project’s multilingual corpus, semantic matching becomes indispensable. This method utilises vector embeddings to identify conceptually similar passages, even when they share no common linguistic vocabulary, thereby enabling cross-lingual conceptual comparisons. A hybrid approach allows for the combination of both methods with adjustable weighting.\nFurthermore, the tool offers distinct matching modes: ‘Standard’ for balanced performance, ‘Comprehensive’ for maximum recall (albeit slower), and ‘Selective’ for higher precision (faster). A crucial ‘sanity check’ involves a lexical match between Newton’s Latin Opticks (1719) and its English counterpart (1718). As anticipated, the ‘Standard’ mode yields no matches, confirming the ineffectiveness of lexical matching across languages. Interestingly, the ‘Comprehensive’ mode reveals three matches, likely indicating instances of English text, such as a preface, embedded within the Latin edition.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-match-results-and-semantic-match-expectations",
    "href": "chapter_ai-nepi_006.html#lexical-match-results-and-semantic-match-expectations",
    "title": "6  The VERITRACE Project",
    "section": "6.19 Lexical Match Results and Semantic Match Expectations",
    "text": "6.19 Lexical Match Results and Semantic Match Expectations\n\n\n\nSlide 21\n\n\nThe VERITRACE system visually presents lexical match results with automatic highlighting of shared terms, displaying the source passage on the left and the comparison passage on the right, alongside a similarity score. A ‘sanity check’ involving a lexical comparison of Newton’s English Opticks (1718) against itself demonstrates the system’s precision: it yields a 100% normalised match score, a 99.7% coverage score, and a perfect 100% quality score. This comparison, involving 1,140 passages and over 1.2 million individual comparisons, completes in 23 seconds, identifying 1,137 estimated matches, all falling within the 90-100% similarity range.\nCrucially, when performing a semantic match between a text and its translation, such as the Latin and English versions of Opticks, the researchers anticipate reasonable matches. Despite significant lexical differences, the conceptual similarity between a text and its translation should be accurately captured by semantic embeddings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-match-highlighting-and-semantic-match-results",
    "href": "chapter_ai-nepi_006.html#lexical-match-highlighting-and-semantic-match-results",
    "title": "6  The VERITRACE Project",
    "section": "6.20 Lexical Match Highlighting and Semantic Match Results",
    "text": "6.20 Lexical Match Highlighting and Semantic Match Results\n\n\n\nSlide 22\n\n\nThe VERITRACE system provides automatic highlighting for lexical matches, identifying terms based on overlap and BM25 relevance ranking. This process successfully analyses 1,137 passages, skipping only three deemed too short for matching. Examples demonstrate precise 100% similarity scores for identical or near-identical passages, such as descriptions of ‘Mercury’ in various forms or discussions of chemical solutions and physical phenomena.\nWhen conducting semantic matches between the Latin and English versions of Newton’s Opticks, the results generally appear reasonable, even in the presence of OCR issues, with conceptually similar passages, such as those discussing ‘colors’, aligning effectively. However, the match score functionality requires further development; whilst the quality score remains high, the coverage score is inconsistent. This discrepancy might be partially explained by the Latin edition being considerably longer than its English counterpart. Nevertheless, broader queries suggest that the current embedding model, LaBSE, is ultimately inadequate for the project’s complex requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-matching-interface-and-progress",
    "href": "chapter_ai-nepi_006.html#semantic-matching-interface-and-progress",
    "title": "6  The VERITRACE Project",
    "section": "6.21 Semantic Matching Interface and Progress",
    "text": "6.21 Semantic Matching Interface and Progress\n\n\n\nSlide 23\n\n\nThe VERITRACE interface for semantic matching offers clear configuration options. Users select from ‘Lexical’, ‘Semantic’ (the default for conceptual comparisons), or ‘Hybrid’ match types, whilst also choosing a ‘Standard’, ‘Comprehensive’, or ‘Selective’ matching mode to balance performance and accuracy. A prominent ‘Run Text Matching’ button initiates the analysis. Results are then presented across three distinct tabs: ‘Match Summary’, ‘Match Details’, and ‘Visualization’. During the comparison process, a progress bar indicates the current status, such as ‘Comparing documents… (20%)’, alongside an instruction to run the matching to view results. This module specifically aims to evaluate translation quality through rigorous text comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-match-results-and-model-adequacy",
    "href": "chapter_ai-nepi_006.html#semantic-match-results-and-model-adequacy",
    "title": "6  The VERITRACE Project",
    "section": "6.22 Semantic Match Results and Model Adequacy",
    "text": "6.22 Semantic Match Results and Model Adequacy\n\n\n\nSlide 24\n\n\nWhen conducting semantic matches between the Latin and English versions of Newton’s Opticks, the results generally appear reasonable, even in the presence of OCR issues, with corresponding paragraphs discussing concepts like ‘Mercury’ and ‘colours’ aligning effectively. The system reports a combined match score of 58%, with a coverage score of 36.9% and a high quality score of 91.2%. Analysis of the 421 estimated matches reveals that whilst some achieve 90-100% similarity, the majority fall within the 70-90% range. The Latin query document contains 1,996 passages, compared to 1,140 in the English comparison document, and the query completes 2.2 million comparisons in 25 seconds.\nDespite these promising indicators, the project team expresses concern that the current embedding model, LaBSE, is likely inadequate for the task. This inadequacy may stem from ‘out-of-domain model collapse’, a phenomenon observed when models trained on modern languages are applied to historical, multilingual texts, leading to suboptimal performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#future-challenges-and-model-selection",
    "href": "chapter_ai-nepi_006.html#future-challenges-and-model-selection",
    "title": "6  The VERITRACE Project",
    "section": "6.23 Future Challenges and Model Selection",
    "text": "6.23 Future Challenges and Model Selection\n\n\n\nSlide 25\n\n\nThe VERITRACE project faces several critical challenges on the horizon, particularly concerning model selection and data quality. Whilst LaBSE serves as an initial choice for vector embeddings, the researchers acknowledge the potential superiority of other models, such as XLM-Roberta, intfloat multilingual-e5-large, or historical mBERT, each presenting distinct trade-offs in terms of storage requirements and inference time. A key strategic question remains whether to fine-tune a base model specifically on the project’s unique historical corpus, given its distinct characteristics, or to persist with existing pre-trained models.\nFurthermore, the evolving nature of semantic meaning across centuries poses a significant hurdle; ensuring that texts from 1540 and 1700, written in different languages, occupy a coherent vector space presents a complex problem. Poor OCR quality, a pervasive issue, fundamentally impacts all downstream processes, including the crucial segmentation of texts into sentences and passages. Re-OCR’ing the entire corpus is not feasible, prompting considerations of re-OCR’ing only the lowest quality texts or actively seeking out existing high-quality versions. Finally, scaling and performance represent substantial future concerns; current queries on a mere 132 texts take 15 seconds, implying considerable challenges when expanding to the full 430,000-text corpus, necessitating robust solutions for managing computational resources and query times.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#additional-visual-materials",
    "href": "chapter_ai-nepi_006.html#additional-visual-materials",
    "title": "6  The VERITRACE Project",
    "section": "6.24 Additional Visual Materials",
    "text": "6.24 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n The slide is titled ‘Issues on the Horizon,’ indicating a discussion of future challenges and considerations. The background is a light beige, framed by a thick black border. In the top right corner, a decorative element features stylized plant branches with leaves, rendered in a light purple or lavender color, extending from the corner into the slide area. The content is presented as a series of bullet points addressing various technical and methodological concerns. The first point discusses model selection, stating that ‘LaBSE is just a starting choice – there are other embedding models that might work better: XLM-Roberta, intfloat multilingual-e5-large, historical mBERT, or others. All have trade-offs.’ This highlights the need to evaluate different embedding models for specific tasks. The second point poses a strategic question: ‘Or is this a losing battle and we ought to fine-tune one of these base models on our historical corpus?’ This suggests a debate between using off-the-shelf models versus domain-specific fine-tuning. The third issue addresses diachronic semantic change: ‘Semantic meaning changes over time – how to handle that across centuries?’ This points to the challenge of applying modern language models to historical texts where word meanings may have evolved. The fourth bullet focuses on data quality, specifically OCR errors: ‘Poor-quality OCR would seem to affect everything downstream. Not feasible to re-OCR our entire corpus. Re-OCR the very poor-quality texts? Invest time to find existing high-quality versions?’ This outlines the significant problem of noisy historical data and potential strategies for mitigation. The fifth point notes that ‘Scaling and performance will increasingly become an issue,’ indicating future challenges with computational resources as data volumes grow. Finally, the slide concludes with an open invitation for collaboration or guidance: ‘Advice very welcome!’",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The VERITRACE Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "",
    "text": "Overview\nDr Oliver Eberle, a postdoctoral senior researcher at the Berlin Institute for Learning and Data, offers a comprehensive overview of his work at the intersection of machine learning and the digital humanities. Drawing from his deep background in machine learning, he has forged a path into the humanities through vital collaborations with historians. His research centres on the interpretability of Large Language Models (LLMs) and the broader field of Explainable Artificial Intelligence (XAI).\nThe presentation systematically addresses two principal areas. Firstly, it establishes a foundational understanding of AI, particularly LLMs, and explores the imperative for their transparency and comprehensibility. Secondly, it demonstrates the practical application of AI methodologies to generate novel scientific insights within humanities disciplines, revealing how such tools can uncover new patterns, connections, and interpretations in fields like history and literature.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#a-dual-focus-explainable-ai-and-ai-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#a-dual-focus-explainable-ai-and-ai-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.1 A Dual Focus: Explainable AI and AI in the Humanities",
    "text": "7.1 A Dual Focus: Explainable AI and AI in the Humanities\n\n\n\nSlide 02\n\n\nThis work establishes a dual focus, addressing two principal areas in a systematic fashion. The first area of inquiry delves into Explainable Artificial Intelligence (XAI) and the critical need to understand Large Language Models (LLMs). This segment explores the interpretability, functionality, and transparency of AI systems, specifically tackling the challenge of comprehending the reasoning and decision-making mechanisms within these complex models.\nThe discussion then shifts to the generation of AI-based scientific insights within the humanities. Here, the focus is on applying artificial intelligence methodologies to produce novel research findings. The aim is to uncover new patterns, connections, and interpretations within disciplines such as history, literature, and philosophy, thereby making a significant contribution to scholarly understanding in these fields.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#introducing-explainable-ai-the-first-wave",
    "href": "chapter_ai-nepi_007.html#introducing-explainable-ai-the-first-wave",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.2 Introducing Explainable AI: The First Wave",
    "text": "7.2 Introducing Explainable AI: The First Wave\n\n\n\nSlide 03\n\n\nThe discourse transitions to an introduction of Explainable AI 1.0, with a specific focus on feature attributions. This foundational concept in XAI involves determining the importance or contribution of individual input features to a model’s prediction. By doing so, it enhances the transparency of the model’s decision-making process. Dr Eberle provides a concise overview, defining XAI from the perspective of the machine learning community.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#the-black-box-problem-and-the-need-for-explainability",
    "href": "chapter_ai-nepi_007.html#the-black-box-problem-and-the-need-for-explainability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.3 The Black Box Problem and the Need for Explainability",
    "text": "7.3 The Black Box Problem and the Need for Explainability\n\n\n\nSlide 04\n\n\nHistorically, machine learning concentrated primarily on visual data; a significant shift towards language-based applications has only emerged in the last decade. To address the inherent opacity of these ‘black box’ models, the machine learning community developed Explainable Artificial Intelligence (XAI). For instance, whilst a model might accurately identify a rooster from an image, users typically lack insight into the basis of this prediction. Consequently, a decade of XAI research has been dedicated to understanding and tracing the origins of such predictions.\nA common XAI technique involves generating heatmaps, which visually indicate the specific pixels responsible for a classification, thereby clarifying why a model recognised the rooster. Beyond mere technical transparency, the motivations for XAI are multifaceted. As Samek and colleagues (2017) outline, these include:\n\nVerifying that predictions are reasonable.\nFacilitating the correction of errors by illuminating how they occur.\nAiding the understanding of the underlying problem, as models can uncover surprising solutions.\nEnsuring compliance with evolving regulations, such as the European AI Act.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#post-hoc-explainability-and-its-motivations",
    "href": "chapter_ai-nepi_007.html#post-hoc-explainability-and-its-motivations",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.4 Post-Hoc Explainability and Its Motivations",
    "text": "7.4 Post-Hoc Explainability and Its Motivations\n\n\n\nSlide 05\n\n\nPost-hoc explainability concentrates on interpreting AI decisions after they have been generated. This process typically involves feeding an input, such as an image of a rooster, into a black box AI system, which in turn yields a prediction. To elucidate this prediction, specific explanation methods interact with the system to produce a heatmap.\nWhen overlaid onto the original input, this heatmap visually highlights the pixels most influential in the AI’s decision—for instance, concentrating on the rooster’s head. This methodology, consistent with the framework of Samek et al. (2017), provides crucial insights into the AI’s reasoning. The imperative for such explainability stems from the need to verify predictions, identify flaws and biases, learn more about the underlying problem, and ensure compliance with legislation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#generative-ai-and-new-interpretability-challenges",
    "href": "chapter_ai-nepi_007.html#generative-ai-and-new-interpretability-challenges",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.5 Generative AI and New Interpretability Challenges",
    "text": "7.5 Generative AI and New Interpretability Challenges\n\n\n\nSlide 06\n\n\nThe field of artificial intelligence has undergone a significant paradigm shift, moving from traditional classification models to the era of Generative AI. Whilst older systems primarily performed discrete tasks such as classifying an input image, contemporary generative models exhibit a far broader range of capabilities. These advanced systems can classify, identify similar images, generate novel content, and engage in comprehensive question-and-answer interactions across diverse topics.\nConsequently, grounding a prediction or an answer from a Large Language Model (LLM) in its specific input has become considerably more challenging. The field now requires interpretability methods that extend beyond simple heatmap representations, focusing instead on intricate feature interactions and a more mechanistic understanding of the models’ internal workings. Today’s foundation models function not merely as specialised classifiers but as multi-task ‘world models’, offering profound insights into societal evolution, textual dynamics, and the inherent features of language itself. This evolution necessitates advanced techniques to fully comprehend their complex outputs.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "href": "chapter_ai-nepi_007.html#illustrative-model-failures",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.6 Illustrative Model Failures",
    "text": "7.6 Illustrative Model Failures\n\n\n\nSlide 07\n\n\nAI models, including advanced LLMs, demonstrably make surprising mistakes. In object detection, for example, Lapuschkin and colleagues (Nat Commun ’19) showed that a standard classifier might erroneously base its prediction of a boat on the surrounding water rather than the vessel itself, simply because water presents a correlated and texturally simpler feature to detect.\nIn multi-step planning, contemporary LLMs also exhibit significant limitations. Mondal, Webb, and colleagues (arxiv ’24) tasked a Llama 3.x model with solving the Tower of Hanoi puzzle. The model immediately erred by attempting to move the largest, physically inaccessible disk directly to the final peg. This failure highlights the model’s inability to comprehend the problem’s fundamental physical constraints, underscoring the ongoing challenges in developing truly robust AI systems.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-towards-structured-interpretability",
    "href": "chapter_ai-nepi_007.html#xai-2.0-towards-structured-interpretability",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.7 XAI 2.0: Towards Structured Interpretability",
    "text": "7.7 XAI 2.0: Towards Structured Interpretability\n\n\n\nSlide 08\n\n\nThe discussion now pivots to Explainable AI (XAI) 2.0, which focuses on structured interpretability. This advanced approach aims to transcend the limitations of conventional heatmap representations. It seeks to provide more organised, systematic, and hierarchical explanations for AI model behaviour, with the subsequent sections detailing methodologies that achieve this deeper level of insight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-explanations-for-verification",
    "href": "chapter_ai-nepi_007.html#first-order-explanations-for-verification",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.8 First-Order Explanations for Verification",
    "text": "7.8 First-Order Explanations for Verification\n\n\n\nSlide 09\n\n\nThe exploration of ‘first-order explanations’ provides a methodology for elucidating the behaviour of classifiers, typically by generating heatmaps over classification outputs. A pertinent case study involved a classifier trained on historical table data, where the objective was to distinguish between specific subgroups of these tables. To verify that the classifier was operating meaningfully, heatmaps were employed to ensure the model’s decisions were grounded in relevant features.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#features-and-pairwise-interactions",
    "href": "chapter_ai-nepi_007.html#features-and-pairwise-interactions",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.9 Features and Pairwise Interactions",
    "text": "7.9 Features and Pairwise Interactions\n\n\n\nSlide 10\n\n\nTo enhance model interpretability, the analysis of relevant features and their interactions is paramount. In a first-order analysis of classifier predictions, a model designed to classify historical tables correctly prioritised numerical content. This focus on numerical data served as an effective proxy for identifying such tables, with heatmaps visually confirming the relevance of these individual features.\nMoving to a second-order analysis, the emphasis shifts to pairwise relationships. Here, investigating similarity proved crucial. By computing a dot product between the embeddings of two images, the project team discovered that feature interactions provided an appropriate representation for explaining similarity predictions. This method revealed, for instance, specific interactions between digits that confirmed the identity of two historical tables, thereby verifying the model’s intended functionality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-and-circuit-level-understanding",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-and-circuit-level-understanding",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.10 Higher-Order Interactions and Circuit-Level Understanding",
    "text": "7.10 Higher-Order Interactions and Circuit-Level Understanding\n\n\n\nSlide 11\n\n\nMore recent scholarship has turned to graph structures, discovering that higher-order interactions offer more meaningful insights into model behaviour. When applied to contexts such as citation networks or networks of books and entities, this approach reveals that feature subgraphs or ‘feature walks’—defined as sets of features that collectively become relevant—are particularly significant. Identifying these complex interactions facilitates a deeper, ‘circuit-level’ comprehension of a model’s internal mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-large-language-models",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-large-language-models",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.11 First-Order Attributions in Large Language Models",
    "text": "7.11 First-Order Attributions in Large Language Models\n\n\n\nSlide 12\n\n\nThe presentation now transitions to a focused examination of first-order attributions within Large Language Models (LLMs). Through concrete examples drawn from language and the humanities, this segment explores the direct causal links between inputs and outputs in LLMs, elucidating the fundamental mechanisms that underpin their interpretability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#biased-sentiment-predictions-in-transformer-models",
    "href": "chapter_ai-nepi_007.html#biased-sentiment-predictions-in-transformer-models",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.12 Biased Sentiment Predictions in Transformer Models",
    "text": "7.12 Biased Sentiment Predictions in Transformer Models\n\n\n\nSlide 13\n\n\nIn a study presented at ICML in 2022, Ali and colleagues investigated biased sentiment predictions within Transformer LLMs. Employing first-order attributions in a standard movie review sentiment analysis scenario, their methodology analysed how certain names influenced sentiment predictions. Using heatmaps generated by a novel method for Transformers, they observed a distinct bias: positive sentiment was more frequently associated with male Western names such as Lee, Barry, or Raphael.\nConversely, negative sentiment scores were more probable when names such as Saddam, Castro, or Chan were present. This study clearly identified fine-grained biases, a phenomenon increasingly recognised within the AI community. The work underscores how XAI can prove invaluable for detecting these subtle, yet significant, model biases.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#long-range-dependencies-in-llms",
    "href": "chapter_ai-nepi_007.html#long-range-dependencies-in-llms",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.13 Long-Range Dependencies in LLMs",
    "text": "7.13 Long-Range Dependencies in LLMs\n\n\n\nSlide 14\n\n\nIn a paper for NeurIPS 2024, Jafari and colleagues investigated first-order attributions to understand long-range dependencies in LLMs. They tasked models with generating summaries from extensive inputs, specifically Wikipedia articles, within an 8,000-token context window. The core inquiry was whether models could effectively utilise information from distant parts of the context.\nBy analysing the source of the generated text, the team observed that a token such as ‘1972’ was accurately attributed to information located 5,775 tokens earlier in the context. This information originated from passages referencing ‘Bread’s 1972 album Guitar Man’ and a ‘List of number-one adult contemporary singles of 1972 (U.S.)’. This example compellingly demonstrates an LLM’s capacity to leverage long-range dependencies for precise text generation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#limitations-in-long-range-summarisation",
    "href": "chapter_ai-nepi_007.html#limitations-in-long-range-summarisation",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.14 Limitations in Long-Range Summarisation",
    "text": "7.14 Limitations in Long-Range Summarisation\n\n\n\nSlide 15\n\n\nThe work of Jafari and colleagues (NeurIPS ’24) also revealed a crucial operational characteristic of LLMs. Whilst models can draw information from the entire 8,000-token context window, they predominantly prioritise data located in the later parts of the input. A histogram illustrating this tendency shows that although mid-range and long-range dependencies are observed, their frequency diminishes significantly compared to short-range interactions.\nConsequently, users employing LLMs for text summarisation should anticipate an unbalanced output. The model is more likely to concentrate on information presented closer to the prompt rather than providing a comprehensive, evenly distributed summary of the entire input text, a key consideration for practical applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-interactions-in-text",
    "href": "chapter_ai-nepi_007.html#higher-order-interactions-in-text",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.15 Higher-Order Interactions in Text",
    "text": "7.15 Higher-Order Interactions in Text\n\n\n\nSlide 16\n\n\nBuilding upon prior discussions of embeddings, the presentation now advances to a detailed examination of second and higher-order interactions within textual data. This segment explores complex relationships that extend beyond simple, direct connections, delving into the intricate patterns, contextual dependencies, and emergent properties inherent in language. The methodologies discussed aim to uncover these sophisticated interactions, offering a more nuanced understanding of textual structures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explaining-sentence-similarity-with-bilrp",
    "href": "chapter_ai-nepi_007.html#explaining-sentence-similarity-with-bilrp",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.16 Explaining Sentence Similarity with BiLRP",
    "text": "7.16 Explaining Sentence Similarity with BiLRP\n\n\n\nSlide 17\n\n\nTo explain sentence similarities, Eberle and colleagues (TPAMI 2022) conducted an unsupervised analysis of representations from a pretrained foundation model, using dot products as the similarity metric. Whilst a model might yield a high similarity score for sentences like ‘A cat I really like’ and ‘It is a great cat!’, the underlying reasons often remain opaque.\nTo address this, the team developed the Bilinear Layer-wise Relevance Propagation (BiLRP) method. This technique, formalised by the equation BiLRP(y, x, x') = Σ_{m=1}^{h} LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x) ⊗ LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x'), decomposes the similarity score into contributions from individual tokens or token pairs. The resulting interaction scores clarify the model’s reasoning.\nAnalysis of these token interactions frequently reveals surprisingly simplistic strategies. These include noun matching (both synonyms and identical tokens) and some noun-verb interactions, suggesting that models often rely on a ‘bag of token types’ approach. This finding is crucial for practitioners, as it indicates that the features driving high similarity scores in embeddings may be remarkably simple.\n\n\n\nSlide 18",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-data",
    "href": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-data",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.17 Graph Neural Networks for Structured Data",
    "text": "7.17 Graph Neural Networks for Structured Data\n\n\n\nSlide 19\n\n\nThe exploration of Graph Neural Networks (GNNs) offers a powerful approach for structured predictions. These models are designed to process graph-structured data, such as complex networks of interconnected nodes and edges. The operational flow of a GNN involves feeding an input graph into a processing pipeline, where an iterative ‘interaction’ phase refines information across the graph through recurrent message-passing.\nThis process updates node features based on their neighbours, culminating in a final hidden state that yields the model’s prediction. Crucially for interpretability, GNNs enable the derivation of attributions in terms of ‘walks’, which represent specific interactions of features within the graph structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explaining-predictions-with-walks",
    "href": "chapter_ai-nepi_007.html#explaining-predictions-with-walks",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.18 Explaining Predictions with Walks",
    "text": "7.18 Explaining Predictions with Walks\n\n\n\nSlide 20\n\n\nSchnake and colleagues (TPAMI 2022) developed a conceptual framework for explaining GNN and LLM predictions by leveraging ‘walks’. In their method, a walk-based relevance analysis identifies influential sequences of connected nodes and edges, progressively simplifying the explanation from a full graph to a subgraph of only the most relevant components.\nA key insight from their work is that GNNs, which inherently encode structural information, can be framed as LLMs. The attention network within an LLM effectively determines which tokens can engage in ‘message passing’, a process analogous to node interactions in a GNN. This conceptual alignment enables the application of walk-based interpretability methods to language models, offering a powerful tool for understanding their complex behaviours.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#limitations-of-standard-explanations",
    "href": "chapter_ai-nepi_007.html#limitations-of-standard-explanations",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.19 Limitations of Standard Explanations",
    "text": "7.19 Limitations of Standard Explanations\n\n\n\nSlide 21\n\n\nIn an illustrative example, Schnake and colleagues (TPAMI 2022) demonstrated how node interactions facilitate the learning of complex language structures. Their premise is that the hierarchical nature of language is well-suited for graph representations, enabling models to be trained on sentiment tasks to extract meaningful ‘walks’ or linguistic patterns.\nHowever, standard explanation methods like the Bag-of-Words (BoW) approach prove inadequate. For the sentence, ‘First I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen’, a first-order, BoW-like explanation might incorrectly assign a high positive score to the initial negative clause due to the word ‘like’, failing to interpret the negation. This highlights the critical need for higher-order interaction methods to achieve a more nuanced understanding.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-explanations-for-nuanced-sentiment",
    "href": "chapter_ai-nepi_007.html#higher-order-explanations-for-nuanced-sentiment",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.20 Higher-Order Explanations for Nuanced Sentiment",
    "text": "7.20 Higher-Order Explanations for Nuanced Sentiment\n\n\n\nSlide 22\n\n\nEmploying their more complex, higher-order explanation methods, Schnake and colleagues demonstrated a significantly improved capacity to interpret nuanced language. For the same example sentence, their advanced techniques accurately assigned a negative score to the entire initial clause, correctly identifying its negative sentiment. Conversely, the second part of the sentence received a positive score, reflecting its true meaning.\nThis result illustrates how higher-order methods can precisely capture the hierarchical and often contradictory elements within complex linguistic expressions. The presentation now transitions to its second major section, focusing on AI-based scientific insights within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#ai-for-scientific-insight-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#ai-for-scientific-insight-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.21 AI for Scientific Insight in the Humanities",
    "text": "7.21 AI for Scientific Insight in the Humanities\n\n\n\nSlide 23\n\n\nThe second major section of the presentation focuses on generating AI-based scientific insights within the humanities. Initial explorations in this domain commenced with heatmap-based approaches, specifically applied to a corpus of historical mathematical instruments. This part of the discussion details how artificial intelligence can be leveraged to produce novel scientific understanding, bridging computational methodologies with traditional humanistic inquiry.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-corpora",
    "href": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-corpora",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.22 Extracting Visual Definitions from Corpora",
    "text": "7.22 Extracting Visual Definitions from Corpora\n\n\n\nSlide 24\n\n\nIn a study for the International Journal of Digital Humanities (2023), El-Hajj, Eberle, and their collaborators explored the extraction of visual definitions from historical corpora. They applied computational methods to illustrations of mathematical instruments, developing a classifier to categorise images into classes such as ‘machine’ or ‘mathematical instrument’. This was achieved through class-specific heatmap explanations, which highlighted the visual features most salient for a particular classification.\nThis endeavour necessitated close collaboration with historians, including Matteo Valeriani and Jochen Büttner, to verify the objectivity of these computationally derived definitions. A key finding revealed that the fine-grained scales on mathematical instruments were highly relevant features for the model. This approach demonstrates how computational techniques can derive and visualise the ‘visual definitions’ a model learns, enhancing transparency in digital humanities research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#analysing-early-modern-astronomical-tables",
    "href": "chapter_ai-nepi_007.html#analysing-early-modern-astronomical-tables",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.23 Analysing Early Modern Astronomical Tables",
    "text": "7.23 Analysing Early Modern Astronomical Tables\n\n\n\nSlide 25\n\n\nIn a major cooperative project, Dr Eberle’s team undertook a corpus-level analysis of early modern astronomical tables, focusing on numerical data from the Sphaera Corpus (1472–1650). Historians Matteo Valeriani and Jochen Büttner initiated this collaboration, seeking an automated method for matching tables with similar semantics—a task previously unfeasible at scale.\nThe corpus comprises diverse examples, from TABVLA SINVVM RECTORVM (tables of sines) to TABVLA CLIMA TVM SECVNDVM PARTItionem ueterum (tables of climates). The work draws from both the Sphaera Corpus (Valleriani et al., 2019) and the Sacrobosco Table Corpus (Eberle et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-historian-a-workflow-for-scalable-insight",
    "href": "chapter_ai-nepi_007.html#xai-historian-a-workflow-for-scalable-insight",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.24 xAI-Historian: A Workflow for Scalable Insight",
    "text": "7.24 xAI-Historian: A Workflow for Scalable Insight\n\n\n\nSlide 26\n\n\nThe xAI-Historian project, published by Eberle and colleagues in Science Advances (2024), pioneers a workflow to provide historians with insights at scale. Recognising historical tables as crucial carriers of scientific knowledge, this initiative leverages the Sacrobosco Corpus, an extensive dataset of university textbooks. The project addresses significant machine learning challenges, including heterogeneous data and limited annotations.\nThe system operates through a three-stage workflow:\n\nData Collections involves sourcing diverse historical books.\nAtomization-Recomposition transforms raw tables into structured formats by generating ‘bigram maps’ and then ‘histograms’ (Φ(x)) as statistical embeddings.\nCorpus-Level Analysis embeds these processed tables into a shared space, where proximity signifies similarity, quantified by the dot product y(x, x') = ⟨Φ(x), Φ(x')⟩.\n\nRather than using general-purpose foundation models, the team developed a custom statistical model for bigram detection. This bespoke approach empowers historians to engage in data-driven hypothesis generation and discover novel case studies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#verifying-features-with-xai-and-historians",
    "href": "chapter_ai-nepi_007.html#verifying-features-with-xai-and-historians",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.25 Verifying Features with XAI and Historians",
    "text": "7.25 Verifying Features with XAI and Historians\n\n\n\nSlide 27\n\n\nThe verification of modelling and features in the historical table analysis relies on a collaborative approach involving XAI and historians. As documented by Eberle and colleagues (TPAMI ‘22; Sci Adv ’24), historical tables are represented using a ’bag of bigrams’, such as patterns like ‘01’ or ‘21’. This method employs a learned feature extractor with a hard-coded structure, operating effectively despite limited annotations.\nEmpirical results underscore the efficacy of this approach. An expert ground truth analysis reveals a direct relationship between table density and model performance: higher densities yield superior correlation coefficients. In cluster classification, the bigram model achieved the highest purity (≈0.9), significantly outperforming unigram models (≈0.75) and VGG-16 (≈0.65), demonstrating its superior performance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#analysing-innovation-spread-with-cluster-entropy",
    "href": "chapter_ai-nepi_007.html#analysing-innovation-spread-with-cluster-entropy",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.26 Analysing Innovation Spread with Cluster Entropy",
    "text": "7.26 Analysing Innovation Spread with Cluster Entropy\n\n\n\nSlide 28\n\n\nTo investigate the historical spread of innovation, Eberle and colleagues (Science Advances, 2024) employed cluster entropy analysis. Their methodology involved calculating the ‘difference between the observed cluster entropy H(p) and the maximum attainable entropy at each print location’. This metric quantitatively assesses the concentration or dispersion of innovative activities.\nHistorically, the diverse ‘programmes of types’ produced by various publishing cities could not be analysed at scale. To overcome this, the team devised a novel clustering approach using robust representations from their bespoke bigram model. This method allowed them to quantify innovation diversity across numerous European cities, including London, Paris, Venice, and Wittenberg, providing unprecedented insights into historical intellectual networks.\n\n\n\nSlide 29",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#entropy-and-historical-publishing-programmes",
    "href": "chapter_ai-nepi_007.html#entropy-and-historical-publishing-programmes",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.27 Entropy and Historical Publishing Programmes",
    "text": "7.27 Entropy and Historical Publishing Programmes\n\n\n\nSlide 31\n\n\nContinuing their cluster entropy analysis, Eberle and colleagues used a distance-based clustering approach to quantify the diversity of ‘programmes of types’ from each city. A low entropy score indicated that a city consistently reproduced identical content, signifying a less diverse print programme. Conversely, a higher entropy score denoted a more varied publishing output.\nThis analysis revealed two particularly interesting cases: Frankfurt/Main and Wittenberg, both exhibiting the lowest entropy scores. Frankfurt/Main was confirmed as a recognised centre for reprinting editions. More notably, Wittenberg emerged as a historical anomaly, where political control by Protestant reformers actively limited the print programme. This finding, which matched existing historical intuition, underscores the method’s capacity to detect nuanced historical patterns.\n\n\n\nSlide 32",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-methods-in-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-methods-in-the-humanities",
    "title": "7  Explainable AI and AI-based Scientific Insights in the Humanities",
    "section": "7.28 Conclusion: AI Methods in the Humanities",
    "text": "7.28 Conclusion: AI Methods in the Humanities\n\n\n\nSlide 33\n\n\nThe presentation concludes by summarising the landscape of AI-based methods in the humanities. Whilst humanities and Digital Humanities (DH) scholars have largely concentrated on digitising source material, the automated analysis of these corpora presents non-trivial challenges, primarily due to data heterogeneity and the scarcity of labelled data.\nMultimodality emerges as a significant aspect, underscoring the necessity for AI methods capable of handling diverse data types beyond conventional text. The integration of Machine Learning (ML) with Explainable AI (XAI) holds substantial promise, potentially enabling the scaling of humanities research and fostering novel research directions.\nHowever, the role of Foundation Models and LLMs requires a nuanced understanding. Whilst prompting can facilitate intermediate tasks like labelling and data curation, these models remain limited when addressing more complex scholarly questions. Furthermore, the challenges of low-resource data, exacerbated by ‘scaling laws’ that demand extensive datasets, represent a significant roadblock.\nFinally, the issue of out-of-domain transfer is particularly pertinent for historical and small-scale data. This necessitates thorough evaluation, given that current LLMs are primarily trained for natural language and code generation, which may not align with the specific requirements of humanities data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Explainable AI and AI-based Scientific Insights in the Humanities</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "8  Validation is All You Need",
    "section": "",
    "text": "Overview\nThis presentation details a novel approach to historical inquiry, one that integrates advanced artificial intelligence with rigorous scholarly validation. The authors begin by tracing the swift evolution of Large Language Models (LLMs), charting their progression from the foundational ‘Attention is all you need’ paradigm, which established the Transformer architecture, to the ‘Context is all you need’ phase, which leverages Retrieval Augmented Generation for broader knowledge integration. This trajectory culminates in the emerging ‘Thinking is all you need’ paradigm, an ambitious pursuit of complex reasoning.\nDespite these advances, the discourse identifies significant limitations within current LLMs. The authors highlight their susceptibility to hallucination and their fundamental inability to perform genuine epistemic functions, such as critical evaluation, justification, and the strategic planning of inquiry. To address these deficiencies, they introduce ‘Computational Epistemology’, a nascent discipline focused on computationally validating propositions, arguments, evidence, and actions. This framework necessitates the cultivation of ‘epistemic agency’ within AI systems, enabling them to identify propositions beyond mere sentences, discern arguments within complex texts, and interpret human intentions from historical documents.\nA sophisticated research infrastructure underpins this vision. The authors’ Scholarium initiative provides a meticulously curated repository of scholarly sources, exemplified by the 86-volume Opera Omnia Euler, which serves as a validated content database. This structured approach offers a robust alternative to less reliable embedding-based methods. The technical implementation leverages the Cursor environment, an ‘AI Cockpit’ that integrates multiple multimodal LLMs. A case study reconstructing contributions to the Sanssouci water fountain demonstrates its facility, extracting precise details on personnel, tasks, materials, and remuneration.\nFurthermore, the project relies on a FAIR Infrastructure for data management, utilising Zenodo for long-term storage and publication. Technical support is provided by Open Science Technology, a startup dedicated to maintaining the infrastructure through open source, open access, and open data principles. This comprehensive ecosystem seeks to elevate AI from a generative tool to a reliable partner in scholarly validation and scientific inquiry.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-evolution-of-large-language-models",
    "href": "chapter_ai-nepi_008.html#the-evolution-of-large-language-models",
    "title": "8  Validation is All You Need",
    "section": "8.1 The Evolution of Large Language Models",
    "text": "8.1 The Evolution of Large Language Models\n\n\n\nSlide 02\n\n\nLarge Language Models have undergone a remarkably swift evolution, progressing through distinct stages of competence. Initially, the ‘Attention is all you need’ paradigm established the foundational Transformer architecture, enabling breakthroughs in conversational AI, as exemplified by early GPT models. Subsequently, the focus shifted to ‘Context is all you need’, a recognition of the critical role external information plays in enhancing LLM performance. This stage saw the integration of larger contexts, notably through Retrieval Augmented Generation (RAG) techniques, which allowed models to leverage external knowledge bases.\nCurrently, the latest models advance a ‘Thinking is all you need’ proposition, indicating an emerging capability for complex reasoning, whether executed with or without a predefined plan. Whilst these advancements represent significant strides, the authors contend that a crucial question remains: are these capabilities sufficient, or are vital elements still absent from the current LLM landscape?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#critical-gaps-in-current-llm-paradigms",
    "href": "chapter_ai-nepi_008.html#critical-gaps-in-current-llm-paradigms",
    "title": "8  Validation is All You Need",
    "section": "8.2 Critical Gaps in Current LLM Paradigms",
    "text": "8.2 Critical Gaps in Current LLM Paradigms\n\n\n\nSlide 03\n\n\nDespite their rapid evolution, contemporary Large Language Models exhibit several critical deficiencies concerning epistemic reliability. The authors argue that these models crucially lack an inherent ‘opponent’ mechanism to counter the pervasive issue of hallucination, where systems generate factually incorrect yet plausible outputs. Furthermore, a fundamental conceptual distinction often remains unaddressed: embedding vectors, whilst capturing semantic relationships, do not equate to the comprehensive meaning of expressions.\nConsequently, the authors propose a system that must adhere to principles forbidding the formulation of content merely because it ‘sounds good’ if it is demonstrably false. Similarly, it must not uncritically repeat information from unverified internet media, as such content does not constitute genuine knowledge. Instead, any reliable system should consistently seek what is demonstrably best justified and possess the capacity to formulate optimal plans for scientific inquiry. As no existing model can achieve these vital goals, a new imperative emerges: ‘Validation is all you need’.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#computational-epistemology-and-the-validation-imperative",
    "href": "chapter_ai-nepi_008.html#computational-epistemology-and-the-validation-imperative",
    "title": "8  Validation is All You Need",
    "section": "8.3 Computational Epistemology and the Validation Imperative",
    "text": "8.3 Computational Epistemology and the Validation Imperative\n\n\n\nSlide 04\n\n\nValidation, a cornerstone of rigorous inquiry, encompasses several critical functions. It systematically provides reasons, arguments, and evidence both for and against the truth of a given proposition. Beyond propositional truth, validation also extends to offering justifications for or against the pursuit of specific actions.\nTo address the significant validation gap in current AI, the authors have proposed a novel discipline: ‘Computational Epistemology’. This emerging field focuses on developing methods to imbue computational systems with the capacity for robust validation. Central to this endeavour is the cultivation of ‘epistemic agency’ within AI. Such agency necessitates the ability to identify propositions that transcend mere linguistic sentences, to discern complex arguments embedded within diverse texts, and to interpret accurately the intentions, plans, and actions of individuals as documented in historical records. This approach moves beyond superficial pattern recognition towards a deeper, contextually aware understanding.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-ai-assisted-historical-inquiry-workbench",
    "href": "chapter_ai-nepi_008.html#the-ai-assisted-historical-inquiry-workbench",
    "title": "8  Validation is All You Need",
    "section": "8.4 The AI-Assisted Historical Inquiry Workbench",
    "text": "8.4 The AI-Assisted Historical Inquiry Workbench\n\n\n\nSlide 05\n\n\nThe research team has developed a sophisticated working environment to facilitate AI-assisted historical inquiry. A split-view academic workspace demonstrates its utility. The left panel displays a historical text from the late 18th century, Manger1789.pdf, which details aspects of the reign of Frederick William II of Prussia. This document forms part of a broader investigation into the construction of Sanssouci castle, focusing on the disputed involvement of the eminent mathematician Leonhard Euler in one of the 18th century’s most significant construction failures.\nTo address such complex historical problems, the system allows users to formulate precise queries. For instance, the German query Rekonstruiere welche Personen an der Wasserfontaine welche Arbeiten ausführten (Reconstruct which persons performed which work on the water fountain) was posed to obtain a validated answer supported by proven evidence. After an 11-second processing period, the system generated a detailed list of individuals commissioned for work on the water fountain or Neptune group.\n\nNahl, a sculptor, created the drawings and small-scale models, receiving 200 Thaler.\nBenkert and Heymüller, under a contract dated 6 June 1746, produced models in the required size, for which they received 1,970 Thaler.\nGiese was responsible for casting the figures from lead, processing approximately 600 Zentner of lead, and was remunerated with 6,000 Thaler.\n\nThis analysis, based on specific sections of the historical document, showcases the system’s capacity to extract granular, factual information. The underlying technical framework, known as the Cursor environment, enables the deployment of AI agents for a variety of automated tasks, streamlining the research process.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-a-curated-scholarly-evidence-base",
    "href": "chapter_ai-nepi_008.html#scholarium-a-curated-scholarly-evidence-base",
    "title": "8  Validation is All You Need",
    "section": "8.5 Scholarium: A Curated Scholarly Evidence Base",
    "text": "8.5 Scholarium: A Curated Scholarly Evidence Base\n\n\n\nSlide 06\n\n\nTo overcome the limitations of conventional data processing, the authors established a robust scholarly evidence base termed Scholarium. This initiative employs an AI agent, named Bernoulli, to conduct inquiries across an extensive array of validated sources. A core component of Scholarium is its reliance on a scholarly curated editorial board, which meticulously processes and validates historical documents. A prime example of this rigorous curation is the Opera Omnia Euler, a monumental 86-volume collection representing 120 years of scholarly endeavour.\nThe curated content within Scholarium, complemented by other seminal works such as Opera Bernoulli Euler and Kepler Gesammelte Werke, serves as a sophisticated alternative to less reliable embedding-based approaches. It functions as a detailed inventory of historically proven activities, meticulously validated against original sources. This inventory includes precise chronologies of actions, communicated expressions, terminology usage, and the historical application of tools and materials.\nOnce established, these rich, validated records become amenable to inquiry using accessible multimodal models, such as the latest Gemini 2.5, which adeptly combines information from both text and image. Digital access to these scholarly editions is facilitated through platforms like the Euler Opera Omnia Viewer, accessible via euler-obe.streamlit.app.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#fair-infrastructure-and-technical-support",
    "href": "chapter_ai-nepi_008.html#fair-infrastructure-and-technical-support",
    "title": "8  Validation is All You Need",
    "section": "8.6 FAIR Infrastructure and Technical Support",
    "text": "8.6 FAIR Infrastructure and Technical Support\n\n\n\nSlide 09\n\n\nThe authors’ project leverages a robust FAIR (Findable, Accessible, Interoperable, Reusable) infrastructure to ensure the longevity and accessibility of its research outputs. Central to this is Zenodo, a long-term repository hosted by CERN that facilitates the storage and publication of diverse research data. This platform supports a wide array of research outputs, from astronomical data to spectroscopic measurements.\nTechnical support for this intricate infrastructure is provided by Open Science Technology, a dedicated startup. This entity ensures the seamless operation of the system, adhering to the core principles of open science.\n\nOpen Source: Software code is made freely available.\nOpen Access: Scholarly publications are disseminated broadly.\nOpen Data: Research data is freely available for public use and reuse via an MCP API server, enabling worldwide access by AI models.\nOpen Collaboration: The initiative aims to standardise AI access APIs to knowledge across the global scientific community.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation is All You Need</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nThis chapter details a conceptual inquiry into the representation of Sustainable Development Goal (SDG)-related research within bibliometric databases, employing Large Language Models (LLMs) to detect inherent biases. Ottaviani and Stahlschmidt aimed to utilise LLMs as a technological tool for assessing biases in publications classified across three prominent bibliometric databases: Web of Science, Scopus, and OpenAlex. Their study highlights the critical, yet performative, role of these databases in the sociology of science, acknowledging their influence on academic behaviour, funding, and policy, whilst also noting their susceptibility to political and commercial interests.\nThe research team’s methodology involved training a pre-existing, open-source LLM, DistilGPT2, on a shared corpus of over 15 million publications classified by the three databases. This approach circumvented the resource-intensive process of training an LLM from scratch, whilst ensuring minimal prior semantic knowledge. The project specifically focused on five SDGs related to socio-economic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure).\nA key component of the research design involved crafting 80-120 specific prompts for each SDG, derived from their respective targets, to serve as a benchmark for compliance and bias detection. The fine-tuned LLM processed these prompts using various decoding strategies (top-k, nucleus, contrastive search), generating responses from which noun phrases were extracted. Analysis of these noun phrases across four dimensions—locations, actors, data/metrics, and focuses—revealed a systematic overlook in the bibliometric data. The LLM’s responses consistently neglected disadvantaged categories of individuals, the poorest countries, and sensitive, underrepresented topics explicitly addressed by SDG targets, instead concentrating on economically powerful and highly developed nations. This finding underscores the decisive impact of seemingly objective science-informed practices, such as bibliometric classification, on research representation and, consequently, on policy and resource allocation. The authors acknowledge limitations, including the general framework and the high sensitivity of LLMs to architectural choices, training data, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-background-and-aims",
    "href": "chapter_ai-nepi_009.html#research-background-and-aims",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 Research Background and Aims",
    "text": "9.1 Research Background and Aims\n\n\n\nSlide 02\n\n\nOttaviani and Stahlschmidt embarked on this project with the primary aim of employing Large Language Models (LLMs) as a technological instrument to discern biases. Specifically, they sought to identify biases originating from publications classified within three prominent bibliometric databases. This foundational objective guided their subsequent methodological development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 SDG Classification in Databases",
    "text": "9.2 SDG Classification in Databases\n\n\n\nSlide 06\n\n\nBibliometric databases function as critical digital infrastructures, enabling extensive bibliometric analyses and impact assessments across the scientific community. These systems, however, exhibit a performative nature, fundamentally shaped by particular understandings of the scientific landscape and inherent value attributions, as scholarly works by Whitley (2000) and Winkler (1988) demonstrate. Consequently, they exert considerable influence over the conduct of academics, researchers, funding institutions, and policymakers alike. Crucially, these databases also reflect and respond to diverse political and commercial interests.\nIn recent years, leading bibliometric platforms, including Web of Science, Scopus, and OpenAlex, have implemented classifications designed to align publications with the United Nations Sustainable Development Goals (SDGs). Nevertheless, prior research, notably by Armitage et al. (2020), has revealed that SDG labelling practices across different providers—such as Elsevier, Bergen, and Aurora—produce markedly divergent results, exhibiting very limited overlap. These inconsistencies in classification can distort perceptions of research priorities, thereby potentially affecting resource allocation and policy decisions, often influenced by underlying political and commercial agendas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "href": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Case Study: SDGs in Bibliometric Data",
    "text": "9.3 Case Study: SDGs in Bibliometric Data\n\n\n\nSlide 08\n\n\nThis case study, conducted by Ottaviani and Stahlschmidt (2024), specifically investigates the aggregated effects on the representation of SDG-related research within bibliometric databases following the introduction of LLM-based tools. The authors adopted a methodological approach that involved deploying “little” pre-trained Large Language Models, notably DistilGPT2. They trained these models independently on distinct subsets of publication abstracts, each corresponding to the SDG classifications derived from various bibliometric databases. This LLM technology served a dual purpose: firstly, as a detector of inherent biases within the data; and secondly, as a proof-of-concept exercise demonstrating its potential for automating information extraction to inform research-related decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "href": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 Chain of Dependencies",
    "text": "9.4 Chain of Dependencies\n\n\n\nSlide 10\n\n\nThe research delineates a partial chain of dependencies, illustrating the intricate relationships within the science-policy interface. Initially, SDG classification directly defines the scope and nature of “Research” on SDGs. Subsequently, a diverse array of stakeholders, including researchers, Small and Medium-sized Enterprises (SMEs), governments, and various intermediate figures, actively process this SDG-focused research. This processed research then critically informs “Decision-making to align with SDGs,” which, in turn, directly impacts “Socioeconomic inequalities.”\nCrucially, the study positions the LLM as a “detector of ‘biases’” operating at the level of “Research” on SDGs. The “Introduction of LLM in Research Policy” emerges as a direct consequence of this bias detection, and this intervention ultimately influences “Socioeconomic inequalities.” Fundamentally, LLMs possess the capacity to alter the metadata associated with SDG research, thereby profoundly influencing the advice provided, choices made, indicators developed, and measures implemented within policy frameworks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Actors and SDG Selection",
    "text": "9.5 Actors and SDG Selection\nThe study focused on three principal bibliometric databases: Web of Science, a proprietary platform from Clarivate (US); Scopus, another proprietary service provided by Elsevier (UK); and OpenAlex, an open-access resource formerly associated with Microsoft (US). The authors strategically selected five Sustainable Development Goals to specifically model socio-economic inequalities. These included three SDGs representing the equity or socio dimension: SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities). Additionally, two SDGs were chosen to represent the economic and technological development dimension: SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data",
    "href": "chapter_ai-nepi_009.html#processed-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Processed Data",
    "text": "9.6 Processed Data\nThe research team processed a substantial dataset, comprising a jointly indexed subset of 15,471,336 publications. They meticulously collected these publications by identifying those shared across all three bibliometric databases—Web of Science, Scopus, and OpenAlex—through precise DOI matching. This collection spanned a timeframe from January 2015 to July 2023. The project then assessed the performance of the three distinct classification standards for the five selected SDGs. Consequently, for each SDG, three separate publication subsets emerged, each attributed to a specific bibliometric database. This rigorous approach, applying classification solely to the shared corpus, established a robust benchmark for subsequent comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 SDG Classification: Socio Dimension",
    "text": "9.7 SDG Classification: Socio Dimension\nInitial results from the comparative analysis of SDG-classified papers, focusing on the socio dimension, strongly corroborate the findings of Armitage (2020), demonstrating a consistently minimal overlap in SDG labelling across the bibliometric databases.\nFor SDG 04, pertaining to Quality Education:\n\nWeb of Science identified 124,359 publications (19.1%).\nOpenAlex identified 218,907 (33.6%).\nScopus identified 339,063 (52.2%).\n\nThe overlaps comprised:\n\n59,002 (9.0%) between Web of Science and OpenAlex.\n35,733 (5.5%) between Web of Science and Scopus.\n35,733 (5.5%) between OpenAlex and Scopus.\nA mere 46,711 (7.2%) across all three platforms.\n\nRegarding SDG 05, Gender Equality:\n\nWeb of Science classified 37,324 publications (57.4%).\nOpenAlex classified 71,727 (9.4%).\nScopus classified 82,273 (12.7%).\n\nThe overlaps were similarly low:\n\n31,210 (4.8%) for Web of Science and OpenAlex.\n26,377 (4.1%) for Web of Science and Scopus.\n34,898 (5.4%) for OpenAlex and Scopus.\n38,066 (12.1%) for all three.\n\nA notable observation emerged for SDG 05: whilst Scopus contained publications relevant to this goal, it often did not classify them as such. Furthermore, Web of Science classified approximately 10% of its SDG 05 publications from the field of mathematics, including topics such as geometrical differential equations, suggesting potential misclassification or a failure to capture relevant content comprehensively.\nFor SDG 10, focused on Reducing Inequalities:\n\nWeb of Science identified 95,460 publications (12.2%).\nOpenAlex identified 213,419 (43.3%).\nScopus identified 236,665 (30.2%).\n\nThe overlaps were:\n\n25,277 (3.2%) for Web of Science and OpenAlex.\n22,540 (2.9%) for Web of Science and Scopus.\n18,850 (2.4%) for OpenAlex and Scopus.\nOnly 10,853 (1.3%) across all three databases.\n\nThese figures consistently highlight the significant discrepancies in how different bibliometric databases classify SDG-related research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 SDG Classification: Economic Dimension",
    "text": "9.8 SDG Classification: Economic Dimension\nThe comparative analysis extended to the economic dimension of SDG-classified papers, revealing similar patterns of limited overlap.\nFor SDG 08, focusing on Decent Work and Economic Growth:\n\nWeb of Science identified 82,366 publications (19.0%).\nOpenAlex identified 121,106 (28.0%).\nScopus identified 183,641 (37.8%).\n\nThe overlaps comprised:\n\n16,268 (3.8%) for Web of Science and OpenAlex.\n8,020 (1.9%) for Web of Science and Scopus.\n39,071 (9.0%) for OpenAlex and Scopus.\n10,853 (2.5%) across all three databases.\n\nSimilarly, for SDG 09, concerning Industry, Innovation, and Infrastructure:\n\nWeb of Science classified 230,883 publications (30.3%).\nOpenAlex classified 217,822 (28.6%).\nScopus classified 200,566 (26.4%).\n\nOverlaps included:\n\n25,679 (3.4%) for Web of Science and OpenAlex.\n26,501 (3.5%) for Web of Science and Scopus.\n44,702 (5.9%) for OpenAlex and Scopus.\n15,186 (2.0%) for all three.\n\nThese figures consistently underscore the significant discrepancies in how different bibliometric databases classify SDG-related research, even within the economic dimension.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "href": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.9 LLM Technology and Key Findings",
    "text": "9.9 LLM Technology and Key Findings\nOttaviani and Stahlschmidt modelled the Large Language Model (LLM) technology with a specific objective: to develop an LLM whose knowledge base originated exclusively from publications classified under a particular Sustainable Development Goal by a given bibliometric database. Recognising the substantial resources required to train an LLM from scratch, they adopted a pragmatic compromise: fine-tuning an existing, pre-trained, open-source LLM. DistilGPT2 emerged as the chosen model, primarily due to its fundamental architecture and minimal prior knowledge, which ensured the absence of pre-existing semantic understanding concerning either publications or prompts.\nThe primary finding from this investigation highlights a systematic overlook within the data, specifically regarding certain actors, the world’s poorest countries, and various underrepresented topics. However, the authors acknowledge several limitations. Their framework remains general, implying that specific applied cases might yield different outcomes. Moreover, LLMs inherently demonstrate high sensitivity to their underlying model architecture, the characteristics of their training data, the chosen (hyper-)parameters, and the decoding strategies employed. Whilst Ottaviani and Stahlschmidt partially accounted for training data variability by incorporating three distinct databases, and for decoding strategies by utilising three methods from existing literature, the need for more advanced LLM architectures persists.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "href": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.10 SDG 4: Quality Education Targets",
    "text": "9.10 SDG 4: Quality Education Targets\nSustainable Development Goal 4 mandates the assurance of inclusive and equitable quality education, alongside the promotion of lifelong learning opportunities for all. A series of specific targets underpins this overarching goal, each outlining a crucial aspect of educational development.\nFor instance:\n\nTarget 4.1 stipulates that by 2030, all girls and boys must complete free, equitable, and high-quality primary and secondary education, leading to demonstrably relevant and effective learning outcomes.\nTarget 4.2 focuses on ensuring universal access to quality early childhood development, care, and pre-primary education by the same year, preparing children for primary schooling.\nTarget 4.3 aims for equal access to affordable and quality technical, vocational, and tertiary education, including university, for all women and men by 2030.\nTarget 4.4 seeks to substantially increase the number of young people and adults possessing relevant skills, particularly technical and vocational proficiencies, for employment, decent work, and entrepreneurship.\nTarget 4.5 addresses the elimination of gender disparities in education and guarantees equal access to all educational levels and vocational training for vulnerable populations, including individuals with disabilities, indigenous peoples, and children in precarious situations.\nTarget 4.6 commits to ensuring that all youth and a significant proportion of adults, irrespective of gender, achieve literacy and numeracy by 2030.\n\nThese targets collectively form the framework of the 2030 Agenda for SDGs, as defined by the United Nations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.11 Prompts for Benchmarking",
    "text": "9.11 Prompts for Benchmarking\nTo establish a robust benchmarking system, Ottaviani and Stahlschmidt meticulously crafted prompts for each Sustainable Development Goal. Each SDG comprises a comprehensive list of between eight and twelve specific targets. For every individual target, the authors systematically developed ten diverse questions, or prompts, with each question designed to explore a distinct facet of that target. This rigorous process yielded a specific set of 80 to 120 prompts for each SDG, collectively serving as a critical benchmark or standard for both defining compliance with the SDGs and identifying potential biases within the data. For instance, prompts such as, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” interrogated Target 4.1, which aims to ensure that all girls and boys complete free, equitable, and quality primary and secondary education by 2030.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "href": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.12 Research Design: Database and SDG Specificity",
    "text": "9.12 Research Design: Database and SDG Specificity\nThe research design meticulously outlines the process for analysing SDG-related content within bibliometric databases. The workflow commenced with an initial input comprising a set of abstracts, each classified according to a specific SDG and originating from a particular database. The authors then applied a crucial fine-tuning process to this dataset, utilising the DistilGPT-2 model. The output of this fine-tuning was a specialised “Fine-tuned DistilGPT-2” model, tailored to each specific SDG and database combination.\nSubsequently, a dedicated set of prompts, crafted specifically for each SDG, served as input to the fine-tuned LLM. To generate diverse and relevant outputs, Ottaviani and Stahlschmidt applied three distinct decoding strategies: top-k, nucleus, and contrastive search. The LLM produced corresponding responses for each SDG and database, categorised by the decoding strategy employed. A final post-processing step involved applying a “prompts’ words filter” to these responses, ultimately yielding a refined set of noun phrases pertinent to each SDG and database.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "href": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.13 Illustrative Example: SDG 4 Analysis",
    "text": "9.13 Illustrative Example: SDG 4 Analysis\nAs an illustrative example, the analysis of SDG 4 involved a systematic method of matching noun phrases, extracted from the LLM’s responses, directly against the specific SDG targets. Ottaviani and Stahlschmidt employed four critical data dimensions for this analysis: Locations, Actors, Data/Metrics, and Focuses. For each SDG, this comprehensive approach facilitated a dual assessment: firstly, evaluating the degree of compliance with its stated targets; and secondly, identifying any inherent biases within the data. Notably, the analysis consistently revealed distinct differences across the various bibliometric databases. A structured table organised these findings, categorising them by unique databases and indicating which targets were addressed or not addressed across the dimensions of locations, actors, data/metrics, and focuses.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "href": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.14 Systematic Overlooks by the LLM",
    "text": "9.14 Systematic Overlooks by the LLM\nDespite the LLM’s stimulation, its responses consistently overlooked several critical areas, revealing systematic biases within the underlying data. Geographically, the model rarely addressed African countries, with the notable exception of South Africa, and largely ignored other developing nations, including China (though this observation carried a degree of uncertainty). The model also largely overlooked least developed countries and small island developing states.\nIn terms of human actors, the LLM systematically overlooked vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Furthermore, the model failed to adequately focus on several key thematic areas explicitly outlined in the SDGs. These included vocational training, scholarships, the establishment of safe, non-violent, inclusive, and effective educational environments, the promotion of sustainable lifestyles, human rights, the cultivation of a culture of peace and non-violence, global citizenship, and the appreciation of cultural diversity. Crucially, the LLM also neglected to address the fundamental concepts of free primary and secondary education, and tertiary education.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.15 Cross-SDG Considerations",
    "text": "9.15 Cross-SDG Considerations\nAcross the five Sustainable Development Goals analysed, several consistent patterns emerged. Geographically, the LLM’s responses rarely addressed least developed countries, with South-Saharan Africa, for instance, receiving minimal attention in relation to SDG8. The United States maintained an undeniable dominance in location mentions, followed by South Africa and China as the most frequently cited countries, with the UK and Australia also featuring prominently.\nRegarding metrics, the LLM’s output included references to various data sources, such as the Demographic and Health Surveys (DHS) and the World Values Survey (WVS), alongside mentions of general metrics, indicators, and benchmarks. The discussed research methodologies spanned theoretical frameworks, empirical studies, thematic analysis, market dynamics, and macroeconomics. However, concerning human actors, the analysis consistently revealed a systematic overlook of discriminated and vulnerable categories across all SDGs. Furthermore, whilst the LLM addressed some SDG-specific focuses, it notably omitted the most sensitive topics, such as human trafficking, human exploitation, and migration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-summary",
    "href": "chapter_ai-nepi_009.html#case-study-summary",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.16 Case Study Summary",
    "text": "9.16 Case Study Summary\nThe case study’s findings reveal a critical insight: the integration of Large Language Models as an analytical AI tool, positioned between SDG classification and the policymaking process, exposes a systematic oversight within the scientific publications classified by SDGs. This oversight consistently neglects the most disadvantaged categories of individuals, the world’s poorest countries, and crucial underrepresented topics explicitly targeted by the SDGs. Conversely, the analysis demonstrates that economic superpowers and highly developing nations receive disproportionate attention. These results unequivocally highlight the profound and decisive impact of seemingly objective, science-informed practices, such as the bibliometric classification of SDGs, on the representation of global challenges.\nThe authors acknowledge several limitations. The LLM’s performance exhibits high sensitivity to its model architecture, the characteristics of its training data (though partially mitigated by incorporating three distinct databases), the chosen (hyper-)parameters, and the decoding strategies employed (which were also partially accounted for). Furthermore, the research operates within a general framework, suggesting that its applicability to highly specific contexts may require further investigation. Future work should therefore explore the development of more sophisticated LLM architectures to address these sensitivities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "",
    "text": "Overview\nThis chapter details an innovative approach to extracting citation data from law and humanities scholarship, a domain historically underserved by conventional bibliometric databases. The research team has developed a specialised methodology to overcome the challenges posed by complex, often multilingual, footnotes and the poor coverage of non-STEM fields in existing data sources. At the core of this solution lies the strategic leverage of Large Language Models (LLMs) and Vision Language Models (VLMs), underpinned by a meticulously crafted, TEI XML-encoded gold standard dataset.\nThe project directly addresses critical limitations inherent in current bibliometric tools, such as Web of Science, Scopus, and OpenAlex. These platforms frequently lack comprehensive coverage for pre-digital, non-English, or non-“A-journal” content, whilst also imposing prohibitive costs and restrictive licences. Furthermore, the intricate nature of humanities footnotes, often laden with commentary and varied citation styles, renders traditional machine learning tools largely ineffective.\nTo ensure the reliability of LLM-extracted data, the team has established a robust testing and evaluation framework. This framework relies on a high-quality gold standard dataset, comprising over 1,000 footnotes from 20 open-access articles across multiple languages and historical periods, yielding more than 1,500 references. This dataset is encoded in TEI XML, a well-established standard in digital humanities, which facilitates detailed contextual markup beyond mere reference management.\nA key technological contribution from the authors is “Llamore”, a lightweight Python package engineered for LLM-based reference extraction and performance evaluation. Llamore extracts citation data from raw text or PDFs, outputting TEI XML, and assesses extraction accuracy using the F1-score. It employs an unbalanced assignment problem solver to align extracted references with gold standard data, thereby maximising the total F1-score for robust evaluation.\nInitial evaluations demonstrate Llamore’s efficacy. When tested against the PLOS 1000 biomedical dataset, Llamore (using Gemini 2.0 Flash) achieved an F1-score of 0.62, comparable to Grobid’s 0.61. Crucially, on the specialised footnoted humanities dataset, Llamore significantly outperformed Grobid, achieving an F1-score of 0.45 compared to Grobid’s 0.14. Whilst Grobid remains more resource-efficient for its trained literature, Llamore proves three times more effective for complex footnoted content. This research paves the way for generating comprehensive citation graphs, enabling deeper insights into intellectual history, influence reconstruction, and the reception of ideas within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "href": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.1 Citation Graphs in Intellectual History",
    "text": "10.1 Citation Graphs in Intellectual History\n\n\n\nSlide 02\n\n\nScholars primarily employ citation graphs to illuminate patterns and relationships within knowledge production, particularly in the realm of intellectual history. These analytical tools prove invaluable for reconstructing influences and meticulously measuring the reception of published ideas. Consequently, researchers can identify, for instance, the most-cited authors across specific timeframes. An illustrative example involves an analysis of the Journal of Law and Society, for which an interactive web application provides detailed insights.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "href": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.2 Deficiencies in Existing Bibliometric Databases for SSH",
    "text": "10.2 Deficiencies in Existing Bibliometric Databases for SSH\n\n\n\nSlide 03\n\n\nA significant challenge arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) within existing bibliometric data sources. The research team deems these databases, including prominent platforms such as Web of Science, Scopus, and OpenAlex, fundamentally unusable for the specific domain under investigation, primarily owing to their pervasive lack of relevant data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "href": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.3 Limitations of Commercial and Open Bibliometric Databases",
    "text": "10.3 Limitations of Commercial and Open Bibliometric Databases\n\n\n\nSlide 04\n\n\nBeyond the general inadequacy, specific limitations plague both commercial and open bibliometric databases. Web of Science and Scopus, for instance, impose exorbitant costs and operate under highly restrictive licences; consequently, the authors advocate for a decisive shift away from reliance on these proprietary systems. Whilst OpenAlex offers the significant advantage of open access, it nonetheless presents considerable shortcomings for Social Sciences and Humanities research. Specifically, it frequently lacks comprehensive coverage for numerous “A-journals,” provides insufficient data from the pre-digital era, and typically omits content published in languages other than English.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "href": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity",
    "text": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity\n\n\n\nSlide 05\n\n\nThe Zeitschrift für Rechtssoziologie, a German Journal for Law and Society established in 1980, exemplifies the pervasive issue of poor data coverage within the humanities. Its citation data, as observed in bibliometric databases, only demonstrates significant improvement after the year 2000, with minimal records available for the preceding decades. This deficiency stems from several factors. Primarily, the humanities attract less commercial interest compared to STEM, medicine, and economics, which typically dominate large bibliometric databases. Moreover, these databases prioritise the “impact factor” for scientific evaluation, a metric largely irrelevant to research in intellectual history.\nCrucially, the literature of interest in humanities scholarship frequently features highly complex footnotes, colloquially termed “footnotes from hell.” These are not mere citations; they often incorporate extensive commentary and various “messy” elements, all embedded within a considerable amount of non-citation “noise,” making automated extraction profoundly challenging.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "href": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.5 Limitations of Traditional Tools and Promise of LLMs",
    "text": "10.5 Limitations of Traditional Tools and Promise of LLMs\nCreating training data presents a formidable challenge, necessitating a labour-intensive annotation process that demands substantial time investment. Furthermore, existing tools, particularly those reliant on traditional machine learning methods such as Conditional Random Forests, exhibit poor performance when confronted with complex footnotes. For instance, ExCite’s performance, as documented by Boulanger and Iurshina in 2022, reveals consistently low extraction and segmentation accuracies across various training datasets.\nNevertheless, Large Language Models (LLMs) offer a promising avenue for resolution. Early experiments conducted in 2022 with models like text-davinci-003 already demonstrated the significant power of LLMs in extracting references from highly unstructured textual data. Newer models promise even better results, whilst Vision Language Models (VLMs) extend this capability to direct PDF processing. The research team is currently exploring various methods to harness these advanced models effectively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "href": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.6 Ensuring Trustworthiness and Robust Evaluation",
    "text": "10.6 Ensuring Trustworthiness and Robust Evaluation\n\n\n\nSlide 13\n\n\nA fundamental concern centres on the trustworthiness of results generated by Large Language Models. Instances of LLMs fabricating non-existent citations, as exemplified by a lawyer’s disastrous use of ChatGPT in federal court, underscore this critical issue. Consequently, a guiding principle dictates against attempting to solve problems for which no validation data exists.\nTo address this, the authors necessitate a robust testing and evaluation solution. This solution comprises three essential components:\n\nA high-quality Gold Standard dataset.\nA flexible framework capable of adapting to the rapidly evolving technological landscape.\nSolid testing and evaluation algorithms designed to produce comparable metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "href": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.7 TEI-Annotated Gold Standard Dataset Development",
    "text": "10.7 TEI-Annotated Gold Standard Dataset Development\n\n\n\nSlide 16\n\n\nThe research team has embarked upon compiling a comprehensive training dataset, specifically designed for dual utility as evaluation data, employing TEI XML encoding. This choice stems from TEI’s status as a well-established, meticulously specified, and comprehensive standard for text interchange within the humanities and digital editorics. Unlike more constrained bibliographical standards such as CSL or BibTeX, TEI encompasses a broader array of phenomena, extending beyond mere reference management to facilitate the encoding of contextual information, including citation intention. Furthermore, its adoption enables the integration of existing TEI XML corpora from various digital editorics projects, thereby supporting the testing of generalisation and robustness features.\nDespite its advantages, the TEI standard presents certain challenges, both conceptual—concerning the distinction between pointers and references—and technical—regarding constrained elements versus elliptic material. The dataset development process involves several stages: initially, capturing PDF screenshots; subsequently, segmenting the reference string to isolate the citation from surrounding non-reference text within footnotes; and finally, parsing the content into a structured data format, utilising TEI elements such as biblStruct, analytic, monogr, author, title, imprint, date, biblScope, and biblRef. This dataset is currently under active development.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "href": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.8 Dataset Strategy Evolution and Tooling Integration",
    "text": "10.8 Dataset Strategy Evolution and Tooling Integration\nThe strategy for constructing this dataset has evolved significantly. Initially, the focus centred on compiling data directly relevant to the primary research question. More recently, however, the authors made a strategic decision to incorporate PDFs, thereby enabling the utilisation of Vision Language Model (VLM) mechanisms. The overarching aim now involves publishing the complete dataset, encompassing everything from the raw PDFs to the meticulously parsed references. To achieve this, the team is sourcing material from open-access journals. The current scope involves coding over 1,000 footnotes derived from 20 articles, spanning several languages and a broad historical timeframe, which are expected to yield more than 1,500 references. Notably, even multiple occurrences of the same work are encoded separately to capture their distinct contexts.\nA significant benefit of adopting the TEI XML standard lies in the extensive tooling available for this interoperable format. Grobid, a widely recognised tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. This alignment facilitates direct performance comparisons with Grobid and enables the provision of new training data to the Grobid team, fostering collaborative advancement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "href": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package",
    "text": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package\nThe research team has developed “Llamore”, an acronym for Large LANguage MOdels for Reference Extraction, as a dedicated Python package. This tool serves a dual purpose: it extracts citation data from either raw text or PDF documents, leveraging multimodal Large Language Models, and subsequently evaluates the performance of this extraction. Specifically, Llamore processes textual or PDF inputs to generate references in TEI XML format, whilst also accepting gold standard references to produce an F1-score as an evaluation metric. The design of Llamore prioritises two key objectives: it remains lightweight, comprising fewer than 2,000 lines of code, and ensures broad compatibility with both open and closed Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "href": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.10 Llamore Implementation and Workflow",
    "text": "10.10 Llamore Implementation and Workflow\nLlamore is readily available on Pypi, enabling straightforward installation via pip. The extraction workflow commences with defining an extractor, which is contingent upon the specific Large Language Model selected. For instance, the OpenAI extractor offers broad compatibility, as many open model serving frameworks, including Olama and VLLM, provide OpenAI-compatible APIs. Subsequently, users supply either a PDF or raw text as input to this extractor, which then yields the extracted references. These references can then be exported conveniently to an XML file. For evaluation purposes, users import the dedicated F1 class and provide both the gold standard references and the extracted references to compute the macro average of performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "href": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment",
    "text": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment\nLlamore employs the F1-score as its primary evaluation metric, a well-established standard for comparing structured data. This score represents the harmonic mean of Precision and Recall, where Precision is calculated as the ratio of matches to predicted elements, and Recall as the ratio of matches to gold elements. An F1-score of 1 signifies perfect extraction, whilst a score of 0 indicates no matches whatsoever. For instance, an extracted reference might align perfectly with a gold reference on fields such as analytic_title, monographic_title, surname, and publication_date, yet exhibit a mismatch in the forename due to a minor discrepancy like an extraneous dot in the gold standard.\nA more complex challenge arises in aligning multiple extracted references with their corresponding gold references. Llamore addresses this as an unbalanced assignment problem. The system computes F1-scores for every possible combination of extracted and gold references, subsequently constructing a matrix from these scores. It then leverages SciPy’s solver to maximise the total F1-score whilst ensuring a unique assignment between the extracted and gold references. Following this alignment, the individual F1-scores are macro-averaged to provide an overall performance metric.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "href": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.12 Performance Evaluation of Llamore",
    "text": "10.12 Performance Evaluation of Llamore\nPerformance evaluations of Llamore reveal distinct capabilities across different datasets. When tested against the PLOS 1000 dataset, comprising 1,000 PDFs from the biomedical field, Llamore (utilising Gemini 2.0 Flash) achieved an exact match F1-score of 0.62, closely aligning with Grobid’s score of 0.61. This indicates comparable performance on literature for which Grobid was specifically trained. However, it is crucial to note that Grobid maintains a significant advantage in resource efficiency, requiring orders of magnitude less computational power than Gemini.\nConversely, on the custom humanities dataset, which features complex footnoted literature, Llamore demonstrated a marked superiority. Grobid struggled considerably, yielding an F1-score of merely 0.14, whilst Llamore (Gemini 2.0 Flash) achieved a substantially higher score of 0.45. This represents approximately a threefold improvement in performance for the challenging domain of footnoted humanities scholarship.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "href": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.13 Conclusion and Takeaways",
    "text": "10.13 Conclusion and Takeaways\nGrobid remains the preferred choice for literature upon which it has been specifically trained, primarily owing to its significantly faster processing speed and reduced resource intensity. Conversely, Llamore, when paired with Gemini, demonstrates approximately three times better performance for the challenging domain of footnoted literature. This performance specifically pertains to pure reference extraction, excluding contextual or cross-referencing information.\nA critical aspect of utilising open-source databases, such as OpenAlex, involves the burden of quality assurance falling directly upon the user. The authors advise a cautious approach towards both OpenAlex and commercial databases like Web of Science and Scopus, as their quality assurance priorities may not align with specific research questions. Large Language Models, whilst powerful, can produce false positives by inventing citations, thus necessitating the attainment of reliable results before any large-scale application. The ultimate ambition extends beyond mere reference string extraction to obtaining reliable results for nuanced contextual information, such as whether a citation is approving or not.\nAnalysis reveals that Grobid’s poor performance on humanities data stems from its training data being out of distribution for this domain. Large Language Models, however, exhibit their own distinct failure modes. These include difficulty discerning ambiguous elements, such as whether a number represents a volume or a page, and being misled by capitalisation. They frequently misidentify personal names appearing within titles as authors and struggle with specialised terminology like Idem, Derselbe, passim, ibid, or n.d.. Furthermore, canonical citations found in fields such as Bible studies, Roman law, and classical literature, along with ellipses, abbreviations, and cross-references, present considerable challenges.\nThe requisite F1-score for a gold standard dataset is contingent upon the analytical ambition; a lower score might suffice for identifying broad tendencies, whilst high accuracy demands a much higher score. For less precise needs, such as fuzzy searching in bibliographical databases using only a title and author, an exact match may not be essential. Nevertheless, the current stage of research prioritises achieving highly reliable results to facilitate the extraction of richer contextual information. A human-in-the-loop approach has been considered for the dataset establishment workflow, where Llamore could pre-annotate data for subsequent human correction. However, intermediate stages, such as merely marking the referring string without full parsing, still necessitate substantial manual effort.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "",
    "text": "Overview\nThis presentation introduces a novel AI-driven solution, comprising Ghostwriter and EverythingData, which a collaborative team from Dans (Data Archiving and Networked Services) and GESIS (Leibniz Institute for the Social Sciences) has meticulously crafted. Their work addresses the burgeoning challenge of information overload within scientific disciplines. The system facilitates knowledge production by enabling users to ‘chat with papers’, directly engaging with the core research question: is it feasible to construct an AI solution for conversational interaction with specific collections of academic literature?\nThe system architecture integrates Retrieval-Augmented Generation (RAG) with a hybrid of vector and graph database methodologies. EverythingData constitutes the backend, processing raw document collections—such as full-text articles from the method-data-analysis (mda) journal—into a structured knowledge base. This process involves comprehensive term extraction, the generation of embeddings using various Machine Learning algorithms and Large Language Models (LLMs), and subsequent enrichment through external knowledge graphs like Wikidata.\nFor implementation, the team employs a vector store, specifically Q-drant, for content embeddings and a graph database for metadata, formally expressed using the Croissant ML standard. This hybrid ‘GraphRAG’ model combines the semantic understanding afforded by vector spaces with the relational insights provided by knowledge graphs. It fosters a ‘locally’ implemented Distributed AI where the LLM functions dually as an intuitive interface and a powerful reasoning engine.\nGhostwriter operates as the user-facing interface, facilitating natural language queries. It dynamically generates both a list of relevant documents and an explanatory text, crucially providing transparent sourcing to prevent hallucinations. The system’s iterative approach empowers users to refine their questions. Its entity extraction pipeline meticulously maps query terms to controlled vocabularies and links them to broader knowledge graph representations, thereby supporting immediate multilinguality. This robust architecture ensures that responses are rigorously grounded in the specific collection, offering a controlled search space for ‘close reading’ and enabling the discovery of associations that extend beyond explicit text or metadata.\nFuture developments include expanding the pilot to incorporate bibliometric questions by integrating metadata harvesting. The team also plans to apply the methodology to large-scale official statistical data, such as CBS data via the ODISSEI Portal and the SSHOC.nl project, and to extend its application to specialised cultural heritage datasets, as in the MuseIT case. Key challenges ahead involve rigorously evaluating the workflow’s efficacy and developing advanced visualisation techniques to represent the spatial and temporal evolution of research topics. The project champions a sustainable, local AI approach, reducing reliance on large industry models and fostering collaborative development within the academic community.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#science-dynamics-and-ai",
    "href": "chapter_ai-nepi_011.html#science-dynamics-and-ai",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.1 Science Dynamics and AI",
    "text": "11.1 Science Dynamics and AI\n\n\n\nSlide 02\n\n\nThe evolution of scientific disciplines consistently demonstrates both growth and increasing differentiation, leading to an overwhelming volume of information. This expansion presents a significant practical challenge: the imperative to efficiently review, evaluate, and select pertinent content. Fundamentally, the creation of new knowledge, whether within individual minds or across broader academia, relies upon the ability to locate and comprehend existing information.\nWhilst machines, particularly recent advancements in Artificial Intelligence, have undeniably fostered this growth, a critical question arises: can they also actively support the knowledge production process itself? This inquiry forms a core challenge within the domain of Information Retrieval. At Dans, senior research engineer Slava Tikhonov has pioneered extensive experimentation, meticulously building complex data pipelines. His intricate work, though challenging to unravel, has inspired the current endeavour. The primary objective involves applying these AI-driven methodologies to manage the burgeoning flood of information, thereby addressing fundamental information retrieval challenges.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#presentation-outline-and-research-question",
    "href": "chapter_ai-nepi_011.html#presentation-outline-and-research-question",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.2 Presentation Outline and Research Question",
    "text": "11.2 Presentation Outline and Research Question\n\n\n\nSlide 03\n\n\nThis presentation addresses the central research question: can an AI solution be developed to facilitate conversational interaction with academic papers? The introductory segment will delve into foundational concepts, encompassing information retrieval principles, the dynamics of human-machine interaction, and the sophisticated techniques of Retrieval-Augmented Generation (RAG) within generative AI.\nA concrete use case focuses on the method-data-analysis (mda) journal, providing a specific domain for application. The discussion will then introduce the bespoke workflow underpinning this ‘local’ or tailored AI solution, highlighting two principal components: Ghostwriter, which functions as the user interface, and EverythingData, a comprehensive term encapsulating the entire backend infrastructure. The talk will subsequently provide illustrations of both front-end and back-end operations, culminating in a summary and an outlook on future directions. Specifically, the AI solution aims to enable users to chat with papers derived from a carefully curated selection.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-approach-a-new-ir-interface",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-approach-a-new-ir-interface",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.3 The Ghostwriter Approach: A New IR Interface",
    "text": "11.3 The Ghostwriter Approach: A New IR Interface\n\n\n\nSlide 04\n\n\nThe initial challenge in effective information retrieval centres upon formulating the precise question, identifying the most appropriate information source, and accurately interpreting the resultant findings. The project team proposes the Ghostwriter approach as a novel interface to address these complexities, contrasting it with existing query interaction models.\nTraditionally, information retrieval (IR) involves a query interacting with a single database representation. This necessitates the user’s explicit knowledge of the schema and its typical values to obtain results—a scenario likened to ‘Me and a database’. A more advanced model, exemplified by Google Features and schema.org, involves a query interacting with a data collection underpinned by connected structured databases or graphs. This is akin to ‘Me and a librarian’, where the system offers suggestions for similar or improved queries based on schema connections.\nThe advent of Large Language Models (LLMs) introduced a paradigm where a query interacts directly with an LLM, resembling ‘Me and a library’ or ‘Me and a round of experts’. Here, the system interprets natural language input and suggests results, also expressed in natural language.\nThe Ghostwriter approach, however, represents a significant advancement. It involves a query interacting with a local LLM, a specific target data collection, and a network of additional data interpretation sources accessed via APIs. The authors describe this sophisticated interaction metaphorically as ‘Me chatting with experts and librarians at the same time’. Ghostwriter dynamically creates a family of terms related to the query, identifies pertinent structured information, and returns a comprehensive list of results. Its iterative application empowers users to reformulate their questions, fostering a deeper understanding of what they genuinely seek to ask and what the available data space can realistically provide.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-core-components",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-core-components",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.4 Ghostwriter and EverythingData: Core Components",
    "text": "11.4 Ghostwriter and EverythingData: Core Components\n\n\n\nSlide 05\n\n\nScientifically, the Ghostwriter and EverythingData system aligns with the broader discourse surrounding Retrieval Augmented Generation (RAG). This approach integrates two primary ingredients: a vector space and a graph. The team meticulously constructs the vector space from the content of data files, where information is encoded into embeddings that capture inherent properties and their attributes. These embeddings are computed using diverse Machine Learning algorithms and can leverage various Large Language Models.\nConversely, the graph constitutes a metadata layer, seamlessly integrated with a range of ontologies and controlled vocabularies, including those pertinent to responsible AI. The authors formally express this graph structure using the Croissant ML standard. The overarching vision for this system proposes combining both graph and vector representations into a singular model, termed ‘GraphRAG’. This innovative approach aims for a locally implemented Distributed AI, where the LLM functions dually as an intuitive interface between human users and the AI, and as a powerful reasoning engine.\nIn terms of implementation, the LLM is robustly connected to a RAG library, which effectively embodies the knowledge graph. This connection enables the LLM to navigate through extensive datasets and consume embeddings as contextual input for its operations. The conceptual design, which draws inspiration from seminal works such as ‘The GraphRAG Manifesto’ by Phipa Rathie, illustrates the progressive evolution of AI systems, moving from basic LLMs to a complex, interconnected graph structure that symbolises a fully realised knowledge-augmented system.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#vector-versus-graph-databases-for-rag",
    "href": "chapter_ai-nepi_011.html#vector-versus-graph-databases-for-rag",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.5 Vector versus Graph Databases for RAG",
    "text": "11.5 Vector versus Graph Databases for RAG\n\n\n\nSlide 07\n\n\nA comparative analysis, drawing upon insights from TheAiEdge.io, elucidates the distinct paradigms of Vector and Graph Databases for Retrieval Augmented Generation (RAG). The Vector Database approach systematically processes information by first partitioning data into discrete chunks. Subsequently, these chunks are encoded into numerical vectors using an LLM, then indexed and stored. For retrieval, a posed question is similarly transformed into an embedding, and the system identifies the nearest neighbours—semantically similar chunks—to provide as context for an LLM-generated response.\nIn contrast, the Graph Database approach commences with an LLM extracting relational information between entities from raw data, such as ‘MACHINE LEARNING’ and ‘FUN’ linked by ‘IS’. This process forms a structured knowledge representation organised as a network of interconnected entities and relationships, which is then stored in a Graph database. When a question concerning a specific entity arises, retrieval involves extracting a relevant subgraph from the database to pass as context to an LLM.\nFurther insights into vector space models reveal a document-term matrix structure, enriched by LLMs identifying semantically similar words and by knowledge graphs capturing deeper meaning. This yields a vector space founded upon distinct ‘classes’ of words, which in turn determine weights and facilitate a more sophisticated understanding of document similarity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-architecture",
    "href": "chapter_ai-nepi_011.html#system-architecture",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.6 System Architecture",
    "text": "11.6 System Architecture\nThe Ghostwriter and EverythingData system architecture systematically transforms raw textual data into a structured, queryable knowledge base. The process commences with a collection of full-text articles, exemplified by the MDA Journal, serving as the primary data source.\nThis full text undergoes comprehensive processing within the EverythingData backend. Information is initially stored in a Vector Store, Q-drant, which houses all data. From this store, the system executes several critical operations: it extracts key terms, converts text segments into numerical vector representations (embeddings), and adds further contextual information through enrichments. Crucially, selected terms are then expressed as structured data within a graph, subsequently enriched with external knowledge from Wikidata. This structured and enriched data then feeds into a Vector space RAG-Graph.\nUsers interact with the system via the Ghostwriter interface, posing natural language queries such as ‘Explain male breadwinner model to me?’. The system processes this query through the Vector space RAG-Graph, producing two distinct outputs: a list of relevant documents and a concise explanatory text. The authors clarify that whilst the system begins with MDA Journal articles, any document collection could serve as input. Coupling this process with knowledge graphs enhances the value of the embeddings, ultimately feeding into a unified system that delivers both document lists and summarised responses.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-content-indexing",
    "href": "chapter_ai-nepi_011.html#ghostwriter-content-indexing",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.7 Ghostwriter: Content Indexing",
    "text": "11.7 Ghostwriter: Content Indexing\n\n\n\nSlide 08\n\n\nGhostwriter functions as a dedicated tool for content indexing, enabling the ingestion of documents and webpages into a queryable collection. The mda methods, data, analyses journal website serves as a primary content source. The Ghostwriter tool itself, residing on the gesis Leibniz-Institut Sozialwissenschaften platform, presents distinct sections for ‘Ask Questions’ and ‘Add Page’.\nWithin the ‘Add Page’ section, users can input a webpage URL or RSS feed, with the system supporting various source types. The interface also displays available collections and management capabilities. The current scope involves a test collection of 100 articles, acquired through web scraping, with the tool accessible at https://gesis.now.museum.\nThe speaker, whilst initially cautious about Large Language Models, systematically deconstructed the training process to identify specific applications. This work, though demonstrated for academic papers, is versatile enough for any web content. Crucially, the system prevents hallucinations by exclusively relying on the ingested source material; if information is unavailable, it explicitly states, ‘I don’t know’. The system achieves complex query responses by combining a relatively simple LLM with sophisticated knowledge graphs, ensuring a curated collection is built by adding papers incrementally.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#interactive-queries-and-source-transparency",
    "href": "chapter_ai-nepi_011.html#interactive-queries-and-source-transparency",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.8 Interactive Queries and Source Transparency",
    "text": "11.8 Interactive Queries and Source Transparency\n\n\n\nSlide 09\n\n\nThe Ghostwriter system facilitates interactive engagement with academic literature. When a user poses a question, such as ‘explain male breadwinner model to me’, the system provides a detailed explanation within its ‘Answer’ section. A ‘Sources’ section transparently lists the academic papers that informed the answer, complete with relevance scores.\nFor instance, the response to the male breadwinner model query cites ‘The Past, Present and Future of Factorial Survey Experiments’ by Treischl and ‘Gender and Survey Participation’ by Becker, both from the MDA GESIS journal. The system’s output comprises both an explanation and a list of documents, notably including those that may not explicitly contain the query terms, thereby indicating a sophisticated semantic search. The tool operates on MDA papers and is accessible at https://gesis.now.museum.\nThe speaker emphasises that, unlike some generative AI models, this implementation provides precise references and demonstrably avoids hallucination because it accurately identifies the information’s origin. The underlying mechanism involves splitting each paper into small, uniquely identified blocks. The LLM technique then intelligently connects and retrieves these blocks, applying weights and leveraging knowledge graphs to predict which text segments will most effectively respond to a given question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-refinement-and-verifiability",
    "href": "chapter_ai-nepi_011.html#iterative-refinement-and-verifiability",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.9 Iterative Refinement and Verifiability",
    "text": "11.9 Iterative Refinement and Verifiability\n\n\n\nSlide 10\n\n\nThe Ghostwriter system employs an iterative approach to ensure accurate, verifiable answers. This is exemplified by its query refinement capability: an initial broad query, such as ‘Explain the male Breadwinner model to me?’, can be refined into a more specific follow-up, like ‘Explain how data were collected on male breadwinner model?’.\nFor such refined queries, the system transparently states if direct information is unavailable. Nevertheless, it provides valuable contextual information, noting, for example, that a study utilised German data and employed a mixed-methods research strategy. The ‘Sources’ section lists relevant academic papers, such as Treischl’s ‘The Past, Present and Future of Factorial Survey Experiments’, complete with relevance scores and direct download URLs. An interactive ‘Chat’ column allows users to pose follow-up questions directly related to specific documents. The speaker underscores that if the system lacks information, it explicitly states this limitation, whilst users retain the ability to add missing articles, thereby enhancing the knowledge base for future queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#entity-extraction-and-multilingual-support",
    "href": "chapter_ai-nepi_011.html#entity-extraction-and-multilingual-support",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.10 Entity Extraction and Multilingual Support",
    "text": "11.10 Entity Extraction and Multilingual Support\n\n\n\nSlide 11\n\n\nBehind the operational facade lies a sophisticated entity extraction pipeline. This pipeline meticulously annotates terms with semantic meaning by mapping them to controlled vocabularies, effectively transforming raw data from a numerical vector space representation into a structured knowledge graph. Furthermore, the system extends this process by linking extracted entities to broader knowledge graph representations, with Wikidata serving as a prominent example. This comprehensive linking establishes a ‘ground truth’ against which the accuracy of LLM-generated answers can be rigorously assessed.\nA key capability of the system is its immediate multilinguality. This is particularly critical when processing papers in languages such as Chinese or German and expecting reliable answers in English. The underlying processes, including entity extraction and linking, inherently support multiple languages. Ultimately, an LLM completes the feedback loop by synthesising the insights derived from the knowledge graph operations into concise, human-readable ‘explanatory text’ summaries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#query-mapping-to-graph-representations",
    "href": "chapter_ai-nepi_011.html#query-mapping-to-graph-representations",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.11 Query Mapping to Graph Representations",
    "text": "11.11 Query Mapping to Graph Representations\n\n\n\nSlide 12\n\n\nThe system meticulously processes user queries by mapping them to a graph representation, whilst simultaneously annotating the query strings with ‘facts’. For instance, the input query ‘explain male breadwinner model to me’ undergoes an initial fact extraction phase, where semantic triples and relationships are identified. These facts often reference ontological relationships, such as ‘gender roles societal expectations(owl:partOf) male breadwinner model’, indicating that societal expectations related to gender roles constitute a component of the male breadwinner model.\nFollowing this extraction, a comprehensive graph representation is constructed, comprising an array of structured objects. Each object precisely defines a relationship between two concepts; for example, one entry explicitly states concept1: 'gender roles', concept2: 'male breadwinner model', and relationship: 'owl:partOf'. The speaker elaborates that this fact extraction process systematically splits the question into smaller, manageable pieces, leveraging a knowledge organisation system that can iteratively generate new levels of terms.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "href": "chapter_ai-nepi_011.html#entity-linking-with-wikidata",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.12 Entity Linking with Wikidata",
    "text": "11.12 Entity Linking with Wikidata\n\n\n\nSlide 13\n\n\nThe system meticulously links entities to the Wikidata ontology, exemplified by the disambiguation and mapping of the term ‘male’ to structured knowledge. Various examples illustrate the polysemy of this term, each associated with a Wikidata entity, its label, a description, a Concept URI, and a similarity score derived from LLM embeddings.\nThe highest similarity score is attributed to ‘male given name’, indicating a strong LLM association. Other significant associations include ‘male’ as human sex or gender and ‘male organism’. The system also identifies technical usages, such as ‘male connector’, and distinguishes homographs like ‘Malé’, the capital of the Maldives. The speaker clarifies that this process involves linking all entities to Wikidata, thereby obtaining identifiers rather than free strings. These identifiers are inherently linked to multilingual translations, providing access to all properties and enabling queries in diverse languages.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#conceptual-translation-with-gemma3",
    "href": "chapter_ai-nepi_011.html#conceptual-translation-with-gemma3",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.13 Conceptual Translation with Gemma3",
    "text": "11.13 Conceptual Translation with Gemma3\n\n\n\nSlide 14\n\n\nThe system demonstrates robust multilingual support by treating the core of a query, such as ‘bread winner model’, as a distinct conceptual entity. A Large Language Model, specifically Gemma3, then produces comprehensive translations for this concept. The underlying mechanism employs a structured data representation, akin to RDF/Turtle, to define and translate the concept of a ‘male breadwinner model’.\nThis involves defining the central concept with a unique URI and associating it with an extensive list of languages, from Czech to Ukrainian. For each language, a precise translation is provided; for example, the Chinese translation is ‘男性主導收入模式’. The frequent inclusion of gendered terms in many translations underscores the specific nature of the concept. The speaker confirms that this process enables the question to be translated into hundreds of languages, with all such translations serving as queries to the LLM.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#system-functionality-and-benefits",
    "href": "chapter_ai-nepi_011.html#system-functionality-and-benefits",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.14 System Functionality and Benefits",
    "text": "11.14 System Functionality and Benefits\n\n\n\nSlide 15\n\n\nThis system offers a robust pipeline that functions effectively for any collection, systematically creating a semantic index and a local chatbot. The capability for ‘local chatting with papers’ significantly supports human ‘close reading’, offering an interactive tool for in-depth textual analysis.\nFurthermore, the system effectively ‘contains the search space’ by integrating the user’s question and the specific collection into a particular area within the networked space of scientific knowledge. Crucially, it enables users to gain information extending beyond what is explicitly expressed in the text or annotated in the metadata. This is achieved by forging associations at both the natural language and the Knowledge Organisation Systems level, uncovering implicit knowledge and connections. The Ghostwriter interface, built upon the EverythingData workflow, enables users to find related documents and to refine their queries.\nThe speaker elaborates on the concept of ‘ground truth’, explaining that knowledge has been decoupled from questions and papers, allowing it to be stored independently of the model. This separation permits the use of entirely different models to produce the same list of identifiers, thereby enabling the creation of benchmarks for future generations of scientists.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#future-directions",
    "href": "chapter_ai-nepi_011.html#future-directions",
    "title": "11  AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach",
    "section": "11.15 Future Directions",
    "text": "11.15 Future Directions\n\n\n\nSlide 16\n\n\nThe ‘MDA case’ currently serves as a pilot, yet future developments aim to incorporate bibliometric questions, necessitating the integration of metadata harvesting. This progression distinguishes between ‘chatting-with-papers’ and ‘chatting-about-ensemble of papers’. The project intends to expand its application to other significant use cases, including CBS (Dutch Statistical Office) data, which is intrinsically linked to the ODISSEI Portal and the SSHOC.nl project. Furthermore, the scope will extend to specialised Dataverse instances concerning performing arts and inclusion, exemplified by the MuseIT case.\nCritical methodological questions for the future revolve around rigorously evaluating the workflow and developing advanced visualisation techniques to display new findings effectively. An API facilitates an automatic mode for agentic architectures, enabling the collection of results and the identification of novel knowledge within papers. The speaker underscored the inherent value of this local approach, highlighting its benefits over reliance on large, less controllable, and often costly external machines. This ‘chatting with papers’ is conceptualised as engaging with an ‘invisible college’, not merely for definitive facts, but primarily to provoke and support the human thought process in formulating precise research questions.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Solutions for Academic Information Retrieval: The Ghostwriter and EverythingData Approach</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "",
    "text": "Overview\nThis report systematically documents the application of Retrieval-Augmented Generation (RAG) systems within philosophy, particularly addressing the discipline’s stringent requirements for linguistic and semantic accuracy. Paul Näger presents a compelling perspective on RAG, highlighting its capacity to resolve core limitations inherent in Large Language Models (LLMs), such as restricted access to full texts, finite context windows, and challenges in attribution. Whilst LLMs excel at generating generalisable statistical rules for text production, they are not designed for verbatim text learning. This poses a critical necessity for philosophical inquiry, which demands deep engagement with original sources and their fine-grained formulations. Consequently, RAG systems emerge as a vital solution.\nNäger’s presentation explores diverse applications, ranging from pedagogical tools to advanced research functionalities. For instance, RAG enables students to interact conversationally with philosophical corpora, such as Locke’s Oeuvre, fostering deeper textual understanding. For researchers, RAG facilitates efficient fact-finding in handbooks, exploration of previously unexamined corpora, identification of passages for close reading, and the precise answering of specific research questions.\nA practical RAG implementation, utilising the Stanford Encyclopedia of Philosophy (SEP) as its data source, demonstrates these capabilities. Initially conceived as a community tool, the project evolved into a qualitative study investigating optimal RAG system configurations for philosophical contexts. This research meticulously examines model choices, including generative LLMs (e.g., gpt-4o-mini) and embedding models, alongside the intricate tuning of hyperparameters such as the number of retrieved documents (top-k), input/output token limits, generation temperature, and chunk size and overlap.\nThe methodology employs a theoretically grounded trial-and-error approach, emphasising the criticality of robust evaluation standards for assessing complex, unstructured philosophical propositions. A key finding reveals that chunking content into main sections, despite their average length (approximately 3000 words) exceeding the embedding model’s cutoff (512 words), yields superior results. This efficacy largely stems from the highly systematised nature of the SEP. Furthermore, the system incorporates reranking as an additional step to enhance retrieval accuracy. It leverages a generative LLM for more advanced semantic differentiation, albeit at increased computational cost.\nUltimately, RAG systems offer significant advantages by integrating verbatim corpora and specialised domain knowledge, thereby reducing hallucinations and enabling precise citation. Their effective deployment, however, necessitates careful tweaking, rigorous evaluation with representative question sets, and the indispensable involvement of domain experts, particularly when exploring unfamiliar corpora. Challenges persist, notably the degradation of answer quality when relevant documents are scarce. Paradoxically, RAGs tend to perform less effectively on broad overview questions, suggesting a need for more flexible, potentially agentic RAG architectures in future developments.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-core-llm-challenges-with-rag-systems",
    "href": "chapter_ai-nepi_012.html#addressing-core-llm-challenges-with-rag-systems",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.1 Addressing Core LLM Challenges with RAG Systems",
    "text": "12.1 Addressing Core LLM Challenges with RAG Systems\n\n\n\nSlide 03\n\n\nPhilosophical inquiry frequently poses complex questions. One might seek to elucidate Aristotle’s theory of matter within his Physics or trace the evolution of Einstein’s concept of locality across his works, from early relativity papers to his 1948 publication on ‘Quantenmechanik und Wirklichkeit’. Whilst Large Language Models (LLMs) like ChatGPT can furnish reasonably differentiated responses to these queries, they exhibit several fundamental limitations. Retrieval-Augmented Generation (RAG) systems specifically address these challenges, offering a robust framework for enhancing LLM performance in knowledge-intensive domains.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-system-architecture-and-workflow",
    "href": "chapter_ai-nepi_012.html#rag-system-architecture-and-workflow",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.2 RAG System Architecture and Workflow",
    "text": "12.2 RAG System Architecture and Workflow\n\n\n\nSlide 04\n\n\nA RAG system fundamentally integrates external data sources to augment the capabilities of Large Language Models. This architecture necessitates a dedicated data source, which, for philosophical research, might comprise a specific corpus such as the complete works of Aristotle or Einstein. Researchers retrieve relevant documents from this corpus, typically employing semantic search, though hybrid or classic search methods also remain viable options. Subsequently, these retrieved text chunks dynamically augment the prompts directed to the LLM. Crucially, this augmentation process directly resolves a significant limitation of standalone LLMs: their lack of direct access to full, original texts. Whilst LLMs may have encountered these texts during their training, they cannot reliably quote specific passages or avoid factual inaccuracies, often leading to ‘hallucinations’.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#overcoming-llm-limitations-access-and-verbatim-learning",
    "href": "chapter_ai-nepi_012.html#overcoming-llm-limitations-access-and-verbatim-learning",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.3 Overcoming LLM Limitations: Access and Verbatim Learning",
    "text": "12.3 Overcoming LLM Limitations: Access and Verbatim Learning\n\n\n\nSlide 05\n\n\nLarge Language Models, despite their sophisticated conversational abilities, inherently lack direct access to the complete texts they discuss. Consequently, when prompted to quote specific sections from a paper, an LLM may either admit its inability or, more problematically, generate fabricated content. This limitation stems from their training methodology; LLMs are not engineered to memorise texts verbatim. Instead, their design explicitly prevents rote learning, compelling them to acquire generalisable statistical rules for text production. Philosophical research, however, with its profound emphasis on linguistic and semantic accuracy, critically depends upon direct engagement with original textual sources and their precise, fine-grained formulations. RAG systems, by providing explicit access to these corpora, directly address this fundamental disparity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-context-window-limitations-and-attribution",
    "href": "chapter_ai-nepi_012.html#addressing-context-window-limitations-and-attribution",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.4 Addressing Context Window Limitations and Attribution",
    "text": "12.4 Addressing Context Window Limitations and Attribution\n\n\n\nSlide 06\n\n\nBeyond facilitating direct textual access, RAG systems adeptly navigate two further critical challenges posed by Large Language Models. Firstly, they mitigate the issue of a limited context window. Although models like ChatGPT 4.0 boast a substantial context of 128,000 tokens, extensive corpora can rapidly exhaust this capacity. RAG systems circumvent this by intelligently retrieving and supplying only the most pertinent text chunks, thereby ensuring that the LLM operates within its operational limits whilst still receiving highly relevant information. Secondly, RAG systems inherently resolve the attribution problem. They furnish explicit citations for the information provided, mirroring the functionality observed in platforms like Perplexity, where numbered references link claims directly to their source documents. This capability is paramount for academic rigour, ensuring the verifiability and trustworthiness of generated content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-system-workflow-query-retrieval-augmentation-generation",
    "href": "chapter_ai-nepi_012.html#rag-system-workflow-query-retrieval-augmentation-generation",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.5 RAG System Workflow: Query, Retrieval, Augmentation, Generation",
    "text": "12.5 RAG System Workflow: Query, Retrieval, Augmentation, Generation\n\n\n\nSlide 07\n\n\nThe operational workflow of a RAG system follows a systematic, multi-stage process. Initially, a user submits a query to a dedicated application. This application then initiates a retrieval query, directing it towards various data sources, which may include vector databases or APIs. Upon receiving this query, the data sources return relevant text chunks to the application. Crucially, the application then combines the original user query with these newly retrieved chunks, forming an ‘augmented’ query. This enriched input is subsequently transmitted to a Large Language Model (LLM) for processing. The LLM, leveraging both the query and the contextual chunks, generates a comprehensive answer, which it relays back to the application. Finally, the application delivers this refined answer to the user, ensuring that responses are both informative and grounded in specific, verifiable sources.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#philosophical-applications-of-rag-systems-didactics-and-research",
    "href": "chapter_ai-nepi_012.html#philosophical-applications-of-rag-systems-didactics-and-research",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.6 Philosophical Applications of RAG Systems: Didactics and Research",
    "text": "12.6 Philosophical Applications of RAG Systems: Didactics and Research\n\n\n\nSlide 08\n\n\nRAG systems offer transformative potential across various facets of philosophical engagement. Fundamentally, they enable conversational interaction with extensive philosophical corpora, such as Locke’s complete works, mirroring the intuitive style of ChatGPT whilst providing significantly more detailed domain knowledge and a verifiable verbatim text basis. This capability proves invaluable for didactics; repeated questioning becomes a highly instructive method for students to progressively deepen their understanding of complex texts, moving from general concepts to intricate details. For instance, students can explore Locke’s epistemology or his theory of matter.\nMoreover, RAG systems hold considerable promise for research. They facilitate efficient fact-finding within handbooks, streamlining the process of locating specific information for orientation, remarks, or footnotes. Researchers can also employ these systems to explore previously unexamined corpora, provided the texts are first digitised, gaining a comprehensive overview of their contents. Furthermore, RAG aids in identifying precise passages for close reading that directly pertain to specific research questions. Ultimately, these systems aspire to furnish detailed answers to at least components of complex research questions, painting a compelling vision for future philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#rag-for-philosophical-corpora-enhanced-domain-knowledge",
    "href": "chapter_ai-nepi_012.html#rag-for-philosophical-corpora-enhanced-domain-knowledge",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.7 RAG for Philosophical Corpora: Enhanced Domain Knowledge",
    "text": "12.7 RAG for Philosophical Corpora: Enhanced Domain Knowledge\n\n\n\nSlide 09\n\n\nThe overarching concept driving the application of RAG systems in philosophy centres on enabling conversational interaction with extensive philosophical corpora, such as the complete works of John Locke. This approach aims to replicate the intuitive user experience of platforms like ChatGPT. Crucially, however, it significantly enhances the interaction by providing a far more detailed domain-specific knowledge base and, critically, a verifiable verbatim textual foundation, ensuring scholarly rigour and precision.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#pedagogical-utility-deepening-textual-engagement",
    "href": "chapter_ai-nepi_012.html#pedagogical-utility-deepening-textual-engagement",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.8 Pedagogical Utility: Deepening Textual Engagement",
    "text": "12.8 Pedagogical Utility: Deepening Textual Engagement\n\n\n\nSlide 10\n\n\nFor pedagogical purposes, RAG systems offer a remarkably effective instructional approach. Students can engage with challenging philosophical texts, such as Locke’s Essay Concerning Human Understanding, by initiating a conversational dialogue. This allows them to begin with broad inquiries, like Locke’s general philosophical tenets, and then progressively delve into more specific areas, such as his ideas on epistemology or his theory of matter. Through this iterative questioning, RAG systems provide a dynamic and instructive pathway for students to achieve a profound understanding of complex textual content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#research-applications-fact-finding-and-corpus-exploration",
    "href": "chapter_ai-nepi_012.html#research-applications-fact-finding-and-corpus-exploration",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.9 Research Applications: Fact-Finding and Corpus Exploration",
    "text": "12.9 Research Applications: Fact-Finding and Corpus Exploration\n\n\n\nSlide 11\n\n\nIn the realm of research, RAG systems prove indispensable for tasks such as fact-finding within handbooks, providing essential orientation, facilitating remarks, and generating accurate footnotes. Historically, scholars manually consulted physical books; now, whilst LLMs can offer information, its reliability remains questionable, often leading to hallucination. Consequently, robust RAG systems become critical for ensuring the veracity of retrieved facts. Furthermore, these systems enable the exploration of previously unexamined corpora. Once digitised, such texts can be interrogated conversationally, yielding deeper overviews of their content and unlocking new avenues for scholarly investigation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advanced-research-applications-close-reading-and-question-answering",
    "href": "chapter_ai-nepi_012.html#advanced-research-applications-close-reading-and-question-answering",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.10 Advanced Research Applications: Close Reading and Question Answering",
    "text": "12.10 Advanced Research Applications: Close Reading and Question Answering\n\n\n\nSlide 12\n\n\nBeyond basic fact-finding, RAG systems significantly advance philosophical research by enabling the precise identification of passages for close reading that directly pertain to a specific research question. Ultimately, these systems hold the potential to furnish detailed answers, at least to components of complex research questions. This capability paints a compelling vision for the future of philosophical inquiry, promising to streamline and deepen scholarly engagement with intricate textual and conceptual challenges.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#example-rag-implementation-stanford-encyclopedia-of-philosophy",
    "href": "chapter_ai-nepi_012.html#example-rag-implementation-stanford-encyclopedia-of-philosophy",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.11 Example RAG Implementation: Stanford Encyclopedia of Philosophy",
    "text": "12.11 Example RAG Implementation: Stanford Encyclopedia of Philosophy\n\n\n\nSlide 13\n\n\nPaul Näger and his team developed an illustrative RAG system, leveraging the Stanford Encyclopedia of Philosophy (SEP) as its foundational data source. They systematically scraped the content of this well-regarded online handbook and converted it into markdown format, preparing it for integration into the RAG architecture.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#initial-aim-community-tool",
    "href": "chapter_ai-nepi_012.html#initial-aim-community-tool",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.12 Initial Aim: Community Tool",
    "text": "12.12 Initial Aim: Community Tool\n\n\n\nSlide 14\n\n\nInitially, Näger’s project aimed to develop a practical and beneficial tool specifically for the academic community, providing a valuable resource for philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#evolving-aims-from-tool-to-qualitative-study",
    "href": "chapter_ai-nepi_012.html#evolving-aims-from-tool-to-qualitative-study",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.13 Evolving Aims: From Tool to Qualitative Study",
    "text": "12.13 Evolving Aims: From Tool to Qualitative Study\n\n\n\nSlide 16\n\n\nPaul Näger and his team implemented a RAG system utilising the Stanford Encyclopedia of Philosophy as its data source. However, initial attempts to configure the system using conventional textbook approaches for retrieval and generation produced unsatisfactory results; indeed, the answers proved inferior to those generated by ChatGPT alone. This unexpected outcome prompted a significant re-evaluation of the project’s objectives, shifting its primary aim from merely developing a functional tool to undertaking a comprehensive qualitative study on the optimal setup of RAG systems specifically tailored for philosophical applications.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#qualitative-study-focus-model-choices-and-hyperparameter-tuning",
    "href": "chapter_ai-nepi_012.html#qualitative-study-focus-model-choices-and-hyperparameter-tuning",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.14 Qualitative Study Focus: Model Choices and Hyperparameter Tuning",
    "text": "12.14 Qualitative Study Focus: Model Choices and Hyperparameter Tuning\n\n\n\nSlide 17\n\n\nNäger’s qualitative study meticulously investigates two critical areas for optimising RAG system performance in philosophy. Firstly, it scrutinises model choices, specifically evaluating the efficacy of various generative Large Language Models and their corresponding embedding models. Secondly, the study delves into the intricate process of hyperparameter tuning. This involves systematically adjusting parameters such as the number of documents retrieved (top-k), the maximum input and output token lengths, the temperature or top-p settings for text generation, and the optimal chunk size and overlap for document segmentation. Each of these parameters profoundly influences the quality and relevance of the generated responses.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#methodological-challenges-retrieval-semantic-mismatch-and-reranking",
    "href": "chapter_ai-nepi_012.html#methodological-challenges-retrieval-semantic-mismatch-and-reranking",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.15 Methodological Challenges: Retrieval Semantic Mismatch and Reranking",
    "text": "12.15 Methodological Challenges: Retrieval Semantic Mismatch and Reranking\n\n\n\nSlide 18\n\n\nBeyond model selection and hyperparameter tuning, Näger and his team confronted additional methodological challenges, notably the issue of retrieval semantic mismatch. To address this, they implemented reranking, an advanced technique designed to refine the relevance of retrieved documents. Their overarching methodology employs a theoretically grounded trial-and-error approach, systematically assessing how various adjustments improve answer quality. Crucially, sound evaluation remains paramount, particularly given the complex nature of philosophical propositions, which rarely reduce to simple, atomic facts. Evaluating the factual accuracy of these nuanced statements presents a significant challenge, demanding rigorous and context-sensitive assessment criteria.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#frontend-overview-configuration-and-comparative-answers",
    "href": "chapter_ai-nepi_012.html#frontend-overview-configuration-and-comparative-answers",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.16 Frontend Overview: Configuration and Comparative Answers",
    "text": "12.16 Frontend Overview: Configuration and Comparative Answers\n\n\n\nSlide 19\n\n\nThe system’s frontend, as demonstrated by Näger, provides a comprehensive interface for configuration and comparative analysis. Users can specify the Generative Model, currently set to gpt-4o-mini, and define prompt token limits, with a model capacity of 128,000 tokens and a user-defined limit of 15,000. Furthermore, the interface allows setting the number of texts to retrieve, typically 15. A ‘Persona’ text area meticulously instructs the model to act as an ‘expert philosopher’, ensuring ‘meticulous and precise’ answers. Users input their philosophical questions, such as ‘What is priority monism?’, into a dedicated field. The system then presents a comparative output, displaying both an ‘Answer with LLM alone’—serving as a benchmark—and an ‘Answer with RAG’, facilitating direct qualitative assessment. Complementing these answers, a ‘Retrieved Texts Overview’ table details the source ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and whether each text was ‘included’ in the prompt, offering full transparency into the retrieval process.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#backend-code-and-output-details",
    "href": "chapter_ai-nepi_012.html#backend-code-and-output-details",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.17 Backend Code and Output Details",
    "text": "12.17 Backend Code and Output Details\n\n\n\nSlide 20\n\n\nNäger’s team crafted the system’s backend in Python, orchestrating the intricate processes of text handling and retrieval. Functions such as print_section_overview and print_paragraph_overview provide detailed insights into how text sections and paragraphs are processed, whilst print_overview_intro displays the total document count within the dataset. Conditional calls throughout the code exemplify a modular design, facilitating flexible text processing and retrieval logic. The system’s output meticulously lists the retrieved texts, detailing article names, specific section headings, and crucially, indicating which texts were fully included in the prompt and which were truncated due to token limitations, thereby ensuring transparency in the information provided to the LLM.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-corpora",
    "href": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-corpora",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.18 Optimising Chunk Size for Philosophical Corpora",
    "text": "12.18 Optimising Chunk Size for Philosophical Corpora\n\n\n\nSlide 21\n\n\nOptimising chunk size represents a critical hyperparameter in RAG system development. Näger and his team explored three primary chunking strategies: employing a fixed number of words (e.g., 500), segmenting by paragraphs, or dividing content into main sections, whether at a lower or higher hierarchical level. Surprisingly, chunking into main sections, inclusive of their headings, yielded the most favourable results. This outcome proved counter-intuitive, given that the average section length of approximately 3000 words substantially exceeded the embedding model’s typical cutoff of 512 words. This efficacy, however, likely stems from the highly systematised structure of the Stanford Encyclopedia of Philosophy, where the initial 500 words of a section often encapsulate its core ideas. Such a strategy might prove less effective for more heterogeneous or less rigorously structured texts. Consequently, future work plans to investigate embedding models with extended context windows, such as Cohere Embed 3, to better accommodate these longer semantic units.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#reranking-addressing-retrieval-semantic-mismatch",
    "href": "chapter_ai-nepi_012.html#reranking-addressing-retrieval-semantic-mismatch",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.19 Reranking: Addressing Retrieval Semantic Mismatch",
    "text": "12.19 Reranking: Addressing Retrieval Semantic Mismatch\n\n\n\nSlide 22\n\n\nReranking constitutes a crucial additional step within the retrieval pipeline, specifically designed to mitigate the problem of false positives, where initially retrieved texts lack true relevance to the query. The primary aim of reranking involves reordering documents based on their actual pertinence. To achieve this, Näger’s team employs a generative Large Language Model (gLLM) to evaluate the relevance of the texts. This approach leverages the gLLM’s superior semantic differentiation capabilities, which significantly surpass those of simpler embedding models. The evaluation process incorporates specific scoring categories, including the informativeness of the text and the length of its relevant passages, culminating in a comprehensive total score. Whilst reranking demonstrably yields highly effective results, it concurrently incurs a substantial increase in computational costs.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advantages-of-rag-systems-in-scientific-tasks",
    "href": "chapter_ai-nepi_012.html#advantages-of-rag-systems-in-scientific-tasks",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.20 Advantages of RAG Systems in Scientific Tasks",
    "text": "12.20 Advantages of RAG Systems in Scientific Tasks\n\n\n\nSlide 23\n\n\nRAG systems offer compelling advantages for scientific tasks. They seamlessly integrate verbatim corpora alongside specialised domain knowledge, thereby furnishing more detailed answers and significantly reducing the incidence of hallucinations. Furthermore, these systems inherently facilitate the precise citation of relevant documents, a critical feature for academic integrity. Consequently, the RAG architecture proves exceptionally well-suited for assisting across a wide spectrum of scientific endeavours, enhancing both the accuracy and trustworthiness of AI-generated content.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#cautions-and-challenges-in-rag-implementation",
    "href": "chapter_ai-nepi_012.html#cautions-and-challenges-in-rag-implementation",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.21 Cautions and Challenges in RAG Implementation",
    "text": "12.21 Cautions and Challenges in RAG Implementation\n\n\n\nSlide 24\n\n\nWhilst RAG systems offer substantial benefits, their effective deployment necessitates careful consideration of several cautions and challenges. Firstly, RAG systems inherently demand extensive tweaking; optimal settings are highly contingent upon the specific corpus and the nature of the questions posed. Secondly, rigorous evaluation remains paramount, requiring a representative set of questions and meticulously prepared expected answers. When exploring previously unexamined corpora, the indispensable involvement of domain experts, such as philosophers, becomes critical for accurate assessment. A significant challenge arises when no relevant documents are retrieved, leading to a marked decrease in answer quality, which then necessitates prompt adjustment. Paradoxically, RAG systems often yield inferior results for widely discussed overview questions, such as ‘What are the central arguments against scientific realism?’ This phenomenon occurs because RAGs, by design, concentrate on local information, which can inadvertently distract from the broader perspective required for comprehensive overview responses. Consequently, future developments must focus on crafting more flexible systems, particularly agentic RAG architectures, capable of discerning between question types and adapting their approach accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#additional-visual-materials",
    "href": "chapter_ai-nepi_012.html#additional-visual-materials",
    "title": "12  RAG systems solve central problems of LLMs",
    "section": "12.22 Additional Visual Materials",
    "text": "12.22 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n\n\n\nSlide 25\n\n\nThis slide, titled “SEP RAG Overview”, presents a detailed view of the system’s frontend. A “Configuration” section, which can be initialised, includes an “Options” subsection. Here, the “Generative Model” is set to gpt-4o-mini, with a “Prompt Token Limit (Model)” of 128,000 and a user-defined “Prompt Token Limit (here)” of 15,000. The system is configured to retrieve 15 texts. A “Persona” text area instructs the model to act as an “expert philosopher”, ensuring “meticulous and precise” answers. The “Philosophical Question” field contains the query: “What is priority monism?”. Below the “Generate answer” button, the slide presents two comparative answers. The “Answer with LLM alone” defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with pluralism and mereological nihilism, and citing examples like the universe and philosophers such as Spinoza. The “Answer with RAG”, titled “Response to ‘What is priority monism?’”, offers a similar but more nuanced definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative. It notes the contrast with existence monism, discusses mathematical expressions, ontological priority, and relevance to quantum mechanics (entangled systems). Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. Finally, a “Retrieved Texts Overview” table lists five documents, detailing their ‘file_names’, ‘sec_heading’, ‘distances’ (relevance scores), ’length_/_token’, ‘total_token’, and ‘included’ status. The table indicates that the RAG system retrieved relevant sections from ‘monism’ and ‘disability-care-rationing’ files, with the first two ‘monism’ entries fully included, and a third ‘monism’ entry partially included. The ‘distances’ column suggests lower values indicate higher relevance. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.\n\n\n\nSlide 26\n\n\nThis slide, also titled “SEP RAG Overview”, visually divides into “Frontend” on the left and “Backend: Python Code” on the right. The frontend section displays a collapsed “Configuration” panel and an expanded “Options” panel, allowing selection of gpt-4o-mini as the “Generative Model” and setting token limits (128,000 model, 15,000 user-defined). The “Persona” is set to “expert philosopher”, and 15 texts are to be retrieved. The “Philosophical Question” is “What is priority monism?”. The interface then presents comparative answers from an “LLM alone” and “RAG”, with the RAG answer providing a more detailed, source-attributed philosophical definition. A “Retrieved Texts Overview” table details source files, section headings, distances, token lengths, and inclusion status. The “Backend: Python Code” section displays Python function definitions, including print_section_overview, print_paragraph_overview, and print_overview_intro, illustrating the modular and configurable nature of the backend’s text processing and retrieval logic. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “6”.\n\n\n\nSlide 27\n\n\nTitled “SEP RAG Details 1”, this slide focuses on the “Frontend: Input section”. The interface features a “Configuration” section with an “Initialize” button. The “Options” section provides detailed settings: “Generative Model” is gpt-4o-mini, “Prompt Token Limit (Model)” is 128,000, and “Prompt Token Limit (here)” is 15,000. The “Persona” instruction is “You are an expert philosopher. You answer meticulously and precisely.” The system is set to retrieve 15 texts. The “Philosophical Question” input box contains “What is priority monism?”. A “Generate answer” button is present at the bottom. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “7”.\n\n\n\nSlide 28\n\n\nThis slide, “SEP RAG Details 2”, presents the “Frontend: Output-section answers”. It features a side-by-side comparison of answers to ‘What is priority monism?’. The ‘Answer with LLM alone’ defines priority monism as a metaphysical position where a single, fundamental entity is ontologically prior to its constituent parts, contrasting it with mereological nihilism and citing examples like the universe and philosophers such as Spinoza. The ‘Answer with RAG’, titled “Response to ‘What is priority monism?’”, provides a more detailed, augmented definition, asserting the existence of exactly one basic concrete object (the universe or cosmos) whose parts are derivative and dependent on the fundamental whole (Text 0). It contrasts this with existence monism, discusses mathematical expressions, and highlights its relevance to emergent properties in quantum mechanics. Historically, it associates Plato and Spinoza with this view and notes its recent traction against competing doctrines. The RAG answer text is truncated, indicated by a scrollbar. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 29\n\n\nContinuing the “SEP RAG Details 2” theme, this slide focuses on a “Comparative setup for qualitative evaluation” of output answers. It displays the “Answer with LLM alone” on the left, serving as a ‘benchmark’, which defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its derived parts, contrasting it with pluralism or mereological nihilism. On the right, the “Answer with RAG”, titled “Response to ‘What is priority monism?’”, provides a definition augmented by retrieved information, citing ‘Text 0’ multiple times. This RAG answer highlights the existence of exactly one basic concrete object (the universe), contrasts it with existence monism, discusses its mathematical expression, and notes its relevance to quantum mechanics and emergent properties. It also attributes the view to Plato and Spinoza and contrasts it with priority pluralism and nihilism. The RAG answer text is partially visible, with a scroll bar indicating more content. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 30\n\n\nThis slide, “SEP RAG Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The left section, “Answer with LLM alone”, explains priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its parts, which derive their existence from the whole. It contrasts this with pluralism and mereological nihilism, using the universe as an example and mentioning Spinoza. The right section, “Answer with RAG”, presents the “Response to ‘What is priority monism?’”. This augmented answer defines priority monism as the existence of one basic concrete object (the universe), with parts being derivative. It highlights phrases marked with ‘(Text 0)’ for source attribution, contrasts it with ‘existence monism’, mentions its mathematical expression, and its crucial role in understanding emergent properties in quantum mechanics. It also associates the view with ‘Plato and Spinoza’ and contrasts it with ‘priority pluralism’ and ‘nihilism’. The text on the right is truncated. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 31\n\n\nThe slide, “SEP RAG Details 2”, continues the “Comparative setup for qualitative evaluation” of answers. The “Answer with LLM alone” on the left defines priority monism as a metaphysical position where a single ‘whole’ is ontologically prior to its constituent parts, citing Spinoza and contrasting it with pluralism. The “Answer with RAG” on the right, titled “Response to ‘What is priority monism?’”, provides a definition with specific phrases highlighted and marked with ‘(Text 0)’, indicating retrieved information. This RAG-generated answer highlights ‘existence monism’ as a contrast, states ‘there exists exactly one basic entity’ (the cosmos), and notes its ‘crucial for understanding emergent properties found in quantum mechanics’. It associates ‘Plato’ and Spinoza with this view and contrasts it with ‘priority pluralism’ and ‘nihilism’. The RAG answer ends abruptly, suggesting an interactive or truncated output. The footer indicates “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “8”.\n\n\n\nSlide 32\n\n\nTitled “SEP RAG Details 3”, this slide presents the “Frontend: Output section retrieved texts” under the heading “Retrieved Texts Overview:”. It features a table with six columns: ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/_token’, ‘total_token’, and ‘included’. The table lists 15 retrieved text segments. For instance, the first entry is from ‘monism’, section ‘## 3. Priority Monism’, with a distance of 0.448, length of 12515 tokens, total tokens 12515, and marked ‘Yes’ for inclusion. The ‘distances’ column likely represents a similarity score, with lower values indicating higher relevance. The ‘included’ column shows that only the top two most relevant sections were fully included, and the third was partially included, suggesting a cutoff based on relevance or a maximum token limit for the context provided to the language model. The ‘total_token’ column tracks the cumulative token count of all retrieved sections. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.\n\n\n\nSlide 33\n\n\nThis slide, “SEP RAG Details 3”, continues to detail the “Frontend: Output section retrieved texts” under “Retrieved Texts Overview:”. It displays a comprehensive table of 15 retrieved text segments, indexed from 0 to 14. The table’s columns are ‘file_names’, ‘sec_heading’, ‘distances’, ’length_/token’, ‘total_token’, and ‘included’. The ‘file_names’ include ‘monism’, ‘disability-care-rationing’, ‘neutral-monism’, and others. The ‘sec_heading’ specifies the section (e.g., ‘## 3. Priority Monism’, ‘## Abstract’). ‘Distances’ range from 0.448 to 1.241, indicating relevance. ’length/_token’ shows segment lengths, and ‘total_token’ tracks cumulative token count. The ‘included’ column indicates ‘Yes’, ‘No’, or ‘Partially’. Several rows are highlighted: the first two ‘monism’ entries are yellow-highlighted and marked ‘Yes’ for inclusion. The third ‘monism’ entry is yellow-highlighted and ‘Partially’ included. A ‘disability-care-rationing’ entry is highlighted in reddish-pink and marked ‘No’. This highlighting likely draws attention to specific retrieval outcomes and inclusion criteria. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “9”.\n\n\n\nSlide 34\n\n\nThis slide introduces “Chunk size” as a “hyperparameter”, visually indicated by a green arrow pointing from the term to the label. The main body of the slide is blank, suggesting it serves as an introductory concept or a placeholder for further discussion on this key configurable parameter in RAG methodology. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 35\n\n\nThe slide, titled “Chunk size” and labelled as a “hyperparameter”, presents “Options:” for determining chunk size. These options are listed as:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThis outlines different strategies for segmenting text or data. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 36\n\n\nThis slide, titled “Chunk size” and identified as a “hyperparameter”, reiterates the “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nIt then states the “Best result: chunking into main sections (including headings).” A rationale is provided: “philosophical facts are rarely short and isolated, they need some space for presentation”. This leads to the conclusion: “best to stick to longer semantic units”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 37\n\n\nThe slide, titled “Chunk size” and labelled as a “hyperparameter”, lists “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nIt states the “Best result: chunking into main sections (including headings).” The rationale provided is that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) section highlights a key observation: “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this discrepancy, the slide notes “Nevertheless best results.” Another rationale explains that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 38\n\n\nThis slide, titled “Chunk size” and identified as a “hyperparameter”, details the “Options:” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThe “Best result:” was “chunking into main sections (including headings)”. The rationale is that “philosophical facts are rarely short and isolated, they need some space for presentation”, thus “best to stick to longer semantic units”. A “NB:” (Nota Bene) section highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words).” Despite this, it notes “Nevertheless best results.” Another rationale states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. A “planned:” section outlines future work: “try emb. models with longer context window like Cohere Embed 3”, suggesting experimentation with larger input sizes. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 39\n\n\nThe slide, titled “Chunk size” and explicitly labelled as a “hyperparameter”, begins by listing “Options” for chunking:\n\nfixed number of words (e.g. 500)\nparagraphs\nsections\n\nThe “Best result” is stated as “chunking into main sections (including headings)”. An observation notes that “philosophical facts are rarely short and isolated, they need some space for presentation”, leading to the conclusion “best to stick to longer semantic units”. A “NB” (Nota Bene) point highlights that the “Average length of sections (approx. 3000 words) has been much longer than cutoff of the embedding model (512 words). Nevertheless best results.” A “planned:” section indicates future work to “try emb. models with longer context window like Cohere Embed 3”. Another observation states that in “highly systematically ordered documents, beginnings of sections contain the main theme”. The slide concludes with a crucial lesson: “effective chunking highly depends on the specifics of the corpus and the kind of questions”. The footer includes “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “10”.\n\n\n\nSlide 40\n\n\nThis slide introduces “Reranking”, prominently displayed and clarified as an “additional step to retrieval”. This visual element emphasises the sequential nature of reranking within an information processing pipeline. The main body of the slide is blank, indicating that further details or diagrams related to reranking would likely be presented incrementally or on subsequent slides. The footer contains “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.\n\n\n\nSlide 41\n\n\nThis presentation slide focuses on “Reranking” as an “additional step to retrieval”. The main content area identifies the core problem that reranking aims to solve: “not all retrieved texts are relevant to the question (false positives)”. This highlights a common challenge in information retrieval where an initial search might yield documents that are not truly pertinent to the user’s query, necessitating a refinement step. The footer displays “Paul Näger, 3.4.25”, “RAG in HPSS”, and page number “11”.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG systems solve central problems of LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "",
    "text": "Overview\nL. Gautheron and Mike Schneider, in a collaborative endeavour with Schneider from the University of Missouri, have embarked upon a developing project. This initiative addresses fundamental questions within the philosophy of science, integrating advanced computational methods, including those discussed in recent academic discourse, with sophisticated social network analysis techniques. Gautheron and Schneider systematically explore the concept of ‘plural pursuit’ within the context of quantum gravity research.\nTheir approach outlines three distinct steps. First, the authors establish the philosophical framework and introduce the case study. Second, they construct a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, they confront this empirical reconstruction with physicists’ ‘top-down’ intuitions regarding their field’s structure. The project ultimately seeks to determine whether quantum gravity research exemplifies plural pursuit, characterised by independent communities pursuing distinct paradigms in parallel.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.1 Introduction to Plural Pursuit in Quantum Gravity",
    "text": "13.1 Introduction to Plural Pursuit in Quantum Gravity\n\n\n\nSlide 02\n\n\nGautheron and Schneider’s developing project, a collaborative endeavour with Mike Schneider from the University of Missouri, directly addresses fundamental questions in the general philosophy of science. The authors integrate computational methods, previously discussed in academic forums, with social network analysis techniques to achieve their objectives. Their presentation systematically unfolds in three distinct steps.\nInitially, Gautheron and Schneider establish the philosophical framing and introduce quantum gravity as the central case study. Subsequently, they propose a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, the authors confront this empirically derived reconstruction with physicists’ own ‘top-down’ intuitions concerning the field’s inherent structure. The overarching themes, prominently displayed on the accompanying slide, encompass ‘Quantum gravity and plural pursuit in science’, ‘Plural pursuit across scales: a bottom-up approach’, and ‘The physicists’ intuition: a top-down approach’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#presentation-structure-and-core-themes",
    "href": "chapter_ai-nepi_015.html#presentation-structure-and-core-themes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.2 Presentation Structure and Core Themes",
    "text": "13.2 Presentation Structure and Core Themes\n\n\n\nSlide 03\n\n\nGautheron and Schneider systematically unfold their presentation in three distinct steps. Initially, they establish the philosophical framing and introduce quantum gravity as the central case study. Subsequently, the authors propose a ‘bottom-up’ reconstruction of the quantum gravity research landscape. Finally, they confront this empirically derived reconstruction with physicists’ own ‘top-down’ intuitions concerning the field’s inherent structure.\nThe accompanying slide visually reinforces these core themes, prominently highlighting ‘Quantum gravity and plural pursuit in science’. It also presents ‘Plural pursuit across scales: a bottom-up approach’ and ‘The physicists’ intuition: a top-down approach’ as integral, albeit visually de-emphasised, components.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-fundamental-problem-of-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#the-fundamental-problem-of-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.3 The Fundamental Problem of Quantum Gravity",
    "text": "13.3 The Fundamental Problem of Quantum Gravity\n\n\n\nSlide 04\n\n\nA central, enduring challenge in fundamental physics involves formulating a quantum theory of gravity. This profound problem necessitates reconciling our understanding of phenomena at very small scales, governed by quantum mechanics, with our knowledge of very large scales, described by general relativity. Physicists have attempted numerous solutions, with string theory emerging as the most prominent amongst them.\nTo comprehensively account for this multifaceted situation, Gautheron and Schneider introduce the concept of ‘plural pursuit’. The slide explicitly titles this section ‘A plurality of approaches to quantum gravity’, framing the core problem as ‘how to formulate a quantum theory of gravity?’ and prompting a discussion on ‘Attempted solutions:’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit-in-scientific-inquiry",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit-in-scientific-inquiry",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.4 Defining Plural Pursuit in Scientific Inquiry",
    "text": "13.4 Defining Plural Pursuit in Scientific Inquiry\n\n\n\nSlide 05\n\n\nMike Schneider defines ‘plural pursuit’ as ‘distinct yet concurrent instances of normal science, dedicated to a common problem-solving goal’. In this specific context, the shared objective involves reconciling quantum theory with gravitation. Each instance of normal science, crucially, finds its articulation through a social community intrinsically linked to an intellectual disciplinary matrix. This concept draws parallels with Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes.\nConsequently, Gautheron and Schneider pose an empirical question: does quantum gravity research exemplify plural pursuit, meaning it comprises independent communities concurrently pursuing distinct paradigms? The accompanying slide reiterates the problem and enumerates specific attempted solutions, including String theory, Supergravity, Loop quantum gravity (encompassing spin foams), Causal set theory, and Asymptotic safety.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-methodology",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-methodology",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.5 Bottom-Up Reconstruction Methodology",
    "text": "13.5 Bottom-Up Reconstruction Methodology\n\n\n\nSlide 06\n\n\nGautheron and Schneider systematically gathered a substantial data corpus, comprising approximately 200,000 abstracts and titles from fundamental physics literature. Their methodology then proceeded in two distinct steps. First, they reconstructed the intellectual structure of the field through linguistic analysis, employing the Bertopic pipeline. This process involved spatialising documents into an embedding space, followed by unsupervised clustering at a highly fine-grained level.\nThis meticulous approach yielded 600 distinct topics, a granularity deemed necessary for accurately capturing niche research areas, some of which might involve as few as 100 papers. Consequently, the authors assigned each physicist a ‘specialty’, determined by the most prevalent topic across their publications, thereby partitioning authors according to the field’s inherent linguistic and intellectual structure. Second, Gautheron and Schneider conducted a comprehensive social network analysis. This involved constructing a co-authorship graph, where individual physicists functioned as nodes and co-authorship relationships formed the edges. Applying a community detection method to this network, the authors recovered approximately 800 communities from a total of 30,000 physicists, providing an alternative partition of authors that directly reflects the field’s social structure. The accompanying slide visually reinforces the problem and attempted solutions, whilst explicitly linking them to the concept of ‘plural pursuit’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-as-a-one-to-one-mapping",
    "href": "chapter_ai-nepi_015.html#conceptualising-plural-pursuit-as-a-one-to-one-mapping",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.6 Conceptualising Plural Pursuit as a One-to-One Mapping",
    "text": "13.6 Conceptualising Plural Pursuit as a One-to-One Mapping\n\n\n\nSlide 07\n\n\nWithin this framework, Gautheron and Schneider define plural pursuit as a precise one-to-one mapping between distinct communities and their corresponding intellectual topics. The authors visualise this relationship using a correlation matrix, where communities align with topics. An ideal scenario, indicative of a clear division of labour, would manifest as a block-diagonal matrix, signifying that each community dedicates itself entirely to a single topic. The accompanying slide, titled ‘Normal science and plural pursuit’, explicitly defines plural pursuit as ‘distinct yet concurrent instances of normal science, dedicated to a common problem-solving goal’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-dependency-in-research-landscapes",
    "href": "chapter_ai-nepi_015.html#addressing-scale-dependency-in-research-landscapes",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.7 Addressing Scale-Dependency in Research Landscapes",
    "text": "13.7 Addressing Scale-Dependency in Research Landscapes\n\n\n\nSlide 08\n\n\nGautheron and Schneider observe that directly applying the fine-grained partitions, comprising 600 topics and 800 communities, results in a highly complex and unintelligible correlation matrix. This challenge, the authors explain, stems from several issues inherent in fine-graining. Firstly, the level of topic fine-graining often proves arbitrary; for instance, string theory might appear scattered across numerous distinct topics. Secondly, micro-social processes significantly shape communities, enabling multiple communities to undertake large research programmes concurrently.\nMore fundamentally, the computational notions of ‘topic’ and ‘community’ are inherently scale-dependent. Conceptually, research programmes themselves exhibit nested structures, allowing for the division of, for example, string theory into families and subfamilies. Consequently, Gautheron and Schneider argue that identifying instances of plural pursuit demands addressing this inherent ambiguity arising from observations at different scales. The accompanying slide reiterates the definition of plural pursuit whilst elaborating that ‘Each instance of normal science is articulated by community × intellectual disciplinary matrix’, citing Kuhn’s paradigms, Laudan’s research traditions, and Lakatos’ research programmes as examples.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-the-research-landscape",
    "href": "chapter_ai-nepi_015.html#hierarchical-reconstruction-of-the-research-landscape",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.8 Hierarchical Reconstruction of the Research Landscape",
    "text": "13.8 Hierarchical Reconstruction of the Research Landscape\n\n\n\nSlide 09\n\n\nTo address the challenges of scale-dependency, Gautheron and Schneider propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, the authors implement a hierarchical clustering approach, commencing with 600 fine-grained topics and progressively merging them using an agglomerative clustering technique. Concurrently, for the community structure, they employ a hierarchical stochastic block model from the outset, which learns a multi-level partition into progressively coarser communities.\nThese hierarchical structures collectively induce a notion of scale, enabling observation of the system at various levels; for instance, one can visualise the co-authorship network with scientists’ specialties indicated by colour at different linguistic coarse-graining levels. Nevertheless, Gautheron and Schneider note that a persistent challenge remains: the chosen scale for observation is still arbitrary, complicating the selection of an appropriate level for either the topic or community structure. The accompanying slide reiterates the core definitions whilst posing the central empirical question: ‘Is quantum gravity research an instance of plural pursuit? (i.e., independent communities pursuing different paradigms in parallel)’.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#necessity-of-adaptive-topic-coarse-graining",
    "href": "chapter_ai-nepi_015.html#necessity-of-adaptive-topic-coarse-graining",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.9 Necessity of Adaptive Topic Coarse-Graining",
    "text": "13.9 Necessity of Adaptive Topic Coarse-Graining\n\n\n\nSlide 11\n\n\nCrucially, the selection of scale significantly impacts the resulting correlation matrix, leading to markedly different interpretations of the research landscape. Consequently, Gautheron and Schneider propose an adaptive topic coarse-graining strategy to address this variability. The rationale underpinning this approach is that whilst fine-grained topics capture subtle linguistic nuances, some of these distinctions hold no practical consequence for scientists’ collaborative capacities.\nTheir primary objective, therefore, involves systematically removing degrees of freedom from the initial fine-grained partition without compromising any information essential for comprehending the field’s social structure. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the ‘Linguistic analysis’ process, whilst also indicating the presence of a ‘Social network analysis’ component.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-methodology",
    "href": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-methodology",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.10 Adaptive Topic Coarse-Graining Methodology",
    "text": "13.10 Adaptive Topic Coarse-Graining Methodology\n\n\n\nSlide 12\n\n\nGautheron and Schneider employed the Minimum Description Length (MDL) criterion to select an appropriate scale for topic coarse-graining. This criterion aims to minimise a quantity that judiciously balances two critical requirements: the linguistic partition’s efficacy in explaining the field’s social structure, and the imperative for a partition that avoids excessive complexity or fine-graining. The authors iteratively refined the 600-topic hierarchical tree, continuing as long as this refinement improved the MDL criterion.\nThe procedure ceased when further increases in complexity no longer yielded sufficient information gain regarding the social structure. This adaptive strategy successfully reduced the initial 600 topics to a more manageable 50 coarse-grained topics. Notably, whilst many topics were aggregated, Gautheron and Schneider meticulously preserved certain small-scale linguistic topics, underscoring their crucial role in comprehending the social structure. This outcome, the authors contend, validated the initial fine-grained classification, as some of these smaller topics proved indispensable for the social analysis. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘2287748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the ‘Linguistic analysis’ process, whilst also indicating the presence of a ‘Social network analysis’ component.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#labelling-and-focusing-coarse-grained-topics",
    "href": "chapter_ai-nepi_015.html#labelling-and-focusing-coarse-grained-topics",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.11 Labelling and Focusing Coarse-Grained Topics",
    "text": "13.11 Labelling and Focusing Coarse-Grained Topics\n\n\n\nSlide 13\n\n\nThe coarse-graining process ultimately yielded 50 distinct topics. To enhance their interpretability, Gautheron and Schneider assigned labels to these topics by extracting representative engrams. For the purpose of this study, the authors’ analysis specifically focused on those topics directly involving quantum gravity. The accompanying slide, titled ‘Clustering pipeline’, details the ‘Linguistic analysis’ pipeline, which commences with the spatialisation of documents into an embedding space (L.1), proceeds to unsupervised clustering (L.2) that identified K=611 clusters, and culminates in the derivation of individual ‘Specialty σ_i’ (L.3).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#matching-intellectual-topics-to-social-communities",
    "href": "chapter_ai-nepi_015.html#matching-intellectual-topics-to-social-communities",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.12 Matching Intellectual Topics to Social Communities",
    "text": "13.12 Matching Intellectual Topics to Social Communities\n\n\n\nSlide 14\n\n\nGautheron and Schneider employed a correlation matrix to systematically match the coarse-grained topics with the field’s community structures across various scales. For each topic, the authors meticulously identified the community that most effectively explained its presence across different levels of the community hierarchy. Their analysis yielded several key observations: some expansive topics, such as a prominent purple cluster, exhibited no clear ties to specific communities, suggesting a broad, pervasive interest across the entire field.\nConversely, other topics, notably string theory, demonstrated a robust correspondence with a community structure situated at the third level of the hierarchy. Interestingly, certain quantum gravity research programmes, such as loop quantum gravity, aligned with communities found at much lower, more fine-grained levels of the hierarchy. Furthermore, the study revealed intricate nested structures; for instance, a smaller community, whilst embedded within a larger string theory community, simultaneously maintained strong ties to a distinct intellectual topic, holography. Ultimately, this comprehensive analysis indicated an absence of a clear division of labour, instead demonstrating a complex entanglement between different scales, as Gautheron and Schneider observe. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1: Community detection C=819).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuition-on-field-structure",
    "href": "chapter_ai-nepi_015.html#top-down-approach-physicists-intuition-on-field-structure",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.13 Top-Down Approach: Physicists’ Intuition on Field Structure",
    "text": "13.13 Top-Down Approach: Physicists’ Intuition on Field Structure\n\n\n\nSlide 15\n\n\nTo gather a ‘top-down’ perspective, Gautheron and Schneider surveyed the founding members of the International Society for Quantum Gravity. The authors specifically asked respondents to ‘Provide a list of quantum gravity approaches that come to your mind as structuring the total research landscape in quantum gravity.’ Despite some disagreement amongst the physicists, Gautheron and Schneider successfully compiled a comprehensive and detailed list of approaches that partition the field.\nFor further analysis, the authors specifically focused on String theory, Supergravity, and Holography. This particular focus arose from Gautheron and Schneider’s observation that physicists themselves expressed uncertainty regarding whether these three approaches should be considered distinct, with some arguing for their fundamental connection to string theory, notwithstanding their clear historical and conceptual differences. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-top-down-intuition",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-top-down-intuition",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.14 Confronting Bottom-Up Reconstruction with Top-Down Intuition",
    "text": "13.14 Confronting Bottom-Up Reconstruction with Top-Down Intuition\n\n\n\nSlide 16\n\n\nGautheron and Schneider trained a classifier to predict the specific approach of papers from their text embeddings, utilising the all-MiniLM-L6-v2 model applied to titles and abstracts, alongside hand-coded labels. This classifier enabled the authors to directly confront the supervised, ‘top-down’ list of approaches with the ‘bottom-up’ reconstruction. Their results demonstrated varied effectiveness: whilst the classifier performed commendably for certain approaches, exhibiting a strong correspondence to topics emergent from the bottom-up analysis, it proved less effective for approaches that were either phenomenological or lacked a ‘full-fledged conceptual framework’. Conversely, it achieved high accuracy for ‘well-defined and conceptually autonomous’ frameworks.\nA particularly significant finding emerged from the authors’ bottom-up approach: a large string theory cluster that comprehensively encompassed both supergravity and string theory. This empirical observation notably converged with the uncertainty expressed by physicists regarding the true separation of these two areas, despite their distinct historical trajectories and conceptual underpinnings, as Gautheron and Schneider highlight. The accompanying slide, titled ‘Clustering pipeline’, specifies the dataset as ‘228748 abstracts+titles of theoretical physics listed on Inspire HEP’ and visually outlines the complete pipeline, encompassing both Linguistic analysis (L1, L2, L3) and Social network analysis (S1).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#implications-for-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#implications-for-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.15 Implications for Plural Pursuit in Quantum Gravity",
    "text": "13.15 Implications for Plural Pursuit in Quantum Gravity\n\n\n\nSlide 17\n\n\nGautheron and Schneider observe that the extensive overlap between communities engaged in supergravity and string theory research suggests a challenging meaningful separation between them, even if a small contingent of individuals continues to pursue supergravity independently. This observation aligns with the authors’ bottom-up assessment, which, by systematically removing linguistic nuances devoid of social structural consequences, consolidates these areas. This convergence occurs despite the initial linguistic clusters accurately acknowledging their conceptual distinctions.\nIn conclusion, Gautheron and Schneider contend that socio-epistemic systems are demonstrably observable at multiple scales, implying that the very notions of communities and disciplinary matrices are inherently scale-dependent. Consequently, identifying genuine configurations of plural pursuit necessitates a meticulous matching of these structures across various scales, as their work demonstrates. Furthermore, Gautheron and Schneider’s bottom-up reconstruction of the quantum gravity research landscape possesses the capacity to either confirm or critically re-assess physicists’ established intuitions. Crucially, the increasing power of computational methods now enables scholars to revisit and challenge long-held philosophical insights, particularly intuitions concerning paradigms or communities within specific contexts such as quantum gravity. As a final reflection, paraphrasing Clausewitz, ‘Computation is the continuation of philosophy by other means’. The accompanying slide, titled ‘Plural pursuit across communities and topics’, visually defines plural pursuit as a ‘one-to-one mapping between communities and topics’ and presents a ‘Community-topic correlation matrix’ with red and blue blocks, noting that ‘A block-diagonal matrix would imply that communities specialize into distinct domains.’",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#additional-visual-materials",
    "href": "chapter_ai-nepi_015.html#additional-visual-materials",
    "title": "13  Quantum gravity and plural pursuit in science",
    "section": "13.16 Additional Visual Materials",
    "text": "13.16 Additional Visual Materials\nThe following slides provide supplementary visual information relevant to the presentation:\n\n\n\nSlide 10\n\n\n\nThis slide serves as a summary, titled ‘Summary’ in large green font at the top left. The main content area lists three key topics, likely representing the structure or main points of the presentation. The first point, ‘Quantum gravity and plural pursuit in science’, is displayed in a faded, light green colour. The second point, ‘Plural pursuit across scales: a bottom-up approach’, is prominently highlighted in a vibrant green, suggesting it is the current focus or a primary takeaway. The third point, ‘The physicists’ intuition: a top-down approach’, is also in a faded, light green colour. This visual emphasis indicates a progression through or a specific focus on the ‘bottom-up approach’ within the context of plural pursuit across scales. A persistent black banner at the top left of the slide reiterates these three topics, with ‘Plural pursuit across scales: a bottom-up approach’ also highlighted in white text, reinforcing its current relevance. The bottom of the slide features a black footer bar with the authors’ names, ‘L. Gautheron, Mike D. Schneider’, on the left. On the right, a green section of the footer displays the presentation title, ‘Plural pursuit across scales’, and the slide number ‘6 / 23’. Standard presentation navigation icons are also visible in the bottom centre.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum gravity and plural pursuit in science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "",
    "text": "Overview\nIn a comparative study, Francis Lareau and Christophe Malaterre evaluated the performance of Latent Dirichlet Allocation (LDA) and BERTopic, two prominent topic modelling approaches, across various levels of textual granularity. Topic modelling, a computational method for extracting latent themes from a corpus, has become an essential analytical tool for navigating large volumes of scientific literature, particularly within the history, philosophy, and sociology of science (HPSS). The technique facilitates the identification of research trends and paradigm shifts, the discernment of thematic substructures, and the analysis of evolving scientific vocabularies.\nPrevious applications of topic modelling have drawn upon diverse textual components, including titles, abstracts, and full texts. This observation prompted Lareau and Malaterre to investigate a critical question: does applying topic modelling solely to titles or abstracts suffice, or is full-text analysis indispensable? The question carries significant weight, given the substantial resources required to obtain, preprocess, and analyse comprehensive full-text corpora.\nTo address this, the authors constituted a corpus of scientific articles, meticulously isolating their title, abstract, and full-text sections. They then applied both LDA and BERTopic to each of these textual components, generating six distinct topic models. Finally, the team subjected these models to rigorous qualitative and quantitative comparison to determine their relative merits.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "href": "chapter_ai-nepi_016.html#topic-modelling-in-hpss",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.1 Topic Modelling in HPSS",
    "text": "14.1 Topic Modelling in HPSS\n\n\n\nSlide 02\n\n\nTopic modelling has established itself as an indispensable tool within the History, Philosophy, and Sociology of Science (HPSS). The method enables scholars to identify research trends and paradigm shifts (Griffiths and Steyvers, 2004) and to reveal the substructures and interrelations of themes (Blei and Lafferty, 2007). Furthermore, it supports the analysis of scientific vocabulary evolution (Chavalarias and Cointet, 2013) and aids in uncovering hidden biases within scientific discourse (Sugimoto et al., 2013).\nAcademics also employ topic modelling to study the sociology of scientific communities (Gerow et al., 2018), analyse the interdisciplinary nature of various fields (Hyeyoung et al., 2022), and enhance the historiography of both science (Mimno, 2012) and art (Browman, 2023). A crucial observation from this body of work is the varied application of topic modelling to different textual structures, including titles, abstracts, and full texts.\nThis variety directly informs the central research question posed by Lareau and Malaterre: is a comprehensive full-text analysis truly necessary, or can one achieve comparable results using only titles or abstracts? This inquiry is particularly urgent given the considerable investment of time and resources needed to manage full-text corpora. Their methodology therefore involved a systematic comparison of models derived from each textual level.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "href": "chapter_ai-nepi_016.html#comparing-lda-and-bertopic",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.2 Comparing LDA and BERTopic",
    "text": "14.2 Comparing LDA and BERTopic\n\n\n\nSlide 05\n\n\nThis study provides a rigorous comparison of two leading topic modelling approaches: Latent Dirichlet Allocation (LDA) and BERTopic. Both methodologies operate on shared fundamental postulates: that documents can be represented by numerical vectors, that topics become identifiable through linguistic regularities, and that machine learning can automate the detection of these patterns.\nLDA, a classical statistical technique, employs a vector representation derived from word counts within documents. In this framework, topics manifest as latent variables adhering to Dirichlet’s law. A key advantage of LDA is its capacity to effectively process long texts, making it suitable for analysing titles, abstracts, or full texts.\nConversely, BERTopic represents a more contemporary, modular approach. It leverages vector representations from large language models, originally building upon BERT, from which it derives its name. Within BERTopic, topics correspond to clusters of documents that reflect their topological densities. Whilst traditionally limited in handling extensive texts, recent advancements have enabled BERTopic to process significantly longer documents—up to approximately 131,000 tokens. Consequently, Lareau and Malaterre specifically tested BERTopic with novel embedding techniques designed to manage such substantial text lengths.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#astrobiology-corpus-and-qualitative-framework",
    "href": "chapter_ai-nepi_016.html#astrobiology-corpus-and-qualitative-framework",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.3 Astrobiology Corpus and Qualitative Framework",
    "text": "14.3 Astrobiology Corpus and Qualitative Framework\n\n\n\nSlide 06\n\n\nFor their analysis, the authors used an Astrobiology corpus previously explored by Malaterre and Lareau (2023). Following a thorough evaluation, they selected a full-text LDA model comprising 25 distinct topics as a reference. The team meticulously analysed each topic by examining its most representative words and documents, subsequently assigning a descriptive name based on its key terms.\nThey then calculated the mutual correlation between these topics, based on their co-occurrence within documents. A community detection algorithm subsequently identified four thematic clusters, designated A, B, C, and D, and distinguished by red, green, yellow, and blue colour variations. The findings are presented in a graph that illustrates the correlations amongst the 25 topics, complete with their labels and cluster affiliations. In this visualisation, line thickness signifies correlation strength, whilst circle size indicates a topic’s prevalence across the corpus.\nThis comprehensive framework facilitates a qualitative comparison of the six topic models. For instance:\n\nCluster A (red/pink) encompasses topics such as the effects of space on health (A-CELL-PLANT-ANIMAL), microbial survival (A-RADIATION-SPORE), and social studies of astrobiology (A-LIFE-CIVILIZATION).\nCluster B (green/teal) includes themes like prebiotic chemistry (B-ORGANIC-MOLECULE), properties of amino acids (B-AMINO-ACID), and the origin of chirality (B-CHIRALITY).\nCluster C (yellow/orange) features topics such as planetary atmospheres (C-ATMOSPHERE) and the dynamics of planetary systems (C-PLANET-STAR).\nCluster D (blue) covers areas like geological biosignatures (D-STRUCTURE-GEOLOGY) and the characterisation of Mars (D-MARS).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#quantitative-evaluation-metrics",
    "href": "chapter_ai-nepi_016.html#quantitative-evaluation-metrics",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.4 Quantitative Evaluation Metrics",
    "text": "14.4 Quantitative Evaluation Metrics\n\n\n\nSlide 07\n\n\nLareau and Malaterre compared the topic models using four distinct quantitative metrics.\n\nThe Adjusted Rand Index evaluates the similarity between two document clusterings whilst correcting for chance agreement.\nTopic diversity quantifies the proportion of unique top words within a given topic model.\nJoint recall assesses the extent to which the top words collectively represent the documents classified within each topic.\nThe coherence metric, specifically Coherence CV, determines whether the top words meaningfully co-occur by measuring the average cosine relative distance between them. This measure, derived from Syed and Sprout (2017), incorporates Normalised Pointwise Mutual Information (NPMI) and cosine similarity to provide a robust evaluation of topic quality.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#assessing-model-similarity",
    "href": "chapter_ai-nepi_016.html#assessing-model-similarity",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.5 Assessing Model Similarity",
    "text": "14.5 Assessing Model Similarity\n\n\n\nSlide 09\n\n\nThe Adjusted Rand Index provides a clear measure of similarity amongst the six topic models, where a value of zero signifies random clustering. Analysis of this metric reveals that the LDA model applied to titles is the most divergent, consistently yielding values below 0.20. For example, its correlation with the LDA full-text model is a mere 0.13.\nIn contrast, all other models demonstrate superior overall correspondence, with their Adjusted Rand Index values generally exceeding 0.20. The BERTopic models exhibit a particularly strong internal consistency, with inter-model values frequently surpassing 0.35; the similarity between BERTopic full-text and BERTopic abstract is 0.36, whilst that between BERTopic abstract and BERTopic title is 0.38.\nFurthermore, the BERTopic Abstract model emerges as a central configuration, correlating effectively with all other models except for the outlier LDA title model, and consistently achieving values above 0.30.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#lda-performance-across-granularities",
    "href": "chapter_ai-nepi_016.html#lda-performance-across-granularities",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.6 LDA Performance Across Granularities",
    "text": "14.6 LDA Performance Across Granularities\n\n\n\nSlide 10\n\n\nA more granular analysis of the LDA models offers detailed insights into their comparative performance. When comparing LDA Full-text with LDA Abstract, as depicted in Table A, the authors observed a generally good fit. This correspondence is evidenced by a high proportion of shared documents between topics, visually represented by a reddish diagonal in the organised data.\nHowever, this comparison also revealed specific structural changes. Three full-text topics disappeared entirely in the abstract model, whilst another three fragmented into multiple, more specific topics. Conversely, three entirely new topics emerged in the abstract model, and three others resulted from the merger of several full-text topics.\nIn stark contrast, the comparison between LDA Full-text and LDA Title, presented in Table B, demonstrated a poor fit. This significant disparity indicates an extensive reorganisation of topics, characterised by the disappearance of numerous full-text topics and the emergence of many new, unrelated title-based topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#bertopic-performance-across-granularities",
    "href": "chapter_ai-nepi_016.html#bertopic-performance-across-granularities",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.7 BERTopic Performance Across Granularities",
    "text": "14.7 BERTopic Performance Across Granularities\n\n\n\nSlide 11\n\n\nThe authors’ assessment of the BERTopic models reveals distinct levels of fit across different textual granularities. The comparison between LDA Full-text and BERTopic Full-text (Table C) showed an average overall fit, highlighting significant reorganisation: eight LDA topics disappeared, and six split. Conversely, five new BERTopic topics emerged, and one resulted from a merger. This analysis also identified four small classes and one notably large class, presenting a potential issue with class balance.\nIn Table D, the comparison between LDA Full-text and BERTopic Abstract indicated a relatively good overall fit. Here, four LDA topics disappeared and six split, whilst two new BERTopic topics appeared and four resulted from mergers. Crucially, the classes remained well-balanced.\nFinally, Table E, comparing LDA Full-text with BERTopic Title, showed an average fit. This configuration saw seven LDA topics disappear and one split, whilst seven new BERTopic topics emerged. Similar to the full-text model, this configuration also produced three small classes and one large class. Across all BERTopic models, the top-words assessment consistently indicated well-formed topics, with the A-Radiation spore topic demonstrating remarkable robustness.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#lda-top-word-analysis",
    "href": "chapter_ai-nepi_016.html#lda-top-word-analysis",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.8 LDA Top-Word Analysis",
    "text": "14.8 LDA Top-Word Analysis\n\n\n\nSlide 12\n\n\nA detailed assessment of top words across the LDA Full-Text, LDA Abstract, and LDA Title models reveals distinct patterns of topic correspondence and evolution. Certain topics, such as A-Radiation-spore and A-Life-civilization, demonstrate robust consistency across all LDA models. They retain core top words like ‘radiation’, ‘spore’, and ‘space’ for the former, and ‘life’, ‘civilization’, and ‘universe’ for the latter.\nConversely, other topics exhibit splitting behaviour. The B-Chemistry topic, initially characterised by terms such as ‘reaction’, ‘product’, and ‘synthesis’ in the full-text model, fragments across the abstract and title models, yielding distinct sets of top words in each. The authors note that the fragmentation within the LDA title model is particularly challenging to interpret without further analysis.\nFurthermore, the team observed instances of topic merger. The B-Amino-acid and B-Protein-gene-RNA topics, distinct in the full-text model, coalesce into new, more generalised topics within both the abstract and title models. This merger, encompassing terms like ‘protein’, ‘code’, and ‘genetic’, forms a logically coherent, broader thematic category.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#bertopic-top-word-analysis",
    "href": "chapter_ai-nepi_016.html#bertopic-top-word-analysis",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.9 BERTopic Top-Word Analysis",
    "text": "14.9 BERTopic Top-Word Analysis\n\n\n\nSlide 13\n\n\nFurther assessment of top words across the three BERTopic models consistently revealed well-formed topics. The A-Radiation spore topic, for instance, demonstrated remarkable robustness across all models, maintaining its core thematic focus.\nWhilst the A-Life civilization topic also remained comparatively stable, it exhibited some splitting behaviour. This fragmentation led to the formation of narrower, more specific topics centred on extraterrestrial life, incorporating terms such as ‘fermi’, ‘drake’, and ‘seti’. Similarly, the splitting of the B-Chemistry topic also resulted in more granular themes, featuring terms like ‘peptide’, ‘amino’, and ‘montmorillonite’.\nConversely, the B-Amino-acid and B-Protein-gene-RNA topics merged into new, broader categories within the BERTopic models. These consolidated topics encompass terms such as ‘genetic’, ‘code’, and ‘protein’, creating more generalised thematic groupings.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#performance-by-coherence",
    "href": "chapter_ai-nepi_016.html#performance-by-coherence",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.10 Performance by Coherence",
    "text": "14.10 Performance by Coherence\n\n\n\nSlide 14\n\n\nLareau and Malaterre evaluated the performance of all six models across a range of 5 to 50 topics, focusing first on the Coherence CV metric, which assesses the meaningfulness of a topic’s top words. Their analysis revealed that titles consistently yielded the poorest coherence, with scores generally falling below 0.4.\nConversely, abstract-based models demonstrated superior coherence compared to full-text models, often achieving scores above 0.6. Overall, BERTopic models exhibited better coherence than LDA for both abstracts and titles, with BERTopic Abstract consistently outperforming LDA Abstract. This performance gap, however, tended to diminish as the number of topics increased.\nUltimately, BERTopic Abstract emerged as the clear winner in terms of coherence for its category. Whilst the full-text models, BERTopic Fulltext and LDA Fulltext, generally displayed the highest absolute coherence scores—typically exceeding 0.7—the performance of BERTopic Abstract was exceptional for a model based on summary text.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#performance-by-diversity",
    "href": "chapter_ai-nepi_016.html#performance-by-diversity",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.11 Performance by Diversity",
    "text": "14.11 Performance by Diversity\n\n\n\nSlide 15\n\n\nThe diversity metric, which quantifies the proportion of distinct top words representing the topics, also underwent rigorous evaluation. The authors observed a general trend where diversity tended to decrease as the number of topics increased.\nNotably, models utilising titles consistently offered the best diversity. Furthermore, BERTopic models demonstrated superior diversity compared to LDA across all text types. Ultimately, BERTopic Title emerged as the winner in terms of diversity, closely followed by BERTopic Full-text. In contrast, the full-text models generally yielded the lowest diversity scores.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#performance-by-joint-recall",
    "href": "chapter_ai-nepi_016.html#performance-by-joint-recall",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.12 Performance by Joint Recall",
    "text": "14.12 Performance by Joint Recall\n\n\n\nSlide 16\n\n\nJoint recall, a metric assessing how effectively top words represent the documents within each topic, provided further insights. The analysis revealed that models utilising titles consistently exhibited the poorest recall. BERTopic Title generally fell below 0.5, and LDA Title remained below 0.6.\nConversely, full-text models significantly outperformed their abstract and title counterparts. BERTopic Fulltext consistently achieved recall values above 0.9, whilst LDA Fulltext approached a perfect 1.0, particularly as the number of topics exceeded 20.\nOverall, LDA demonstrated superior joint recall to BERTopic across all textual granularities. LDA Fulltext emerged as the clear winner, with BERTopic Fulltext also showing commendably high recall. The BERTopic Abstract model performed well, achieving values between 0.7 and 0.8.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#overall-performance-and-recommendations",
    "href": "chapter_ai-nepi_016.html#overall-performance-and-recommendations",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.13 Overall Performance and Recommendations",
    "text": "14.13 Overall Performance and Recommendations\n\n\n\nSlide 17\n\n\nIn summarising the overall model performance, Lareau and Malaterre emphasise that no single model represents an absolute optimum. Instead, the most suitable choice depends entirely on the specific research objectives.\nFor instance, if the primary goal is to discover main topics without requiring precise document classification, then BERTopic on full texts performs commendably, despite some class imbalance. Similarly, whilst BERTopic on titles proved suboptimal in overall metrics, it nonetheless generated robust topics that also appeared in other, better-performing models.\nThe authors strongly advise against using LDA on titles, given its consistently poor performance across nearly every assessment. They recommend conducting topic modelling on either abstracts or full texts, utilising both LDA and BERTopic. This dual approach allows for a cross-validation of findings, provided the application does not result in the misclassification of documents pertinent to specific topics. The LDA Abstract and BERTopic Abstract models consistently achieved a strong balance of high overall fit, top-word quality, coherence, diversity, and joint recall.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#key-findings-and-future-directions",
    "href": "chapter_ai-nepi_016.html#key-findings-and-future-directions",
    "title": "14  A Comparative Study of LDA and BERTopic Performance Across Text Levels",
    "section": "14.14 Key Findings and Future Directions",
    "text": "14.14 Key Findings and Future Directions\n\n\n\nSlide 18\n\n\nThis research yielded several key findings regarding topic modelling performance across different text levels. Firstly, title-based models consistently demonstrated poor performance. The inherent lack of information in titles can lead to the false classification of documents, although the authors note that the BERTopic title model still produced some meaningful topics. This suggests a need to balance well-defined topics with comprehensive document coverage.\nSecondly, full-text models presented their own challenges. LDA models, for instance, could produce loosely defined topics with overly broad coverage, whilst BERTopic models sometimes generated overly narrow topics, resulting in poor document coverage and class size imbalances. Thirdly, abstract-based models consistently performed well, exhibiting results that were consistent with the LDA full-text model.\nA fourth finding was the notable robustness of certain topics across all models, which facilitates meta-analytic methods for identifying the most stable themes. This consistency also opens the possibility of using relative distance between models to pinpoint an optimal configuration. In this study, BERTopic Abstract emerged as the strongest candidate.\nFinally, these findings prompt a crucial question about the future of topic modelling: is it time for new models? Lareau and Malaterre contend that it is, highlighting the potential to leverage structural information from full texts, abstracts, and titles simultaneously to extract even more meaningful sets of topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>A Comparative Study of LDA and BERTopic Performance Across Text Levels</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "",
    "text": "Overview\nThis presentation introduces a novel architectural approach for Large Language Models (LLMs), which the speaker terms the Time Transformer. The innovation directly addresses a fundamental limitation of current models, which derive only an implicit understanding of time from statistical patterns within their training data. The speaker highlights that whilst existing models demonstrate remarkable capabilities, their lack of explicit temporal conditioning can lead to inconsistencies when processing information that evolves, such as historical facts or linguistic trends.\nThe proposed Time Transformer integrates a dedicated temporal dimension directly into the token embeddings, thereby enabling the model to learn and reproduce changing linguistic patterns as a direct function of time. The authors validated this concept using a small generative LLM trained on a highly constrained dataset of Met Office weather reports. Their work demonstrates the model’s ability to capture and reproduce time-dependent linguistic shifts with high efficiency. The presentation explores the theoretical underpinnings of this approach, details the model architecture and data preparation, and presents two experiments demonstrating its efficacy in learning synthetic temporal drifts. Furthermore, it outlines potential applications, including historical analysis and instruction-tuned models, whilst acknowledging challenges related to fine-tuning and data curation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#temporal-limitations-in-current-llms",
    "href": "chapter_ai-nepi_017.html#temporal-limitations-in-current-llms",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.1 Temporal Limitations in Current LLMs",
    "text": "15.1 Temporal Limitations in Current LLMs\n\n\n\nSlide 02\n\n\nCurrent Large Language Models possess an inherently implicit understanding of time, derived statistically from the vast textual corpora used for their training. Whilst these models exhibit a profound grasp of temporal concepts, their comprehension stems from subtle cues embedded within the data rather than from explicit temporal conditioning. The authors contend that explicit time awareness would demonstrably enhance their utility, particularly within historical analysis and across a broader spectrum of applications.\nConsider, for instance, two sentences that differ solely in their temporal context: ‘The primary architecture for processing text through Neural Networks is LSTM’ and ‘The primary architecture for processing text through Neural Networks is Transformer.’ Without explicit temporal information, these statements, representing different states of affairs in 2017 and 2025 respectively, directly contradict one another within an LLM’s training data. The model must then arbitrarily favour one, inevitably making an error regarding the other.\nFurthermore, a discernible recency bias often influences LLM predictions, favouring more contemporary information. Current methods, such as prompt engineering, merely attempt to exploit the model’s implicit temporal understanding, a process the speaker likens to ‘fishing in the dark’ for desired outcomes. To overcome these limitations, the authors propose integrating time directly into the token embeddings of Transformer-based LLMs. This architectural modification aims to render LLMs explicitly time-aware, enabling them to learn and reproduce evolving linguistic patterns as a direct function of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "href": "chapter_ai-nepi_017.html#formalising-temporal-dependence",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.2 Formalising Temporal Dependence",
    "text": "15.2 Formalising Temporal Dependence\n\n\n\nSlide 03\n\n\nFundamentally, Large Language Models operate by estimating the probability distribution over their vocabulary for the next token, conditioned on a sequence of preceding tokens. This process is mathematically represented as p(x_n | x_1, …, x_{n-1}). In the real world, however, the likelihood of a token appearing is not static; it is intrinsically dependent on time, thus becoming p(x_n | x_1, …, x_{n-1}, t).\nExtending this principle, the joint probability for an entire sequence of tokens uttered at a particular time t is expressed as the product of these conditional probabilities: p(x_1, …, x_n | t) = ∏ p(x_k | x_1, …, x_{k-1}, t). Despite this inherent temporal variability, current LLM training processes frequently treat these probability distributions as static. Consequently, during inference, these models can only reflect temporal drift through in-context learning, a mechanism that relies on the immediate context provided rather than an explicit, integrated understanding of time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#modelling-temporal-drift",
    "href": "chapter_ai-nepi_017.html#modelling-temporal-drift",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.3 Modelling Temporal Drift",
    "text": "15.3 Modelling Temporal Drift\n\n\n\nSlide 04\n\n\nA significant challenge in current LLM training lies in the treatment of inherently time-dependent probability distributions as static. This simplification means that whilst the real-world likelihood of a token is a direct function of time—for instance, the probability of ‘transformer’ completing a sentence was effectively zero in 2017—LLMs primarily reflect such temporal drift only through in-context learning during inference.\nTo improve upon this, the authors sought more effective methods for modelling these dynamic, time-dependent probability distributions. Existing strategies, such as ‘time slicing’—where distinct models are trained for specific temporal segments—prove remarkably data-inefficient, as they assume static distributions within those slices. A more streamlined and integrated approach is therefore imperative.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-time-transformer-concept",
    "href": "chapter_ai-nepi_017.html#the-time-transformer-concept",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.4 The Time Transformer Concept",
    "text": "15.4 The Time Transformer Concept\n\n\n\nSlide 05\n\n\nTo overcome the limitations of implicit temporal understanding, the authors propose an innovative solution termed the Time Transformer. This concept centres on a remarkably simple yet profound architectural adjustment: reserving a single dimension within the token embedding space specifically for time. This dedicated dimension explicitly conveys the utterance date for each token sequence, thereby providing direct temporal context.\nThe initial implementation employs a non-trainable, min-max normalised ‘day of the year’ as the time embedding. The team strategically chose this feature to exploit natural seasonal variations inherent in their chosen dataset, such as the prevalence of snow in winter or heat in summer. The framework, however, readily accommodates alternative time embeddings as required.\nFor their proof of concept, the authors selected Met Office weather reports as the primary dataset. This text corpus is characterised by its limited vocabulary and simple, repetitive language, making it an ideal candidate for initial validation. The UK’s national meteorological service issues these daily reports, and historical data remains accessible through its digital archive.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-curation-and-processing",
    "href": "chapter_ai-nepi_017.html#dataset-curation-and-processing",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.5 Dataset Curation and Processing",
    "text": "15.5 Dataset Curation and Processing\n\n\n\nSlide 06\n\n\nThe team systematically acquired the dataset by scraping daily weather reports from Met Office PDFs spanning the years 2018 to 2024. This process yielded approximately 2,500 reports, each comprising between 150 and 200 words. For text processing, they employed tf.keras.layers.TextVectorization, standardising the input by converting text to lowercase and stripping punctuation.\nCrucially, the tokenization process avoided sub-word segmentation and deliberately neglected case and interpunctuation, reflecting the inherently simple nature of the language. This straightforward approach resulted in a remarkably concise vocabulary of just 3,395 unique words across the entire seven-year corpus. An illustrative example, the Daily Weather Summary for Sunday 04 August 2019, details showery rain and mist, whilst its Daily Extremes table highlights a highest maximum temperature of 27.5°C recorded in Writtle, Essex.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#baseline-model-architecture",
    "href": "chapter_ai-nepi_017.html#baseline-model-architecture",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.6 Baseline Model Architecture",
    "text": "15.6 Baseline Model Architecture\n\n\n\nSlide 07\n\n\nTo establish a baseline for language pattern learning, the authors constructed a modest-sized, decoder-only Transformer architecture, which they term the Vanilla model. This architecture processes input through an Embedding Layer (d_model=512), followed by Positional Encoding and a Dropout layer (rate=0.1). Subsequently, the input traverses a stack of four Multi-Head Attention Decoder Blocks. The final output from these layers feeds into a Dense Layer, sized to the vocabulary, which produces the model’s predictions.\nThis compact model contains 39 million parameters, a stark contrast to models such as GPT-4, which commands 1.8 trillion parameters. Despite its modest scale, the model trains with remarkable efficiency on an HPC cluster, completing each epoch in merely 11 seconds. During training, its accuracy steadily improved, with validation accuracy stabilising around 0.38, demonstrating its capacity to reproduce the language patterns observed in the weather reports.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#integrating-the-temporal-dimension",
    "href": "chapter_ai-nepi_017.html#integrating-the-temporal-dimension",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.7 Integrating the Temporal Dimension",
    "text": "15.7 Integrating the Temporal Dimension\n\n\n\nSlide 08\n\n\nThe established Vanilla model demonstrates a robust capacity to reproduce the language of weather reports. When provided with a seed sequence such as ‘During the night, a band…’, the model autoregressively generates coherent and contextually relevant text that closely mimics actual forecasts.\nThe transition to a Time Transformer involves a remarkably minimal architectural adjustment. Instead of embedding all information within a 512-dimensional latent semantic space, the authors reserve one dimension specifically for temporal data. This dedicated dimension explicitly informs each token about the precise date on which its sequence was uttered. The current implementation employs a non-trainable, min-max normalised ‘day of the year’ as this time embedding, a choice driven by the desire to leverage natural seasonal variations inherent in weather data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#dataset-suitability-for-time-aware-models",
    "href": "chapter_ai-nepi_017.html#dataset-suitability-for-time-aware-models",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.8 Dataset Suitability for Time-Aware Models",
    "text": "15.8 Dataset Suitability for Time-Aware Models\n\n\n\nSlide 09\n\n\nTo test their proposed time-aware model rigorously, the authors required a dataset characterised by restricted, repetitive language and a small vocabulary, thereby simplifying the task of pattern learning. The UK Met Office weather reports proved an ideal choice, being readily accessible online from the national meteorological service. The team also identified the TinyStories dataset as a potential alternative for future investigations.\nThe data, originally presented as monthly PDFs, was systematically scraped for the period spanning 2018 to 2024, yielding approximately 2,500 reports. The intentionally simple tokenization process, which eschewed sub-word segmentation and neglected case, resulted in a compact vocabulary of just 3,400 words. This linguistic simplicity makes the dataset an excellent testbed for isolating and analysing the model’s ability to learn temporal dependencies.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#a-comparative-architecture",
    "href": "chapter_ai-nepi_017.html#a-comparative-architecture",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.9 A Comparative Architecture",
    "text": "15.9 A Comparative Architecture\n\n\n\nSlide 10\n\n\nThe Time Transformer represents a minimal yet impactful architectural adjustment to the standard Transformer decoder. In a conventional Vanilla Transformer, the input flows directly into an Embedding Layer (d_model=512), followed by Positional Encoding, Dropout, and a series of Decoder Layers before the final output.\nConversely, the Time Transformer introduces two distinct inputs: textual data and temporal data. The textual input undergoes embedding into a d_model of 511, whilst the temporal data is embedded into a dedicated d_model of 1. These two embedded streams are then concatenated, maintaining the overall embedding dimension, before proceeding through the identical sequence of Positional Encoding, Dropout, and Decoder Layers. The time dimension itself is implemented as a non-trainable, min-max normalised ‘day of the year’. This explicit integration directly addresses the challenge that real-world token likelihoods are inherently time-dependent, a dynamic often overlooked by conventional training processes.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-1-synonymic-succession",
    "href": "chapter_ai-nepi_017.html#experiment-1-synonymic-succession",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.10 Experiment 1: Synonymic Succession",
    "text": "15.10 Experiment 1: Synonymic Succession\n\n\n\nSlide 11\n\n\nTo assess the Time Transformer’s capacity for learning temporal drift efficiently, the authors conducted an experiment they term synonymic succession. This involved injecting a synthetic, time-dependent drift directly into the training data: the word ‘rain’ was progressively replaced by ‘liquid sunshine’ throughout the year. The objective was to ascertain whether the model could reproduce this engineered temporal dependence in its predictions.\nThe probability of this replacement followed an S-shaped curve, commencing near zero in January and ascending to approach 1.00 by the year’s end. Analysis of monthly occurrences in the generated sequences clearly demonstrated the model’s successful capture of this drift. Occurrences of ‘Rain’ predominated in the earlier months, whilst ‘Liquidsunshine’ became significantly more frequent and eventually dominant in the latter half of the year.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experiment-2-collocation-fixation",
    "href": "chapter_ai-nepi_017.html#experiment-2-collocation-fixation",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.11 Experiment 2: Collocation Fixation",
    "text": "15.11 Experiment 2: Collocation Fixation\n\n\n\nSlide 13\n\n\nA second experiment, described as ‘changing a weather pattern’ or fixation of a collocation, further investigated the model’s ability to capture temporal shifts. This involved injecting a synthetic change in co-occurrence: the pattern ‘rain’ followed by any word other than ‘and’ was progressively altered to ‘rain and snow’. Linguistically, this process simulates the fixation of a collocation, akin to an established phrase such as ‘bread and butter’.\nThe results, visualised through monthly comparisons, clearly demonstrated the injected temporal shift. Occurrences of ‘Rain Only’ were notably higher in the first half of the year, whilst ‘Rain and Snow’ became significantly more frequent in the latter half. Furthermore, an analysis of attention weights revealed that the token ‘snow’ consistently exhibited the highest attention on ‘rain’, indicating that the model successfully learned this evolving co-occurrence pattern.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#conclusions-and-future-directions",
    "href": "chapter_ai-nepi_017.html#conclusions-and-future-directions",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.12 Conclusions and Future Directions",
    "text": "15.12 Conclusions and Future Directions\n\n\n\nSlide 14\n\n\nThe conducted research establishes a clear proof of concept: Transformer-based models can be efficiently rendered time-aware by integrating a dedicated temporal dimension within their token embeddings. This innovation opens several compelling applications. A foundational Time Transformer, for instance, could provide an exceptional basis for downstream tasks involving historical data. Moreover, an instruction-tuned variant would empower users to ‘talk to a specific time’, potentially yielding superior results even in common usage.\nBeyond temporal dynamics, the methodology readily extends to modelling dependencies on other contextual dimensions, such as country or genre. Future research endeavours include benchmarking this approach against explicit time-token methods and testing for potential increases in training efficiency. Nevertheless, challenges persist. Uncertainty surrounds the feasibility of fine-tuning due to the architectural modifications. Furthermore, the approach necessitates a departure from metadata-free self-supervised learning, plunging the team into the complexities of data curation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#supplementary-resource",
    "href": "chapter_ai-nepi_017.html#supplementary-resource",
    "title": "15  Time-aware large language models towards a novel architecture for historical analysis",
    "section": "15.13 Supplementary Resource",
    "text": "15.13 Supplementary Resource\n\n\n\nSlide 16\n\n\nA supplementary resource, specifically a ChatGPT conversation, is available for further exploration at the following URL: https://chatgpt.com/c/67b8237a-2a48-8012-9862-80af84830a17.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-aware large language models towards a novel architecture for historical analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers within the Cascade project, a Marie Curie doctoral network, meticulously explore the computational analysis of semantic change. PhD student Sophia Aguilar leads this investigation, focusing on modelling diverse contextual factors and their intricate interplay. Building upon previous work that modelled distinct context types in isolation, the current objective is to integrate these approaches, thereby illuminating their complex interactions.\nThe chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a pivotal pilot study. Linguists engaged in this endeavour examine how language adapts to real-world transformations, drawing upon register theory and principles of rational communication. The study aims to detect periods of significant linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately comprehend the linguistic mechanisms and communicative drivers underpinning these transformations. To this end, the authors propose a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics, positioning context as a central signal and seeking to overcome limitations of existing methods in capturing the interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot\nWithin the Cascade project, a Marie Curie doctoral network, researchers meticulously investigate the computational analysis of semantic change. PhD student Sophia Aguilar spearheads efforts to model context comprehensively, examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing upon the Royal Society Corpus (RSC). This historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors:\n\nSituational (where)\nTemporal (when)\nExperiential (what)\nInterpersonal (who)\nTextual (how)\nCausal (why)\n\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal higher divergence due to substantial linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time—for example, from 1725 to 1845—reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models\nBeyond temporal detection, the investigation delves into paradigmatic context and the dynamics of conceptual change, referencing work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand who spearheads and propagates these linguistic and conceptual shifts, Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, the research team examines shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, visualised through graphs plotting surprisal against year, supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community using that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. Aguilar’s proposed framework positions context as a central signal for modelling language dynamics, identifying Graph Convolutional Networks (GCNs) as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four stages:\n\nData Sampling: This stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: The process begins by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the authors refine network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research acknowledges several profound questions that delineate its current limitations and chart future directions. A primary concern involves the nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview\nThe research team investigates the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the investigators reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]