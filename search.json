[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-NEPI Conference Proceedings",
    "section": "",
    "text": "1 Preface\nThis enhanced edition of the AI-NEPI Conference Proceedings contains presentations on Large Language Models for History, Philosophy and Sociology of Science, held April 2-4, 2025.\nThe chapters in this book have been generated from comprehensive XML content reports, providing structured access to the rich discussions and insights shared during the conference.\nEach chapter includes:\n\nStructured presentation content with key sections\nSlide images synchronized with the presentation flow\n\nComplete speaker abstracts and overviews\nDetailed transcriptions of the presentations\n\nThis enhanced format allows readers to follow both the visual presentation materials and the detailed content in an integrated manner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI-NEPI Conference Proceedings - Enhanced Edition</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html",
    "href": "chapter_ai-nepi_001.html",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "",
    "text": "Overview\nThe workshop, “Large Language Models for the History, Philosophy and Sociology of Science,” convened to explore the application of advanced AI methods within historical, philosophical, and sociological inquiries into science. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collaboratively organised this distinguished event, which emerged from two distinct yet complementary initiatives.\nFirstly, the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932), provided a foundational interest in training large language models on physics texts and analysing conceptual issues within the discipline. Secondly, Gerd Graßhoff, a long-standing advocate for AI integration in the history and philosophy of science, particularly for understanding scientific discovery processes, proposed a workshop on novel AI-assisted methodologies. These converging interests consequently led to the joint organisation of the current event.\nThe NEPI project specifically investigates the internal communication dynamics of the ATLAS collaboration at CERN, aiming to elucidate how such a prominent, large-scale research collaboration collectively generates new knowledge. This research employs both network analysis to map communication structures and semantic tools, including Large Language Models, to trace the flow of ideas within these networks. The workshop attracted significant interest, receiving over 50 paper submissions, from which the organisers selected 16 for presentation. It quickly reached full capacity for in-person attendance and garnered a substantial online audience, totalling approximately 220 registered participants. The programme features keynotes from leading researchers: Pierluigi Cassotti and Nina Tahmasebi, who focus on large-scale text analysis for cultural and societal change, and Iryna Gurevych, who addresses the elevation of Natural Language Processing to the cross-document level. Logistical arrangements include structured question-and-answer sessions, an Etherpad for comments, a dedicated discussion session, and various networking opportunities. Recording protocols ensure the capture of presentations for future dissemination on the NEPI YouTube channel, subject to presenter consent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "href": "chapter_ai-nepi_001.html#workshop-genesis-and-scope",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.1 Workshop Genesis and Scope",
    "text": "2.1 Workshop Genesis and Scope\n\n\n\nSlide 02\n\n\nThe workshop, titled “Large Language Models for the History, Philosophy and Sociology of Science,” emerged from two distinct yet complementary initiatives. Adrian Wüthrich, Arno Simons, Michael Zichert, and Gerd Graßhoff collectively organised this event. One primary impetus stemmed from the Network Epistemology in Practice (NEPI) project, an ERC Consolidator Grant (Nr. 101044932). Within this project, Arno Simons pioneered the training of one of the earliest large language models specifically on physics texts, whilst Michael Zichert employed similar models to analyse conceptual issues prevalent in physics.\nConcurrently, Gerd Graßhoff, a long-standing collaborator and proponent of AI integration within the history and philosophy of science, particularly for analysing scientific discovery processes, conceived a workshop focused on novel AI-assisted methodologies. Consequently, these converging interests led to a joint endeavour, culminating in the present workshop. The NEPI project specifically investigates the internal communication of the ATLAS collaboration at CERN, the particle physics laboratory. Researchers aim to understand how one of the largest and most prominent research collaborations collectively generates new knowledge. This investigation employs network analysis to elucidate communication structures and semantic tools, including large language models, to trace the flow of ideas within these intricate networks.\nThe call for papers for this workshop garnered significant interest, attracting over 50 submissions, from which the organisers selected 16 for presentation. On-site attendance quickly reached full capacity, whilst a substantial online audience also registered, bringing the total participation to approximately 220 individuals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "href": "chapter_ai-nepi_001.html#recording-protocols-and-consent",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.2 Recording Protocols and Consent",
    "text": "2.2 Recording Protocols and Consent\n\n\n\nSlide 03\n\n\nThe workshop sessions are currently undergoing recording. Attendees provided their consent for this during the registration process. A single camera captures the proceedings, specifically directed towards the presenter. Audio recording relies on four microphones, supplemented by an iPhone serving as a backup audio recorder. Subject to the presenters’ explicit consent, the videos of the talks, encompassing the subsequent discussion, will be uploaded to the NEPI YouTube Channel following the workshop. Crucially, the discussion segments will feature only the audio and video of the presenter, ensuring the privacy of the audience. Individuals requiring further information or wishing to withdraw their consent should approach the organisers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "href": "chapter_ai-nepi_001.html#workshop-logistics-and-interaction-guidelines",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.3 Workshop Logistics and Interaction Guidelines",
    "text": "2.3 Workshop Logistics and Interaction Guidelines\n\n\n\nSlide 04\n\n\nGiven the large group size and the limited time allocated for presentations and subsequent questions, participants are requested to keep their questions and comments concise and pertinent. Following each presentation, the organisers will collect up to four questions or comments, enabling the presenter to respond to them collectively, thereby optimising time. An Etherpad provides dedicated sections for each talk, alongside a general section, allowing participants to place their comments appropriately. Furthermore, a dedicated discussion session on the second day will facilitate the pooling and collective discussion of frequently arising questions and comments.\nBeyond the formal sessions, the workshop offers ample opportunities for informal networking amongst researchers and fellows. These include generous lunch and coffee breaks, a modest reception, and a workshop dinner. Notably, seating for the dinner is highly limited, reserved exclusively for participants who received confirmation of their attendance. Coffee breaks and refreshments are available on-site. Lunch and the reception will take place in Room H 2051, located down the hall, one floor below the main workshop area.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "href": "chapter_ai-nepi_001.html#keynote-1-large-scale-text-analysis-for-cultural-and-societal-change",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change",
    "text": "2.4 Keynote 1: Large-scale Text Analysis for Cultural and Societal Change\n\n\n\nSlide 05\n\n\nThe first keynote address, scheduled shortly, features Pierluigi Cassotti and Nina Tahmasebi from the University of Gothenburg. Nina Tahmasebi directs the “Change is Key” research programme, whilst Pierluigi Cassotti contributes as a researcher within the project. Their work has gained considerable recognition for its focus on semantic change detection. This research encompasses both technical aspects, such as the development of benchmarks, and broader methodological considerations, including the application of data science methods to questions within the humanities. This dual focus renders their expertise particularly relevant to the workshop’s objectives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "href": "chapter_ai-nepi_001.html#keynote-2-elevating-nlp-to-the-cross-document-level",
    "title": "2  Large Language Models for the History, Philosophy and Sociology of Science (Workshop)",
    "section": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level",
    "text": "2.5 Keynote 2: Elevating NLP to the Cross-Document Level\n\n\n\nSlide 06\n\n\nIryna Gurevych will deliver the second keynote address tomorrow late afternoon. She leads the Ubiquitous Knowledge Processing (UKP) Lab at the Technical University Darmstadt. Her research primarily concentrates on information extraction, semantic text processing, and machine learning. Crucially, her work also extends to the application of Natural Language Processing (NLP) techniques within the social sciences and humanities, aligning perfectly with the interdisciplinary focus of this workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html",
    "href": "chapter_ai-nepi_003.html",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "",
    "text": "Overview\nThis chapter comprehensively explores the application of Large Language Models (LLMs) within the History, Philosophy, and Sociology of Science (HPSS) domain. It commences by providing a foundational primer on LLM architectures, specifically detailing the Transformer model and its derivatives, BERT and GPT. The discussion then transitions to summarising current applications of LLMs in HPSS research, categorising them into data handling, knowledge structuring, knowledge dynamics, and knowledge practices.\nCrucially, the authors identify key distinctions in LLM types and adaptation strategies, including pre-training, fine-tuning, and Retrieval-Augmented Generation (RAG) pipelines. The chapter highlights the accelerating interest in LLMs across academic journals, whilst acknowledging persistent concerns regarding computational resources, model opaqueness, data scarcity, and the absence of standardised benchmarks.\nThe authors conclude by offering critical reflections on HPSS-specific challenges, such as the historical evolution of concepts and sparse, multilingual data. They advocate for enhanced LLM literacy amongst researchers and a steadfast adherence to HPSS methodologies to prevent technological tools from dictating research objectives. Ultimately, this chapter underscores the potential of LLMs to bridge qualitative and quantitative research approaches within the HPSS field, fostering new avenues for interdisciplinary inquiry.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#introduction-to-large-language-models-and-hpss-applications",
    "href": "chapter_ai-nepi_003.html#introduction-to-large-language-models-and-hpss-applications",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.1 Introduction to Large Language Models and HPSS Applications",
    "text": "3.1 Introduction to Large Language Models and HPSS Applications\nThis chapter establishes a foundational understanding of Large Language Models (LLMs) and their specific applications within the History, Philosophy, and Sociology of Science (HPSS). Primarily, it aims to furnish a concise primer on LLMs, detailing their adaptation to diverse scientific domains. Subsequently, the discussion summarises current LLM applications pertinent to HPSS research. Finally, the authors offer critical reflections, intended to stimulate further discussion throughout the workshop. Given the varied technical backgrounds of the audience, the initial segment focuses on accessible explanations of complex LLM concepts.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#the-transformer-architecture-foundation-of-modern-llms",
    "href": "chapter_ai-nepi_003.html#the-transformer-architecture-foundation-of-modern-llms",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.2 The Transformer Architecture: Foundation of Modern LLMs",
    "text": "3.2 The Transformer Architecture: Foundation of Modern LLMs\nThe foundational architecture underpinning contemporary Large Language Models is the Transformer. Vaswani and colleagues originally introduced this model in their seminal 2017 paper, “Attention is all you need” (vaswani2017attention?). Initially conceived for language translation tasks, such as converting German to English, this architecture comprises two interconnected streams: an encoder and a decoder.\nThe encoder, positioned on the left, processes the entire input sentence simultaneously. Crucially, it enables each word within the input to interact with every other word, thereby constructing a comprehensive contextual representation of the complete sentence. Words initially feed into this component and subsequently encode into numerical representations.\nConversely, the decoder, located on the right, generates the output sentence. This component operates sequentially, predicting each subsequent word based solely on its predecessors; it cannot access future words. The numerical representations from the encoder transmit to the decoder, which then transforms them back into words. Each generated word subsequently feeds back into the decoder’s input, forming a loop that continues until the complete sentence is produced. Within both the encoder and decoder, various layers refine contextualised word embeddings, progressively enhancing their contextual understanding.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#encoder-based-models-bert-for-bidirectional-contextual-understanding",
    "href": "chapter_ai-nepi_003.html#encoder-based-models-bert-for-bidirectional-contextual-understanding",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.3 Encoder-Based Models: BERT for Bidirectional Contextual Understanding",
    "text": "3.3 Encoder-Based Models: BERT for Bidirectional Contextual Understanding\n\n\n\nSlide 03\n\n\nFollowing the introduction of the Transformer model, Devlin and colleagues promptly began re-engineering its individual streams to develop pre-trained language models (devlin2018bert?). These models, rather than focusing solely on translation, excel at understanding and generating language, making them highly adaptable for various Natural Language Processing (NLP) tasks with minimal additional training.\nA prominent example of such a model is BERT, or Bidirectional Encoder Representations from Transformers. BERT specifically leverages the encoder side of the Transformer architecture. Its defining characteristic is its capacity for bidirectional, full-context understanding; every word within the input stream can interact with all other words, thereby constructing a comprehensive contextual representation of the entire input at once. During its pre-training phase, BERT learns by predicting randomly masked words within a text. Consequently, BERT-like models are primarily designed for coherent sentence understanding, distinguishing them from models focused on generating novel text. Despite newer developments, the BERT family of models maintains a significant presence and utility in the field.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#decoder-based-models-gpt-for-unidirectional-generative-capabilities",
    "href": "chapter_ai-nepi_003.html#decoder-based-models-gpt-for-unidirectional-generative-capabilities",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.4 Decoder-Based Models: GPT for Unidirectional Generative Capabilities",
    "text": "3.4 Decoder-Based Models: GPT for Unidirectional Generative Capabilities\n\n\n\nSlide 04\n\n\nConversely, the GPT models, standing for Generative Pre-trained Transformers, derive from the decoder side of the Transformer architecture, as detailed by Radford and colleagues in 2018 (radford2018improving?). These models possess a unidirectional generative capability, meaning they can only consider preceding words when generating new text. This structural difference enables GPT models to produce novel language, a feature that powers widely used applications such as ChatGPT.\nNotably, GPT models fundamentally differ from BERT models: whilst GPT excels at generating new words, BERT primarily focuses on understanding the full context of an input. Beyond these two primary types, the field also features hybrid architectures that combine encoders and decoders, alongside decoder-only models engineered to emulate encoder-like functionalities, such as XLNet and XLM. Understanding this core distinction between generative models, which produce language, and full-context models, which coherently understand sentences, remains paramount for effective application.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-llms",
    "href": "chapter_ai-nepi_003.html#evolution-and-specialisation-of-scientific-llms",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.5 Evolution and Specialisation of Scientific LLMs",
    "text": "3.5 Evolution and Specialisation of Scientific LLMs\n\n\n\nSlide 06\n\n\nThe landscape of Large Language Models has undergone rapid and diverse evolution between 2018 and 2024, particularly within scientific domains. Researchers categorise these models primarily by their underlying architecture: encoders, such as the BERT-type models; decoders, exemplified by GPT-type models; and hybrid encoder-decoder configurations, alongside other distinct architectures.\nEarly popular examples of domain-specific models include BioBERT, Specter, and Cyber. Subsequently, the field has witnessed the development of specialised LLMs tailored for a wide array of scientific disciplines, encompassing biomedicine, chemistry, material science, climate science, mathematics, physics, and social science. For researchers in the History, Philosophy, and Sociology of Science, this proliferation signifies a substantial opportunity either to leverage existing domain-specific models or to develop bespoke solutions. Analysis of the current landscape indicates a greater prevalence of encoder models in scientific applications compared to decoder models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-llms",
    "href": "chapter_ai-nepi_003.html#strategies-for-domain-and-task-adaptation-of-llms",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.6 Strategies for Domain and Task Adaptation of LLMs",
    "text": "3.6 Strategies for Domain and Task Adaptation of LLMs\n\n\n\nSlide 07\n\n\nAdapting Large Language Models to specific scientific languages and tasks involves several distinct strategies. The most fundamental approach is pre-training, wherein a model initially learns language by predicting either the next token, as seen in GPT models, or random masked words, characteristic of BERT models. However, this process demands prohibitive computational resources and vast datasets, rendering it generally unfeasible for individual researchers.\nA more accessible alternative is continued pre-training. This involves taking an already pre-trained model and further training it on domain-specific language; for instance, the authors’ own research has refined a BERT model using physics texts. Beyond this, researchers can adapt models by adding extra parameters or layers atop pre-trained architectures, which then train for specific downstream tasks such as sentiment analysis or named entity recognition.\nWhilst prompt-based adaptation offers another avenue for guiding model behaviour without extensive retraining, contrastive learning represents a crucial method for generating sentence or document embeddings. This technique, often leveraging BERT models, produces embeddings where semantically similar inputs position closely in the embedding space, whilst dissimilar inputs keep distant. Sentence BERT, for example, is a widely adopted approach that facilitates the creation of comprehensive sentence and document embeddings from initial word embeddings.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-pipelines",
    "href": "chapter_ai-nepi_003.html#retrieval-augmented-generation-rag-pipelines",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.7 Retrieval-Augmented Generation (RAG) Pipelines",
    "text": "3.7 Retrieval-Augmented Generation (RAG) Pipelines\n\n\n\nSlide 09\n\n\nRetrieval-Augmented Generation (RAG) constitutes a sophisticated pipeline system rather than a singular Large Language Model. This architecture typically integrates at least two or more models that operate in concert. Specifically, BERT-type models are employed to assess the semantic similarity between a user’s query and a corpus of documents. Concurrently, generative models are tasked with formulating responses, drawing upon the context retrieved by the BERT-type components.\nThe RAG workflow unfolds systematically: a user’s query, such as “What are LLMs?”, first encodes into a sentence embedding by a BERT-type model. This model then queries a database of relevant documents, identifying and retrieving passages that exhibit the highest semantic similarity. Subsequently, these retrieved passages seamlessly integrate into the prompt provided to a generative model. Armed with this augmented context, the generative model then synthesises a comprehensive answer. This sophisticated pipeline is now a ubiquitous feature in contemporary LLM applications, exemplified by ChatGPT’s capacity to search the internet and present relevant results. More broadly, advanced reasoning models and agents represent complex systems that combine LLMs with a diverse array of other computational tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#key-distinctions-in-large-language-model-paradigms",
    "href": "chapter_ai-nepi_003.html#key-distinctions-in-large-language-model-paradigms",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.8 Key Distinctions in Large Language Model Paradigms",
    "text": "3.8 Key Distinctions in Large Language Model Paradigms\n\n\n\nSlide 10\n\n\nTo navigate the complex landscape of Large Language Models effectively, several key distinctions warrant careful consideration.\n\nFirstly, models fundamentally differ in their underlying architecture, broadly categorised as encoder-based, decoder-based, or hybrid encoder-decoder configurations.\nSecondly, a diverse array of fine-tuning strategies exists, each designed to adapt pre-trained models for specific tasks.\nThirdly, a critical conceptual difference lies between word embeddings and sentence embeddings, as these represent distinct levels of semantic representation.\nFinally, it is imperative to recognise the varying levels of abstraction at which LLMs operate: they can function as individual models, as integral components within larger pipelines such as RAG systems, or as sophisticated agents that combine LLMs with a multitude of other computational tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#applications-of-llms-in-history-philosophy-and-sociology-of-science",
    "href": "chapter_ai-nepi_003.html#applications-of-llms-in-history-philosophy-and-sociology-of-science",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.9 Applications of LLMs in History, Philosophy, and Sociology of Science",
    "text": "3.9 Applications of LLMs in History, Philosophy, and Sociology of Science\n\n\n\nSlide 11\n\n\nA recent survey of Large Language Model users within History, Philosophy, and Sociology of Science (HPSS) research has delineated four primary categories of application.\n\nDealing with data and sources: LLMs prove invaluable for facilitating the parsing and extraction of diverse information, including publication types, acknowledgements, and citations. Moreover, they enable more dynamic interaction with sources through summarisation and RAG-type conversational interfaces.\nAnalysis of knowledge structures: LLMs contribute significantly to this area. This encompasses the extraction of specific entities, such as scientific instruments, celestial bodies, or chemicals, alongside the mapping of complex relationships, including disciplinary boundaries, interdisciplinary fields, and science-policy discourses.\nInvestigation of knowledge dynamics: The models offer powerful tools for this purpose. This involves tracing conceptual histories—for instance, the evolution of terms like “theory” in Digital Humanities or “virtual” and “Planck” in physics. Furthermore, LLMs can assist in detecting novelty, identifying breakthrough papers, and pinpointing emerging technologies.\nEnhancement of knowledge practices: LLMs support argument reconstruction by identifying premises, conclusions, and causal links within texts. Citation context analysis, an established HPSS tradition often now confined to evaluatory purposes, gains renewed utility for broader HPSS tasks. Additionally, LLMs facilitate nuanced discourse analysis, enabling the detection of subtle linguistic features such as hedge sentences, specialised jargon, and instances of boundary work.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#observations-and-concerns-in-hpss-llm-adoption",
    "href": "chapter_ai-nepi_003.html#observations-and-concerns-in-hpss-llm-adoption",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.10 Observations and Concerns in HPSS LLM Adoption",
    "text": "3.10 Observations and Concerns in HPSS LLM Adoption\n\n\n\nSlide 12\n\n\nCurrent trends indicate an accelerating interest in Large Language Models within the academic community, extending even to journals not traditionally associated with computational methods, such as Scientometrics and JASIST. This growing adoption reflects the profound semantic capabilities of these models, which increasingly appeal to qualitative researchers and philosophers.\nThe degree of customisation in LLM application varies considerably, ranging from the straightforward, off-the-shelf use of tools like ChatGPT to the development of entirely new architectures, alongside custom pre-training and fine-tuning. Despite this burgeoning interest, several recurring concerns persist. Researchers frequently highlight the overwhelming computational resources required, the inherent opaqueness of many models, and a pervasive lack of both sufficient training data and standardised benchmarks.\nFurthermore, the necessity of navigating trade-offs between different model types, such as BERT-like and GPT-like architectures, underscores that no single model serves all purposes. Rather, the selection of an adequate model remains contingent upon the specific research objective. Nevertheless, a discernible trend towards greater accessibility is emerging, exemplified by tools like BERTopic for topic modelling, which are gaining widespread adoption due to their user-friendliness and robust maintenance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#reflections-on-hpss-specific-challenges-and-llm-integration",
    "href": "chapter_ai-nepi_003.html#reflections-on-hpss-specific-challenges-and-llm-integration",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.11 Reflections on HPSS-Specific Challenges and LLM Integration",
    "text": "3.11 Reflections on HPSS-Specific Challenges and LLM Integration\n\n\n\nSlide 13\n\n\nIntegrating Large Language Models into History, Philosophy, and Sociology of Science (HPSS) necessitates a careful consideration of several HPSS-specific challenges.\n\nFirstly, the historical evolution of concepts and language presents a significant hurdle. As LLMs predominantly train on modern language, they may exhibit biases when applied to historical texts. This necessitates either the development of bespoke models or the judicious application of existing ones, whilst acknowledging their inherent limitations.\nSecondly, HPSS adopts a reconstructive and critically reflective perspective, involving reading “between the lines” to understand authorial context and subtle discursive strategies, such as boundary work. Current LLMs do not inherently train for such nuanced interpretation, demanding innovative approaches to align model capabilities with HPSS methodologies.\nThirdly, practical data issues persist, including sparse datasets, the presence of multiple languages, old scripts, and a general lack of digitalisation.\n\nAddressing these challenges requires cultivating robust LLM literacy amongst researchers. This involves familiarising oneself with the tools and theoretical underpinnings of LLMs, Natural Language Processing (NLP), and Deep Learning (DL). Researchers must learn to identify the most appropriate architecture and training regimen for their specific problems, whilst collectively developing shared datasets and benchmarks tailored to HPSS needs. Although advancements in natural language processing may gradually reduce the need for extensive coding, a foundational understanding remains crucial to avoid merely generating aesthetically pleasing but analytically shallow visualisations.\nCrucially, HPSS researchers must remain steadfast in their methodological principles. This entails translating complex HPSS problems into NLP tasks without compromising the original research focus or purpose. Nevertheless, LLMs present unprecedented opportunities for bridging qualitative and quantitative approaches, fostering interdisciplinary collaboration. Furthermore, it is pertinent to reflect upon HPSS’s own historical contributions to the pre-history of these models, exemplified by the co-word analysis developed by scholars like Colon and Ari Rip in the 1980s, which emerged from an Actor-Network Theory perspective and demonstrates a long-standing engagement with computational tools for conceptual analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_003.html#additional-visual-materials",
    "href": "chapter_ai-nepi_003.html#additional-visual-materials",
    "title": "Large Language Models for the History, Philosophy, and Sociology of Science",
    "section": "3.12 Additional Visual Materials",
    "text": "3.12 Additional Visual Materials\nThe following slides provide supplementary visual information that complements the main chapter content:\n\n3.12.1 Slide 02\n\n\n\nSlide 02\n\n\nThe slide presents the agenda for the presentation, framed as “Today’s Menu”. The main sections to be covered are listed as: “Primer on LLMs” (Large Language Models), “Applications in HPSS” (History, Philosophy, and Sociology of Science), and “Reflections”. This slide serves as a roadmap for the audience.\n\n\n3.12.2 Slide 08\n\n\n\nSlide 08\n\n\nThis slide, titled “Transformer”, presents the complete Transformer model architecture, now augmented with specific examples of Large Language Models (LLMs). On the “Encoder” side, “BERT” is shown, characterised as “bidirectional full-context” and linked to “LLMs for HPSS ?” and “Vocab”. On the “Decoder” side, “GPT” is introduced, characterised as “unidirectional generative” and also linked to “LLMs for HPSS ?” and “Vocab”. The full internal components of the Transformer (Input/Output Embedding, Positional Encoding, Multi-Head Attention, Feed Forward, Add & Norm, Softmax, Linear) are visible. The slide includes citations for “Vaswani et al. 2017: Attention is all you need”, “Devlin et al. 2018. BERT: Pre-training of…”, and “Radford et al. 2018. Improving Language…”.\n\n\n3.12.3 Slide 14\n\n\n\nSlide 14\n\n\nThe slide, titled “Applications in HPSS”, provides a comprehensive list of potential uses for Large Language Models within the History, Philosophy, and Sociology of Science. These applications are categorised into four main areas: “Dealing with data and sources”, which includes “Parsing and extracting (publication types, acknowledgements, citations)” and “Interacting with sources (summarisation, RAG-type chatting)”; “Knowledge structures”, covering “Entity extraction (scientific instruments, celestial bodies, chemicals)” and “Mappings (disciplines, interdisciplinary fields, science-policy discourses)”; “Knowledge dynamics”, addressing “Conceptual histories (“theory” in DH, “virtual” and “Planck” in physics)” and “Novelty (breakthrough papers, emerging technologies)”; and “Knowledge practices”, encompassing “Argument reconstruction (premises & conclusions, causality)”, “Citation context analysis (purpose, sentiment)”, and “Discourse analysis (hedge sentences, jargon, boundary work)”.\n\n\n3.12.4 Slide 15\n\n\n\nSlide 15\n\n\nThe slide, titled “Applications in HPSS”, continues the discussion by highlighting current trends and challenges. It notes “Accelerating interest in LLMs”, even in “non-computational journals”. It describes “Varying degrees of customisation”, ranging “From architectural tweakings and custom pretraining over custom fine-tuning to off-the-shelf use of ChatGPT”. The slide also lists “Repeating concerns” such as “Overwhelming computational resources, opaqueness, lack of training data, lack of benchmarks, trade-offs between model types (BERT-like vs. GPT-like)”. Finally, it points to a “Trend toward accessibility”, exemplified by the question “E.g. BERTopic as the new pyLDAvis?”.\n\n\n3.12.5 Slide 16\n\n\n\nSlide 16\n\n\nThe slide, titled “Reflections”, presents key considerations and future directions for LLMs in HPSS. It outlines “Acknowledging HPSS-specific challenges”, including “Historical evolution of concepts and language”, adopting a “Reconstructive perspective, reading between the lines, reflecting social implications”, and dealing with “Sparse data, multiple languages, old scripts, lack of digitalisation”. It emphasises “Building LLM literacy” by encouraging familiarity with “LLMs, NLP, and DL, both tools and theory”, learning “what’s the right architecture and training for our problems”, and developing “our own shared datasets and benchmarks”. Lastly, it stresses “Staying true to HPSS methodologies”, which involves translating “HPSS problems into NLP tasks without losing our focus”, exploring “New opportunities for bridging qualitative and quantitative approaches”, and reflecting on “HPSS’ own role in LLM pre-history (e.g. co-word analysis)”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models for the History, Philosophy, and Sociology of Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html",
    "href": "chapter_ai-nepi_004.html",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "",
    "text": "Overview\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila have developed OpenAlex Mapper, an innovative tool designed to mitigate critical generalisation and validation challenges pervasive in History, Philosophy, and Sociology of Science (HPSS) research. This presentation introduces the tool, clarifies its technical underpinnings, demonstrates its interactive capabilities, and examines its manifold applications within transdisciplinary contexts.\nThe methodology centres on fine-tuning the Specter 2 language model to enhance its recognition of disciplinary boundaries. Subsequently, the team sampled 300,000 English abstracts from the OpenAlex database, a comprehensive and openly accessible repository of scholarly material. Engineers embedded these abstracts and reduced their dimensionality to two dimensions using Uniform Manifold Approximation and Projection (UMAP), thereby creating a foundational 2D base map. OpenAlex Mapper then allows users to submit arbitrary queries, downloading and embedding the corresponding records before projecting them onto this pre-trained UMAP model.\nThe interactive map facilitates in-depth investigation of specific terms, authors, temporal distributions, and citation networks. Significantly, the tool provides a rigorous quantitative framework that grounds qualitative, heuristic investigations, enabling researchers to trace the diffusion of models, map the distribution of concepts, and analyse method usage patterns across vast interdisciplinary samples. Examples include tracking the Hopfield model’s adoption, visualising model templates like Ising and Sherrington-Kirkpatrick, and contrasting the spatial distribution of concepts such as “phase transition” and “emergence,” alongside methods like Random Forest and Logistic Regression.\nDespite its utility, the system acknowledges several qualifications. It relies on the OpenAlex database, which, whilst robust, is not without imperfections, particularly concerning disciplinary representation. The current language model processes English-only sources, and the embedding step necessitates the presence of abstracts or well-formed titles. Furthermore, the UMAP algorithm, a stochastic process, introduces inherent trade-offs in dimensionality reduction, meaning the 768 dimensions of the Specter model cannot be perfectly represented in two, leading to potential misalignments. A working paper, Philosophy at Scale: Introducing OpenAlex Mapper, offers more detailed technical insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "href": "chapter_ai-nepi_004.html#openalex-mapper-architecture-and-workflow",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.1 OpenAlex Mapper Architecture and Workflow",
    "text": "4.1 OpenAlex Mapper Architecture and Workflow\nMaximilian Noichl, Andrea Loetgers, and Taya Knuuttila crafted OpenAlex Mapper, a novel tool funded by an ERC grant focused on “possible life.” This innovation aims to introduce the tool, clarify its high-level technical operations, demonstrate its practical application, and ultimately discuss its utility for research within the History, Philosophy, and Sociology of Science (HPSS).\nThe core workflow of OpenAlex Mapper comprises several distinct stages. Initially, the team fine-tuned the Specter 2 embedding model, specifically to enhance its recognition of disciplinary boundaries. This process involved training the model on a dataset of articles originating from highly similar disciplinary backgrounds, with UMAP dimensionality reduction providing a visualisation of this training. Notably, these adjustments constituted minor modifications to the language model, rather than a comprehensive retraining effort.\nSubsequently, for base-map preparation, the researchers leveraged the OpenAlex database, a vast and inclusive repository of scholarly material that surpasses the scale of Web of Science or Scopus. OpenAlex offers fully open data, facilitating easy batch querying and free accessibility, which distinguishes it from many proprietary alternatives. From this extensive database, the team sampled 300,000 random articles, imposing minimal restrictions beyond requiring reasonably well-formed English abstracts. These abstracts then underwent embedding using the previously fine-tuned Specter 2 model. Engineers further reduced these embeddings to two dimensions through Uniform Manifold Approximation and Projection (UMAP), yielding both a 2D base map and a trained UMAP model.\nFor individual user queries, OpenAlex Mapper allows submission of arbitrary searches to the OpenAlex database. The tool downloads the relevant records—for instance, the first 1,000 for demonstration purposes—and embeds their abstracts using the identical fine-tuned language model. These new embeddings are then projected through the pre-trained UMAP model, ensuring that the queried articles acquire positions on the two-dimensional map consistent with their hypothetical presence during the original layout process. The resulting interactive map is accessible online and available for download via data mappers, offering features such as temporal distributions and citation graph overlays. Users can access the slides and interactive tool via maxnoichl.eu/talk, whilst a version with a higher latency GPU setup is also available for processing larger queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "href": "chapter_ai-nepi_004.html#interactive-demonstration-of-openalex-mapper",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.2 Interactive Demonstration of OpenAlex Mapper",
    "text": "4.2 Interactive Demonstration of OpenAlex Mapper\nThe OpenAlex Mapper tool, accessible via https://m7n-openalex-mapper.hf.space, offers a straightforward user experience. Users initiate their investigation by searching the OpenAlex database directly through its comprehensive search interface, for example, by entering a query such as “scale-free network models.”\nIn the backend, the system efficiently downloads the initial 1,000 records pertinent to the search query, a limit imposed to optimise processing time. Subsequently, it embeds all abstracts from these downloaded records. If the user enables the option, the tool also processes the citation graph, enriching the analytical output. The primary output manifests as a projection of the search results onto a pre-existing grey base map, visually representing the disciplinary landscape.\nCrucially, the map is fully interactive, empowering users to delve into specific data points. For instance, one can investigate the presence of a term like “coriander” within unexpected fields such as epidemiology or public health, gaining nuanced insights into interdisciplinary connections. The demonstration showcased queries for both “coriander,” a standard OpenAlex example, and “scale-free network models.” Furthermore, the developers have made an alternative setup available, featuring a higher latency GPU, which accommodates larger and more complex queries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "href": "chapter_ai-nepi_004.html#applications-in-history-philosophy-and-sociology-of-science-hpss",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)",
    "text": "4.3 Applications in History, Philosophy, and Sociology of Science (HPSS)\nOpenAlex Mapper primarily addresses the persistent challenges of generalisation and validation that arise from the reliance on small samples and case studies within History, Philosophy, and Sociology of Science (HPSS). Whilst traditional HPSS methods—such as the close reading of scholarly papers, direct interaction with scientists, and studies conducted by researchers with scientific training—offer invaluable detailed, close-up views of scientific processes, they often struggle to scale. Generalising these granular insights to the vast, global, and rapidly evolving landscape of contemporary science presents a significant hurdle.\nOpenAlex Mapper contributes by providing rigorous quantitative methods that effectively ground qualitative, heuristic investigations. A key feature of the tool is its capacity to trace all analytical results directly back to their original textual sources, ensuring transparency and scholarly rigour.\nThe tool supports several specific applications, offering compelling examples of its utility. Researchers can trace the diffusion of particular models, such as the Hopfield model, to ascertain where it genuinely “stuck” or achieved widespread adoption and sustained reference across diverse scientific domains. Furthermore, the system facilitates the investigation of “model templates”—conceptual frameworks defining models of similar structure that emerge in disparate scientific fields, potentially structuring science in ways orthogonal to established disciplines. Examples like the Ising, Hopfield, and Sherrington-Kirkpatrick models often appear at specific, non-continuous locations on the base map, providing crucial insights for ongoing debates concerning model transfer in science.\nBeyond models, OpenAlex Mapper enables the mapping of concept distribution. For instance, it can visually contrast the spread of “phase transition” (depicted in blue) with “emergence” (in orange), broadening such analyses into interdisciplinary contexts and circumventing common problems associated with acquiring specific datasets. Finally, the tool proves invaluable for analysing method usage. It reveals distinguishable patterns of specific methods within interdisciplinary contexts; for example, neuroscientists frequently employ Random Forest algorithms, whilst researchers in psychiatry or mental health often utilise Logistic Regression. This observation prompts profound philosophical questions regarding the underlying reasons for these patterns and their implications for debates on machine learning in science versus classical statistics, and the concept of “theory-free science.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "href": "chapter_ai-nepi_004.html#limitations-and-future-considerations",
    "title": "4  Introducing OpenAlex Mapper",
    "section": "4.4 Limitations and Future Considerations",
    "text": "4.4 Limitations and Future Considerations\nWhilst OpenAlex Mapper offers significant analytical capabilities, its application is subject to several important qualifications. The system’s efficacy inherently depends on the OpenAlex database, which, despite its overall reasonable data quality compared to other available sources, is not without imperfections. Notably, certain disciplines, such as law, may exhibit underrepresentation within the database, potentially skewing comprehensive analyses.\nThe current language model processes English-only sources, which somewhat limits the tool’s global scope. Nevertheless, this constraint poses less of a problem for investigations focused on the more recent history of science. In principle, the integration of multilingual models could remedy this limitation, although the availability of high-quality, science-trained multilingual models remains scarce. Furthermore, the embedding step of the methodology necessitates that sources include either abstracts or well-formed titles, thereby restricting the range of processable data.\nCrucially, the method relies heavily on the Uniform Manifold Approximation and Projection (UMAP) algorithm, which presents its own set of imperfections. As a stochastic algorithm, UMAP generates one specific output amongst many possible configurations. Moreover, the algorithm must make inherent trade-offs during dimensionality reduction; the 768 dimensions of the Specter language model cannot be perfectly compressed into two, inevitably leading to some degree of “pushing and pulling and misaligning” of data points.\nFor those seeking further information, the presentation slides are available online at maxnoichl.eu/talk. Additionally, a working paper, titled Philosophy at Scale: Introducing OpenAlex Mapper, provides more exhaustive technical details regarding the tool’s development and operation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing OpenAlex Mapper</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html",
    "href": "chapter_ai-nepi_005.html",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "",
    "text": "Overview\nThis report details a procedural investigation into genre classification for historical medical periodicals. The ActDisease project, undertaken by the team at Uppsala University and funded by the European Research Council (ERC), seeks to illuminate how patient organisations profoundly shaped modern medicine in 20th-century Europe. Specifically, the project explores the evolution of disease concepts, illness experiences, and medical practices. Its primary analysis focuses on periodicals, predominantly magazines, published by ten European patient organisations across Sweden, Germany, France, and Great Britain, spanning the period from approximately 1890 to 1990.\nThe extensive dataset, comprising over 96,000 digitised pages, presents considerable challenges, particularly regarding Optical Character Recognition (OCR) accuracy. OCR accuracy suffers due to complex layouts and varied font types. Consequently, the project team has diligently explored post-OCR correction methods, leveraging instruction-tuned generative models. A central motivation for genre classification stems from the inherent diversity of text types within these historical periodicals; disparate content, such as administrative reports, advertisements, and humour sections, frequently co-exists on a single page. Traditional topic models and simple term counts often exhibit a bias towards the most prevalent text types, thereby obscuring nuanced communicative strategies.\nTo address these limitations, the project employs genre classification, defining genre as a class of documents united by a shared communicative purpose. This approach facilitates a multi-faceted exploration of the historical material, enabling comparative studies of communicative strategies across different countries, diseases, and publications over time. Furthermore, it supports fine-grained analysis of term distributions and topic models within specific genre categories, thereby enriching historical argumentation. Initial experiments in genre classification have utilised zero-shot (without prior examples) and few-shot (with a small number of examples) techniques, specifically employing the Llama-3.1 8b Instruct model to categorise diverse textual examples, including poetry, academic reports, legal documents, advertisements, instructive content, organisational reports, and patient narratives.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#project-introduction-and-scope",
    "href": "chapter_ai-nepi_005.html#project-introduction-and-scope",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.1 Project Introduction and Scope",
    "text": "5.1 Project Introduction and Scope\n\n\n\nSlide 01\n\n\nThe ActDisease project, an initiative at Uppsala University’s Department of History of Science and Ideas, comprehensively studies historical medical periodicals. This procedural report specifically details the project team’s experiments in genre classification for these historical texts. The project aims to refine analytical methods for understanding the intricate textual landscape of past medical discourse.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-materials",
    "href": "chapter_ai-nepi_005.html#the-actdisease-project-objectives-and-materials",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.2 The ActDisease Project: Objectives and Materials",
    "text": "5.2 The ActDisease Project: Objectives and Materials\n\n\n\nSlide 01\n\n\nThe ActDisease project, formally titled “Acting out Disease: How Patient Organizations Shaped Modern Medicine”, is an ERC-funded research endeavour (ERC-2021-STG 101040999) dedicated to the history of patient organisations in 20th-century Europe. This initiative investigates how these organisations influenced the evolution of disease concepts, illness experiences, and medical practices between approximately 1890 and 1990. The project concentrates on ten distinct European patient organisations, drawing material from Sweden, Germany, France, and Great Britain. Its primary source material comprises periodicals, predominantly magazines, published by these organisations. A historical image of Heligoland, Germany, dating from around 1890-1900, visually underscores the project’s scope, notably as the founding location of the Hay Fever Association of Heligoland in 1897.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#actdisease-dataset-overview",
    "href": "chapter_ai-nepi_005.html#actdisease-dataset-overview",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.3 ActDisease Dataset Overview",
    "text": "5.3 ActDisease Dataset Overview\n\n\n\nSlide 01\n\n\nThe ActDisease project utilises a substantial, recently digitised private collection of patient organisation magazines, collectively encompassing 96,186 pages. This comprehensive dataset is organised by country, disease, page count, and year coverage. German periodicals cover allergy/asthma, diabetes, and multiple sclerosis. Swedish sources address allergy/asthma, diabetes, and lung diseases. French publications focus on diabetes and rheumatism/paralysis, whilst UK materials include diabetes and rheumatism. Visual examples, such as the covers of “BRA Review” and “Allergia”, illustrate the diverse range of periodicals within the collection.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "href": "chapter_ai-nepi_005.html#digitisation-challenges-and-post-ocr-correction",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.4 Digitisation Challenges and Post-OCR Correction",
    "text": "5.4 Digitisation Challenges and Post-OCR Correction\n\n\n\nSlide 01\n\n\nDigitisation of the ActDisease dataset primarily employed ABBYY FineReader Server 14 for Optical Character Recognition (OCR). While this tool generally performs well on common layouts and fonts, significant challenges persist. Complex layouts, slanted text, rare fonts, and inconsistent scan or photo quality frequently impede accurate recognition, leading to errors. Consequently, the digitised texts exhibit remaining issues, notably OCR errors, which are particularly prevalent in German and French materials, alongside instances of disrupted reading order. To mitigate these inaccuracies, the project conducted specific experiments on post-OCR correction for German texts, leveraging instruction-tuned generative models. Danilova and Aangenendt’s work highlights a recurring problem: OCR errors frequently appear in creative textual elements, such as advertisements, humour pages, and poems, due to their often unconventional formatting (DanilovaAangenendt202X?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "href": "chapter_ai-nepi_005.html#rationale-for-genre-classification",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.5 Rationale for Genre Classification",
    "text": "5.5 Rationale for Genre Classification\nThe project team thoroughly examined the textual content within the ActDisease materials, observing a profound diversity of text types that consistently appear across all magazines. Crucially, different textual forms frequently co-exist within a single page; for instance, an administrative report might appear alongside an advertisement and a humour section. This inherent textual heterogeneity poses a significant challenge for conventional analytical approaches. Yearly and decade-based topic models, along with simple term counts, often fail to account for this co-occurrence, leading to a probable bias towards the most frequently appearing text types and obscuring nuanced communicative strategies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conceptualising-genre-for-historical-analysis",
    "href": "chapter_ai-nepi_005.html#conceptualising-genre-for-historical-analysis",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.6 Conceptualising Genre for Historical Analysis",
    "text": "5.6 Conceptualising Genre for Historical Analysis\nRecognising the limitations of aggregate analyses, genre emerged as a particularly useful concept for distinguishing between various text types. In the field of Language Technology, leading scholars define genre as a class of documents united by a shared communicative purpose (Swales1990?; Biber1988?), a definition that proves highly applicable to this historical research. Implementing genre classification directly supports the project’s core objective: exploring the historical material from multiple perspectives to enable more robust historical arguments.\nThis analytical approach offers two significant benefits. Firstly, it facilitates the study of evolving communicative strategies over time, enabling comparisons across different countries, diseases, and publications. Secondly, it enables a more granular analysis of term distributions and topic models specifically within identified genre groups, providing deeper insights into the nuances of historical discourse.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-poetry",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-poetry",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.7 Illustrative Genre Examples: Poetry",
    "text": "5.7 Illustrative Genre Examples: Poetry\n\n\n\nSlide 03\n\n\nWithin the ActDisease dataset, diverse genres manifest across the historical periodicals. One such example is poetry, which appears embedded within the magazine content. A visual representation highlights a section of text, clearly labelled “Poetry,” demonstrating the integration of these creative forms into the broader publication.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-academic-reports",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-academic-reports",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.8 Illustrative Genre Examples: Academic Reports",
    "text": "5.8 Illustrative Genre Examples: Academic Reports\n\n\n\nSlide 03\n\n\nAnother distinct genre identified within the periodicals is the academic report. For instance, the project team encountered reports detailing studies on the pancreas. A visual excerpt from the magazines illustrates this genre, with a highlighted section explicitly indicating an “Academic report” within the textual columns.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-legal-documents",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-legal-documents",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.9 Illustrative Genre Examples: Legal Documents",
    "text": "5.9 Illustrative Genre Examples: Legal Documents\n\n\n\nSlide 04\n\n\nThe collection also contains legal documents, exemplifying another genre present in the historical periodicals. One specific instance includes a deed of covenant. A visual highlight within the magazine layout directs attention to a section marked “Legal,” demonstrating the inclusion of formal legal texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-advertisements",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-advertisements",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.10 Illustrative Genre Examples: Advertisements",
    "text": "5.10 Illustrative Genre Examples: Advertisements\n\n\n\nSlide 04\n\n\nAdvertisements constitute a further genre found within the ActDisease materials. Illustratively, the periodicals feature advertisements for products such as chocolate specifically formulated for diabetics. A visual cue on the slide highlights an “Advertisement” section, showcasing how commercial content integrated into these historical publications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-instructive-content",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-instructive-content",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.11 Illustrative Genre Examples: Instructive Content",
    "text": "5.11 Illustrative Genre Examples: Instructive Content\n\n\n\nSlide 04\n\n\nInstructive content forms a significant genre within the periodicals, offering practical guidance to readers. This category encompasses various forms, including recipes, general doctor’s advice, and specific dietary recommendations. A highlighted portion of the magazine layout, labelled “Instructive/Guidance,” visually confirms the presence of such informative texts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-organisational-reports",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-organisational-reports",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.12 Illustrative Genre Examples: Organisational Reports",
    "text": "5.12 Illustrative Genre Examples: Organisational Reports\n\n\n\nSlide 04\n\n\nPatient organisation reports represent another key genre within the collection, documenting the internal workings and public face of these groups. These reports typically detail organisational meetings and various activities undertaken by the associations. A visual highlight on the slide directs attention to a section designated “Patient Organisation report,” illustrating this administrative and communicative function.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#illustrative-genre-examples-patient-narratives",
    "href": "chapter_ai-nepi_005.html#illustrative-genre-examples-patient-narratives",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.13 Illustrative Genre Examples: Patient Narratives",
    "text": "5.13 Illustrative Genre Examples: Patient Narratives\n\n\n\nSlide 04\n\n\nThe periodicals also feature narratives that recount the lived experiences of patients, offering invaluable insights into historical illness and care. These “Patient Experiences” provide personal perspectives on health and disease. A highlighted segment within the magazine layout, explicitly labelled “Patient Experiences,” demonstrates the inclusion of these poignant personal accounts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_005.html#conclusion-and-future-directions",
    "href": "chapter_ai-nepi_005.html#conclusion-and-future-directions",
    "title": "Genre Classification for Historical Medical Periodicals, ActDisease Project",
    "section": "5.14 Conclusion and Future Directions",
    "text": "5.14 Conclusion and Future Directions\nThis report has detailed the procedural investigation into genre classification for historical medical periodicals within the ActDisease project. By employing genre classification, the project effectively addresses the challenges posed by the textual heterogeneity of historical patient organisation magazines, moving beyond the limitations of aggregate analyses. This approach enables a more nuanced understanding of communicative strategies and facilitates granular analysis of specific textual forms.\nThe initial experiments utilising the Llama-3.1 8b Instruct model with zero-shot and few-shot techniques demonstrate the feasibility and potential of this methodology for categorising diverse genres, from poetry to patient narratives. Future work will focus on refining these classification models, expanding the genre taxonomy, and integrating the genre-classified data into broader historical analyses. This will allow for deeper comparative studies across different diseases, countries, and time periods, ultimately enriching our understanding of how patient organisations shaped modern medicine.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Genre Classification for Historical Medical Periodicals, ActDisease Project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html",
    "href": "chapter_ai-nepi_006.html",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "",
    "text": "Overview\nThe VERITRACE project, a five-year ERC Starting Grant initiative (2023-2028) entitled “Traces de la Verité: The reappropriation of ancient wisdom in early modern natural philosophy,” operates from the Vrije Universiteit Brussel (VUB). Research Professor Cornelis J. Schilt leads the project team, which comprises five members: a classicist, two historians, and Dr. Jeffrey Wolf, who serves as the digital humanities specialist. This ambitious endeavour meticulously traces the influence of the early modern ‘ancient wisdom’ or Prisca Sapientia tradition on the development of natural philosophy during the early modern period.\nThe team focuses on a core corpus of 140 works, including seminal texts such as the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and the Corpus Hermeticum. Historical evidence confirms the tradition’s profound impact; for instance, Isaac Newton engaged with the Sibylline Oracles, whilst Johannes Kepler demonstrated familiarity with the Corpus Hermeticum. Beyond these well-documented instances, the VERITRACE team seeks to uncover a much broader, often overlooked, network of texts and authors—dubbed the ‘great unread’—that also engaged with this tradition.\nTo achieve these objectives, the project employs a computational approach to the History, Philosophy, and Sociology of Science (HPSS). This approach involves large-scale, multilingual exploration to identify both direct (lexical) and indirect (semantic) textual re-use across a vast corpus. Functioning akin to an “early modern plagiarism detector,” the system aims to reveal ignored networks of texts, passages, themes, topics, and authors, potentially uncovering novel patterns in intellectual history.\nThe VERITRACE team leverages a diverse multilingual dataset of approximately 430,000 printed texts, spanning nearly 200 years (1540-1728). These texts originate from three primary digital sources: Early English Books Online (EEBO), Gallica (French National Library), and the Bavarian State Library. State-of-the-art digital techniques—including keyword search, text matching, topic modelling, and sentiment analysis—facilitate the analysis of this extensive corpus.\nSignificant challenges arise from variable OCR quality in raw library-provided texts, the complexities of early modern typography and semantics across multiple languages, and the sheer volume of data. The VERITRACE team addresses these by utilising Large Language Models (LLMs) in two key ways: GPT-based LLMs serve as “judges” for enriching and cleaning metadata, whilst BERT-based LLMs generate vector embeddings to encode the semantic meaning of textual passages for advanced matching.\nThe VERITRACE web application, currently in its alpha version, provides a user interface for these capabilities. It features sections for corpus statistics, a metadata explorer, a keyword search function, planned analytical tools (topic modelling, LSA, diachronic analysis), a text reader, and a sophisticated text matching tool. This application operates on an Elasticsearch backend. Despite its promising capabilities, the project faces ongoing challenges related to LLM hallucinations in metadata processing, the adequacy of current embedding models (such as LaBSE) for historical multilingual data, semantic drift over time, and the substantial scaling and performance requirements for processing the full corpus.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#about-the-veritrace-project",
    "href": "chapter_ai-nepi_006.html#about-the-veritrace-project",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.1 About the VERITRACE Project",
    "text": "6.1 About the VERITRACE Project\n\n\n\nSlide 03\n\n\nThe VERITRACE project, a five-year ERC Starting Grant initiative (2023-2028), bears the full title “Traces de la Verité: The reappropriation of ancient wisdom in early modern natural philosophy.” Based at the Vrije Universiteit Brussel (VUB), this significant undertaking operates under the leadership of its Principal Investigator, Research Professor Cornelis J. Schilt. Professor Schilt’s team comprises five dedicated members: a classicist, two historians, and Dr. Jeffrey Wolf, who serves as the digital humanities specialist.\nA core objective for the VERITRACE team involves meticulously tracing the influence of the early modern ‘ancient wisdom’ tradition, also known as Prisca Sapientia, upon the evolution of early modern natural philosophy and science. This tradition manifests in a specific corpus of 140 key texts, which form the team’s close reading focus. Notable examples include the Chaldean Oracles, the Sibylline Oracles, the Orphic Hymns, and, perhaps most famously, the Corpus Hermeticum.\nHistorical evidence already confirms the profound impact of this tradition on scientific thought; for instance, Isaac Newton demonstrably read the Sibylline Oracles, whilst Johannes Kepler possessed a deep understanding of the Corpus Hermeticum. Nevertheless, the VERITRACE team aims to delve deeper, seeking to uncover a far broader array of networks and texts that engaged with this ancient wisdom. This includes a vast collection of works often termed the ‘great unread,’ which historians have typically overlooked due to their sheer volume and authorship by lesser-known figures.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#computational-methodologies-for-hpss",
    "href": "chapter_ai-nepi_006.html#computational-methodologies-for-hpss",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.2 Computational Methodologies for HPSS",
    "text": "6.2 Computational Methodologies for HPSS\n\n\n\nSlide 04\n\n\nThe VERITRACE team establishes a robust framework for computational History, Philosophy, and Sociology of Science (HPSS), primarily aiming to facilitate large-scale, multilingual exploration within historical research. A key capability involves identifying textual re-use, distinguishing between direct and indirect forms. Direct re-use encompasses lexical similarities, such as direct quotations that may remain uncited. Conversely, indirect re-use focuses on semantic connections, identifying paraphrases or conceptually similar passages, even when direct linguistic overlap is minimal.\nThis functionality effectively creates an “early modern plagiarism detector,” enabling scholars to discern subtle and overt textual appropriations. Ultimately, this approach seeks to uncover previously ignored networks of texts, passages, themes, topics, and authors, thereby enriching our understanding of intellectual connections. Through this systematic analysis, the VERITRACE team anticipates revealing novel patterns within the intellectual history and philosophy of science. Furthermore, the system incorporates dedicated tools for comprehensive keyword searching, enhancing the exploratory capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#large-scale-multilingual-data-set-and-analytical-techniques",
    "href": "chapter_ai-nepi_006.html#large-scale-multilingual-data-set-and-analytical-techniques",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.3 Large-Scale Multilingual Data Set and Analytical Techniques",
    "text": "6.3 Large-Scale Multilingual Data Set and Analytical Techniques\n\n\n\nSlide 05\n\n\nThe VERITRACE team has assembled a substantial and diverse multilingual dataset, exclusively comprising printed works; handwritten materials remain outside the current scope. This extensive collection draws from three distinct multilingual data sources, encompassing texts in at least six different languages. The temporal span of the corpus covers approximately 200 years, commencing in 1540 and concluding in 1728, shortly after Isaac Newton’s death.\nSpecifically, the primary data sources include the Early English Books Online (EEBO), which offers freely downloadable content, alongside materials acquired from Gallica, the digital library of the French National Library. The Bavarian State Library constitutes the largest contributor to the corpus. Cumulatively, these sources provide approximately 430,000 texts for analysis. To explore this vast dataset, the VERITRACE team employs state-of-the-art digital techniques, including keyword search, sophisticated text matching algorithms, topic modelling, and sentiment analysis, amongst other advanced methodologies.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#overcoming-data-challenges-with-large-language-models",
    "href": "chapter_ai-nepi_006.html#overcoming-data-challenges-with-large-language-models",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.4 Overcoming Data Challenges with Large Language Models",
    "text": "6.4 Overcoming Data Challenges with Large Language Models\n\n\n\nSlide 06\n\n\nThe VERITRACE project confronts several core challenges inherent in processing historical textual data. Firstly, the variable quality of Optical Character Recognition (OCR) presents a significant hurdle. Libraries supply raw text in various formats—including XML, HOCR, and HTML files—without accompanying ground truth page images. This absence profoundly impacts all subsequent downstream processing stages. Secondly, the complexities of early modern typography and semantics, spanning at least six different languages, introduce further difficulties. Finally, the sheer volume of data—hundreds of thousands of texts printed across Europe over two centuries—necessitates robust computational strategies.\nTo address these challenges, the VERITRACE team integrates Large Language Models (LLMs) in two distinct capacities. On the decoder side, GPT-based LLMs function as “judges” to enrich and clean the project’s metadata. This application remains a work in progress, not yet fully functional. Conversely, on the encoder side, BERT-based LLMs generate vector embeddings. These embeddings serve to encode the semantic meaning of sentences and short passages within the textual corpus, thereby facilitating the crucial text matching functionalities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#experimental-application-of-llms-for-metadata-enrichment",
    "href": "chapter_ai-nepi_006.html#experimental-application-of-llms-for-metadata-enrichment",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.5 Experimental Application of LLMs for Metadata Enrichment",
    "text": "6.5 Experimental Application of LLMs for Metadata Enrichment\n\n\n\nSlide 07\n\n\nThe VERITRACE team has explored an experimental application of LLMs to automate the highly tedious process of comparing bibliographic metadata records. The primary task involves determining whether pairs of records—originating from both low-quality and high-quality sources such as the Universal Short Title Catalogue (USTC)—refer to the same underlying printed text. This process previously necessitated individual team members reviewing as many as 10,000 such pairs, highlighting the critical need for automation.\nThe adopted LLM approach employs a “bench” or “panel of judges,” comprising a chain of LLMs, including Primary, Secondary, Tiebreaker, and Expert models for handling edge cases. These models are tasked with judging matches, providing detailed reasoning, and assigning confidence levels to each decision. The VERITRACE team validates LLM outputs through comparison against a ground truth dataset, followed by a final review. The system incorporates extensive prompt guidelines, specifying field priorities, match criteria, and non-match indicators, to guide the LLMs’ decision-making process.\nCurrently, this remains a work in progress. A major challenge stems from hallucinations in the LLM output, where models generate non-existent records. Whilst requesting more structured output can mitigate hallucinations, this often results in more generic and less helpful responses, particularly in the reasoning provided. Consequently, striking the right balance proves challenging, often described as “more art than science.” Despite these difficulties, the approach holds significant potential for substantial time savings. The project currently utilises open-source models, such as Llama, for these operations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-veritrace-web-application-alpha-version",
    "href": "chapter_ai-nepi_006.html#the-veritrace-web-application-alpha-version",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.6 The VERITRACE Web Application: Alpha Version",
    "text": "6.6 The VERITRACE Web Application: Alpha Version\n\n\n\nSlide 11\n\n\nThe VERITRACE web application currently exists in an alpha version, representing an extremely new development that remains unavailable to the public. Its primary purpose involves demonstrating the project’s envisioned future capabilities. Engineers are presently testing a BERT-based Large Language Model, specifically LaBSE, to generate vector embeddings for every passage within the textual corpus. However, initial assessments suggest this model will likely prove “not good enough” for the project’s comprehensive requirements, despite demonstrating functionality in specific instances. For the current presentation, screenshots serve as a substitute for a live demonstration of the website.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#the-complex-data-processing-pipeline",
    "href": "chapter_ai-nepi_006.html#the-complex-data-processing-pipeline",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.7 The Complex Data Processing Pipeline",
    "text": "6.7 The Complex Data Processing Pipeline\n\n\n\nSlide 12\n\n\nThe VERITRACE team relies upon a complex, 15-stage data processing pipeline, meticulously designed to transform raw textual data—received in formats such as XML, HOCR, and HTML—into a structured form suitable for the backend Elasticsearch database. This intricate process involves numerous critical stages, including the extraction of text into standardised text files, the generation of precise position mappings, robust text segmentation, and comprehensive OCR quality assessment. Each of these stages demands careful optimisation to ensure data integrity and usability. Crucially, the generation of vector embeddings occurs towards the end of this pipeline. This entire workflow necessitates significant background effort, underscoring that the process is far from a simple, automated operation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#corpus-exploration-and-metadata-management",
    "href": "chapter_ai-nepi_006.html#corpus-exploration-and-metadata-management",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.8 Corpus Exploration and Metadata Management",
    "text": "6.8 Corpus Exploration and Metadata Management\n\n\n\nSlide 15\n\n\nThe VERITRACE web application organises its functionalities into five primary sections: Explore, Metadata Explorer, Search, Analyse, Read, and Match. The Explore section serves to provide comprehensive statistics about the corpus, drawing data directly from a Mongo database. Currently, this section displays information for 427,305 metadata records, detailing aspects such as language distribution, data sources, documents by decade, and publication places.\nBeyond this overview, the Metadata Explorer enables users to delve into the rich metadata associated with each individual text. For instance, examining “Sibyllina oraculo, ex veteribus codicibus emendata” reveals detailed bibliographic information. A crucial feature involves language identification, which processes every text down to approximately 50 characters. This granular approach is essential because many early modern texts are multilingual, whilst their primary metadata often indicates only a single language. For “Sibyllina oraculo,” this process identified 15% of the text as Greek and 85% as Latin, clearly indicating its substantively multilingual nature. Furthermore, the system assesses OCR quality on a page-by-page basis, rather than assigning a single quality metric to an entire book. This assessment presents a significant challenge, however, as it must be performed solely on the raw text due to the absence of ground truth page images.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#advanced-search-and-analytical-capabilities",
    "href": "chapter_ai-nepi_006.html#advanced-search-and-analytical-capabilities",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.9 Advanced Search and Analytical Capabilities",
    "text": "6.9 Advanced Search and Analytical Capabilities\n\n\n\nSlide 16\n\n\nThe web application incorporates a robust Search section, offering standard keyword search functionality. For example, a query for “Hermes” within the current prototype corpus of 132 files yields 22 documents containing 332 matches. Whilst this prototype index already occupies 15 gigabytes, the full corpus is projected to require terabytes of storage. Beyond basic keyword searches, the system supports complex Elasticsearch queries, enabling users to perform highly specific investigations. These include field queries, such as author:kepler 'hermes', alongside boolean operators (AND, OR), nested queries, and proximity queries, which allow users to specify terms appearing within a defined word distance, for instance, “Hermes and Plato mentioned within 10 words.”\nA dedicated Analyse section is also planned, though not yet implemented. This future component will integrate advanced analytical tools, including Topic Modelling, Latent Semantic Analysis (LSA), and Diachronic Analysis, further enhancing scholarly exploration of the corpus.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#text-viewing-and-the-veritrace-match-tool",
    "href": "chapter_ai-nepi_006.html#text-viewing-and-the-veritrace-match-tool",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.10 Text Viewing and the VERITRACE Match Tool",
    "text": "6.10 Text Viewing and the VERITRACE Match Tool\n\n\n\nSlide 19\n\n\nThe web application features a Read section, enabling scholars to view original text images. This functionality integrates a Mirador viewer, providing access to PDFs of every text within the corpus, alongside their associated metadata.\nCrucially, the VERITRACE Match Tool serves as the primary mechanism for identifying textual re-use across different documents. This tool offers various comparison modes: users can compare a single document against another, or against a collection of multiple documents—for example, comparing Newton’s Latin Opticks with all of Kepler’s works present in the database. A more ambitious goal involves enabling comparisons of a single text against the entire corpus, though this presents significant computational challenges regarding processing time and user experience.\nThe tool provides extensive user control, exposing numerous parameters for fine-tuning, such as setting a minimum similarity score. It supports distinct match types: lexical matching, which employs keyword analysis to identify vocabulary similarities, and semantic matching, which utilises vector embeddings to uncover conceptually similar passages, even in the absence of shared vocabulary—a vital feature for multilingual analysis. Furthermore, a hybrid matching option combines both approaches with adjustable weighting. Users can also select from different matching modes: a standard mode, a comprehensive mode that demands more computing power and time but aims for exhaustive results, and a faster mode for quicker queries.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#lexical-matching-case-study-newtons-opticks",
    "href": "chapter_ai-nepi_006.html#lexical-matching-case-study-newtons-opticks",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.11 Lexical Matching Case Study: Newton’s Opticks",
    "text": "6.11 Lexical Matching Case Study: Newton’s Opticks\n\n\n\nSlide 20\n\n\nA case study involving Isaac Newton’s Latin edition of Opticks (1719) and its English counterpart (1718) demonstrates the lexical matching capabilities. Given the distinct languages, a lexical match was expected to yield no results. Indeed, when run in standard mode, the tool confirmed this expectation, finding no matches, which serves as a crucial sanity check for the system’s design. However, employing the comprehensive mode revealed three matches. This outcome suggests the Latin edition likely incorporates some English text, possibly within a preface or other ancillary sections.\nThe system presents a detailed match summary, including a quality score reflecting the average similarity of the matches and a coverage score indicating the proportion of text covered by these matches. For instance, a query might involve approximately 1.3 million comparisons to generate its results. The match overview further details source and comparison passages, providing individual similarity scores and automatically highlighting matching terms to facilitate user analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_006.html#semantic-matching-and-future-challenges",
    "href": "chapter_ai-nepi_006.html#semantic-matching-and-future-challenges",
    "title": "6  Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project",
    "section": "6.12 Semantic Matching and Future Challenges",
    "text": "6.12 Semantic Matching and Future Challenges\n\n\n\nSlide 26\n\n\nWhen applying semantic matching to Newton’s Latin and English Opticks, significant matches were anticipated, given that one text is largely a translation of the other, implying strong conceptual similarity. Indeed, the semantic matches appear reasonable despite existing OCR issues, with passages discussing similar concepts, such as colours, aligning effectively.\nNevertheless, the semantic model’s performance exhibits certain limitations. The overall match score remains underdeveloped and lacks accuracy, whilst the quality score, though appearing high (e.g., 91.2%), contrasts with a low coverage score (e.g., 36.9%). This low coverage might accurately reflect genuine differences in text length or content between the editions, as the Latin version is notably longer. Overall, based on other queries, the current embedding model, LaBSE, is deemed inadequate for the task. The VERITRACE team hypothesises that this inadequacy stems from an “out-of-domain model collapse,” as the model was trained on modern languages rather than early modern texts, which possess distinct semantics, typography, and OCR quality.\nSeveral critical issues remain on the horizon for the project. The choice of embedding model is paramount; whilst LaBSE served as a starting point, alternatives such as XLM-Roberta, intfloat multilingual-e5-large, or historical mBERT warrant investigation, each presenting trade-offs between accuracy, storage requirements, and inference time. A key consideration involves whether to fine-tune a base model on the project’s unique historical corpus, given its distinct characteristics. Furthermore, the challenge of semantic drift—how meaning changes across centuries and different languages (e.g., from 1540 to 1700)—requires careful methodological consideration.\nThe pervasive issue of poor OCR quality significantly affects all downstream processes, including the fundamental task of segmenting text into sentences and passages. Re-OCRing the entire corpus is not feasible; therefore, solutions may involve re-OCRing only the very poor-quality texts or investing time in sourcing existing high-quality versions. Finally, scaling and performance present substantial challenges. The current prototype, comprising merely 132 texts, already incurs query times of 15 seconds. Scaling this to the full corpus of 430,000 texts, which will amount to terabytes of data, necessitates robust solutions to ensure usability and responsiveness.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html",
    "href": "chapter_ai-nepi_007.html",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "",
    "text": "Overview\nThis chapter explores the interpretability of Large Language Models (LLMs) and their application within the humanities, emphasising transparency and scientific insight. The discussion unfolds in two principal areas: first, the development of methods for understanding complex AI models, particularly explainable AI (XAI); second, the utilisation of AI to generate scientific insights within the humanities. The team has progressed from early XAI 1.0 techniques, such as feature attributions and heatmaps, to more sophisticated XAI 2.0 approaches, which encompass structured interpretability, feature interactions, and mechanistic perspectives. These advanced methods address the challenges posed by generative AI models, which function as multi-task and world models.\nEberle and his colleagues highlight critical issues such as model biases, long-range dependency limitations in summarisation tasks, and the simplistic strategies models often employ for tasks like sentence similarity. To overcome these, the team has developed novel techniques, including second-order explanations for token interactions and higher-order explanations for graph structures, which reveal complex language hierarchies.\nWithin the humanities, the project demonstrates AI’s capacity to extract visual definitions from historical corpora, exemplified by the classification of mathematical instruments. A significant initiative, the xAI-Historian project, focuses on corpus-level analysis of early modern astronomical tables, specifically the Sacrobosco Corpus (1472-1650), comprising 76,000 pages of university textbooks. This endeavour addresses the heterogeneity and limited annotations characteristic of historical data, where conventional OCR and foundation models often fail. The team developed a statistical model capable of detecting bigram representations within these tables, enabling the automated matching of similar semantics at scale. This methodology facilitates data-driven hypothesis generation and the discovery of historical anomalies, such as the politically controlled print programme in Wittenberg. The project ultimately aims to scale humanities research and foster novel research directions by integrating machine learning and explainable AI, whilst acknowledging the persistent challenges of low-resource and out-of-domain data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#presentation-structure-and-research-focus",
    "href": "chapter_ai-nepi_007.html#presentation-structure-and-research-focus",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.1 Presentation Structure and Research Focus",
    "text": "7.1 Presentation Structure and Research Focus\n\n\n\nSlide 01\n\n\nOliver Eberle, a Senior Researcher at the Berlin Institute for Learning and Data (BIFOLD) at TU Berlin, presented this work. Eberle’s background in machine learning has naturally led him into the digital humanities through collaborations with historians. This chapter systematically divides into two principal parts. The initial section explores explainable AI (XAI) and the development of methods and approaches to comprehend the internal workings of highly complex models, particularly Large Language Models. The subsequent part then demonstrates how researchers leverage AI to derive scientific insights, specifically within humanities applications. The overarching themes encompass the interpretability of LLMs, fostering transparency, exploring practical applications, and generating scientific insights.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#introduction-to-explainable-ai-xai-1.0",
    "href": "chapter_ai-nepi_007.html#introduction-to-explainable-ai-xai-1.0",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.2 Introduction to Explainable AI (XAI) 1.0",
    "text": "7.2 Introduction to Explainable AI (XAI) 1.0\n\n\n\nSlide 01\n\n\nExplainable AI (XAI) 1.0 primarily focuses on feature attributions. Historically, machine learning research has predominantly centred on visual data, particularly images. However, a significant shift has occurred over the past decade, with the field demonstrating a burgeoning interest in language data. A core challenge in machine learning involves comprehending the operations of “black box” models. In typical classification scenarios, such as image classification, users receive a prediction yet possess no insight into the underlying basis for that classification. This opacity necessitates methods to elucidate model decisions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#post-hoc-explainability-and-its-rationale",
    "href": "chapter_ai-nepi_007.html#post-hoc-explainability-and-its-rationale",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.3 Post-Hoc Explainability and its Rationale",
    "text": "7.3 Post-Hoc Explainability and its Rationale\n\n\n\nSlide 01\n\n\nFor approximately a decade, the field of explainable AI has dedicated its research to tracing the origins of model predictions. A common explanatory method involves generating heatmaps, which delineate the specific pixels responsible for an image classification. For instance, a heatmap might highlight the rooster’s head, indicating its significance in the model’s recognition of a rooster.\nBeyond specific examples, the broader rationale for explainability encompasses several critical objectives. Firstly, it enables the verification of predictions, ensuring that models operate reasonably. Secondly, it facilitates the identification of flaws and biases, allowing researchers to correct errors and comprehend how models make mistakes. Thirdly, explainability offers insights into the underlying problem, as models frequently uncover surprising solutions. Finally, and increasingly importantly, it ensures compliance with evolving legislation, such as the European AI Act. This post-hoc explainability represented the standard XAI scenario until approximately five years ago, as Samek and colleagues (2017) documented.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#transition-to-generative-ai-and-foundation-models",
    "href": "chapter_ai-nepi_007.html#transition-to-generative-ai-and-foundation-models",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.4 Transition to Generative AI and Foundation Models",
    "text": "7.4 Transition to Generative AI and Foundation Models\n\n\n\nSlide 03\n\n\nThe current era is defined by generative AI, where models exhibit extensive multi-task capabilities. These systems can perform classification, identify similar images, generate novel images, and answer diverse questions across various topics. This expanded functionality necessitates a move beyond conventional heatmap representations towards more sophisticated interpretability methods. Eberle and his colleagues now consider feature interactions and adopt more mechanistic perspectives to understand these complex models. Today’s foundation models function not merely as multi-task systems but also as “world models,” offering profound insights into societal structures, the evolution of text over time, and specific features inherent in textual data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#illustrative-model-mistakes-and-limitations",
    "href": "chapter_ai-nepi_007.html#illustrative-model-mistakes-and-limitations",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.5 Illustrative Model Mistakes and Limitations",
    "text": "7.5 Illustrative Model Mistakes and Limitations\n\n\n\nSlide 04\n\n\nModels frequently exhibit surprising and significant errors. One well-known example in object detection involves a standard classifier predicting a “boat” based on the surrounding water, rather than the vessel itself. The model identifies water as a correlated feature with an easily detectable texture, leading to this misattribution, as Lapuschkin and colleagues (2019) documented in Nature Communications.\nA more recent illustration of multi-step planning mistakes in Large Language Models (LLMs) emerges from the Tower of Hanoi puzzle. When tasked with moving disks from a left peg to a right peg, the LLM immediately errs by attempting to move the largest, inaccessible disk directly to the final peg. This demonstrates a fundamental failure to comprehend the physical constraints inherent in the problem, a finding Mondal and Webb (2024) highlighted.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability-and-first-order-attributions",
    "href": "chapter_ai-nepi_007.html#xai-2.0-structured-interpretability-and-first-order-attributions",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.6 XAI 2.0: Structured Interpretability and First-Order Attributions",
    "text": "7.6 XAI 2.0: Structured Interpretability and First-Order Attributions\n\n\n\nSlide 05\n\n\nThe evolution of explainable AI has led to XAI 2.0, which prioritises structured interpretability and seeks to move beyond the limitations of simple heatmap visualisations. First-order explanations, which attribute predictions to individual features, prove particularly useful for elucidating classifier decisions. For instance, in a project involving historical data, Eberle and his team trained a classifier to distinguish subgroups within historical tables, such as numerical tables. To verify the model’s efficacy, they employed heatmaps, which confirmed that the classifier accurately focused on the numerical content, thereby correctly identifying numerical tables. This demonstrated the model’s reliance on meaningful features for its predictions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#second-order-explanations-pairwise-relationships-and-similarity",
    "href": "chapter_ai-nepi_007.html#second-order-explanations-pairwise-relationships-and-similarity",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.7 Second-Order Explanations: Pairwise Relationships and Similarity",
    "text": "7.7 Second-Order Explanations: Pairwise Relationships and Similarity\n\n\n\nSlide 05\n\n\nBeyond individual feature attributions, the authors have explored second-order features, specifically focusing on pairwise relationships. This approach proves crucial for explaining similarity scores derived from embeddings. When computing a dot product to ascertain the similarity between two inputs, such as images or textual segments, interaction scores between their constituent features provide the explanation. For example, by analysing interactions between digits in historical tables, the team could verify that their model correctly identified identical tables, confirming its intended operation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#higher-order-explanations-graph-structures-and-contextual-bias",
    "href": "chapter_ai-nepi_007.html#higher-order-explanations-graph-structures-and-contextual-bias",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.8 Higher-Order Explanations: Graph Structures and Contextual Bias",
    "text": "7.8 Higher-Order Explanations: Graph Structures and Contextual Bias\n\n\n\nSlide 06\n\n\nIn more recent work, Eberle and his colleagues have investigated higher-order interactions within graph structures, finding them particularly meaningful for interpretability. This approach applies to various networks, such as citation graphs or networks of books and entities, especially when models are trained on classification tasks. A significant observation emerged: models predominantly focus on the later parts of the context, prioritising information presented more recently in the input. Conversely, they are considerably less likely to extract information from the very beginning of the context, a trend evident on a log scale of counts. This finding carries crucial implications for LLM summarisation; models are unlikely to produce balanced summaries of entire texts, instead tending to concentrate on data presented closer to the user’s prompt.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biased-sentiment-predictions",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-in-llms-biased-sentiment-predictions",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.9 First-Order Attributions in LLMs: Biased Sentiment Predictions",
    "text": "7.9 First-Order Attributions in LLMs: Biased Sentiment Predictions\n\n\n\nSlide 07\n\n\nAli and colleagues have applied first-order attributions to Large Language Models (LLMs) to understand feature importance, particularly in the context of biased sentiment predictions. This involves analysing which names exert the most or least influence in shifting a review’s sentiment towards either a positive or negative classification. Examples illustrate this phenomenon: phrases such as “malcolm d. lee,” “the sally jesse raphael atmosphere,” “some sort of martha stewart decorating program run am ok,” and “of cuban leader fidel castro” demonstrate negative relevance. Conversely, “dave barry’s,” “the coe n brothers,” and “a jackie chan movie” exhibit positive relevance. Ali and colleagues (2022) published this investigation into biased sentiment predictions in Transformer LLMs at ICML.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#first-order-attributions-for-long-range-dependencies-in-llms",
    "href": "chapter_ai-nepi_007.html#first-order-attributions-for-long-range-dependencies-in-llms",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.10 First-Order Attributions for Long-Range Dependencies in LLMs",
    "text": "7.10 First-Order Attributions for Long-Range Dependencies in LLMs\n\n\n\nSlide 08\n\n\nJafari and colleagues investigated first-order attributions to understand long-range dependencies within Large Language Models (LLMs), specifically when generating text summaries from extensive inputs, up to an 8,000-token context window. The central question concerned the extent of token dependencies. Their analysis revealed that models predominantly focus on the later sections of the context, prioritising information presented more recently. Conversely, they are significantly less likely to extract information from the very beginning of the context, a pattern evident on a log scale of counts. This finding implies that LLMs may not produce balanced summaries; instead, they tend to concentrate on data positioned closer to the user’s prompt. Jafari and colleagues (2024) published this work in MambaLRP at NeurIPS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#explaining-sentence-similarities-and-embeddings",
    "href": "chapter_ai-nepi_007.html#explaining-sentence-similarities-and-embeddings",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.11 Explaining Sentence Similarities and Embeddings",
    "text": "7.11 Explaining Sentence Similarities and Embeddings\n\n\n\nSlide 09\n\n\nA significant challenge involves elucidating the rationale behind specific sentence similarity scores. Eberle and his team address this through unsupervised analysis of representations extracted from a pretrained foundation model, employing dot-products. This process begins with a forward prediction or encoder, where a pair of sentences, such as “A cat I really like.” and “It is a great cat!”, undergo processing through multiple layers to yield embeddings, culminating in a similarity score (e.g., 0.92).\nTo explain this score, their method progresses to token interactions. Second-order explanations generate interaction scores between individual tokens, revealing the specific elements contributing to high similarity. In toy examples, their findings consistently indicate reliance on noun matching strategies, including synonyms and identical noun tokens, alongside some noun-verb interactions and separator/other token interactions. This suggests that models, compelled to compress vast amounts of information, often resort to surprisingly simplistic strategies.\nFurther analysis extends to corpus-level interaction patterns, visualised through a triangular matrix that illustrates the interaction strength between various part-of-speech tags. Ultimately, this research highlights that the features models utilise to assign high similarity scores can be remarkably simple. Eberle and colleagues (2022) published this work in TPAMI, with further contributions by Vasileiou and Eberle (2024) at NAACL.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions-and-language-understanding",
    "href": "chapter_ai-nepi_007.html#graph-neural-networks-for-structured-predictions-and-language-understanding",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.12 Graph Neural Networks for Structured Predictions and Language Understanding",
    "text": "7.12 Graph Neural Networks for Structured Predictions and Language Understanding\n\n\n\nSlide 11\n\n\nSchnake and colleagues have explored Graph Neural Networks (GNNs) for structured predictions, where attributions manifest as “walks” that represent feature interactions. A crucial insight reveals that GNNs, which inherently encode structural information, can be conceptualised as Large Language Models (LLMs), given that attention networks in LLMs essentially facilitate token message passing akin to graph structures.\nApplying this framework to language understanding, the team identified a limitation with first-order attributions: they often fail to capture the intricate complexity of natural language, particularly negations. For instance, in the sentence “First I didn’t like the boring pictures…”, a first-order attribution might incorrectly assign a high score to “boring pictures” due to the presence of “boring.” However, by employing higher-order interaction methods, their system accurately reflects the hierarchical structure of natural language. In the same example, the entire negative first sentence receives a negative score, whilst the subsequent positive part of the sentence is correctly weighted, demonstrating a more nuanced understanding. Schnake and colleagues (2022) published this work on higher-order explanations for GNNs in TPAMI.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-historical-corpora",
    "href": "chapter_ai-nepi_007.html#extracting-visual-definitions-from-historical-corpora",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.13 Extracting Visual Definitions from Historical Corpora",
    "text": "7.13 Extracting Visual Definitions from Historical Corpora\n\n\n\nSlide 12\n\n\nIn their initial foray into the humanities, El-Hajj and Eberle, amongst others, applied heatmap-based approaches to extract visual definitions from historical corpora. They focused on a corpus of mathematical instruments, comprising historical illustrations, with the objective of classifying these instruments into categories such as “machine” or “mathematical instrument.” This endeavour involved close collaboration with historians, including Matteo Valleriani and Jochen Büttner, to develop objective criteria for these visual definitions. Through rigorous verification with domain experts, the team ensured the meaningfulness of the definitions. A key finding revealed that the fine-grained scales present on mathematical instruments were highly relevant features for the model’s classification decisions. El-Hajj and Eberle, amongst others, published this research on explainability and transparency in digital humanities in the International Journal of Digital Humanities (2023).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables-xai-historian",
    "href": "chapter_ai-nepi_007.html#corpus-level-analysis-of-early-modern-astronomical-tables-xai-historian",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.14 Corpus-Level Analysis of Early Modern Astronomical Tables (xAI-Historian)",
    "text": "7.14 Corpus-Level Analysis of Early Modern Astronomical Tables (xAI-Historian)\n\n\n\nSlide 12\n\n\nThe largest collaborative project undertaken involved a corpus-level analysis of early modern astronomical tables. This initiative focused on numerical tables from the Sphera Corpus, a collection of early modern texts spanning from 1472 to 1650. Specifically, the Sacrobosco Table Corpus, comprising 76,000 pages of university textbooks from the same period, formed the core dataset. Historians approached the team seeking an automated method for matching tables with similar semantics, a task previously unfeasible at scale.\nThe machine learning challenge proved substantial due to the data’s extreme heterogeneity, the scarcity of annotations, and the inadequacy of conventional OCR and foundation models. In response, Eberle and his colleagues developed a bespoke workflow designed to empower historians with insights at scale. This led to the concept of the xAI-Historian, an academic who leverages AI and explainable AI to facilitate data-driven hypothesis generation and the discovery of specific case studies. This significant work draws upon the Sphera Corpus (Valleriani and colleagues, 2019) and the Sacrobosco Table Corpus (Eberle and colleagues, 2024).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#verifying-modelling-and-features-using-xai-and-historians",
    "href": "chapter_ai-nepi_007.html#verifying-modelling-and-features-using-xai-and-historians",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.15 Verifying Modelling and Features using XAI and Historians",
    "text": "7.15 Verifying Modelling and Features using XAI and Historians\n\n\n\nSlide 13\n\n\nThis project fundamentally considers historical tables as crucial carriers of scientific knowledge processes, particularly mathematisation. Rather than attempting to process entire tables with generic foundation models—which prove ineffective for this out-of-domain data—Eberle and his team crafted a specialised statistical model. This bespoke model detects bigram representations, such as ‘01’ or ‘21’, within the tables. Given the limited annotations available, their approach combines a learned feature extractor with a hard-coded structure.\nVerification of the model’s efficacy involves confirming that it consistently detects identical bigrams across different inputs, for example, identifying ‘38’ on two distinct tables. This rigorous validation ensures the model operates as intended and its decisions can be trusted. The overall workflow encompasses three key steps:\n\ndata collection from images of old books;\nan atomisation-recomposition phase involving input tables, bigram maps, and histograms;\na corpus-level analysis that generates historical table embeddings and assesses data similarity.\n\nEberle and colleagues (2022) detailed this methodology in TPAMI, with further insights in Science Advances (Eberle and colleagues, 2024).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#cluster-entropy-analysis-to-investigate-innovation-in-print-programmes",
    "href": "chapter_ai-nepi_007.html#cluster-entropy-analysis-to-investigate-innovation-in-print-programmes",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.16 Cluster Entropy Analysis to Investigate Innovation in Print Programmes",
    "text": "7.16 Cluster Entropy Analysis to Investigate Innovation in Print Programmes\n\n\n\nSlide 14\n\n\nEberle and his team employed cluster entropy analysis, specifically measuring the difference between observed cluster entropy H(p) and maximum attainable entropy H(p_max), to investigate the diffusion of innovation across Europe. This method analysed the print output of various cities, each serving as a publishing location. The process involved utilising representations derived from the bigram model, followed by distance-based clustering to ascertain the number of distinct content clusters each city produced. Entropy then quantified the diversity of these print programmes: a low entropy score indicated a consistent reproduction of the same content, whilst a higher score signified a more diverse programme.\nThis analysis yielded two compelling case studies. Frankfurt/Main exhibited one of the lowest entropy scores, confirming its historical reputation as a major centre for reprinting editions. More notably, Wittenberg also registered a very low score, revealing a historical anomaly. Here, the political control exerted by Protestant reformers, particularly Melanchthon, actively restricted the curriculum and, consequently, the print programme. This finding not only detected a previously unquantified historical anomaly but also corroborated existing historical intuition and supporting evidence. This work is documented in Sphera publication EPISD-626 and by Eberle and colleagues (2024) in Science Advances.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "href": "chapter_ai-nepi_007.html#conclusion-ai-based-methods-for-the-humanities",
    "title": "7  Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.",
    "section": "7.17 Conclusion: AI-based Methods for the Humanities",
    "text": "7.17 Conclusion: AI-based Methods for the Humanities\n\n\n\nSlide 15\n\n\nIn conclusion, whilst humanities and digital humanities scholars have largely concentrated on the digitisation of source material, the automated analysis of these corpora presents significant challenges due to their inherent heterogeneity and scarcity of labels. Nevertheless, the integration of machine learning (ML) and explainable AI (XAI) offers substantial potential to scale humanities research and foster novel research directions.\nFoundation Models and Large Language Models (LLMs) can effectively assist with intermediate tasks such as labelling, data curation, and error correction. However, their utility remains limited when addressing more complex research questions. Significant roadblocks persist, including the challenges posed by low-resource data for machine learning, particularly concerning scaling laws. Furthermore, out-of-domain transfer, especially for historical and small-scale datasets, demands rigorous evaluation, as current LLMs are primarily trained and aligned for natural language processing and code generation, not historical data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interpretability for LLMs: Transparency, Applications and Scientific Insights in the Humanities.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html",
    "href": "chapter_ai-nepi_008.html",
    "title": "Modeling Science",
    "section": "",
    "text": "Overview\nThe chapter, “Modelling Science,” critically examines the role of computational tools, particularly Large Language Models (LLMs), in scientific inquiry and scholarly content generation. It begins by tracing the conceptual evolution of LLM capabilities, acknowledging their significant advancements. However, it immediately pivots to a rigorous analysis of their persistent and fundamental limitations. The authors highlight the pervasive issue of hallucination and the inherent lack of epistemic agency in current LLM outputs, which poses significant challenges to their reliable application in academic and scientific domains. This initial discussion establishes the imperative for a paradigm shift: moving from uncritical reliance on LLM generation to a more robust, validated approach.\nA central argument of this chapter is the assertion that “Validation is all you need.” This principle posits that any proposition or action, especially within scholarly contexts, demands rigorous support and verification. This underpins the subsequent introduction of practical solutions designed to address the identified LLM deficiencies. The discussion then emphasises the foundational role of meticulously curated sources, illustrating this through a demonstration of software for digital text analysis. This sets the stage for the unveiling of Scholarium, a novel, registry-based platform conceived as a comprehensive, curated scholarly database. Distinct from conventional embedding-centric systems, Scholarium champions a structured approach to managing and validating scholarly content, thereby facilitating multimodal AI inquiry by prioritising data integrity and verifiable provenance.\nTo operationalise this vision, the chapter introduces the AI Cockpit as the primary user interface. This interface is designed to seamlessly integrate LLM capabilities within a framework of robust data stewardship and advanced document analysis, emphasising the controlled application of AI within a validated environment. Furthermore, the discussion extends to the foundational infrastructure supporting this ecosystem, highlighting platforms like Zenodo as exemplars of FAIR (Findable, Accessible, Interoperable, Reusable) data principles. These principles are crucial for developing community-driven and AI-ready scholarly resources. The chapter concludes by outlining the guiding philosophy of OpenScienceTechnology, which emphasises principles of sustainability, openness, and technical support. Collectively, these components articulate a compelling vision for a new era of digital scholarship that leverages AI’s potential whilst rigorously upholding scientific integrity and epistemic responsibility.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#llm-competence-conceptual-evolution-and-persistent-challenges",
    "href": "chapter_ai-nepi_008.html#llm-competence-conceptual-evolution-and-persistent-challenges",
    "title": "Modeling Science",
    "section": "8.1 LLM Competence: Conceptual Evolution and Persistent Challenges",
    "text": "8.1 LLM Competence: Conceptual Evolution and Persistent Challenges\n\n\n\nSlide 02\n\n\nSlide 2, titled “LLM: Evolution of competence,” outlines a conceptual progression in the development of Large Language Models (LLMs). This evolution begins with the foundational concept of “Attention is all you need” [Vaswani et al., 2017], which marked a significant leap in sequence processing and epitomises early conversational models like Chat with GPT. The progression then moves to “Context is all you need,” recognising the importance of incorporating broader external information. This is often achieved through Retrieval-Augmented Generation (RAG) [Lewis et al., 2020], which provides a larger and more relevant context for LLM responses. The aspirational third stage, “Thinking is all you need,” points towards models capable of genuine reasoning, whether through pre-defined plans or more emergent cognitive processes.\nHowever, despite this depicted evolution, a critical examination reveals significant unmet needs and fundamental limitations within current LLM technologies. A core challenge lies in countering hallucinations, as current models often prioritise fluency and coherence over factual accuracy. This issue stems from a deeper conceptual misunderstanding: embedding vectors, whilst powerful for representing semantic similarity, are not synonymous with the “meanings of impressions [or] expressions” in a true cognitive sense. The authors contend that LLMs should not formulate responses that merely “sound good but are false,” nor uncritically repeat “what others put out on media, internet media,” as such outputs do not constitute genuine knowledge or justified answers.\nCrucially, existing models fundamentally lack the capacity for higher-order cognitive functions such as seeking “what is best justified” or the ability to “make plans for scientific inquiry.” These capabilities, essential for true intelligence and academic rigour, are entirely missing from current LLM architectures. The presenter emphatically concludes that “no model can do that,” indicating a profound gap that current technological paradigms offer “no hope” of achieving. This suggests that the “evolution of competence” depicted on the slide is, in many critical aspects, far from complete.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#addressing-llm-limitations-the-imperative-of-validation-and-epistemic-agency",
    "href": "chapter_ai-nepi_008.html#addressing-llm-limitations-the-imperative-of-validation-and-epistemic-agency",
    "title": "Modeling Science",
    "section": "8.2 Addressing LLM Limitations: The Imperative of Validation and Epistemic Agency",
    "text": "8.2 Addressing LLM Limitations: The Imperative of Validation and Epistemic Agency\n\n\n\nSlide 03\n\n\nSlide 3, titled “What is missing?”, critically examines fundamental limitations and necessary improvements for Large Language Models (LLMs). A primary concern is the pervasive issue of hallucination; the authors propose introducing an ‘Opponent to counter hallucination’ to ensure factual accuracy and reliability. Furthermore, the slide highlights a crucial conceptual misunderstanding, asserting that ‘Embedding vectors are NOT meanings of expressions’. This underscores the need for a deeper, more nuanced understanding of linguistic and semantic representation beyond mere statistical associations. These points collectively articulate the pressing need for LLMs to move beyond superficial linguistic generation towards grounded, verifiable knowledge.\nBuilding upon these identified gaps, the slide prescribes vital ethical and epistemological directives for LLM development. These include strict adherence to truthfulness—“Do not formulate what sounds good but is false”—and a caution against uncritical dissemination: “Do not repeat what others put out as internet media.” Instead, the authors contend that LLMs must adhere to principles that guide them to “Seek what is best justified” and to “Make best plan for inquiry,” emphasising a commitment to reasoned thought and rigorous investigation. The presenter powerfully frames the overarching solution to these challenges as “validation,” positing it as the essential element for providing reasons, arguments, and evidence for or against the truth of a proposition, and critically, for or against the pursuit of actions.\nTo bridge this validation gap and foster genuine epistemic capabilities in LLMs, the presenter proposes a new discipline: computational epistemology. This emerging field aims to develop the methods and methodologies required to imbue AI with what the authors term “epistemic agency.” This crucial capability extends beyond mere sentence generation. It enables LLMs to identify propositions within complex texts, discern arguments in diverse sources (including historical inquiries), and interpret the intentions, plans, and actions of individuals documented within historical records. Ultimately, the authors aim to equip LLMs with the capacity for critical assessment and justified reasoning, moving them towards a more robust and ethically sound form of artificial intelligence.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-epistemic-agency-in-computational-inquiry",
    "href": "chapter_ai-nepi_008.html#the-imperative-of-validation-and-epistemic-agency-in-computational-inquiry",
    "title": "Modeling Science",
    "section": "8.3 The Imperative of Validation and Epistemic Agency in Computational Inquiry",
    "text": "8.3 The Imperative of Validation and Epistemic Agency in Computational Inquiry\n\n\n\nSlide 04\n\n\nThe foundational premise of this discussion asserts that “Validation is all you need,” emphasising that any proposition or course of action necessitates robust support. Validation, in this context, extends beyond mere assertion; it demands comprehensive reasons, compelling arguments, and concrete evidence both for and against the truth of a statement, as well as for or against the pursuit of specific actions. This rigorous approach is intrinsically linked to the field of Computational Epistemology, which seeks to formalise and automate the processes of knowledge acquisition, justification, and reasoning. Central to this framework, the authors define Epistemic Agency as the capacity to identify propositions—moving beyond mere linguistic sentences to grasp underlying claims—engage in sophisticated argumentation, and comprehend the intentions, plans, and actions of individuals.\nThe authors apply this theoretical framework practically within a computational working environment designed to tackle complex inquiries. This is illustrated by a historical case study concerning the 18th-century construction of Sanssouci castle. A long-standing historical dispute revolves around the precise involvement of the renowned mathematician Leonhard Euler in this project and whether his contribution, or that of others, led to one of the biggest construction failures of the era. To address such open questions, the system provides an “inquiry window” where users can formulate precise questions, such as “Reconstruct which persons performed which work on the water fountain.” The objective is not merely to retrieve information, but to obtain a “validated answer”—a qualified, reliable response supported by proven evidence, free from hearsay.\nTo assist in these inquiries, the system employs an AI agent, presumptively named Bernoulli, which embodies the principles of epistemic agency. This agent navigates and processes vast quantities of historical sources, moving far beyond the limitations of simply reading a single PDF or relying on basic indexing and token matching. The true challenge lies in aggregating and intelligently synthesising information from all available sources to construct a coherent, evidence-based narrative. This computational approach aims to provide the depth of validation and analytical rigour demanded by the slide’s core premise, transforming intricate historical debates into computationally tractable and verifiable inquiries.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-foundational-role-of-curated-sources-in-digital-text-analysis",
    "href": "chapter_ai-nepi_008.html#the-foundational-role-of-curated-sources-in-digital-text-analysis",
    "title": "Modeling Science",
    "section": "8.4 The Foundational Role of Curated Sources in Digital Text Analysis",
    "text": "8.4 The Foundational Role of Curated Sources in Digital Text Analysis\n\n\n\nSlide 05\n\n\nSlide 5 visually presents a live demonstration of a sophisticated software application designed by the authors for digital humanities research. The interface on the left pane displays a historical document, specifically a text titled ‘Allerdurchlauchtigsten, Großmächtigsten König und Herrn Friedrich Wilhelm dem Zweiten König von Preußen’, indicating a focus on historical and archival materials. Complementing this, the right pane, labelled ‘Personen und Aufgaben in der Bibliothek’, showcases the platform’s capability for structured information extraction. It highlights entities such as ‘Nahl (Bildhauer)’, ‘Benkert und Heymüller’, and ‘Giese’, along with associated monetary values and actions. This demonstrates the software’s utility in uncovering specific details and relationships within complex historical narratives.\nCrucial to the efficacy of such advanced digital analysis, as the presenter elaborates, is the underlying quality of the source material. The authors emphasise the involvement of a “scholarly curated editorial board which has worked on the sources” as a key component for successful scholarly work in this domain. This highlights that whilst digital tools provide powerful analytical capabilities, their output is only as reliable as the input data, necessitating meticulous human scholarship in preparing and verifying historical texts.\nThe Opera Omnia of Euler exemplifies the immense scale and dedication required for such foundational work. This monumental project, comprising 86 volumes, involved approximately 120 years of sustained effort by numerous scholars. It culminated in the complete editing of all Euler’s letters and 866 publications just two years prior. This rigorous, long-term scholarly endeavour in text preparation is fundamental, providing the high-fidelity, meticulously edited digital sources that enable the precise and insightful text analysis capabilities demonstrated by the application.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-a-novel-platform-for-curated-validated-scholarly-content",
    "href": "chapter_ai-nepi_008.html#scholarium-a-novel-platform-for-curated-validated-scholarly-content",
    "title": "Modeling Science",
    "section": "8.5 Scholarium: A Novel Platform for Curated, Validated Scholarly Content",
    "text": "8.5 Scholarium: A Novel Platform for Curated, Validated Scholarly Content\n\n\n\nSlide 06\n\n\nThe sixth slide introduces Scholarium, a pioneering platform designed by the authors as a comprehensive, curated scholarly database, distinct from conventional data embeddings. As highlighted by the presenter, this innovative system provides a structured, validated source for historical information, moving beyond typical data representation methods. The slide visually reinforces this by showcasing examples of meticulously curated scholarly works such as the Opera Bernoulli Euler, Kepler Gesammelte Werke, and Brahe Opera Omnia, underscoring the platform’s commitment to complete and authoritative historical records.\nScholarium’s core strength lies in its ability to compile highly detailed, chronologically organised inventories of historically proven activities. The presenter elaborated on the depth of this curation, which encompasses chronologies of actions, expressions communicated between individuals, the evolving use of terminology and language by a person, and even the historical application of tools and material objects. This granular level of detail ensures a rich, contextually grounded understanding of historical figures and their contributions, all rigorously validated by primary sources and forming a very detailed inventory of historical records.\nThe visual component of the slide, featuring a screenshot of a digital library interface displaying ‘LEONHARD EULER BRIEFWECHSEL OPERA OMNIA SERIES QUARTA A: COMMERCIUM EPISTOLICUM VOL. I,’ exemplifies the practical application of Scholarium’s curated content. This screenshot perfectly illustrates how the platform provides direct access to meticulously organised and historically verified records, such as extensive correspondences, which are fundamental to academic research and understanding the intellectual lineage of historical thought.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#scholarium-a-registry-based-framework-for-structured-scholarly-content-and-multimodal-ai-inquiry",
    "href": "chapter_ai-nepi_008.html#scholarium-a-registry-based-framework-for-structured-scholarly-content-and-multimodal-ai-inquiry",
    "title": "Modeling Science",
    "section": "8.6 Scholarium: A Registry-Based Framework for Structured Scholarly Content and Multimodal AI Inquiry",
    "text": "8.6 Scholarium: A Registry-Based Framework for Structured Scholarly Content and Multimodal AI Inquiry\n\n\n\nSlide 07\n\n\nThe authors introduce Scholarium as a novel framework centred on a registry-based approach for managing and curating scholarly content, explicitly positioning itself as an alternative to embedding-centric systems. This structured methodology supports a “Curated Scholarly Editorial Board,” implying a robust system for organising and validating academic contributions. The framework meticulously categorises various dimensions of scholarly work, including personal actions like communication acts (e.g., publications, reports), and the nuanced nature of statements (e.g., implications, arguments, inquiries). Furthermore, it captures the specific academic methodologies employed, detailing the use of language, terminology, concepts, relations, models, methods, tools, devices, data, information, evidence, and sources, thereby providing a granular and comprehensive record of scholarly endeavours.\nTo significantly enhance the utility of these precisely recorded elements, the authors integrate them with advanced artificial intelligence capabilities. The slide highlights the presence of both an AI API and a Model Context Protocol (MCP) API, as evidenced by the Anthropic webpage screenshot. This indicates a strategic design to enable programmatic interaction with sophisticated AI models, allowing for structured and context-aware inquiry into the amassed scholarly data. These “records,” once established, become readily queryable by accessible multimodal AI models.\nCrucially, the presenter emphasises that such models, exemplified by the latest versions of Gemini 2.5, are particularly well-suited for the task due to their ability to seamlessly combine and process information from diverse modalities, including text and images. This multimodal capability is vital for effectively extracting insights and solving complex analytical requirements from the rich, categorised content stored within the Scholarium registry, thereby facilitating a deeper understanding and interaction with scholarly knowledge.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#the-ai-cockpit-user-interface-llm-integration-and-data-stewardship",
    "href": "chapter_ai-nepi_008.html#the-ai-cockpit-user-interface-llm-integration-and-data-stewardship",
    "title": "Modeling Science",
    "section": "8.7 The AI Cockpit: User Interface, LLM Integration, and Data Stewardship",
    "text": "8.7 The AI Cockpit: User Interface, LLM Integration, and Data Stewardship\n\n\n\nSlide 08\n\n\nSlide 8 introduces the AI Cockpit, which serves as the primary user interface for the system. It facilitates advanced document analysis and information extraction. As depicted on the left side of the slide, this interface boasts robust integration with a variety of leading Large Language Models (LLMs), including Claude, Gemini, Llama, and the specialised LettreAI on Cursor. This multi-model support ensures flexibility and powerful analytical capabilities for diverse document types and analytical tasks. The accompanying screenshot on the right visually demonstrates the AI Cockpit’s operational view, displaying a historical document such as ‘Manger1789.pdf’ alongside its associated analysis pane. This showcases the system’s ability to perform structured information extraction directly within the intuitive interface.\nThe data generated through the AI Cockpit’s powerful extraction capabilities necessitates a robust and sustainable strategy for storage and dissemination. To address this, the project team commits to adhering to FAIR (Findable, Accessible, Interoperable, Reusable) data principles. The authors have selected Zenodo, a repository hosted by CERN in Geneva, as their long-term solution for storing and publishing research data. This choice is critical for ensuring the longevity and broad accessibility of the valuable structured information extracted by the system.\nZenodo’s infrastructure provides a reliable and persistent platform, enabling the hosting of these research data for many years. This thereby supports open science and facilitates future research and validation. This comprehensive approach, combining a sophisticated AI-powered user interface for data extraction with a dedicated, FAIR-compliant repository, establishes a complete pipeline for academic research data management, from initial processing to long-term preservation and publication.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#zenodo-enabling-fair-data-through-community-and-ai-ready-infrastructure",
    "href": "chapter_ai-nepi_008.html#zenodo-enabling-fair-data-through-community-and-ai-ready-infrastructure",
    "title": "Modeling Science",
    "section": "8.8 Zenodo: Enabling FAIR Data Through Community and AI-Ready Infrastructure",
    "text": "8.8 Zenodo: Enabling FAIR Data Through Community and AI-Ready Infrastructure\n\n\n\nSlide 09\n\n\nSlide 9, titled ‘FAIR Infrastructure,’ provides a concrete illustration of these principles through the Zenodo platform, prominently featured with its subtitle. Zenodo serves as a vital open repository that enables researchers to share and discover various types of research outputs, including datasets, publications, and software. As depicted in the accompanying screenshot, Zenodo facilitates community engagement and content discoverability through features such as ‘Featured communities,’ exemplified by ‘AAS Journals,’ which outlines specific publishing and community submission processes. The platform also highlights ‘Recent uploads,’ showcasing how new research outputs, such as the specific dataset “Wide spectral range optical characterization of terbium gallium garnet (TGG) single crystal by universal dispersion mode” uploaded by Franka, Daniel et al., become immediately available and discoverable to the wider scientific community.\nBeyond its user-facing functionalities, dedicated technical expertise underpins the robust operation of such an infrastructure. Entities like Open Science Technology, a startup instrumental in maintaining the platform’s technical stability and evolving capabilities, provide this critical support. A significant development in this regard is the implementation of a Model Context Protocol (MCP) API server. This server specifically standardises the access of artificial intelligence models to the vast trove of data hosted on the platform, thereby facilitating worldwide inquiries and automated knowledge extraction. This forward-looking approach directly addresses the growing need for machine-actionable data, significantly enhancing the interoperability and reusability aspects of FAIR principles.\nWhilst acknowledging the dynamic landscape of technological advancements, this initiative represents a proactive attempt to standardise AI access APIs for global scientific knowledge. This commitment to standardisation, coupled with an ethos of open collaboration, ensures that the infrastructure remains adaptable and accessible, not only for human users but also for emerging AI-driven research paradigms. By continually developing such sophisticated access mechanisms, Zenodo reinforces its role as a pivotal component in the evolving ecosystem of open science and FAIR data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_008.html#opensciencetechnology-pillars-of-sustainable-digital-scholarship",
    "href": "chapter_ai-nepi_008.html#opensciencetechnology-pillars-of-sustainable-digital-scholarship",
    "title": "Modeling Science",
    "section": "8.9 OpenScienceTechnology: Pillars of Sustainable Digital Scholarship",
    "text": "8.9 OpenScienceTechnology: Pillars of Sustainable Digital Scholarship\n\n\n\nSlide 10\n\n\nSlide 10, “Technical Support: OpenScienceTechnology,” outlines the foundational principles governing the technical and operational philosophy of the presented scholarly support system. These principles—Open Source, Open Access, Open Data (supported by components like the Model Context Protocol (MCP) API Server), and Open Collaboration—are central to ensuring the longevity and utility of digital scholarly achievements. The speaker underscored the critical importance of a maintained infrastructure for scientific achievements, citing the 110-year legacy of the Euler edition as a testament to sustained scholarly work. This long-term perspective highlights the necessity of these open principles to prevent digital projects from “evaporating” once initial funding ceases, a prevalent issue in digital humanities.\nA key aspect of this framework is the commitment to Open Access and Open Data, which directly addresses common pitfalls in digital scholarship. The speaker emphasised the proactive resolution of copyright issues, contrasting this with traditional publishing models that often impose exclusive rights, leading to project “dead ends.” This commitment ensures that valuable scholarly work remains accessible and usable for future research. Furthermore, the discussion on computational epistemology reinforced the need for ‘Open Data’ that is not merely available but also complete and structured. Whilst not inherently against unstructured texts, the system advocates for their use in a structured manner to facilitate precise and verifiable answers. This is particularly crucial for historical research where complete chronologies and negations demand robust, prepared data beyond the capabilities of current unstructured AI approaches like embeddings or RAG.\nThe principles of Open Source and Open Collaboration underpin the transferability and scalability of this approach across diverse scholarly domains. The speaker acknowledged the inherent friction between maintaining high quality and achieving broad scalability, suggesting that collaborative frameworks are essential to bridge this gap. The vision extends to developing computational epistemology where models can be explicitly instructed with rules for causal reasoning, propositional logic, and scientific experimental justification. This highlights how Open Collaboration in developing and applying such methodologies, coupled with careful data curation strategies for LLM applications, can enable more rigorous and reliable scholarly engagement with AI, ensuring that human expertise remains central whilst leveraging technological advancements. Ultimately, this comprehensive approach fosters a sustainable and ethically sound future for digital scholarship.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html",
    "href": "chapter_ai-nepi_009.html",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "",
    "text": "Overview\nThis chapter details a conceptual inquiry into the representation of Sustainable Development Goal (SDG)-related research within bibliometric databases, employing Large Language Models (LLMs) to detect inherent biases. Ottaviani and Stahlschmidt aimed to utilise LLMs as a technological tool for assessing biases in publications classified across three prominent bibliometric databases: Web of Science, Scopus, and OpenAlex. Their study highlights the critical, yet performative, role of these databases in the sociology of science, acknowledging their influence on academic behaviour, funding, and policy, whilst also noting their susceptibility to political and commercial interests.\nThe research team’s methodology involved training a pre-existing, open-source LLM, DistilGPT2, on a shared corpus of over 15 million publications classified by the three databases. This approach circumvented the resource-intensive process of training an LLM from scratch, whilst ensuring minimal prior semantic knowledge. The project specifically focused on five SDGs related to socio-economic inequalities: SDG4 (Quality Education), SDG5 (Gender Equality), SDG10 (Reduced Inequalities), SDG8 (Decent Work and Economic Growth), and SDG9 (Industry, Innovation, and Infrastructure).\nA key component of the research design involved crafting 80-120 specific prompts for each SDG, derived from their respective targets, to serve as a benchmark for compliance and bias detection. The fine-tuned LLM processed these prompts using various decoding strategies (top-k, nucleus, contrastive search), generating responses from which noun phrases were extracted. Analysis of these noun phrases across four dimensions—locations, actors, data/metrics, and focuses—revealed a systematic overlook in the bibliometric data. The LLM’s responses consistently neglected disadvantaged categories of individuals, the poorest countries, and sensitive, underrepresented topics explicitly addressed by SDG targets, instead concentrating on economically powerful and highly developed nations. This finding underscores the decisive impact of seemingly objective science-informed practices, such as bibliometric classification, on research representation and, consequently, on policy and resource allocation. The authors acknowledge limitations, including the general framework and the high sensitivity of LLMs to architectural choices, training data, and decoding strategies.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-background-and-aims",
    "href": "chapter_ai-nepi_009.html#research-background-and-aims",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.1 Research Background and Aims",
    "text": "9.1 Research Background and Aims\n\n\n\nSlide 02\n\n\nOttaviani and Stahlschmidt embarked on this project with the primary aim of employing Large Language Models (LLMs) as a technological instrument to discern biases. Specifically, they sought to identify biases originating from publications classified within three prominent bibliometric databases. This foundational objective guided their subsequent methodological development.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "href": "chapter_ai-nepi_009.html#sdg-classification-in-databases",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.2 SDG Classification in Databases",
    "text": "9.2 SDG Classification in Databases\n\n\n\nSlide 06\n\n\nBibliometric databases function as critical digital infrastructures, enabling extensive bibliometric analyses and impact assessments across the scientific community. These systems, however, exhibit a performative nature, fundamentally shaped by particular understandings of the scientific landscape and inherent value attributions, as scholarly works by Whitley (2000) and Winkler (1988) demonstrate. Consequently, they exert considerable influence over the conduct of academics, researchers, funding institutions, and policymakers alike. Crucially, these databases also reflect and respond to diverse political and commercial interests.\nIn recent years, leading bibliometric platforms, including Web of Science, Scopus, and OpenAlex, have implemented classifications designed to align publications with the United Nations Sustainable Development Goals (SDGs). Nevertheless, prior research, notably by Armitage et al. (2020), has revealed that SDG labelling practices across different providers—such as Elsevier, Bergen, and Aurora—produce markedly divergent results, exhibiting very limited overlap. These inconsistencies in classification can distort perceptions of research priorities, thereby potentially affecting resource allocation and policy decisions, often influenced by underlying political and commercial agendas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "href": "chapter_ai-nepi_009.html#case-study-sdgs-in-bibliometric-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.3 Case Study: SDGs in Bibliometric Data",
    "text": "9.3 Case Study: SDGs in Bibliometric Data\n\n\n\nSlide 08\n\n\nThis case study, conducted by Ottaviani and Stahlschmidt (2024), specifically investigates the aggregated effects on the representation of SDG-related research within bibliometric databases following the introduction of LLM-based tools. The authors adopted a methodological approach that involved deploying “little” pre-trained Large Language Models, notably DistilGPT2. They trained these models independently on distinct subsets of publication abstracts, each corresponding to the SDG classifications derived from various bibliometric databases. This LLM technology served a dual purpose: firstly, as a detector of inherent biases within the data; and secondly, as a proof-of-concept exercise demonstrating its potential for automating information extraction to inform research-related decision-making processes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "href": "chapter_ai-nepi_009.html#chain-of-dependencies",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.4 Chain of Dependencies",
    "text": "9.4 Chain of Dependencies\n\n\n\nSlide 10\n\n\nThe research delineates a partial chain of dependencies, illustrating the intricate relationships within the science-policy interface. Initially, SDG classification directly defines the scope and nature of “Research” on SDGs. Subsequently, a diverse array of stakeholders, including researchers, Small and Medium-sized Enterprises (SMEs), governments, and various intermediate figures, actively process this SDG-focused research. This processed research then critically informs “Decision-making to align with SDGs,” which, in turn, directly impacts “Socioeconomic inequalities.”\nCrucially, the study positions the LLM as a “detector of ‘biases’” operating at the level of “Research” on SDGs. The “Introduction of LLM in Research Policy” emerges as a direct consequence of this bias detection, and this intervention ultimately influences “Socioeconomic inequalities.” Fundamentally, LLMs possess the capacity to alter the metadata associated with SDG research, thereby profoundly influencing the advice provided, choices made, indicators developed, and measures implemented within policy frameworks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "href": "chapter_ai-nepi_009.html#actors-and-sdg-selection",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.5 Actors and SDG Selection",
    "text": "9.5 Actors and SDG Selection\nThe study focused on three principal bibliometric databases: Web of Science, a proprietary platform from Clarivate (US); Scopus, another proprietary service provided by Elsevier (UK); and OpenAlex, an open-access resource formerly associated with Microsoft (US). The authors strategically selected five Sustainable Development Goals to specifically model socio-economic inequalities. These included three SDGs representing the equity or socio dimension: SDG4 (Quality Education), SDG5 (Gender Equality), and SDG10 (Reduce Inequalities). Additionally, two SDGs were chosen to represent the economic and technological development dimension: SDG8 (Decent Work and Economic Growth) and SDG9 (Industry, Innovation, and Infrastructure).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#processed-data",
    "href": "chapter_ai-nepi_009.html#processed-data",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.6 Processed Data",
    "text": "9.6 Processed Data\nThe research team processed a substantial dataset, comprising a jointly indexed subset of 15,471,336 publications. They meticulously collected these publications by identifying those shared across all three bibliometric databases—Web of Science, Scopus, and OpenAlex—through precise DOI matching. This collection spanned a timeframe from January 2015 to July 2023. The project then assessed the performance of the three distinct classification standards for the five selected SDGs. Consequently, for each SDG, three separate publication subsets emerged, each attributed to a specific bibliometric database. This rigorous approach, applying classification solely to the shared corpus, established a robust benchmark for subsequent comparative analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-socio-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.7 SDG Classification: Socio Dimension",
    "text": "9.7 SDG Classification: Socio Dimension\nInitial results from the comparative analysis of SDG-classified papers, focusing on the socio dimension, strongly corroborate the findings of Armitage (2020), demonstrating a consistently minimal overlap in SDG labelling across the bibliometric databases.\nFor SDG 04, pertaining to Quality Education:\n\nWeb of Science identified 124,359 publications (19.1%).\nOpenAlex identified 218,907 (33.6%).\nScopus identified 339,063 (52.2%).\n\nThe overlaps comprised:\n\n59,002 (9.0%) between Web of Science and OpenAlex.\n35,733 (5.5%) between Web of Science and Scopus.\n35,733 (5.5%) between OpenAlex and Scopus.\nA mere 46,711 (7.2%) across all three platforms.\n\nRegarding SDG 05, Gender Equality:\n\nWeb of Science classified 37,324 publications (57.4%).\nOpenAlex classified 71,727 (9.4%).\nScopus classified 82,273 (12.7%).\n\nThe overlaps were similarly low:\n\n31,210 (4.8%) for Web of Science and OpenAlex.\n26,377 (4.1%) for Web of Science and Scopus.\n34,898 (5.4%) for OpenAlex and Scopus.\n38,066 (12.1%) for all three.\n\nA notable observation emerged for SDG 05: whilst Scopus contained publications relevant to this goal, it often did not classify them as such. Furthermore, Web of Science classified approximately 10% of its SDG 05 publications from the field of mathematics, including topics such as geometrical differential equations, suggesting potential misclassification or a failure to capture relevant content comprehensively.\nFor SDG 10, focused on Reducing Inequalities:\n\nWeb of Science identified 95,460 publications (12.2%).\nOpenAlex identified 213,419 (43.3%).\nScopus identified 236,665 (30.2%).\n\nThe overlaps were:\n\n25,277 (3.2%) for Web of Science and OpenAlex.\n22,540 (2.9%) for Web of Science and Scopus.\n18,850 (2.4%) for OpenAlex and Scopus.\nOnly 10,853 (1.3%) across all three databases.\n\nThese figures consistently highlight the significant discrepancies in how different bibliometric databases classify SDG-related research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "href": "chapter_ai-nepi_009.html#sdg-classification-economic-dimension",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.8 SDG Classification: Economic Dimension",
    "text": "9.8 SDG Classification: Economic Dimension\nThe comparative analysis extended to the economic dimension of SDG-classified papers, revealing similar patterns of limited overlap.\nFor SDG 08, focusing on Decent Work and Economic Growth:\n\nWeb of Science identified 82,366 publications (19.0%).\nOpenAlex identified 121,106 (28.0%).\nScopus identified 183,641 (37.8%).\n\nThe overlaps comprised:\n\n16,268 (3.8%) for Web of Science and OpenAlex.\n8,020 (1.9%) for Web of Science and Scopus.\n39,071 (9.0%) for OpenAlex and Scopus.\n10,853 (2.5%) across all three databases.\n\nSimilarly, for SDG 09, concerning Industry, Innovation, and Infrastructure:\n\nWeb of Science classified 230,883 publications (30.3%).\nOpenAlex classified 217,822 (28.6%).\nScopus classified 200,566 (26.4%).\n\nOverlaps included:\n\n25,679 (3.4%) for Web of Science and OpenAlex.\n26,501 (3.5%) for Web of Science and Scopus.\n44,702 (5.9%) for OpenAlex and Scopus.\n15,186 (2.0%) for all three.\n\nThese figures consistently underscore the significant discrepancies in how different bibliometric databases classify SDG-related research, even within the economic dimension.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "href": "chapter_ai-nepi_009.html#llm-technology-and-key-findings",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.9 LLM Technology and Key Findings",
    "text": "9.9 LLM Technology and Key Findings\nOttaviani and Stahlschmidt modelled the Large Language Model (LLM) technology with a specific objective: to develop an LLM whose knowledge base originated exclusively from publications classified under a particular Sustainable Development Goal by a given bibliometric database. Recognising the substantial resources required to train an LLM from scratch, they adopted a pragmatic compromise: fine-tuning an existing, pre-trained, open-source LLM. DistilGPT2 emerged as the chosen model, primarily due to its fundamental architecture and minimal prior knowledge, which ensured the absence of pre-existing semantic understanding concerning either publications or prompts.\nThe primary finding from this investigation highlights a systematic overlook within the data, specifically regarding certain actors, the world’s poorest countries, and various underrepresented topics. However, the authors acknowledge several limitations. Their framework remains general, implying that specific applied cases might yield different outcomes. Moreover, LLMs inherently demonstrate high sensitivity to their underlying model architecture, the characteristics of their training data, the chosen (hyper-)parameters, and the decoding strategies employed. Whilst Ottaviani and Stahlschmidt partially accounted for training data variability by incorporating three distinct databases, and for decoding strategies by utilising three methods from existing literature, the need for more advanced LLM architectures persists.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "href": "chapter_ai-nepi_009.html#sdg-4-quality-education-targets",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.10 SDG 4: Quality Education Targets",
    "text": "9.10 SDG 4: Quality Education Targets\nSustainable Development Goal 4 mandates the assurance of inclusive and equitable quality education, alongside the promotion of lifelong learning opportunities for all. A series of specific targets underpins this overarching goal, each outlining a crucial aspect of educational development.\nFor instance:\n\nTarget 4.1 stipulates that by 2030, all girls and boys must complete free, equitable, and high-quality primary and secondary education, leading to demonstrably relevant and effective learning outcomes.\nTarget 4.2 focuses on ensuring universal access to quality early childhood development, care, and pre-primary education by the same year, preparing children for primary schooling.\nTarget 4.3 aims for equal access to affordable and quality technical, vocational, and tertiary education, including university, for all women and men by 2030.\nTarget 4.4 seeks to substantially increase the number of young people and adults possessing relevant skills, particularly technical and vocational proficiencies, for employment, decent work, and entrepreneurship.\nTarget 4.5 addresses the elimination of gender disparities in education and guarantees equal access to all educational levels and vocational training for vulnerable populations, including individuals with disabilities, indigenous peoples, and children in precarious situations.\nTarget 4.6 commits to ensuring that all youth and a significant proportion of adults, irrespective of gender, achieve literacy and numeracy by 2030.\n\nThese targets collectively form the framework of the 2030 Agenda for SDGs, as defined by the United Nations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "href": "chapter_ai-nepi_009.html#prompts-for-benchmarking",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.11 Prompts for Benchmarking",
    "text": "9.11 Prompts for Benchmarking\nTo establish a robust benchmarking system, Ottaviani and Stahlschmidt meticulously crafted prompts for each Sustainable Development Goal. Each SDG comprises a comprehensive list of between eight and twelve specific targets. For every individual target, the authors systematically developed ten diverse questions, or prompts, with each question designed to explore a distinct facet of that target. This rigorous process yielded a specific set of 80 to 120 prompts for each SDG, collectively serving as a critical benchmark or standard for both defining compliance with the SDGs and identifying potential biases within the data. For instance, prompts such as, “How can countries ensure that all girls and boys complete free, equitable and quality primary and secondary education by 2030?” interrogated Target 4.1, which aims to ensure that all girls and boys complete free, equitable, and quality primary and secondary education by 2030.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "href": "chapter_ai-nepi_009.html#research-design-database-and-sdg-specificity",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.12 Research Design: Database and SDG Specificity",
    "text": "9.12 Research Design: Database and SDG Specificity\nThe research design meticulously outlines the process for analysing SDG-related content within bibliometric databases. The workflow commenced with an initial input comprising a set of abstracts, each classified according to a specific SDG and originating from a particular database. The authors then applied a crucial fine-tuning process to this dataset, utilising the DistilGPT-2 model. The output of this fine-tuning was a specialised “Fine-tuned DistilGPT-2” model, tailored to each specific SDG and database combination.\nSubsequently, a dedicated set of prompts, crafted specifically for each SDG, served as input to the fine-tuned LLM. To generate diverse and relevant outputs, Ottaviani and Stahlschmidt applied three distinct decoding strategies: top-k, nucleus, and contrastive search. The LLM produced corresponding responses for each SDG and database, categorised by the decoding strategy employed. A final post-processing step involved applying a “prompts’ words filter” to these responses, ultimately yielding a refined set of noun phrases pertinent to each SDG and database.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "href": "chapter_ai-nepi_009.html#illustrative-example-sdg-4-analysis",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.13 Illustrative Example: SDG 4 Analysis",
    "text": "9.13 Illustrative Example: SDG 4 Analysis\nAs an illustrative example, the analysis of SDG 4 involved a systematic method of matching noun phrases, extracted from the LLM’s responses, directly against the specific SDG targets. Ottaviani and Stahlschmidt employed four critical data dimensions for this analysis: Locations, Actors, Data/Metrics, and Focuses. For each SDG, this comprehensive approach facilitated a dual assessment: firstly, evaluating the degree of compliance with its stated targets; and secondly, identifying any inherent biases within the data. Notably, the analysis consistently revealed distinct differences across the various bibliometric databases. A structured table organised these findings, categorising them by unique databases and indicating which targets were addressed or not addressed across the dimensions of locations, actors, data/metrics, and focuses.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "href": "chapter_ai-nepi_009.html#systematic-overlooks-by-the-llm",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.14 Systematic Overlooks by the LLM",
    "text": "9.14 Systematic Overlooks by the LLM\nDespite the LLM’s stimulation, its responses consistently overlooked several critical areas, revealing systematic biases within the underlying data. Geographically, the model rarely addressed African countries, with the notable exception of South Africa, and largely ignored other developing nations, including China (though this observation carried a degree of uncertainty). The model also largely overlooked least developed countries and small island developing states.\nIn terms of human actors, the LLM systematically overlooked vulnerable populations, persons with disabilities, indigenous peoples, and children in vulnerable situations. Furthermore, the model failed to adequately focus on several key thematic areas explicitly outlined in the SDGs. These included vocational training, scholarships, the establishment of safe, non-violent, inclusive, and effective educational environments, the promotion of sustainable lifestyles, human rights, the cultivation of a culture of peace and non-violence, global citizenship, and the appreciation of cultural diversity. Crucially, the LLM also neglected to address the fundamental concepts of free primary and secondary education, and tertiary education.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "href": "chapter_ai-nepi_009.html#cross-sdg-considerations",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.15 Cross-SDG Considerations",
    "text": "9.15 Cross-SDG Considerations\nAcross the five Sustainable Development Goals analysed, several consistent patterns emerged. Geographically, the LLM’s responses rarely addressed least developed countries, with South-Saharan Africa, for instance, receiving minimal attention in relation to SDG8. The United States maintained an undeniable dominance in location mentions, followed by South Africa and China as the most frequently cited countries, with the UK and Australia also featuring prominently.\nRegarding metrics, the LLM’s output included references to various data sources, such as the Demographic and Health Surveys (DHS) and the World Values Survey (WVS), alongside mentions of general metrics, indicators, and benchmarks. The discussed research methodologies spanned theoretical frameworks, empirical studies, thematic analysis, market dynamics, and macroeconomics. However, concerning human actors, the analysis consistently revealed a systematic overlook of discriminated and vulnerable categories across all SDGs. Furthermore, whilst the LLM addressed some SDG-specific focuses, it notably omitted the most sensitive topics, such as human trafficking, human exploitation, and migration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_009.html#case-study-summary",
    "href": "chapter_ai-nepi_009.html#case-study-summary",
    "title": "9  The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs",
    "section": "9.16 Case Study Summary",
    "text": "9.16 Case Study Summary\nThe case study’s findings reveal a critical insight: the integration of Large Language Models as an analytical AI tool, positioned between SDG classification and the policymaking process, exposes a systematic oversight within the scientific publications classified by SDGs. This oversight consistently neglects the most disadvantaged categories of individuals, the world’s poorest countries, and crucial underrepresented topics explicitly targeted by the SDGs. Conversely, the analysis demonstrates that economic superpowers and highly developing nations receive disproportionate attention. These results unequivocally highlight the profound and decisive impact of seemingly objective, science-informed practices, such as the bibliometric classification of SDGs, on the representation of global challenges.\nThe authors acknowledge several limitations. The LLM’s performance exhibits high sensitivity to its model architecture, the characteristics of its training data (though partially mitigated by incorporating three distinct databases), the chosen (hyper-)parameters, and the decoding strategies employed (which were also partially accounted for). Furthermore, the research operates within a general framework, suggesting that its applicability to highly specific contexts may require further investigation. Future work should therefore explore the development of more sophisticated LLM architectures to address these sensitivities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html",
    "href": "chapter_ai-nepi_010.html",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "",
    "text": "Overview\nThis chapter details an innovative approach to extracting citation data from law and humanities scholarship, a domain historically underserved by conventional bibliometric databases. The research team has developed a specialised methodology to overcome the challenges posed by complex, often multilingual, footnotes and the poor coverage of non-STEM fields in existing data sources. At the core of this solution lies the strategic leverage of Large Language Models (LLMs) and Vision Language Models (VLMs), underpinned by a meticulously crafted, TEI XML-encoded gold standard dataset.\nThe project directly addresses critical limitations inherent in current bibliometric tools, such as Web of Science, Scopus, and OpenAlex. These platforms frequently lack comprehensive coverage for pre-digital, non-English, or non-“A-journal” content, whilst also imposing prohibitive costs and restrictive licences. Furthermore, the intricate nature of humanities footnotes, often laden with commentary and varied citation styles, renders traditional machine learning tools largely ineffective.\nTo ensure the reliability of LLM-extracted data, the team has established a robust testing and evaluation framework. This framework relies on a high-quality gold standard dataset, comprising over 1,000 footnotes from 20 open-access articles across multiple languages and historical periods, yielding more than 1,500 references. This dataset is encoded in TEI XML, a well-established standard in digital humanities, which facilitates detailed contextual markup beyond mere reference management.\nA key technological contribution from the authors is “Llamore”, a lightweight Python package engineered for LLM-based reference extraction and performance evaluation. Llamore extracts citation data from raw text or PDFs, outputting TEI XML, and assesses extraction accuracy using the F1-score. It employs an unbalanced assignment problem solver to align extracted references with gold standard data, thereby maximising the total F1-score for robust evaluation.\nInitial evaluations demonstrate Llamore’s efficacy. When tested against the PLOS 1000 biomedical dataset, Llamore (using Gemini 2.0 Flash) achieved an F1-score of 0.62, comparable to Grobid’s 0.61. Crucially, on the specialised footnoted humanities dataset, Llamore significantly outperformed Grobid, achieving an F1-score of 0.45 compared to Grobid’s 0.14. Whilst Grobid remains more resource-efficient for its trained literature, Llamore proves three times more effective for complex footnoted content. This research paves the way for generating comprehensive citation graphs, enabling deeper insights into intellectual history, influence reconstruction, and the reception of ideas within the humanities.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "href": "chapter_ai-nepi_010.html#citation-graphs-in-intellectual-history",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.1 Citation Graphs in Intellectual History",
    "text": "10.1 Citation Graphs in Intellectual History\n\n\n\nSlide 02\n\n\nScholars primarily employ citation graphs to illuminate patterns and relationships within knowledge production, particularly in the realm of intellectual history. These analytical tools prove invaluable for reconstructing influences and meticulously measuring the reception of published ideas. Consequently, researchers can identify, for instance, the most-cited authors across specific timeframes. An illustrative example involves an analysis of the Journal of Law and Society, for which an interactive web application provides detailed insights.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "href": "chapter_ai-nepi_010.html#deficiencies-in-existing-bibliometric-databases-for-ssh",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.2 Deficiencies in Existing Bibliometric Databases for SSH",
    "text": "10.2 Deficiencies in Existing Bibliometric Databases for SSH\n\n\n\nSlide 03\n\n\nA significant challenge arises from the extremely poor coverage of historical Social Sciences and Humanities (SSH) within existing bibliometric data sources. The research team deems these databases, including prominent platforms such as Web of Science, Scopus, and OpenAlex, fundamentally unusable for the specific domain under investigation, primarily owing to their pervasive lack of relevant data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "href": "chapter_ai-nepi_010.html#limitations-of-commercial-and-open-bibliometric-databases",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.3 Limitations of Commercial and Open Bibliometric Databases",
    "text": "10.3 Limitations of Commercial and Open Bibliometric Databases\n\n\n\nSlide 04\n\n\nBeyond the general inadequacy, specific limitations plague both commercial and open bibliometric databases. Web of Science and Scopus, for instance, impose exorbitant costs and operate under highly restrictive licences; consequently, the authors advocate for a decisive shift away from reliance on these proprietary systems. Whilst OpenAlex offers the significant advantage of open access, it nonetheless presents considerable shortcomings for Social Sciences and Humanities research. Specifically, it frequently lacks comprehensive coverage for numerous “A-journals,” provides insufficient data from the pre-digital era, and typically omits content published in languages other than English.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "href": "chapter_ai-nepi_010.html#challenges-in-humanities-data-coverage-and-footnote-complexity",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity",
    "text": "10.4 Challenges in Humanities Data Coverage and Footnote Complexity\n\n\n\nSlide 05\n\n\nThe Zeitschrift für Rechtssoziologie, a German Journal for Law and Society established in 1980, exemplifies the pervasive issue of poor data coverage within the humanities. Its citation data, as observed in bibliometric databases, only demonstrates significant improvement after the year 2000, with minimal records available for the preceding decades. This deficiency stems from several factors. Primarily, the humanities attract less commercial interest compared to STEM, medicine, and economics, which typically dominate large bibliometric databases. Moreover, these databases prioritise the “impact factor” for scientific evaluation, a metric largely irrelevant to research in intellectual history.\nCrucially, the literature of interest in humanities scholarship frequently features highly complex footnotes, colloquially termed “footnotes from hell.” These are not mere citations; they often incorporate extensive commentary and various “messy” elements, all embedded within a considerable amount of non-citation “noise,” making automated extraction profoundly challenging.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "href": "chapter_ai-nepi_010.html#limitations-of-traditional-tools-and-promise-of-llms",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.5 Limitations of Traditional Tools and Promise of LLMs",
    "text": "10.5 Limitations of Traditional Tools and Promise of LLMs\nCreating training data presents a formidable challenge, necessitating a labour-intensive annotation process that demands substantial time investment. Furthermore, existing tools, particularly those reliant on traditional machine learning methods such as Conditional Random Forests, exhibit poor performance when confronted with complex footnotes. For instance, ExCite’s performance, as documented by Boulanger and Iurshina in 2022, reveals consistently low extraction and segmentation accuracies across various training datasets.\nNevertheless, Large Language Models (LLMs) offer a promising avenue for resolution. Early experiments conducted in 2022 with models like text-davinci-003 already demonstrated the significant power of LLMs in extracting references from highly unstructured textual data. Newer models promise even better results, whilst Vision Language Models (VLMs) extend this capability to direct PDF processing. The research team is currently exploring various methods to harness these advanced models effectively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "href": "chapter_ai-nepi_010.html#ensuring-trustworthiness-and-robust-evaluation",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.6 Ensuring Trustworthiness and Robust Evaluation",
    "text": "10.6 Ensuring Trustworthiness and Robust Evaluation\n\n\n\nSlide 13\n\n\nA fundamental concern centres on the trustworthiness of results generated by Large Language Models. Instances of LLMs fabricating non-existent citations, as exemplified by a lawyer’s disastrous use of ChatGPT in federal court, underscore this critical issue. Consequently, a guiding principle dictates against attempting to solve problems for which no validation data exists.\nTo address this, the authors necessitate a robust testing and evaluation solution. This solution comprises three essential components:\n\nA high-quality Gold Standard dataset.\nA flexible framework capable of adapting to the rapidly evolving technological landscape.\nSolid testing and evaluation algorithms designed to produce comparable metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "href": "chapter_ai-nepi_010.html#tei-annotated-gold-standard-dataset-development",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.7 TEI-Annotated Gold Standard Dataset Development",
    "text": "10.7 TEI-Annotated Gold Standard Dataset Development\n\n\n\nSlide 16\n\n\nThe research team has embarked upon compiling a comprehensive training dataset, specifically designed for dual utility as evaluation data, employing TEI XML encoding. This choice stems from TEI’s status as a well-established, meticulously specified, and comprehensive standard for text interchange within the humanities and digital editorics. Unlike more constrained bibliographical standards such as CSL or BibTeX, TEI encompasses a broader array of phenomena, extending beyond mere reference management to facilitate the encoding of contextual information, including citation intention. Furthermore, its adoption enables the integration of existing TEI XML corpora from various digital editorics projects, thereby supporting the testing of generalisation and robustness features.\nDespite its advantages, the TEI standard presents certain challenges, both conceptual—concerning the distinction between pointers and references—and technical—regarding constrained elements versus elliptic material. The dataset development process involves several stages: initially, capturing PDF screenshots; subsequently, segmenting the reference string to isolate the citation from surrounding non-reference text within footnotes; and finally, parsing the content into a structured data format, utilising TEI elements such as biblStruct, analytic, monogr, author, title, imprint, date, biblScope, and biblRef. This dataset is currently under active development.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "href": "chapter_ai-nepi_010.html#dataset-strategy-evolution-and-tooling-integration",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.8 Dataset Strategy Evolution and Tooling Integration",
    "text": "10.8 Dataset Strategy Evolution and Tooling Integration\nThe strategy for constructing this dataset has evolved significantly. Initially, the focus centred on compiling data directly relevant to the primary research question. More recently, however, the authors made a strategic decision to incorporate PDFs, thereby enabling the utilisation of Vision Language Model (VLM) mechanisms. The overarching aim now involves publishing the complete dataset, encompassing everything from the raw PDFs to the meticulously parsed references. To achieve this, the team is sourcing material from open-access journals. The current scope involves coding over 1,000 footnotes derived from 20 articles, spanning several languages and a broad historical timeframe, which are expected to yield more than 1,500 references. Notably, even multiple occurrences of the same work are encoded separately to capture their distinct contexts.\nA significant benefit of adopting the TEI XML standard lies in the extensive tooling available for this interoperable format. Grobid, a widely recognised tool for reference and information extraction, notably employs TEI XML for its training and evaluation processes. This alignment facilitates direct performance comparisons with Grobid and enables the provision of new training data to the Grobid team, fostering collaborative advancement.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "href": "chapter_ai-nepi_010.html#introducing-llamore-a-reference-extraction-and-evaluation-package",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package",
    "text": "10.9 Introducing Llamore: A Reference Extraction and Evaluation Package\nThe research team has developed “Llamore”, an acronym for Large LANguage MOdels for Reference Extraction, as a dedicated Python package. This tool serves a dual purpose: it extracts citation data from either raw text or PDF documents, leveraging multimodal Large Language Models, and subsequently evaluates the performance of this extraction. Specifically, Llamore processes textual or PDF inputs to generate references in TEI XML format, whilst also accepting gold standard references to produce an F1-score as an evaluation metric. The design of Llamore prioritises two key objectives: it remains lightweight, comprising fewer than 2,000 lines of code, and ensures broad compatibility with both open and closed Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "href": "chapter_ai-nepi_010.html#llamore-implementation-and-workflow",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.10 Llamore Implementation and Workflow",
    "text": "10.10 Llamore Implementation and Workflow\nLlamore is readily available on Pypi, enabling straightforward installation via pip. The extraction workflow commences with defining an extractor, which is contingent upon the specific Large Language Model selected. For instance, the OpenAI extractor offers broad compatibility, as many open model serving frameworks, including Olama and VLLM, provide OpenAI-compatible APIs. Subsequently, users supply either a PDF or raw text as input to this extractor, which then yields the extracted references. These references can then be exported conveniently to an XML file. For evaluation purposes, users import the dedicated F1 class and provide both the gold standard references and the extracted references to compute the macro average of performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "href": "chapter_ai-nepi_010.html#llamores-evaluation-methodology-f1-score-and-reference-alignment",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment",
    "text": "10.11 Llamore’s Evaluation Methodology: F1-Score and Reference Alignment\nLlamore employs the F1-score as its primary evaluation metric, a well-established standard for comparing structured data. This score represents the harmonic mean of Precision and Recall, where Precision is calculated as the ratio of matches to predicted elements, and Recall as the ratio of matches to gold elements. An F1-score of 1 signifies perfect extraction, whilst a score of 0 indicates no matches whatsoever. For instance, an extracted reference might align perfectly with a gold reference on fields such as analytic_title, monographic_title, surname, and publication_date, yet exhibit a mismatch in the forename due to a minor discrepancy like an extraneous dot in the gold standard.\nA more complex challenge arises in aligning multiple extracted references with their corresponding gold references. Llamore addresses this as an unbalanced assignment problem. The system computes F1-scores for every possible combination of extracted and gold references, subsequently constructing a matrix from these scores. It then leverages SciPy’s solver to maximise the total F1-score whilst ensuring a unique assignment between the extracted and gold references. Following this alignment, the individual F1-scores are macro-averaged to provide an overall performance metric.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "href": "chapter_ai-nepi_010.html#performance-evaluation-of-llamore",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.12 Performance Evaluation of Llamore",
    "text": "10.12 Performance Evaluation of Llamore\nPerformance evaluations of Llamore reveal distinct capabilities across different datasets. When tested against the PLOS 1000 dataset, comprising 1,000 PDFs from the biomedical field, Llamore (utilising Gemini 2.0 Flash) achieved an exact match F1-score of 0.62, closely aligning with Grobid’s score of 0.61. This indicates comparable performance on literature for which Grobid was specifically trained. However, it is crucial to note that Grobid maintains a significant advantage in resource efficiency, requiring orders of magnitude less computational power than Gemini.\nConversely, on the custom humanities dataset, which features complex footnoted literature, Llamore demonstrated a marked superiority. Grobid struggled considerably, yielding an F1-score of merely 0.14, whilst Llamore (Gemini 2.0 Flash) achieved a substantially higher score of 0.45. This represents approximately a threefold improvement in performance for the challenging domain of footnoted humanities scholarship.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "href": "chapter_ai-nepi_010.html#conclusion-and-takeaways",
    "title": "10  Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset",
    "section": "10.13 Conclusion and Takeaways",
    "text": "10.13 Conclusion and Takeaways\nGrobid remains the preferred choice for literature upon which it has been specifically trained, primarily owing to its significantly faster processing speed and reduced resource intensity. Conversely, Llamore, when paired with Gemini, demonstrates approximately three times better performance for the challenging domain of footnoted literature. This performance specifically pertains to pure reference extraction, excluding contextual or cross-referencing information.\nA critical aspect of utilising open-source databases, such as OpenAlex, involves the burden of quality assurance falling directly upon the user. The authors advise a cautious approach towards both OpenAlex and commercial databases like Web of Science and Scopus, as their quality assurance priorities may not align with specific research questions. Large Language Models, whilst powerful, can produce false positives by inventing citations, thus necessitating the attainment of reliable results before any large-scale application. The ultimate ambition extends beyond mere reference string extraction to obtaining reliable results for nuanced contextual information, such as whether a citation is approving or not.\nAnalysis reveals that Grobid’s poor performance on humanities data stems from its training data being out of distribution for this domain. Large Language Models, however, exhibit their own distinct failure modes. These include difficulty discerning ambiguous elements, such as whether a number represents a volume or a page, and being misled by capitalisation. They frequently misidentify personal names appearing within titles as authors and struggle with specialised terminology like Idem, Derselbe, passim, ibid, or n.d.. Furthermore, canonical citations found in fields such as Bible studies, Roman law, and classical literature, along with ellipses, abbreviations, and cross-references, present considerable challenges.\nThe requisite F1-score for a gold standard dataset is contingent upon the analytical ambition; a lower score might suffice for identifying broad tendencies, whilst high accuracy demands a much higher score. For less precise needs, such as fuzzy searching in bibliographical databases using only a title and author, an exact match may not be essential. Nevertheless, the current stage of research prioritises achieving highly reliable results to facilitate the extraction of richer contextual information. A human-in-the-loop approach has been considered for the dataset establishment workflow, where Llamore could pre-annotate data for subsequent human correction. However, intermediate stages, such as merely marking the referring string without full parsing, still necessitate substantial manual effort.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html",
    "href": "chapter_ai-nepi_011.html",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#overview",
    "href": "chapter_ai-nepi_011.html#overview",
    "title": "11  Science dynamics and AI",
    "section": "",
    "text": "This presentation details the development of an AI solution designed to facilitate interaction with scholarly papers, addressing the increasing volume of scientific information. Researchers from DANS, the data archive of the Royal Netherlands Academy of Arts and Sciences, and GESIS, an archive also engaged in research, collaborated on this project. Their primary objective involved constructing an AI system that enables users to \"chat\" with selected academic texts, thereby enhancing information retrieval processes.\n\n  The solution comprises two principal components: Ghostwriter, serving as the user interface, and EverythingData, which manages the underlying backend workflow. Ghostwriter functions as a novel information retrieval interface, allowing simultaneous interaction with both structured data (metaphorically, a \"librarian\" representing knowledge organisation systems) and natural language content (an \"expert\"). This approach aligns with the broader scientific discourse surrounding Retrieval Augmented Generation (RAG), specifically advocating for a GraphRAG methodology.\n\n  The system's core ingredients include a vector space, constructed from data file content encoded as embeddings via various machine learning algorithms and Large Language Models (LLMs), and a graph, which represents a metadata layer integrated with diverse ontologies and controlled vocabularies, including those for responsible AI, expressed using the Croissant ML standard. The vision centres on unifying these graph and vector representations within a single model, creating a local, distributed AI where the LLM acts as both an interface and a reasoning engine. This engine connects to a \"RAG library\" (the graph), navigates datasets, and consumes embeddings (vectors) as contextual information.\n\n  Implementation begins with ingesting a collection of articles, such as those from the *methods, data, analyses* (MDA) journal. EverythingData processes these articles, storing information in a vector store (Qdrant) and performing operations like term extraction, embedding construction, and enrichment. Crucially, the system couples these processes with knowledge graphs, thereby contextualising embeddings and enhancing their value. Users formulate natural language queries within the Ghostwriter interface, prompting the system to return a list of relevant documents and an explanatory summary.\n\n  To prevent hallucinations, the system explicitly states when information is not directly available within the provided text, offering users the option to add new papers. Backend operations involve an entity extraction pipeline that annotates terms with semantic meaning by mapping them to controlled vocabularies, transitioning from vector space to the knowledge graph. Entities are further linked to the Wikidata ontology, providing immediate multilingual support by enabling queries in various languages. The LLM then synthesises these extracted text pieces to produce the final explanatory output.\n\n  The pipeline demonstrates versatility, functioning with any collection, including webpages, RSS feeds, and trusted data repositories like Dataverse instances. This capability enables localised interaction with papers, supporting close reading and containing the search space by situating questions within specific areas of scientific knowledge. The system extracts information beyond explicit text and metadata, forging associations at both natural language and Knowledge Organisation System levels. The Ghostwriter interface specifically facilitates the identification of related documents and the refinement of queries.\n\n  A key innovation lies in decoupling knowledge from questions and papers, storing this knowledge externally (e.g., as Wikidata identifiers). This separation allows for benchmarking different models against established Knowledge Organisation Systems, fostering future advancements in scientific inquiry. The project actively collaborates with industry partners, including Google and Meta, to ensure the sustainability of this KOS-based approach. During a live demonstration, the Ghostwriter interface successfully processed queries regarding \"rational choice theory\" and \"utility in rational choice theory,\" providing accurate summaries and references, whilst showcasing its multilingual capabilities by processing a German-language paper. The system employs a 1 billion parameter LLM (Llama), capable of local execution, and segments papers into small, identifiable blocks, leveraging LLM techniques and knowledge graphs to predict relevant text segments. This local processing capability offers greater control and cost-effectiveness compared to reliance on large, external models, ultimately supporting human thought processes in formulating research questions rather than providing definitive answers. The system has already demonstrated scalability, handling approximately 300,000 documents for Harvard University.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#presentation-objectives-and-core-research-question",
    "href": "chapter_ai-nepi_011.html#presentation-objectives-and-core-research-question",
    "title": "11  Science dynamics and AI",
    "section": "11.1 Presentation Objectives and Core Research Question",
    "text": "11.1 Presentation Objectives and Core Research Question\n\n\n\nSlide 02\n\n\n    This presentation addresses a pivotal research question: whether an AI solution can be effectively constructed to facilitate interactive \"chatting\" with a curated selection of academic papers. The discussion commences by introducing fundamental concepts in information retrieval, exploring the intricate dynamics of human-machine interaction, and detailing the principles of Retrieval-Augmented Generation (RAG) within generative AI.\n\n    A specific use case, drawing upon articles from the *methods, data, analyses* (MDA) journal, illustrates the practical application of the developed system. The presentation then introduces the underlying workflow of a \"local\" or \"tailored AI solution,\" which comprises two key components: Ghostwriter, serving as the interface, and EverythingData, encompassing the comprehensive backend operations. Subsequent sections provide detailed illustrations of both front-end and back-end functionalities, culminating in a summary and an outlook on future developments.\n\n    The broader context for this endeavour stems from the pervasive challenge of managing the overwhelming flood of information in contemporary science. This initiative represents a collaborative effort between DANS, the data archive of the Royal Netherlands Academy of Arts and Sciences, and GESIS, an institution that combines archiving with active research.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-new-paradigm-for-information-retrieval",
    "href": "chapter_ai-nepi_011.html#the-ghostwriter-interface-a-new-paradigm-for-information-retrieval",
    "title": "11  Science dynamics and AI",
    "section": "11.2 The Ghostwriter Interface: A New Paradigm for Information Retrieval",
    "text": "11.2 The Ghostwriter Interface: A New Paradigm for Information Retrieval\n\n\n\nSlide 03\n\n\n    The Ghostwriter approach introduces a novel interface for information retrieval, fundamentally altering how users interact with data. This system conceptualises information access through a series of evolving metaphors, each representing increasing sophistication in interaction. Initially, querying a single database, akin to \"Me and a database,\" necessitates explicit knowledge of the schema and its typical values to yield results, embodying a classic information retrieval challenge.\n\n    Progressing beyond this, the interaction with \"Me and a librarian\" signifies engagement with a single data collection or space, where connected structured databases or graphs operate in the background. This \"librarian\" metaphor specifically refers to structured data, encompassing knowledge organisation systems (KOS) and pre-existing classifications. Further still, the advent of Large Language Models (LLMs) transforms the interaction into \"Me and a library\" or \"Me and a round of experts,\" where natural language becomes the primary mode of engagement.\n\n    Crucially, the Ghostwriter interface claims to enable simultaneous \"chatting with experts and librarians.\" This advanced interaction is facilitated by a local LLM, integrated with a target data collection or space, and embedded within a network of additional data interpretation sources via APIs. This innovative design allows users to leverage both structured knowledge and the nuanced understanding provided by natural language processing in a unified environment.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-and-graphrag-architecture",
    "href": "chapter_ai-nepi_011.html#retrieval-augmented-generation-rag-and-graphrag-architecture",
    "title": "11  Science dynamics and AI",
    "section": "11.3 Retrieval Augmented Generation (RAG) and GraphRAG Architecture",
    "text": "11.3 Retrieval Augmented Generation (RAG) and GraphRAG Architecture\n\n\n\nSlide 04\n\n\n    Scientifically, this project situates itself within the broader discourse of Retrieval Augmented Generation (RAG). A particularly insightful resource on this topic is Philip Rustle's paper from Neo4j, which offers a comprehensive introduction to the field. The system's architecture relies on two main ingredients: a vector space and a graph.\n\n    The vector space is meticulously constructed from the content of data files, with information encoded into embeddings. Various Machine Learning (ML) algorithms and diverse Large Language Models (LLMs) compute these embeddings. Concurrently, a graph component represents a sophisticated metadata layer. This graph integrates seamlessly with a range of ontologies and controlled vocabularies, notably including those pertaining to responsible AI, and adheres to the Croissant ML standard for its expression.\n\n    The overarching vision for this system involves the integration of both graph and vector representations into a singular, cohesive model, termed GraphRAG. This model is designed for implementation as a \"local\" Distributed AI, where the LLM performs a dual function: serving as the primary interface between human users and the AI, and operating as a robust reasoning engine. In practice, the LLM connects to a \"RAG library,\" which embodies the graph structure. Through this connection, the LLM navigates extensive datasets and consumes embeddings, which represent the vector component, as crucial contextual information.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-workflow",
    "href": "chapter_ai-nepi_011.html#ghostwriter-and-everythingdata-workflow",
    "title": "11  Science dynamics and AI",
    "section": "11.4 Ghostwriter and EverythingData Workflow",
    "text": "11.4 Ghostwriter and EverythingData Workflow\n    The Ghostwriter and EverythingData workflow commences with a designated collection of articles, exemplified by those from the *methods, data, analyses* (MDA) journal. This system, however, demonstrates versatility, accepting any collection of documents as its input. The \"EverythingData\" component, representing the upper part of the workflow, orchestrates a series of operations.\n\n    Initially, this component stores information within a vector store, specifically utilising Qdrant. Subsequently, it executes various processes, including term extraction, the construction of embeddings, and further enrichments. A critical aspect involves the integration with knowledge graphs, which couples the processed information to contextualise the embeddings. This integration significantly enhances the value of specific words, phrases, and embeddings by providing a richer context.\n\n    All processed data then converges into a unified vector space, termed the RAG-Graph. The Ghostwriter interface directly interacts with this RAG-Graph vector space, allowing users to formulate queries as natural language questions. In response, the system delivers a list of relevant documents and an explanatory summary, generated by the underlying machinery. A key design principle ensures that the system does not hallucinate; it precisely identifies the source of its information.\n\n    From an implementation perspective, the system meticulously splits each paper into small, manageable blocks, assigning a unique identifier to every block. Leveraging advanced LLM techniques, the system intelligently connects and retrieves these blocks, employing weights in conjunction with knowledge graphs to accurately predict which pieces of text will best respond to a specific question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-document-indexing-and-collection-management",
    "href": "chapter_ai-nepi_011.html#ghostwriter-document-indexing-and-collection-management",
    "title": "11  Science dynamics and AI",
    "section": "11.5 Ghostwriter: Document Indexing and Collection Management",
    "text": "11.5 Ghostwriter: Document Indexing and Collection Management\n\n\n\nSlide 06\n\n\n    Ghostwriter functions as a sophisticated tool for indexing documents and webpages, enabling their organisation into distinct collections. The system initially ingested articles from the *methods, data, analyses* (MDA) journal, creating a test collection of approximately 100 articles scraped directly from its website. This demonstrates the system's capacity to process and manage specific datasets.\n\n    Crucially, Ghostwriter exhibits considerable input flexibility, capable of ingesting virtually any content from the web, including spreadsheets, individual webpages, entire websites via crawling, and RSS feeds. A core design principle ensures that the system relies exclusively on the ingested source data, thereby preventing hallucinations. Rather than employing large, complex LLMs, the system leverages a more modest 1 billion parameter LLM to address intricate questions, achieving accuracy through its integration with knowledge graphs. The primary objective remains to furnish only factual information present within the paper, explicitly stating \"I don't know\" if the requested information is unavailable in the source material.\n\n    The user interface, accessible via the \"Ask Questions\" section on the GESIS website, provides an intuitive input field for queries. Users can readily create new collections, manage available collections (such as the pre-selected \"mda\" collection), and add new content through options like \"Single Webpage,\" \"Website Crawler,\" or \"RSS Feed.\" The Ghostwriter system, specifically for MDA papers, is accessible online via `https://gesis.now.museum`.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#ghostwriter-chatting-with-papers-and-hallucination-prevention",
    "href": "chapter_ai-nepi_011.html#ghostwriter-chatting-with-papers-and-hallucination-prevention",
    "title": "11  Science dynamics and AI",
    "section": "11.6 Ghostwriter: Chatting with Papers and Hallucination Prevention",
    "text": "11.6 Ghostwriter: Chatting with Papers and Hallucination Prevention\n    Ghostwriter's core functionality centres on enabling users to \"chat\" directly with academic papers. For instance, when presented with the query \"explain male breadwinner model to me,\" the system delivers a concise explanation alongside precise references. These references, unlike those often generated by general-purpose LLMs, are accurate and directly traceable to the original sources, such as \"The Past, Present and Future of Factorial Survey Experiments\" and \"Gender and Survey Participation.\"\n\n    A fundamental design principle ensures that the system does not hallucinate; it maintains an exact awareness of its information sources. This precision is achieved through a meticulous implementation strategy: each paper undergoes a process of segmentation into small, discrete blocks, with a unique identifier assigned to every block. The underlying LLM then employs sophisticated techniques to connect and retrieve these blocks, applying weights in conjunction with knowledge graphs to accurately predict which text segments are most pertinent to a given question. The interface clearly presents the answer, followed by a \"Sources\" section that lists the contributing papers, complete with a \"Chat\" button, title, reference URL, and a relevance score.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#iterative-query-refinement-and-content-addition",
    "href": "chapter_ai-nepi_011.html#iterative-query-refinement-and-content-addition",
    "title": "11  Science dynamics and AI",
    "section": "11.7 Iterative Query Refinement and Content Addition",
    "text": "11.7 Iterative Query Refinement and Content Addition\n\n\n\nSlide 08\n\n\n    The Ghostwriter system incorporates an iterative approach to prevent hallucinations, explicitly stating when information is unavailable. For example, if a user poses the question \"explain how data was collected on male breadwinner model,\" and the direct information is not present in the indexed texts, the system will respond by stating, \"According to the provided text, there is no direct information about how data was collected on the male breadwinner model,\" whilst potentially referring to other related articles.\n\n    This design encourages users to actively participate in refining the knowledge base. Should a user locate an article containing the missing information, they can utilise the \"Add Paper\" button to ingest it into the system. Consequently, upon subsequent queries regarding the same topic, the system will then be able to provide a comprehensive response, drawing upon the newly added content. The sources section continues to list relevant papers, such as \"The Past, Present and Future of Factorial Survey Experiments\" and \"Gender of Interviewer Effects,\" even when direct answers are not found for specific sub-queries.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#behind-the-screens-entity-extraction-and-multilinguality",
    "href": "chapter_ai-nepi_011.html#behind-the-screens-entity-extraction-and-multilinguality",
    "title": "11  Science dynamics and AI",
    "section": "11.8 Behind the Screens: Entity Extraction and Multilinguality",
    "text": "11.8 Behind the Screens: Entity Extraction and Multilinguality\n\n\n\nSlide 09\n\n\n    Behind the Ghostwriter interface, a sophisticated entity extraction pipeline operates, meticulously annotating terms with semantic meaning. This process involves mapping terms to controlled vocabularies, effectively transitioning data from the vector space into a structured knowledge graph. Crucially, the system links these extracted entities to broader knowledge graph representations, notably leveraging Wikidata. This linking is paramount as it establishes a \"ground truth,\" enabling the validation of LLM-generated answers and ensuring their accuracy.\n\n    Furthermore, the system provides immediate and robust multilinguality support. This capability is critical; it allows users to pose questions in one language, such as English, and receive reliable answers even when the source papers are written in entirely different languages, like Chinese or German. Ultimately, the LLM synthesises these disparate pieces of text to produce a coherent, explanatory summary.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#fact-extraction-and-knowledge-organisation-systems",
    "href": "chapter_ai-nepi_011.html#fact-extraction-and-knowledge-organisation-systems",
    "title": "11  Science dynamics and AI",
    "section": "11.9 Fact Extraction and Knowledge Organisation Systems",
    "text": "11.9 Fact Extraction and Knowledge Organisation Systems\n\n\n\nSlide 10\n\n\n    The fact extraction process within Ghostwriter meticulously dissects a user's query into smaller, manageable pieces. The system then maps this query to a comprehensive graph representation, whilst simultaneously annotating the query strings with specific \"facts.\" For instance, a query such as \"explain male breadwinner model to me\" prompts the extraction of pertinent facts, including \"gender roles societal expectations,\" \"male breadwinner model economic systems,\" and \"male breadwinner model social structures,\" amongst others.\n\n    The resulting graph representation encapsulates key concepts like \"gender roles,\" \"male breadwinner model,\" \"economic systems,\" \"patriarchal society,\" and \"feminine role,\" complete with their interrelationships and assigned importance scores. This entire mechanism operates as a dynamic Knowledge Organisation System (KOS), capable of repeated querying. Each iteration can generate new, deeper levels of detail beneath the initial terms, providing an increasingly granular understanding of the subject matter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#linking-entities-to-wikidata-for-multilingual-support",
    "href": "chapter_ai-nepi_011.html#linking-entities-to-wikidata-for-multilingual-support",
    "title": "11  Science dynamics and AI",
    "section": "11.10 Linking Entities to Wikidata for Multilingual Support",
    "text": "11.10 Linking Entities to Wikidata for Multilingual Support\n\n\n\nSlide 11\n\n\n    A crucial step in the Ghostwriter workflow involves linking all extracted entities directly to Wikidata. This process transforms free-form strings into standardised identifiers, which are inherently connected to a wealth of multilingual translations. Consequently, the system gains the profound ability to comprehend questions posed in various languages, ensuring that the underlying knowledge remains accessible regardless of the query's linguistic origin. The similarity measure, which quantifies the relevance of a link, is derived directly from the LLM embeddings. For example, the term \"Male\" might be linked to the Wikidata entity `Q12308941`, representing a male given name, with an associated similarity score of 0.3429.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#multilingual-query-translation",
    "href": "chapter_ai-nepi_011.html#multilingual-query-translation",
    "title": "11  Science dynamics and AI",
    "section": "11.11 Multilingual Query Translation",
    "text": "11.11 Multilingual Query Translation\n    The system's robust multilingual support treats the core query, such as \"bread winner model,\" as a conceptual entity rather than a fixed string. An integrated LLM, specifically Gemma3, then generates translations of this concept into hundreds of languages. This comprehensive set of translations subsequently forms the complete query submitted to the LLM. For instance, the concept is translated into Chinese as \"男性主要收入模式 (nánxì zhǔyào shōurù móshì),\" alongside translations into Czech, Danish, Dutch, English, French, German, Greek, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Slovak, Spanish, Swedish, and Ukrainian, ensuring broad linguistic coverage.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#summary-of-pipeline-capabilities-and-future-vision",
    "href": "chapter_ai-nepi_011.html#summary-of-pipeline-capabilities-and-future-vision",
    "title": "11  Science dynamics and AI",
    "section": "11.12 Summary of Pipeline Capabilities and Future Vision",
    "text": "11.12 Summary of Pipeline Capabilities and Future Vision\n\n\n\nSlide 13\n\n\n    The developed pipeline demonstrates remarkable versatility, functioning effectively with any collection of documents, whether sourced from webpages, websites, RSS feeds, or trusted data repositories such as Dataverse instances. This capability enables the creation of a \"semantic index\" and a localised chatbot for diverse collections, thereby supporting \"close reading\" by human researchers.\n\n    The system strategically contains the search space by situating user questions and their associated collections within specific, relevant areas of the networked scientific knowledge domain. Crucially, it facilitates the acquisition of information that extends beyond what is explicitly stated in the text or annotated in the metadata. This is achieved by forging intricate associations at both the natural language level and within Knowledge Organisation Systems (KOS), effectively simulating a dialogue with the \"experts\" or \"invisible colleges\" that underpin the scholarly papers.\n\n    The Ghostwriter interface itself offers dual functionalities: it assists users in identifying related documents and provides mechanisms for refining questions or queries. A significant innovation lies in the system's ability to decouple knowledge from specific questions and papers, storing this knowledge externally, for instance, as Wikidata identifiers. This separation establishes a robust \"ground truth,\" allowing researchers to benchmark different models by comparing the identifier lists they produce in response to identical questions. This method readily identifies models unsuitable for particular tasks, whilst simultaneously leveraging KOS for the benefit of future generations of scientists. The project actively collaborates with industry leaders, including Google and Meta, to ensure the long-term sustainability of this KOS-centric approach, which the developers firmly believe represents the future of information management in science.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_011.html#live-demonstration-querying-and-multilingual-capabilities",
    "href": "chapter_ai-nepi_011.html#live-demonstration-querying-and-multilingual-capabilities",
    "title": "11  Science dynamics and AI",
    "section": "11.13 Live Demonstration: Querying and Multilingual Capabilities",
    "text": "11.13 Live Demonstration: Querying and Multilingual Capabilities\n\n\n\nSlide 13\n\n\n    A live demonstration showcased the \"Ask Questions\" web interface developed by the GESIS Leibniz-Institut für Sozialwissenschaften. The interface displayed the \"mda\" collection, comprising 37,637 vectors, as the selected dataset.\n\n    When presented with the query \"what is the role of utility in the Rational Choice Theory,\" the system promptly provided a detailed explanation, citing various researchers and studies. The sources section accurately listed the contributing papers, including \"The measurement of utility and subjective probability\" and \"Responding to Socially Desirable and Undesirable Topics,\" complete with their respective reference URLs and scores. A subsequent query, \"explain utility in rational choice theory,\" yielded different, yet relevant, pieces of information, consistently pointing back to the same source papers.\n\n    The system also offers an API, enabling an automatic mode for building agentic architectures, where results can be collected and new insights from papers identified. Users can contribute new content via an \"Add page\" button, ensuring the knowledge base remains current. A compelling aspect of the demonstration involved the system's multilingual capability: whilst the query was posed in English, the primary source paper, \"Die Messung von Nutzen und subjektiven Wahrscheinlichkeiten,\" was entirely in German, save for its abstract. The system successfully processed this, demonstrating its linguistic versatility.\n\n    Technically, the system operates efficiently on a local computer, even accommodating paper training processes, utilising a compact 1 billion parameter LLM. This local deployment offers significant advantages in terms of control and cost-effectiveness, particularly for handling private or sensitive information. The system has already proven its scalability, having been developed for Harvard University to manage approximately 300,000 documents. The developers advocate for this local, controlled approach over a full reliance on external models like ChatGPT, emphasising its role in supporting the human thought process and aiding in the formulation of research questions, rather than merely providing definitive answers. The team actively seeks collaborations with external parties who have concrete research questions, offering resources for try-outs and facilitating the handover of the system for further tinkering and refinement.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Science dynamics and AI</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html",
    "href": "chapter_ai-nepi_012.html",
    "title": "12  RAG in HPSS",
    "section": "",
    "text": "Overview\nThis presentation explores the application and optimisation of Retrieval-Augmented Generation (RAG) systems within the humanities, particularly philosophy. The project team developed a RAG system to address the inherent limitations of Large Language Models (LLMs), such as their inability to access full texts, their tendency to hallucinate, and their restricted context windows. The system aims to provide accurate, detailed, and attributable answers to complex philosophical research questions by integrating verbatim corpora and specialised domain knowledge.\nThe core RAG architecture establishes a dedicated data source for document retrieval. This setup requires the integration of a pertinent corpus, such as the complete works of Aristotle or Einstein, to serve as the foundational knowledge base for the system.\nThe retrieval process involves identifying relevant documents from this source using semantic, hybrid, or classic search methods. These retrieved text chunks then augment the LLM’s prompts, enabling more precise and contextually rich responses. A key innovation lies in the system’s capacity to cite original sources, thereby resolving the attribution problem crucial for academic rigour.\nA practical implementation, the Stanford Encyclopedia of Philosophy (SEP) RAG system, serves as a compelling case study. Initial development revealed that a basic RAG configuration yielded suboptimal results, necessitating extensive qualitative study and iterative refinement. This optimisation process involved careful selection of generative LLMs and embedding models, alongside meticulous tuning of hyperparameters such as the number of retrieved documents (top-k), token limits, generation temperature, and chunk size. Notably, chunking by main sections, despite exceeding the embedding model’s typical cutoff, proved most effective for philosophical texts owing to their highly systematised structure.\nFurther enhancements included a reranking mechanism, where a generative LLM evaluates the relevance of retrieved texts to mitigate false positives, albeit at increased computational cost. The system’s frontend provides a comparative interface, displaying answers from both a standalone LLM and the RAG system, alongside a detailed overview of the retrieved texts.\nWhilst RAG systems offer significant advantages—including reduced hallucinations, enhanced detail, and source citation—they present distinct challenges. Their performance is highly dependent on domain-specific refinement and robust evaluation, which necessitates representative question sets and the indispensable involvement of domain experts. Furthermore, RAGs can paradoxically underperform on broad overview questions, as their focus on local information may obscure wider perspectives. This highlights a future need for more flexible, potentially agentic, RAG systems capable of discerning question types and adapting their retrieval strategies accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry",
    "href": "chapter_ai-nepi_012.html#addressing-llm-limitations-in-philosophical-inquiry",
    "title": "12  RAG in HPSS",
    "section": "12.1 Addressing LLM Limitations in Philosophical Inquiry",
    "text": "12.1 Addressing LLM Limitations in Philosophical Inquiry\nPhilosophers frequently pose complex research questions demanding high linguistic and semantic accuracy. For instance, inquiries might concern Aristotle’s theory of matter as presented in his Physics, or the evolution of Einstein’s concept of locality from his early works on relativity through to his 1948 paper, Quantenmechanik und Wirklichkeit. Whilst Large Language Models (LLMs) like ChatGPT can offer reasonably differentiated answers to such queries, they exhibit inherent limitations. Retrieval-Augmented Generation (RAG) systems offer a compelling architectural solution, precisely engineered to surmount these challenges.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#core-rag-system-architecture",
    "href": "chapter_ai-nepi_012.html#core-rag-system-architecture",
    "title": "12  RAG in HPSS",
    "section": "12.2 Core RAG System Architecture",
    "text": "12.2 Core RAG System Architecture\nThe fundamental RAG architecture establishes a dedicated data source from which documents are retrieved. This setup requires the integration of a pertinent corpus, such as the complete works of Aristotle or Einstein, to serve as the foundational knowledge base for the system.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#document-retrieval-and-prompt-augmentation",
    "href": "chapter_ai-nepi_012.html#document-retrieval-and-prompt-augmentation",
    "title": "12  RAG in HPSS",
    "section": "12.3 Document Retrieval and Prompt Augmentation",
    "text": "12.3 Document Retrieval and Prompt Augmentation\nDocument retrieval within the RAG framework typically utilises semantic search, though hybrid or classic search methods also present viable alternatives. Crucially, the system augments the LLM’s prompts with the most relevant text chunks identified during this retrieval phase. This process ensures the language model receives highly pertinent contextual information for generating its responses.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#addressing-the-problem-of-access-and-llm-training-limitations",
    "href": "chapter_ai-nepi_012.html#addressing-the-problem-of-access-and-llm-training-limitations",
    "title": "12  RAG in HPSS",
    "section": "12.4 Addressing the Problem of Access and LLM Training Limitations",
    "text": "12.4 Addressing the Problem of Access and LLM Training Limitations\n\n\n\nSlide 06\n\n\nRAG systems primarily address the LLM’s limited access to full texts. Whilst extensive corpora may inform an LLM’s training data, the model cannot directly access or reliably quote specific chapters or papers during inference. This limitation frequently results in hallucination, wherein the model fabricates information. Although online search functionalities can sometimes enable quoting, copyright restrictions, as encountered with the EPR paper, often impede direct reproduction. Fundamentally, LLM training mechanisms are engineered to acquire generalisable statistical rules for text production, not to memorise texts verbatim. Indeed, explicit mechanisms within their design actively preclude these systems from verbatim reproduction.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#context-window-limitations-and-philosophical-research-needs",
    "href": "chapter_ai-nepi_012.html#context-window-limitations-and-philosophical-research-needs",
    "title": "12  RAG in HPSS",
    "section": "12.5 Context Window Limitations and Philosophical Research Needs",
    "text": "12.5 Context Window Limitations and Philosophical Research Needs\nPhilosophical research inherently necessitates direct engagement with original text sources, requiring deep analysis of their most fine-grained formulations. This necessity directly challenges the issue of LLMs’ limited context windows. RAG systems provide a vital solution by furnishing the language model with only the most relevant text chunks, thereby circumventing context window constraints whilst ensuring access to the precise textual evidence required for rigorous philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#solving-the-attribution-problem-and-general-applications",
    "href": "chapter_ai-nepi_012.html#solving-the-attribution-problem-and-general-applications",
    "title": "12  RAG in HPSS",
    "section": "12.6 Solving the Attribution Problem and General Applications",
    "text": "12.6 Solving the Attribution Problem and General Applications\nRAG systems efficaciously resolve the attribution problem by citing the sources of all provided text chunks. This capability forms the basis for a broader application: enabling users to “chat” with philosophical corpora, such as Locke’s complete works. Their objective is to facilitate interactions yielding significantly more detailed domain knowledge and a verifiable verbatim text basis, thereby surpassing the capabilities of standard LLMs like ChatGPT.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#didactic-applications-of-rag-systems",
    "href": "chapter_ai-nepi_012.html#didactic-applications-of-rag-systems",
    "title": "12  RAG in HPSS",
    "section": "12.7 Didactic Applications of RAG Systems",
    "text": "12.7 Didactic Applications of RAG Systems\nRAG systems present substantial didactic utility, furnishing an instructive methodology for students to engage deeply with complex texts. For instance, students approaching Locke’s Essay Concerning Human Understanding may commence their study by querying the system for Locke’s general ideas, then progressively delve into more specific topics such as his epistemology or theory of matter. This iterative questioning cultivates a structured and efficacious learning progression, guiding students into profound textual engagement.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#research-applications-factual-lookups",
    "href": "chapter_ai-nepi_012.html#research-applications-factual-lookups",
    "title": "12  RAG in HPSS",
    "section": "12.8 Research Applications: Factual Lookups",
    "text": "12.8 Research Applications: Factual Lookups\n\n\n\nSlide 11\n\n\nBeyond didactic uses, RAG systems possess considerable significance for academic research, particularly for factual lookups within handbooks. This functionality directly supplants the traditional method of manually consulting physical books to find relevant pages for orientation, remarks, or footnotes. Crucially, RAG systems surmount the inherent unreliability of standalone LLMs for such precise factual lookups, ensuring the information retrieved is both accurate and verifiable.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advanced-research-applications-and-future-vision",
    "href": "chapter_ai-nepi_012.html#advanced-research-applications-and-future-vision",
    "title": "12  RAG in HPSS",
    "section": "12.9 Advanced Research Applications and Future Vision",
    "text": "12.9 Advanced Research Applications and Future Vision\n\n\n\nSlide 14\n\n\nRAG systems broaden their research utility to encompass the exploration of previously unexamined corpora, though this first requires digitisation of the texts. Furthermore, they enable the identification of specific passages for close reading, directly pertinent to a researcher’s inquiry. Ultimately, these systems possess the potential to furnish detailed answers to at least portions of complex research questions. This comprehensive vision aims to elucidate the transformative capabilities RAG systems could bring to future philosophical research.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#the-stanford-encyclopedia-of-philosophy-rag-example",
    "href": "chapter_ai-nepi_012.html#the-stanford-encyclopedia-of-philosophy-rag-example",
    "title": "12  RAG in HPSS",
    "section": "12.10 The Stanford Encyclopedia of Philosophy RAG Example",
    "text": "12.10 The Stanford Encyclopedia of Philosophy RAG Example\nThe project team has developed an exemplary RAG system employing the Stanford Encyclopedia of Philosophy (SEP) as its primary data source. The SEP, a widely recognised online philosophical handbook, furnishes a robust and authoritative corpus for this application.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#project-aims-and-initial-findings",
    "href": "chapter_ai-nepi_012.html#project-aims-and-initial-findings",
    "title": "12  RAG in HPSS",
    "section": "12.11 Project Aims and Initial Findings",
    "text": "12.11 Project Aims and Initial Findings\nThe project team commenced by scraping the Stanford Encyclopedia’s content into Markdown format, with the initial objective of crafting a practical tool for the philosophical community. However, early trials uncovered a critical insight: a conventional RAG setup, as described in standard textbooks (comprising only retrieval and generation components), yielded answers inferior to those generated by a standalone ChatGPT. This unexpected outcome necessitated a revision of the project’s focus, pivoting towards a qualitative study dedicated to optimising RAG system configurations specifically for philosophical applications.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimisation-strategies-and-evaluation-challenges",
    "href": "chapter_ai-nepi_012.html#optimisation-strategies-and-evaluation-challenges",
    "title": "12  RAG in HPSS",
    "section": "12.12 Optimisation Strategies and Evaluation Challenges",
    "text": "12.12 Optimisation Strategies and Evaluation Challenges\nImproving RAG system performance demands a multifaceted approach, encompassing meticulous refinement and the integration of algorithmic sophistication. Key areas for adjustment involve selecting appropriate generative LLMs and embedding models, alongside fine-tuning various hyperparameters. These parameters include the number of documents retrieved (top-k), the maximum input and output token lengths, the temperature or top-p settings for generation, and the chunk size and overlap. Furthermore, addressing methodological challenges such as retrieval semantic mismatch frequently necessitates more intricate algorithms, including reranking.\nThe overarching methodological approach entails a theoretically grounded iterative process, systematically assessing which measures enhance answer quality. This process is particularly challenging in philosophy, where answers typically manifest as complex, unstructured text rather than atomic facts, such as “Wittgenstein’s last place of living.” Consequently, the authors must rigorously assess whether the generated propositions accurately reflect the underlying facts, a task requiring sophisticated evaluation criteria.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#the-crucial-role-of-sound-evaluation",
    "href": "chapter_ai-nepi_012.html#the-crucial-role-of-sound-evaluation",
    "title": "12  RAG in HPSS",
    "section": "12.13 The Crucial Role of Sound Evaluation",
    "text": "12.13 The Crucial Role of Sound Evaluation\nSound evaluation is paramount for the effective deployment of RAG systems. Nevertheless, assessing the accuracy and quality of complex philosophical propositions poses a considerable challenge, necessitating robust and nuanced evaluation methodologies.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#sep-rag-system-frontend-and-configuration",
    "href": "chapter_ai-nepi_012.html#sep-rag-system-frontend-and-configuration",
    "title": "12  RAG in HPSS",
    "section": "12.14 SEP RAG System Frontend and Configuration",
    "text": "12.14 SEP RAG System Frontend and Configuration\nThe SEP RAG system integrates a user-friendly frontend with a Python-based backend. The frontend features a ‘Configuration’ section, complete with an ‘Initialize’ button, alongside an ‘Options’ section. Within these options, users can select a generative model, with gpt-4o-mini being the default choice, and configure prompt token limits (e.g., model limit of 128,000, system limit of 15,000). A ‘Persona’ setting, which defines the LLM as “an expert philosopher” who answers “meticulously and precisely,” guides the generation process. Users further specify the number of texts to retrieve, typically set at 15. A dedicated ‘Philosophical Question’ text box enables users to input queries, such as “What is priority monism?”, before initiating the answer generation process with a ‘Generate answer’ button.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#comparative-output-display",
    "href": "chapter_ai-nepi_012.html#comparative-output-display",
    "title": "12  RAG in HPSS",
    "section": "12.15 Comparative Output Display",
    "text": "12.15 Comparative Output Display\nThe system features a comparative output display for generated answers. The left side, serving as the benchmark, presents the answer produced by a standalone LLM, such as ChatGPT. Conversely, the right side showcases the answer generated by the RAG system. This side-by-side presentation considerably facilitates the comparison of answer quality between the two approaches.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#retrieved-texts-overview",
    "href": "chapter_ai-nepi_012.html#retrieved-texts-overview",
    "title": "12  RAG in HPSS",
    "section": "12.16 Retrieved Texts Overview",
    "text": "12.16 Retrieved Texts Overview\nThe system’s output includes a crucial component: a comprehensive list of all retrieved texts. For each text, the system furnishes detailed information, encompassing article names, relevant section headings, the text’s length in tokens, the total token count, and its inclusion status—indicating whether the text was fully incorporated into the prompt or truncated owing to prompt limitations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-texts",
    "href": "chapter_ai-nepi_012.html#optimising-chunk-size-for-philosophical-texts",
    "title": "12  RAG in HPSS",
    "section": "12.17 Optimising Chunk Size for Philosophical Texts",
    "text": "12.17 Optimising Chunk Size for Philosophical Texts\nOptimising chunk size constitutes a critical hyperparameter tuning step. Initially, three primary chunking options emerged: a fixed number of words (e.g., 500 tokens or words), paragraphs, or sections (at various hierarchical levels). Surprisingly, chunking the content into main sections, inclusive of their headings, consistently produced the most favourable outcomes. This outcome accords with the nature of philosophical facts, which seldom manifest as brief, isolated statements; instead, they often require substantial textual space for comprehensive presentation, thereby favouring longer semantic units.\nA significant observation emerged from this optimisation: the average length of these optimal sections, approximately 3,000 words, substantially exceeded the embedding model’s typical cutoff of 512 words. This counter-intuitive success likely derives from the highly systematised structure of the Stanford Encyclopedia of Philosophy. Within such meticulously organised documents, the initial 500 words of a section frequently encapsulate its central themes and main ideas. However, this specific chunking strategy may not prove efficacious for more heterogeneous or less rigorously structured texts.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#reranking-for-enhanced-relevance",
    "href": "chapter_ai-nepi_012.html#reranking-for-enhanced-relevance",
    "title": "12  RAG in HPSS",
    "section": "12.18 Reranking for Enhanced Relevance",
    "text": "12.18 Reranking for Enhanced Relevance\nReranking represents an additional, pivotal step within the retrieval process. This technique mitigates the issue of false positives, where not all initially retrieved texts prove pertinent to the user’s query. Reranking’s primary objective is to reorder documents according to their genuine relevance. A generative LLM (gLLM) conducts this evaluation, furnishing a more advanced semantic differentiation capability than the initial embedding model. The gLLM evaluates texts based on scoring categories such as informativeness and the length of relevant passages, subsequently calculating a total score for each document. Whilst this method yields highly favourable results in terms of relevance, it substantially increases computational expenditure.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#results-and-discussion-overview",
    "href": "chapter_ai-nepi_012.html#results-and-discussion-overview",
    "title": "12  RAG in HPSS",
    "section": "12.19 Results and Discussion Overview",
    "text": "12.19 Results and Discussion Overview\nThis section offers a comprehensive overview of the results and discussion concerning RAG systems, with a particular focus on their distinct advantages.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_012.html#advantages-cautions-and-challenges-of-rag-systems",
    "href": "chapter_ai-nepi_012.html#advantages-cautions-and-challenges-of-rag-systems",
    "title": "12  RAG in HPSS",
    "section": "12.20 Advantages, Cautions, and Challenges of RAG Systems",
    "text": "12.20 Advantages, Cautions, and Challenges of RAG Systems\nRAG systems present compelling advantages for academic inquiry. They seamlessly integrate verbatim corpora with specialised domain knowledge, consequently yielding more detailed answers whilst significantly reducing the incidence of hallucinations. Furthermore, these systems enable the citation of pertinent documents, a critical feature for academic integrity. Collectively, these capabilities render RAG configurations exceptionally well-suited for assisting across a broad spectrum of scientific tasks.\nNevertheless, several caveats warrant consideration. RAG systems inherently necessitate extensive refinement; optimal configurations vary considerably depending on the specific corpus and the nature of the questions posed. Crucially, robust evaluation remains paramount, demanding a representative set of questions and their corresponding expected answers. This process highlights the indispensable role of domain experts, particularly when engaging with unexplored corpora.\nSignificant challenges also persist. Should the system fail to retrieve pertinent documents, the quality of the generated answer demonstrably diminishes, thereby necessitating prompt adjustment. Paradoxically, RAG systems often yield suboptimal results for widely discussed overview questions, such as “What are the central arguments against scientific realism?” This phenomenon arises because RAGs tend to concentrate on the local information they retrieve, which can inadvertently obscure the broader perspective essential for comprehensive overview responses. Addressing this limitation necessitates the development of more adaptable systems, including agentic RAG systems, capable of discerning distinct question types and adjusting their retrieval and generation strategies accordingly.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>RAG in HPSS</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html",
    "href": "chapter_ai-nepi_015.html",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "",
    "text": "Overview\nThis chapter presents a novel approach to fundamental questions in the philosophy of science, specifically examining the landscape of quantum gravity research. The authors have developed a sophisticated methodology, integrating advanced linguistic analysis with social network analysis, to reconstruct the intellectual and social structures of scientific fields. Their core objective involves empirically determining whether quantum gravity research exemplifies “plural pursuit”—a state where distinct, concurrent instances of normal science, each articulated by a specific community and intellectual disciplinary matrix, collectively strive towards a common problem-solving goal.\nThe methodology commences with the meticulous collection of 228,748 abstracts and titles from theoretical physics literature, sourced from Inspire HEP. A two-step clustering pipeline then processes this extensive dataset. Linguistic analysis employs the Bertopic pipeline for spatialisation into an embedding space, followed by unsupervised clustering to identify 611 fine-grained topics. Subsequently, the authors assign each physicist a specialty based on their most common topic. Concurrently, social network analysis constructs a co-authorship graph, applying community detection to identify 819 distinct communities from a network of 30,000 physicists.\nRecognising the inherent scale-dependent nature of computational topics and communities, the project introduces a hierarchical clustering approach for both linguistic and social structures. An adaptive topic coarse-graining strategy, guided by the Minimum Description Length (MDL) criterion, refines the initial 600 topics to a more manageable 50. This process preserves only those linguistic nuances crucial for understanding the social structure. The authors then rigorously compare this bottom-up reconstruction with physicists’ top-down intuitions, gathered via a survey of the International Society for Quantum Gravity’s founding members. A Support Vector Machine (SVM) classifier, trained on hand-coded labels and text embeddings, facilitates this crucial comparison.\nKey findings indicate that whilst some top-down approaches align well with emergent bottom-up topics, others—particularly phenomenological or less conceptually autonomous frameworks—do not. Notably, the analysis reveals a significant overlap between “String Theory” and “Supergravity” within the bottom-up clusters. This convergence with expert intuition underscores the practical inseparability of these communities, despite their distinct historical and conceptual foundations. The work concludes by asserting that powerful computational methods offer a robust means to revisit and challenge long-held philosophical insights, effectively positioning computation as a continuation of philosophical inquiry.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#introduction-to-plural-pursuit-in-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.1 Introduction to Plural Pursuit in Quantum Gravity",
    "text": "13.1 Introduction to Plural Pursuit in Quantum Gravity\n\n\n\nSlide 02\n\n\nThis evolving research project, a collaborative endeavour with Mike Schneider from the University of Missouri, addresses fundamental questions within the philosophy of science. The authors strategically combine contemporary analytical methods, previously discussed within the academic community, with social network analysis techniques. Their investigation centres on quantum gravity as a crucial case study. The team aims to construct a comprehensive bottom-up reconstruction of the quantum gravity research landscape, subsequently confronting this empirical model with the established intuitions of physicists regarding their field’s inherent structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#research-methodology-and-structure",
    "href": "chapter_ai-nepi_015.html#research-methodology-and-structure",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.2 Research Methodology and Structure",
    "text": "13.2 Research Methodology and Structure\n\n\n\nSlide 03\n\n\nThis chapter systematically unfolds in three distinct stages. Initially, it introduces the quantum gravity case study, establishing the philosophical framework that underpins the research. Subsequently, the discussion progresses to propose a detailed bottom-up reconstruction of the quantum gravity research landscape. The final stage involves a critical confrontation between this empirically derived bottom-up model and the prevailing intuitions of physicists concerning the organisation of their field.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#the-problem-of-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.3 The Problem of Quantum Gravity",
    "text": "13.3 The Problem of Quantum Gravity\n\n\n\nSlide 04\n\n\nA persistent and profound challenge in contemporary fundamental physics involves formulating a coherent quantum theory of gravity. This endeavour fundamentally seeks to reconcile existing knowledge of phenomena at minuscule scales with observations pertaining to very large scales, thereby bridging a significant conceptual divide in theoretical physics.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#plurality-of-approaches-to-quantum-gravity",
    "href": "chapter_ai-nepi_015.html#plurality-of-approaches-to-quantum-gravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.4 Plurality of Approaches to Quantum Gravity",
    "text": "13.4 Plurality of Approaches to Quantum Gravity\n\n\n\nSlide 06\n\n\nNumerous solutions have been proposed to address the quantum gravity problem, with String Theory emerging as the most prominent amongst them. Other notable attempted solutions encompass Supergravity, Loop Quantum Gravity, Spin Foams, Causal Set Theory, and Asymptotic Safety. To comprehensively account for this diverse landscape of research, the authors introduce the concept of “plural pursuit.”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#defining-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.5 Defining Plural Pursuit",
    "text": "13.5 Defining Plural Pursuit\n\n\n\nSlide 07\n\n\nPlural pursuit designates situations characterised by distinct, yet concurrently active, instances of normal science. Each instance dedicates itself to a singular, overarching problem-solving objective. In the context of fundamental physics, this objective specifically involves reconciling the principles of quantum mechanics with those of gravitation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#articulation-of-normal-science-instances",
    "href": "chapter_ai-nepi_015.html#articulation-of-normal-science-instances",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.6 Articulation of Normal Science Instances",
    "text": "13.6 Articulation of Normal Science Instances\n\n\n\nSlide 08\n\n\nEach instance of normal science finds articulation through a specific social community, intrinsically linked to an intellectual disciplinary matrix. This conceptualisation aligns with, or explicitly appears within, numerous established accounts of research programme structures, notably encompassing Thomas Kuhn’s paradigms, Larry Laudan’s research traditions, and Imre Lakatos’ research programmes.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#empirical-question-quantum-gravity-as-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#empirical-question-quantum-gravity-as-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.7 Empirical Question: Quantum Gravity as Plural Pursuit",
    "text": "13.7 Empirical Question: Quantum Gravity as Plural Pursuit\n\n\n\nSlide 10\n\n\nA central empirical question arises: does quantum gravity research constitute an instance of plural pursuit? This inquiry specifically investigates whether the field comprises independent communities concurrently pursuing distinct paradigms, thereby operating in parallel.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "href": "chapter_ai-nepi_015.html#bottom-up-reconstruction-of-research-landscape",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.8 Bottom-Up Reconstruction of Research Landscape",
    "text": "13.8 Bottom-Up Reconstruction of Research Landscape\nTo address this question, the authors propose a comprehensive bottom-up reconstruction of the quantum gravity research landscape. This reconstruction meticulously analyses both the linguistic and intellectual structures of the field, alongside its intricate social organisation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#data-collection-and-clustering-pipeline-overview",
    "href": "chapter_ai-nepi_015.html#data-collection-and-clustering-pipeline-overview",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.9 Data Collection and Clustering Pipeline Overview",
    "text": "13.9 Data Collection and Clustering Pipeline Overview\n\n\n\nSlide 12\n\n\nThe authors initiated this study by collecting an extensive dataset comprising 228,748 abstracts and titles from fundamental physics literature, primarily sourced from the Inspire HEP database. This substantial corpus then underwent a two-step processing pipeline. The first step involved a linguistic analysis, meticulously reconstructing the intellectual structure of the field, primarily utilising the Bertopic pipeline. Concurrently, a social network analysis proceeded to reconstruct the field’s social structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#linguistic-analysis-spatialisation",
    "href": "chapter_ai-nepi_015.html#linguistic-analysis-spatialisation",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.10 Linguistic Analysis: Spatialisation",
    "text": "13.10 Linguistic Analysis: Spatialisation\n\n\n\nSlide 13\n\n\nThe initial phase of the linguistic analysis involves spatialising the collected documents. This process maps each document into a high-dimensional embedding space, designated as L-1, thereby transforming textual data into a numerical representation suitable for further computational analysis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#linguistic-analysis-unsupervised-clustering",
    "href": "chapter_ai-nepi_015.html#linguistic-analysis-unsupervised-clustering",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.11 Linguistic Analysis: Unsupervised Clustering",
    "text": "13.11 Linguistic Analysis: Unsupervised Clustering\n\n\n\nSlide 14\n\n\nFollowing spatialisation, unsupervised clustering proceeds within the embedding space, designated as L-2. This process yields a highly granular partition of the literature, resulting in 611 distinct topics. Such a fine-grained resolution proves essential for capturing niche approaches within quantum gravity, some of which may encompass as few as one hundred publications.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#linguistic-analysis-specialty-assignment",
    "href": "chapter_ai-nepi_015.html#linguistic-analysis-specialty-assignment",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.12 Linguistic Analysis: Specialty Assignment",
    "text": "13.12 Linguistic Analysis: Specialty Assignment\n\n\n\nSlide 15\n\n\nThe linguistic analysis culminates in assigning a specific specialty to each scientist, denoted as σ_i (L-3). This specialty corresponds to the most frequently occurring topic across an individual’s published works. Consequently, this method effectively partitions each author according to the field’s underlying linguistic and intellectual structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#social-network-analysis-co-authorship-graph",
    "href": "chapter_ai-nepi_015.html#social-network-analysis-co-authorship-graph",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.13 Social Network Analysis: Co-authorship Graph",
    "text": "13.13 Social Network Analysis: Co-authorship Graph\n\n\n\nSlide 16\n\n\nConcurrently with the linguistic analysis, the authors conducted a comprehensive social network analysis. This process commenced with the construction of a co-authorship graph, where individual physicists constitute the nodes, and the edges signify collaborative co-authorship relationships between them.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#social-network-analysis-community-detection",
    "href": "chapter_ai-nepi_015.html#social-network-analysis-community-detection",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.14 Social Network Analysis: Community Detection",
    "text": "13.14 Social Network Analysis: Community Detection\n\n\n\nSlide 18\n\n\nA sophisticated community detection method was subsequently applied to the co-authorship network, designated as S-1. This process successfully identified approximately 800 distinct communities, specifically 819, from the expansive network comprising 30,000 physicists.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#operationalising-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#operationalising-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.15 Operationalising Plural Pursuit",
    "text": "13.15 Operationalising Plural Pursuit\n\n\n\nSlide 19\n\n\nOperationalising the concept of plural pursuit involves establishing a direct one-to-one correspondence between identified communities and their associated topics. This ideal configuration would manifest as a block-diagonal correlation matrix, where communities align exclusively with distinct intellectual domains. Such a matrix would unequivocally signify a clear division of labour and specialisation amongst the various research groups.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#initial-correlation-matrix-complexity",
    "href": "chapter_ai-nepi_015.html#initial-correlation-matrix-complexity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.16 Initial Correlation Matrix Complexity",
    "text": "13.16 Initial Correlation Matrix Complexity\n\n\n\nSlide 20\n\n\nHowever, directly applying the fine-grained partition of the field into communities and topics yields a highly complex and “messy” correlation heatmap. This intricate pattern, whilst indicative of underlying structure, proves exceptionally challenging to interpret meaningfully.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#challenges-with-fine-grained-partitioning",
    "href": "chapter_ai-nepi_015.html#challenges-with-fine-grained-partitioning",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.17 Challenges with Fine-Grained Partitioning",
    "text": "13.17 Challenges with Fine-Grained Partitioning\n\n\n\nSlide 21\n\n\nThe observed complexity stems from two primary issues. Firstly, the chosen level of fine-graining for topic partitioning proves somewhat arbitrary; for instance, String Theory, intuitively a cohesive research programme, might inadvertently scatter across numerous distinct topics. Secondly, large-scale research programmes frequently involve parallel efforts by multiple communities, further complicating a clear one-to-one mapping.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#scale-dependence-of-computational-notions",
    "href": "chapter_ai-nepi_015.html#scale-dependence-of-computational-notions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.18 Scale-Dependence of Computational Notions",
    "text": "13.18 Scale-Dependence of Computational Notions\n\n\n\nSlide 23\n\n\nThese challenges fundamentally derive from a more profound issue: the computational definitions of both “topic” and “community” inherently depend on the scale of analysis. Consequently, both the scientific literature and its underlying social networks permit partitioning at various levels of granularity.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conceptual-nesting-of-research-programmes",
    "href": "chapter_ai-nepi_015.html#conceptual-nesting-of-research-programmes",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.19 Conceptual Nesting of Research Programmes",
    "text": "13.19 Conceptual Nesting of Research Programmes\n\n\n\nSlide 25\n\n\nBeyond a mere technical challenge, this issue reflects a deeper conceptual reality: research programmes are inherently nested. For instance, String Theory encompasses Superstring Theory, which itself branches into Type II and Heterotic variants. Type II further subdivides into Type IIA and Type IIB, whilst Heterotic differentiates into Heterotic SO(32) and Heterotic E8 × E8. Moreover, String Theory also includes Bosonic String Theory and Type I.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#addressing-scale-ambiguity-for-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#addressing-scale-ambiguity-for-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.20 Addressing Scale Ambiguity for Plural Pursuit",
    "text": "13.20 Addressing Scale Ambiguity for Plural Pursuit\n\n\n\nSlide 26\n\n\nConsequently, to accurately identify instances of plural pursuit, the authors must directly address the inherent ambiguity arising from the analysis across disparate scales. This necessitates a methodological approach capable of navigating and resolving such multi-scale complexities.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-clustering-for-topics",
    "href": "chapter_ai-nepi_015.html#hierarchical-clustering-for-topics",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.21 Hierarchical Clustering for Topics",
    "text": "13.21 Hierarchical Clustering for Topics\n\n\n\nSlide 27\n\n\nTo resolve the scale ambiguity, the authors propose a hierarchical reconstruction of the quantum gravity research landscape. For topics, they employ Ward agglomerative clustering, systematically merging the initial 600 fine-grained topics one by one, guided by a predefined objective function.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#hierarchical-clustering-for-communities",
    "href": "chapter_ai-nepi_015.html#hierarchical-clustering-for-communities",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.22 Hierarchical Clustering for Communities",
    "text": "13.22 Hierarchical Clustering for Communities\n\n\n\nSlide 28\n\n\nSimilarly, for the community structure, the authors constructed a hierarchical clustering. This process explicitly leverages a hierarchical stochastic block model from its inception, which inherently learns a multi-level partition, progressively identifying coarser communities from the underlying network.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#observing-socio-epistemic-systems-at-multiple-scales",
    "href": "chapter_ai-nepi_015.html#observing-socio-epistemic-systems-at-multiple-scales",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.23 Observing Socio-Epistemic Systems at Multiple Scales",
    "text": "13.23 Observing Socio-Epistemic Systems at Multiple Scales\n\n\n\nSlide 29\n\n\nThese meticulously constructed hierarchical structures inherently introduce a notion of scale, enabling observation of the socio-epistemic system at various levels of granularity. For instance, within the co-authorship network, each dot symbolises a physicist, with its colour denoting their specific specialty. This specialty remains observable across different levels of coarse-graining applied to the linguistic structure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#multi-level-community-structure-visualisation",
    "href": "chapter_ai-nepi_015.html#multi-level-community-structure-visualisation",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.24 Multi-Level Community Structure Visualisation",
    "text": "13.24 Multi-Level Community Structure Visualisation\n\n\n\nSlide 30\n\n\nCrucially, an analogous multi-level observation capability extends to the community structure, allowing for a comprehensive analysis of its organisation at varying scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#challenge-of-scale-selection",
    "href": "chapter_ai-nepi_015.html#challenge-of-scale-selection",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.25 Challenge of Scale Selection",
    "text": "13.25 Challenge of Scale Selection\n\n\n\nSlide 31\n\n\nDespite the development of hierarchical models, the selection of an appropriate scale for analysis remains an arbitrary decision at this juncture. Determining the optimal scale for either the linguistic or social structure presents a significant challenge.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-strategy",
    "href": "chapter_ai-nepi_015.html#adaptive-topic-coarse-graining-strategy",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.26 Adaptive Topic Coarse-Graining Strategy",
    "text": "13.26 Adaptive Topic Coarse-Graining Strategy\n\n\n\nSlide 33\n\n\nTo address the challenge of scale selection, the authors propose an adaptive topic coarse-graining strategy. This approach acknowledges that many linguistic nuances, whilst conceptually real, may not significantly impact the collaborative capabilities of scientists. Consequently, the method systematically removes degrees of freedom from the initial fine-grained partition, provided this reduction does not diminish the useful information required to comprehend the social structure. This strategy fundamentally relies upon the Minimum Description Length (MDL) criterion, an information-theoretic principle. The MDL criterion identifies the optimal partition by minimising a quantity that judiciously balances two critical factors: the linguistic partition’s explanatory power concerning the field’s social structure, and the imperative to maintain a partition that avoids excessive complexity or fine-graining.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#results-of-adaptive-coarse-graining",
    "href": "chapter_ai-nepi_015.html#results-of-adaptive-coarse-graining",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.27 Results of Adaptive Coarse-Graining",
    "text": "13.27 Results of Adaptive Coarse-Graining\n\n\n\nSlide 34\n\n\nApplying the Minimum Description Length criterion to the initial 600 topics yielded a refined set of just 50 coarse-grained topics. A notable observation from this process is the preservation of certain small-scale linguistic topics, underscoring their critical importance for comprehending the underlying social structure. Conversely, numerous other topics coalesced into much larger, broader categories. This outcome unequivocally confirmed the necessity of commencing with a highly fine-grained classification, as specific small topics demonstrably influence the social dynamics of the field. The authors subsequently assigned descriptive labels to these 50 resulting topics by extracting representative n-grams, focusing specifically on those relevant to quantum gravity.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#matching-coarse-grained-topics-with-community-structures",
    "href": "chapter_ai-nepi_015.html#matching-coarse-grained-topics-with-community-structures",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.28 Matching Coarse-Grained Topics with Community Structures",
    "text": "13.28 Matching Coarse-Grained Topics with Community Structures\n\n\n\nSlide 35\n\n\nThe authors proceeded to match the newly derived coarse-grained topics against the various community structures across different scales, employing a comprehensive correlation matrix. For each topic, represented as a column in the matrix, their analysis sought to identify the community that best explained its prevalence across the hierarchical levels of the community structure. Interestingly, some broad topics, such as a prominent large purple topic, exhibited no specific ties to any single community, suggesting their widespread relevance across the field. Conversely, other topics, notably String Theory, demonstrated a strong correspondence with a specific research programme, which itself aligned with a community structure at the third level of the hierarchy. However, certain quantum gravity research programmes, such as Loop Quantum Gravity, corresponded to communities situated at much lower, more fine-grained levels within the hierarchy. Overall, the findings did not present a clear instance of plural pursuit; instead, the analysis revealed intricate nested structures and entangled scales. For example, a smaller community focused on holography was observed to reside within the broader String Theory community, illustrating complex interdependencies rather than clear divisions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-physicists-intuition",
    "href": "chapter_ai-nepi_015.html#confronting-bottom-up-reconstruction-with-physicists-intuition",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.29 Confronting Bottom-Up Reconstruction with Physicists’ Intuition",
    "text": "13.29 Confronting Bottom-Up Reconstruction with Physicists’ Intuition\n\n\n\nSlide 36\n\n\nThe investigation then transitioned to a critical phase: confronting the empirically derived bottom-up reconstruction with the prevailing intuitions of physicists regarding the inherent structure of their field. This comparative analysis aimed to validate or re-evaluate the computational model against expert understanding.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#surveying-physicists-intuitions",
    "href": "chapter_ai-nepi_015.html#surveying-physicists-intuitions",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.30 Surveying Physicists’ Intuitions",
    "text": "13.30 Surveying Physicists’ Intuitions\n\n\n\nSlide 37\n\n\nTo ascertain expert intuition, the authors conducted a survey targeting the founding members of the International Society for Quantum Gravity. Participants received a specific request: to enumerate the quantum gravity approaches they perceived as fundamentally structuring the entirety of the research landscape.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#identified-quantum-gravity-approaches",
    "href": "chapter_ai-nepi_015.html#identified-quantum-gravity-approaches",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.31 Identified Quantum Gravity Approaches",
    "text": "13.31 Identified Quantum Gravity Approaches\n\n\n\nSlide 38\n\n\nThe survey feedback, whilst revealing some disagreement amongst physicists, collectively yielded a comprehensive list of approaches that effectively partition the field. This extensive list encompasses: Asymptotic Safety, Causal Sets, Dynamical Triangulations, Group Field Theory, Loop Quantum Gravity (LQG), Spin Foams, Noncommutative Geometry, Swampland Conjectures, Modified Dispersion Relations, Doubly Special Relativity (DSR), Quantum Modified Black Holes (BH), Shape Dynamics, Tensor Models, String Theory, Supergravity, and Holography. Notably, the analysis specifically concentrated on String Theory, Supergravity, and Holography. This particular focus arose from the observed divergence in physicists’ opinions regarding their distinctness; some experts considered them fundamentally intertwined with String Theory, despite their clear historical and conceptual differences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#supervised-classification-of-approaches",
    "href": "chapter_ai-nepi_015.html#supervised-classification-of-approaches",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.32 Supervised Classification of Approaches",
    "text": "13.32 Supervised Classification of Approaches\n\n\n\nSlide 40\n\n\nTo facilitate the comparison, the authors trained a Support Vector Machine (SVM) classifier. This model’s primary function involved predicting the specific approach to which individual papers belonged. It leveraged text embeddings, generated by the all-MiniLM-L6-v2 model from both titles and abstracts, and relied upon meticulously hand-coded labels for its training regimen.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#top-down-vs.-bottom-up-comparison",
    "href": "chapter_ai-nepi_015.html#top-down-vs.-bottom-up-comparison",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.33 Top-Down vs. Bottom-Up Comparison",
    "text": "13.33 Top-Down vs. Bottom-Up Comparison\n\n\n\nSlide 41\n\n\nThe supervised, top-down list of approaches underwent a direct confrontation with the bottom-up reconstruction. The analysis revealed that certain approaches exhibited a strong correspondence with the topics emerging from the bottom-up analysis. Conversely, other approaches, particularly those categorised as phenomenological or lacking a fully developed conceptual framework, demonstrated poor correlation. Notably, frameworks characterised by clear definitions and conceptual autonomy consistently showed robust correlations.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#convergence-of-findings-string-theory-and-supergravity",
    "href": "chapter_ai-nepi_015.html#convergence-of-findings-string-theory-and-supergravity",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.34 Convergence of Findings: String Theory and Supergravity",
    "text": "13.34 Convergence of Findings: String Theory and Supergravity\n\n\n\nSlide 42\n\n\nCrucially, the bottom-up approach unveiled a substantial String Theory cluster that effectively encompassed both Supergravity and String Theory. This finding remarkably converges with the prevailing intuition amongst physicists, who often perceive these communities as inseparable, notwithstanding their distinct historical trajectories and conceptual foundations. The coarse-graining process, by systematically eliminating linguistic nuances that bear no consequence for the social structure, effectively consolidated these areas. This outcome thus corroborates the physicists’ assessment that, due to extensive overlap, these communities defy meaningful separation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-scale-dependent-socio-epistemic-systems",
    "href": "chapter_ai-nepi_015.html#conclusions-scale-dependent-socio-epistemic-systems",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.35 Conclusions: Scale-Dependent Socio-Epistemic Systems",
    "text": "13.35 Conclusions: Scale-Dependent Socio-Epistemic Systems\n\n\n\nSlide 43\n\n\nSocio-epistemic systems demonstrably lend themselves to observation across multiple scales. Consequently, the very notions of communities and disciplinary matrices inherently exhibit scale-dependence, necessitating a multi-scalar analytical approach.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-identifying-plural-pursuit",
    "href": "chapter_ai-nepi_015.html#conclusions-identifying-plural-pursuit",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.36 Conclusions: Identifying Plural Pursuit",
    "text": "13.36 Conclusions: Identifying Plural Pursuit\n\n\n\nSlide 44\n\n\nIdentifying configurations indicative of plural pursuit, which fundamentally involve a one-to-one mapping between distinct communities and their corresponding intellectual substrates, necessitates the meticulous matching of these structures across various analytical scales.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-quantum-gravity-reconstruction",
    "href": "chapter_ai-nepi_015.html#conclusions-quantum-gravity-reconstruction",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.37 Conclusions: Quantum Gravity Reconstruction",
    "text": "13.37 Conclusions: Quantum Gravity Reconstruction\n\n\n\nSlide 45\n\n\nFor quantum gravity, specifically, a bottom-up reconstruction of its intricate research landscape offers a powerful means to either corroborate or critically re-evaluate certain long-held intuitions amongst physicists.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_015.html#conclusions-computation-and-philosophy",
    "href": "chapter_ai-nepi_015.html#conclusions-computation-and-philosophy",
    "title": "13  Quantum Gravity and Plural Pursuit in Science",
    "section": "13.38 Conclusions: Computation and Philosophy",
    "text": "13.38 Conclusions: Computation and Philosophy\n\n\n\nSlide 16\n\n\nFinally, the escalating power of computational methods provides an unprecedented capacity to revisit and indeed challenge established philosophical insights. This particularly applies to long-standing intuitions concerning the nature of paradigms or communities within specific scientific contexts, such as quantum gravity. As such, one might provocatively assert, paraphrasing Clausewitz, that computation represents the continuation of philosophy by other means.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quantum Gravity and Plural Pursuit in Science</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html",
    "href": "chapter_ai-nepi_016.html",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "",
    "text": "Overview\nThis chapter details a comparative study assessing the efficacy of Latent Dirichlet Allocation (LDA) and BERTopic models when applied to distinct textual levels: titles, abstracts, and full texts within a scientific corpus. The authors aimed to ascertain whether topic modelling on titles or abstracts suffices, or if full-text analysis remains indispensable, particularly given the substantial resources required for its processing.\nTheir methodology involved constituting a corpus of scientific articles, segmenting these into titles, abstracts, and full texts, and subsequently applying both LDA and BERTopic approaches. A comprehensive analysis, encompassing both qualitative and quantitative methods, facilitated the comparison of the resulting topic models. Key quantitative metrics included the Adjusted Rand Index, Topic Diversity, Joint Recall, and Coherence CV.\nThe authors’ findings indicate that title-based models generally exhibit poor performance, whilst abstract models consistently demonstrate robust and meaningful topic extraction, often aligning well with full-text models. Full-text models, whilst offering comprehensive coverage, can present challenges such as loosely defined topics or class-size imbalances, particularly with BERTopic. Ultimately, the study recommends employing topic modelling on abstracts or full texts with either LDA or BERTopic, provided such approaches do not lead to misclassification of relevant documents.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "href": "chapter_ai-nepi_016.html#research-question-and-methodological-approach",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.1 Research Question and Methodological Approach",
    "text": "14.1 Research Question and Methodological Approach\n\n\n\nSlide 02\n\n\nThis research addresses a critical inquiry: whether applying topic modelling solely to titles or abstracts suffices, or if full-text analysis remains an indispensable requirement. This question gains particular urgency given the substantial resources demanded for the acquisition, preprocessing, and subsequent analysis of extensive full-text corpora.\nTo investigate this, the authors meticulously constituted a corpus of scientific articles. They then systematically identified and segmented the title, abstract, and full-text sections from each article. Subsequently, they applied two prominent topic modelling approaches—Latent Dirichlet Allocation (LDA) and BERTopic—to each of these textual levels. The resulting topic models underwent rigorous analysis and comparison, employing both qualitative and quantitative methodologies.\nThe overall workflow involved segmenting the scientific corpus into titles, abstracts, and full texts. Each segment then served as input for both LDA and BERTopic models. The outputs from these models were then subjected to both qualitative and quantitative scrutiny. This comprehensive approach aimed to provide robust insights into the comparative performance of these models across different textual granularities.\nBeyond this specific investigation, topic modelling itself stands as a vital analytical instrument for processing vast scientific literature, especially within the history, philosophy, and sociology of science. Historically, researchers have deployed topic modelling for diverse tasks, including discerning research trends and paradigm shifts, identifying thematic substructures and interrelationships, and charting the evolution of scientific vocabulary. These prior applications have consistently involved various textual structures, ranging from titles and abstracts to complete full texts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "href": "chapter_ai-nepi_016.html#material-astrobiology-corpus-for-qualitative-comparison",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.2 Material: Astrobiology Corpus for Qualitative Comparison",
    "text": "14.2 Material: Astrobiology Corpus for Qualitative Comparison\n\n\n\nSlide 05\n\n\nThe authors grounded this study in an extensive topic analysis of an astrobiology corpus, previously detailed by Malaterre and Lareau in 2023. Following a thorough evaluation process, they selected a full-text LDA model comprising 25 distinct topics as the primary material for comparison.\nTheir analysis of these 25 topics involved a meticulous examination of their most representative words and documents, enabling the authors to assign a descriptive name to each topic based on its key terms. Subsequently, they compared the topics by calculating their mutual correlation, a metric derived from the topics’ presence within the documents. A community detection algorithm then identified four thematic clusters, designated by letters A, B, C, and D, and visually distinguished by red, green, yellow, and blue colours, respectively.\nA graphical representation visually conveys these findings, illustrating the correlations amongst the 25 topics. This graph incorporates topic labels and the colour variations corresponding to their thematic clusters. Crucially, the thickness of the lines connecting topics denotes the strength of their correlation, whilst the size of each circle reflects the topic’s overall prevalence across all documents. This comprehensive analytical framework enables a robust qualitative comparison of the six distinct topic models under investigation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "href": "chapter_ai-nepi_016.html#methodology-quantitative-analysis-metrics",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.3 Methodology: Quantitative Analysis Metrics",
    "text": "14.3 Methodology: Quantitative Analysis Metrics\n\n\n\nSlide 06\n\n\nFor the quantitative dimension of this study, the authors employed four distinct metrics to compare the various topic models. Firstly, the Adjusted Rand Index (ARI) served to evaluate the similarity between two document clusterings, with a correction applied for chance agreement. This metric precisely quantifies the extent to which documents cluster together, or diverge, across different models.\nSecondly, Topic Diversity assessed the proportion of distinct top words, thereby determining whether individual topics within a given model were characterised by unique vocabulary. Thirdly, Joint Recall measured the average document-topic recall in relation to any topic’s top words, evaluating how effectively these top words collectively represented the documents assigned to each topic. Finally, Coherence CV, calculated as the average cosine relative distance between top words within topics, provided an assessment of whether these top words formed a semantically meaningful grouping. Each of these metrics is underpinned by specific mathematical formulations, ensuring rigorous quantitative comparison.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "href": "chapter_ai-nepi_016.html#results-adjusted-rand-index-between-topic-models",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.4 Results: Adjusted Rand Index Between Topic Models",
    "text": "14.4 Results: Adjusted Rand Index Between Topic Models\n\n\n\nSlide 07\n\n\nThe Adjusted Rand Index (ARI) provides crucial insights into the similarities amongst the six topic models. A value of zero for this metric indicates a clustering equivalent to random assignment. Analysis of the heatmap reveals that the LDA model applied to titles stands out as the most distinct, consistently demonstrating poor similarity with all other models, as evidenced by ARI values below 0.20, depicted by yellow hues in the visualisation.\nConversely, the remaining models generally exhibit a superior overall match, with ARI values consistently exceeding 0.20. Notably, BERTopic models display a stronger internal correspondence, with their inter-model ARI values typically surpassing 0.35. The BERTopic abstract model emerges as particularly central within this network of similarities, demonstrating robust correspondence with every other model, apart from the outlier LDA title model, with values consistently above 0.30. The heatmap visually encapsulates these relationships, where warmer colours signify higher degrees of similarity between the compared topic models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-lda-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.5 Results: LDA Full-text Versus Abstracts and Titles",
    "text": "14.5 Results: LDA Full-text Versus Abstracts and Titles\n\n\n\nSlide 08\n\n\nA more granular analysis of the LDA models provides detailed insights into their interrelationships. Table A, which compares the LDA full-text model with the LDA abstract model, indicates a generally good overall fit. This strong correspondence is evident from the reddish diagonal in the table, signifying that each topic from one model typically aligns with a topic from the other, sharing a high proportion of common documents.\nHowever, this alignment is not without dynamic shifts. Three full-text LDA topics effectively disappear, represented by long horizontal dark grey lines. Conversely, three full-text topics fragment into multiple topics within the abstract model, visible as short horizontal dark grey lines. The abstract model also sees the emergence of three entirely new topics, marked by long vertical dark grey lines, whilst three topics arise from mergers, again indicated by short horizontal dark grey lines. Furthermore, one small class, comprising fewer than 50 documents, is discernible within the abstract topics.\nIn stark contrast, Table B, comparing the LDA full-text model with the LDA title model, reveals a poor overall fit. This disparity necessitates substantial reorganisation, manifested by a proliferation of vertical and horizontal dark lines across the table. This indicates that numerous full-text topics vanish, whilst a considerable number of new abstract topics emerge, highlighting a significant divergence in thematic representation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "href": "chapter_ai-nepi_016.html#results-bertopic-full-text-versus-abstracts-and-titles",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles",
    "text": "14.6 Results: BERTopic Full-text Versus Abstracts and Titles\n\n\n\nSlide 09\n\n\nAnalysis of the BERTopic models, when compared against the LDA full-text baseline, reveals varied performance. Table C, which juxtaposes LDA full-text with BERTopic full-text, indicates an average overall fit. Within this comparison, eight LDA topics vanish along the horizontal axis, whilst six LDA topics fragment into the BERTopic model. Conversely, the vertical axis shows the emergence of five new BERTopic topics, with one topic resulting from mergers. A notable observation from the total document count is the presence of four small classes alongside one exceptionally large class.\nMoving to Table D, the comparison between LDA full-text and BERTopic abstract demonstrates a relatively good overall fit. Here, four LDA topics disappear, whilst six topics undergo splitting. The vertical axis reveals two new BERTopic topics appearing and four topics resulting from mergers. Crucially, this model maintains balanced class sizes.\nFinally, Table E, comparing LDA full-text with BERTopic title, again indicates an average overall fit. In this instance, seven LDA topics disappear, and one topic splits. The vertical axis shows seven new BERTopic topics emerging, with one topic resulting from a merger. The total document count for this model highlights three small classes and one large class. These heatmaps collectively illustrate the proportions of shared documents between topics across these diverse model comparisons.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-lda---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.7 Results: LDA - Comparing Top-words",
    "text": "14.7 Results: LDA - Comparing Top-words\n\n\n\nSlide 11\n\n\nAn examination of the top words within the LDA models revealed that topics were generally well-formed across all iterations. The authors observed several robust topics that maintained strong correspondence across the full-text, abstract, and title models. The topic “A-Radiation-spore” serves as a prime example of this consistency.\nConversely, certain topics from the full-text model fragmented into multiple, more granular topics within both the abstract and title models. For instance, the splitting of “A-Life-civilization” proved semantically coherent, yielding a broader topic encompassing research in astrobiology. However, the fragmentation of “B-Chemistry” presented a more ambiguous case, necessitating further analysis for clear interpretation.\nFurthermore, the study identified instances where topics from the full-text model coalesced into new, merged topics within the abstract and title models. A notable example is the merger of “B-Amino-acid” and “B-Protein-gene-rna” in the LDA abstract model. This particular consolidation formed a more general topic, which aligns logically with the underlying subject matter. The visual representation provides side-by-side tables illustrating the top words for selected topics across these LDA models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "href": "chapter_ai-nepi_016.html#results-bertopic---comparing-top-words",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.8 Results: BERTopic - Comparing Top-words",
    "text": "14.8 Results: BERTopic - Comparing Top-words\n\n\n\nSlide 12\n\n\nContinuing the assessment of top words, the three BERTopic models also yielded relatively well-formed topics. The robustness of “A-Radiation-spore” persisted across all models, including LDA Full-text, BERTopic Full-text, BERTopic Abstract, and BERTopic Title, underscoring its consistent thematic representation.\nWhilst “A-Life-civilization” generally maintained its stability across the BERTopic models, it exhibited some instances of splitting. This fragmentation led to the emergence of more narrowly defined topics specifically pertaining to extraterrestrial life. Similarly, the “B-Chemistry” topic also underwent splitting across the BERTopic models, resulting in a series of more focused thematic areas. The visual data provides comparative tables of top words from selected topics across these models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-coherence",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.9 Results of Quantitative Analysis: Coherence",
    "text": "14.9 Results of Quantitative Analysis: Coherence\n\n\n\nSlide 12\n\n\nThe coherence metric, specifically Coherence CV, provides a quantitative assessment of the semantic meaningfulness of the top words within each topic. Across a range of topics from 5 to 50, distinct patterns emerged. Models based on titles consistently exhibited the poorest coherence. Conversely, abstract models demonstrably outperformed full-text models in this regard.\nFurthermore, BERTopic models generally achieved superior coherence compared to LDA, particularly for abstract and title-based analyses. However, this performance differential tended to diminish as the number of topics increased, indicating a convergence in coherence scores at higher topic counts. Ultimately, the BERTopic abstract model unequivocally emerged as the leading performer in terms of topic coherence. A line graph visually represents these trends, plotting the Coherence CV for each of the six models against varying numbers of topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-diversity",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.10 Results of Quantitative Analysis: Diversity",
    "text": "14.10 Results of Quantitative Analysis: Diversity\n\n\n\nSlide 13\n\n\nRegarding the diversity of top words characterising the topics, a clear trend emerged: diversity generally diminishes as the number of topics increases. Within this context, models derived from titles consistently offered the highest diversity, surpassing their abstract or full-text counterparts.\nMoreover, BERTopic models demonstrated superior diversity compared to LDA across the board. The BERTopic title model ultimately emerged as the top performer in terms of diversity, with the BERTopic full-text model closely trailing. A line graph visually illustrates these diversity trends for each of the six models across varying topic counts.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "href": "chapter_ai-nepi_016.html#results-of-quantitative-analysis-joint-recall",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.11 Results of Quantitative Analysis: Joint Recall",
    "text": "14.11 Results of Quantitative Analysis: Joint Recall\n\n\n\nSlide 13\n\n\nThe Joint Recall metric assesses the efficacy with which the top words collectively represent every document assigned to a given topic. Analysis revealed that models based on titles consistently yielded the poorest recall. Conversely, full-text models demonstrated superior performance compared to their abstract and title counterparts.\nIn terms of algorithmic performance, LDA models generally exhibited better Joint Recall than BERTopic. The LDA full-text and BERTopic full-text models emerged as the leading performers in this category, with the BERTopic abstract model following very closely behind. A line graph visually depicts the micro Joint Recall for each of the six models across a range of topic numbers.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "href": "chapter_ai-nepi_016.html#summary-of-model-performance",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.12 Summary of Model Performance",
    "text": "14.12 Summary of Model Performance\n\n\n\nSlide 14\n\n\nThe authors compiled the individual assessment results to offer a holistic perspective on the models’ performance. For each criterion—overall fit, top-words, coherence, diversity, and joint recall—a circular representation indicates performance: a black circle denotes the highest score, a white circle signifies a lesser score, and a half-black, half-white circle indicates intermediate performance. Crucially, the study underscores that no single model emerges as universally superior; rather, diverse research objectives inherently dictate varying needs and, consequently, different model choices.\nFor instance, if the primary objective involves the discovery of main topics without stringent requirements for precise document classification, then issues such as poor recall or large class sizes might be acceptable. In such scenarios, the BERTopic Full-text model performed commendably, albeit with some observed class imbalance. Similarly, whilst far from optimal, the BERTopic Title model did yield certain robust topics that were consistently identified across other models.\nConversely, if the aim is to achieve maximum document coverage across all topics, then neither BERTopic Full-text nor BERTopic Title is recommended, as both approaches lead to large document classes and, in the case of BERTopic Title, poor recall. Furthermore, the LDA Title model receives a general non-recommendation due to its consistently poor performance across nearly all assessments. In essence, the study advocates for conducting topic modelling on either abstracts or full texts, employing either LDA or BERTopic, provided that such applications do not result in the misclassification of documents pertinent to specific topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "href": "chapter_ai-nepi_016.html#discussion-and-conclusion",
    "title": "14  Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance",
    "section": "14.13 Discussion and Conclusion",
    "text": "14.13 Discussion and Conclusion\n\n\n\nSlide 16\n\n\nThis research yields several pivotal findings. Firstly, title models consistently exhibit poor performance, primarily attributable to the inherent lack of information within titles, which can consequently lead to erroneous document classification. Nevertheless, the BERTopic title model, surprisingly, generated numerous meaningful topics, suggesting that future efforts might focus on striking a balance between precisely defined topics and comprehensive document coverage.\nSecondly, full-text models occasionally encounter difficulties in processing vast quantities of information. With LDA, topics can become more broadly defined and encompass wider coverage, potentially including secondary or transverse themes such as methodologies. Conversely, BERTopic, when applied to full texts, can produce overly narrow topics, resulting in inadequate document coverage and issues with class size.\nThirdly, abstract models consistently demonstrate strong performance with summary information. Their results align remarkably well with the LDA full-text model, as well as with both LDA and BERTopic abstract models. This consistency underscores their utility in capturing core thematic content.\nFourthly, the study highlights the notable robustness of topics. Across the board, the authors identified highly similar topics, a finding that facilitates the application of meta-analytic methods to pinpoint the most enduring and robust themes. Moreover, this consistency suggests the potential for employing relative distance metrics across models to identify an optimal solution; in this study, the BERTopic abstract model emerged as such an optimum, performing exceptionally well across all other metrics.\nFinally, the findings prompt a consideration of new model development. It appears feasible and potentially beneficial to leverage the structural information inherent in documents—specifically, full text, abstract, and title—to extract more semantically rich sets of top words or topics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html",
    "href": "chapter_ai-nepi_017.html",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "",
    "text": "Overview\nThis chapter introduces a novel architectural modification for Large Language Models (LLMs), meticulously designed to imbue them with explicit time awareness. This enhancement promises to significantly improve their utility for historical analysis and a broader spectrum of applications. Current LLMs derive their understanding of time implicitly from statistical patterns within training data, a method that often proves insufficient when processing temporally sensitive information, such as contradictory statements from different eras. The proposed “Time Transformer” architecture directly addresses this limitation by injecting a temporal dimension into each token’s latent semantic representation.\nTo validate this concept, the authors employed a modest Transformer model, training it on a highly restricted, repetitive dataset: daily weather reports from the UK Met Office spanning 2018 to 2024. This dataset, comprising approximately 2,500 reports and a vocabulary of 3,395 words, facilitated rapid experimentation. The vanilla Transformer, featuring four decoder layers and 39 million parameters, successfully reproduced the language patterns characteristic of these weather reports.\nThe core innovation involves reserving a single dimension within the 512-dimensional token embedding for a non-trainable, min-max normalised “day of the year” value. This straightforward modification enables the model to learn and reproduce time-dependent patterns with remarkable efficacy. The authors demonstrated this through two key experiments: firstly, the model accurately reproduced a synthetically injected “rain” to “liquid sunshine” replacement pattern across the year; secondly, it learned a synthetic co-occurrence shift, where “rain” became obligatorily followed by “and snow” towards the year’s end. Further analysis of the attention heads revealed that specific heads specialised in processing this crucial temporal information.\nWhilst proving the concept, the approach presents challenges for large-scale application. These primarily stem from the necessity of training models from scratch and the complex, labour-intensive process of accurately dating historical token sequences. Nevertheless, the research suggests potential for improved training efficiency and highlights the utility of such time-aware embeddings for targeted tasks, such as semantic shift detection in historical corpora.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#limitations-of-current-large-language-models-in-temporal-understanding",
    "href": "chapter_ai-nepi_017.html#limitations-of-current-large-language-models-in-temporal-understanding",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.1 Limitations of Current Large Language Models in Temporal Understanding",
    "text": "15.1 Limitations of Current Large Language Models in Temporal Understanding\nCurrent Large Language Models (LLMs) inherently possess merely an implicit understanding of time, which they derive statistically from the textual patterns observed during their extensive training. Whilst these models demonstrate a remarkable grasp of temporal concepts, their comprehension stems from the implicit cues embedded within the training data. Explicit time awareness, defined as the capacity to learn and reproduce evolving patterns within training data as a function of time, would undoubtedly enhance their application in historical analysis and, indeed, across a broader spectrum of domains.\nConsider, for instance, two sentences that differ only in their final two words: “The primary architectures for processing text through NNs are LSTMs” and “The primary architectures for processing text through NNs are Transformers.” These statements are inherently contradictory, yet human readers instinctively recognise their temporal distinction. When such conflicting information appears within an LLM’s training data without explicit temporal context, these statements compete for the model’s attention. Consequently, the model cannot perfectly fulfil its objective, as a preference for one statement inevitably leads to an error regarding the other.\nDuring inference, when presented with a sequence such as “The primary architectures for processing text through NNs are…”, an LLM typically exhibits a recency bias, often predicting “Transformers” due to its prevalence in more recent data. Manipulating the input context, for example by adding “In 2017” or altering verb tenses, constitutes a form of prompt engineering—an imprecise method of “fishing in the dark” to exploit the model’s implicit temporal understanding. A more robust approach necessitates explicit time awareness, enabling models to accurately reflect temporal shifts in information.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#the-challenge-of-temporal-drift-in-language-models",
    "href": "chapter_ai-nepi_017.html#the-challenge-of-temporal-drift-in-language-models",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.2 The Challenge of Temporal Drift in Language Models",
    "text": "15.2 The Challenge of Temporal Drift in Language Models\nLarge Language Models fundamentally operate by estimating the probability distribution over their vocabulary for the next token, given a preceding sequence of tokens. This process can be formally expressed as p(x_n | x_1, …, x_{n-1}). However, in the real world, the probability of a token appearing within a specific context is not static; rather, it demonstrably depends on time, a relationship represented as p(x_n | x_1, …, x_{n-1}, t). Consequently, the probability for an entire sequence of tokens uttered at a particular time, t, follows the product of these conditional probabilities: p(x_1, x_2, …, x_n | t) = Π_{k=1}^n p(x_k | x_1, …, x_{k-1}, t).\nCrucially, during their training, current LLMs largely treat these complex, time-dependent probability distributions as static. This simplification means that whilst they may implicitly capture some temporal nuances, their ability to reflect genuine temporal drift in the underlying token sequence distributions during inference is limited to in-context learning. This reliance on contextual cues, as previously illustrated, often proves insufficient for tasks requiring precise temporal understanding.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#formalising-time-dependent-probability-distributions",
    "href": "chapter_ai-nepi_017.html#formalising-time-dependent-probability-distributions",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.3 Formalising Time-Dependent Probability Distributions",
    "text": "15.3 Formalising Time-Dependent Probability Distributions\nLarge Language Models fundamentally estimate the probability distribution over their vocabulary for the next token, conditioned on a sequence of preceding tokens, formally expressed as p(x_n | x_1, …, x_{n-1}). Nevertheless, in practical applications, the probability of a token appearing within a given context is not static; instead, it inherently depends on time, a relationship denoted as p(x_n | x_1, …, x_{n-1}, t). This temporal dependency extends to entire sequences of tokens uttered at a specific time, t, where the probability is the product of individual conditional probabilities: p(x_1, x_2, …, x_n | t) = Π_{k=1}^n p(x_k | x_1, …, x_{k-1}, t).\nDespite this inherent temporal variability in real-world data, current LLMs typically treat these probability distributions as static during their training phase. Consequently, during inference, these models can only reflect temporal drift in the underlying token sequence distributions through in-context learning. This limitation underscores the challenge of accurately modelling dynamic linguistic patterns without explicit temporal conditioning.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#introducing-the-time-transformer-architecture",
    "href": "chapter_ai-nepi_017.html#introducing-the-time-transformer-architecture",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.4 Introducing the Time Transformer Architecture",
    "text": "15.4 Introducing the Time Transformer Architecture\n\n\n\nSlide 05\n\n\nTo enhance the temporal understanding of Large Language Models, the authors recognised the necessity of explicitly modelling the time-dependent probability distribution, p(x_n | x_1, …, x_{n-1}, t). Traditional approaches, such as time slicing—training separate models for distinct temporal segments—prove exceptionally data inefficient. A more elegant solution, which the authors termed the “Time Transformer”, proposes a straightforward yet powerful modification: augmenting the latent semantic token features with an additional temporal dimension.\nThis architectural innovation involves extending the standard token embedding, E(x), to include a time component, φ(t), resulting in a new embedding function: E(x, t) = {e_1(x), e_2(x), …, e_{d-1}(x), φ(t)}. Consequently, the Transformer receives a sequence of time-dependent token embeddings, [E(x_1, t), E(x_2, t), …, E(x_{n-1}, t)], which then enables the model to output a truly time-dependent probability distribution, p_θ(x_n | x_1, …, x_{n-1}, t). Crucially, the fundamental training objective remains unchanged; the model continues to minimise the negative log likelihood across all sequences and partial sequences.\nBy directly injecting time into each token’s representation, the model gains the capacity to learn precisely how strongly or weakly this temporal dimension influences individual tokens. For certain words, the temporal context may hold little significance, whilst for others, it proves paramount. The inherent efficiency of the Transformer architecture then allows for the robust statistical processing of this newly integrated temporal information, facilitating a more nuanced and accurate understanding of language evolution.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experimental-validation-and-data-acquisition",
    "href": "chapter_ai-nepi_017.html#experimental-validation-and-data-acquisition",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.5 Experimental Validation and Data Acquisition",
    "text": "15.5 Experimental Validation and Data Acquisition\nTo validate their proposed Time Transformer concept, the authors sought a dataset characterised by restricted language, a compact vocabulary, and repetitive patterns. The daily weather reports issued by the UK’s National Meteorological Service, the Met Office, proved an ideal candidate. These historical reports are readily accessible through their digital archive, located at https://digital.nmla.metoffice.gov.uk/. Another potential resource, “TinyStories”, was also identified as a suitable alternative for similar experimental contexts.\nFor the current study, the team systematically scraped data spanning from 2018 to 2024, yielding approximately 2,500 individual weather reports. This raw text then underwent a process of chunking and tokenisation. Notably, the processing eschewed sub-word tokenisation, whilst the authors deliberately disregarded both case and interpunctuation. This simplified approach resulted in a remarkably concise vocabulary, comprising merely 3,395 unique words across the entire seven-year dataset.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#vanilla-transformer-model-implementation",
    "href": "chapter_ai-nepi_017.html#vanilla-transformer-model-implementation",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.6 Vanilla Transformer Model Implementation",
    "text": "15.6 Vanilla Transformer Model Implementation\n\n\n\nSlide 09\n\n\nTo establish a baseline and confirm the capacity for pattern learning, the authors developed a modest Transformer model. This architecture incorporates four multi-head attention decoder blocks, each meticulously designed. Within each decoder layer, an eight-head multi-head attention mechanism processes input, followed by a normalisation layer, a non-linear feed-forward network, and a subsequent normalisation. A final dense layer then undertakes the critical task of assigning probabilities to each token within the vocabulary.\nThis compact model comprises 39 million parameters, occupying approximately 150 MB of memory—a stark contrast to models like GPT-4, which command 1.8 trillion parameters distributed across 120 layers. Training occurred on two A100 GPUs within an HPC cluster in Munich, demonstrating remarkable efficiency with only 11 seconds required per epoch, a speed attributable to both the diminutive dataset and the model’s modest scale. The complete codebase for the model and its associated experiments is openly accessible via https://github.com/j-buettner/time_transformer. Crucially, the model proficiently learned to reproduce the distinct language of the weather reports. For instance, providing a seed sequence such as “During the night, a band…”, the model generated text virtually indistinguishable from authentic weather forecasts, confirming its successful acquisition of the underlying linguistic patterns.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#time-transformer-architectural-modification",
    "href": "chapter_ai-nepi_017.html#time-transformer-architectural-modification",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.7 Time Transformer Architectural Modification",
    "text": "15.7 Time Transformer Architectural Modification\nThe architectural modification required for the Time Transformer is remarkably minimal, representing a key advantage of the proposed approach. Within the 512-dimensional latent semantic space, the authors specifically reserved one dimension for encoding temporal information. This means the text input occupies 511 dimensions, whilst the time data is allocated a single dimension.\nCrucially, this temporal dimension remains non-trainable. Instead, the authors populated it with a min-max normalised “day of the year” value, calculated as (day of year - 1) / (365 - 1). This particular choice was strategic, designed to exploit the inherent seasonal variations present within the weather report dataset, such as the prevalence of snow in winter or higher temperatures in summer. Nevertheless, the framework remains flexible, readily accommodating any other desired time embedding, offering adaptability for diverse temporal contexts.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#experimental-validation-of-temporal-drift-learning",
    "href": "chapter_ai-nepi_017.html#experimental-validation-of-temporal-drift-learning",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.8 Experimental Validation of Temporal Drift Learning",
    "text": "15.8 Experimental Validation of Temporal Drift Learning\nA central inquiry for the Time Transformer concerned its capacity to efficiently learn temporal drift within the underlying data distribution. The initial experiment, designated “synonymic succession,” directly addressed this by injecting a synthetic temporal drift into the training data. This involved a time-dependent replacement of the word “rain” with “liquid sunshine,” a substitution governed by a sigmoid function tied to the day of the year. The probability of this replacement progressively increased from zero at the year’s commencement to one by its conclusion.\nTo rigorously evaluate the model’s learning, the authors generated a distinct weather prediction for each day of the year. Subsequently, they meticulously counted the monthly frequencies of “rain” versus “liquid sunshine” occurrences within these generated sequences. The results unequivocally demonstrated that the model perfectly reproduced the synthetically introduced pattern. “Rain” appeared predominantly at the beginning of the year, whilst “liquid sunshine” dominated towards the end, with the transition occurring precisely in the middle of the year, exhibiting only minor, statistically expected variations. This outcome robustly confirmed the model’s ability to acquire and replicate time-dependent linguistic shifts.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#advanced-temporal-pattern-learning-and-attention-analysis",
    "href": "chapter_ai-nepi_017.html#advanced-temporal-pattern-learning-and-attention-analysis",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.9 Advanced Temporal Pattern Learning and Attention Analysis",
    "text": "15.9 Advanced Temporal Pattern Learning and Attention Analysis\n\n\n\nSlide 12\n\n\nThe second experiment advanced the investigation by focusing on altering a co-occurrence pattern, effectively demonstrating the “fixation of a collocation” from a variable to an obligatory relationship. The authors synthetically injected a time-dependent change: instances of “rain” not immediately followed by “and” were systematically replaced with “rain and snow.” This intervention, whilst altering a weather pattern from a meteorological perspective, fundamentally modified a linguistic collocation within the model’s domain, akin to fixing the phrase “bread and butter.”\nAgain, the authors assessed the model’s performance by generating one prediction for each day of the year. The results conclusively showed that the model successfully acquired this complex temporal pattern. Towards the year’s end, the generated text almost exclusively featured “raining and snowing,” whilst the beginning of the year saw a mix of “rain” and occasional “rain and snow.” Notably, these early occurrences of “rain and snow” were naturally conditioned on the presence of a “cold system,” indicating the model’s nuanced understanding. Further analysis of the attention heads within the Transformer architecture revealed that specific heads had specialised in processing this intricate temporal pattern, with the attention from “snow” to “rain” across different heads clearly illustrating the model’s learned temporal sensitivity.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_017.html#proof-of-concept-and-future-implications",
    "href": "chapter_ai-nepi_017.html#proof-of-concept-and-future-implications",
    "title": "15  Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis",
    "section": "15.10 Proof of Concept and Future Implications",
    "text": "15.10 Proof of Concept and Future Implications\nThe conducted experiments unequivocally establish a proof of concept: Transformer-based Large Language Models can indeed be rendered explicitly time-aware through the straightforward addition of a temporal dimension to their token embeddings. This fundamental innovation carries profound implications for a diverse array of applications, particularly within historical analysis.\nFuture research could profitably investigate whether this explicit temporal dimension might enhance training efficiency. By providing direct temporal cues, the model could potentially decipher underlying patterns more readily, circumventing the need to infer such information from implicit textual signals alone. Nevertheless, significant challenges impede the immediate, widespread application of this architecture.\nFirstly, the approach necessitates training models from scratch, as existing, pre-trained LLMs cannot be simply fine-tuned to incorporate this novel temporal dimension. Consequently, the computational resources required for any serious, large-scale implementation remain substantial. Secondly, the method compromises the “metadata-free” elegance of traditional self-supervised learning. Each token sequence would demand a precise date assignment, a task fraught with complexity for historical documents, where distinguishing between original utterance dates and reprint dates, for instance, presents considerable curatorial hurdles.\nAs a pragmatic alternative, the authors suggest that the principle could be applied to develop a specialised embedding model, akin to a BERT-like embedder. Such a model, rather than being fully generative, would focus on targeted tasks, learning only the temporal patterns relevant to its specific domain. This approach would mitigate the computational demands and data curation complexities associated with training comprehensive generative models, whilst still leveraging the benefits of explicit time awareness.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Time-Aware Language Models: Towards a Novel Architecture for Historical Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html",
    "href": "chapter_ai-nepi_018.html",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "",
    "text": "Overview\nDiego Alves and Sergei Bagdasarov, with significant contributions from Badr M. Abdullah, have pioneered a comprehensive approach to enrich metadata and conduct diachronic analysis of chemical knowledge within historical scientific texts. This endeavour primarily addresses two objectives: first, enhancing the metadata of historical documents through Large Language Models (LLMs), specifically focusing on article categorisation by scientific discipline, semantic tagging, and abstractive summarisation. Secondly, the project analyses the evolution of the chemical space across various disciplines over time, identifying periods of heightened interdisciplinarity and knowledge transfer.\nThe team meticulously processed the Philosophical Transactions of the Royal Society of London, a diachronic corpus spanning from 1665 to 1996, comprising nearly 48,000 texts and 300 million tokens. Employing the Hermes 2 Pro Llama 3 8B model, the authors crafted a system prompt that instructed the LLM to act as a librarian, generating revised titles, five key topics, concise TL;DR summaries, and hierarchical scientific classifications (primary discipline and sub-discipline) in a structured YAML format. This LLM-driven metadata generation achieved remarkable validity: 99.81% of outputs conformed to the specified format, and 94% of discipline predictions aligned with predefined categories.\nFor the diachronic analysis of chemical knowledge, Alves and Bagdasarov focused on chemistry, biology, and physics. They utilised ChemDataExtractor, a Python module, to identify chemical terms, applying a two-stage extraction process to mitigate noise. Kullback-Leibler Divergence (KLD) served as the core analytical tool, enabling both independent tracking of chemical space evolution within each discipline and pairwise comparisons between disciplines across defined time windows. Their findings reveal significant shifts in disciplinary focus over centuries, including a pronounced peak in chemical articles during the late 18th-century chemical revolution. KLD analysis further illuminated specific chemical substances driving disciplinary change and identified instances of knowledge transfer, where elements transitioned in distinctiveness from one field to another. Visualisations, such as t-SNE projections of summaries, further illustrate the evolving relationships and overlaps between scientific domains. Future work aims to test additional LLMs, refine evaluation metrics, and expand the scope of interdisciplinary analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "href": "chapter_ai-nepi_018.html#introduction-and-research-objectives",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.1 Introduction and Research Objectives",
    "text": "16.1 Introduction and Research Objectives\n\n\n\nSlide 02\n\n\nDiego Alves and Sergei Bagdasarov have embarked upon a comprehensive project, titled “Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts.” This work also involved the significant contributions of Badr M. Abdullah, an expert in Large Language Models.\nThe project unfolds in two distinct yet interconnected parts. The first part explores the application of LLMs to enhance the metadata associated with historical texts, particularly within diachronic corpora. This involves the systematic categorisation of articles by scientific discipline, the assignment of semantic tags or topics, and the generation of abstractive summaries.\nThe second part of the study presents a detailed case study. Here, the authors analyse how the chemical space evolves across different scientific disciplines over time. A primary objective involves identifying specific historical periods that exhibit peaks of interdisciplinarity and significant instances of knowledge transfer between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "href": "chapter_ai-nepi_018.html#the-philosophical-transactions-corpus-for-diachronic-scientific-inquiry",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry",
    "text": "16.2 The Philosophical Transactions Corpus for Diachronic Scientific Inquiry\n\n\n\nSlide 03\n\n\nCentral to this research lies an interest in understanding the diachronic evolution of scientific English, particularly how it transformed into an optimised medium for expert-to-expert communication. Beyond this linguistic focus, Alves and Bagdasarov also analyse phenomena such as knowledge transfer and identify influential papers and authors throughout history.\nThe Philosophical Transactions of the Royal Society of London serves as the primary corpus for this investigation. First published in 1665, this esteemed journal holds the distinction of being the oldest scientific journal in continuous publication, maintaining a high reputation to this day. Crucially, it played a pivotal role in shaping scientific communication, notably by establishing the peer-reviewed paper as a fundamental means for disseminating scientific knowledge.\nWithin this extensive corpus reside numerous influential contributions. The 17th century, for instance, saw Isaac Newton’s seminal “New Theory about Light and Colours” published in 1672. Moving into the 18th century, Benjamin Franklin’s “The ‘Philadelphia Experiment’ (the Electrical Kite)” marked another significant entry. Later, in the 19th century, James Clerk Maxwell’s “On the Dynamical Theory of the Electromagnetic Field” (1865) further enriched the collection. Whilst these landmark papers underscore the journal’s scientific rigour, the corpus also contains more curious articles, such as “Monfieur Autour’s Speculations of the Changes, likely to be discovered in the Earth and Moon, by their respective Inhabitants,” which describes lunar inhabitants. Nevertheless, the project’s interest lies not in the scientific validity or fact-checking of these papers, but rather in their linguistic and historical characteristics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "href": "chapter_ai-nepi_018.html#characteristics-and-pre-existing-metadata-of-the-royal-society-corpus",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus",
    "text": "16.3 Characteristics and Pre-existing Metadata of the Royal Society Corpus\n\n\n\nSlide 20\n\n\nThe research team leverages the latest iteration of the Royal Society Corpus, specifically RSC 6.0 Full. This extensive dataset encompasses over three centuries of scientific communication, spanning from 1665 to 1996. It comprises approximately 48,000 distinct texts, accumulating to a substantial 300 million tokens.\nThe corpus already incorporates various metadata attributes, including author, century, year, volume, Digital Object Identifier (DOI), journal, language, and title. Previously, researchers applied Latent Dirichlet Allocation (LDA) topic modelling to infer fields of research categories and classify the diverse papers. However, this LDA approach often yielded mixed classifications, blending distinct disciplines, their sub-disciplines, and even text types, such as “observations” and “reporting.” Consequently, a clear need emerged to enhance this existing metadata and generate additional, more refined attributes, prompting the authors’ integration of Large Language Models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "href": "chapter_ai-nepi_018.html#large-language-models-for-information-management-and-knowledge-organisation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.4 Large Language Models for Information Management and Knowledge Organisation",
    "text": "16.4 Large Language Models for Information Management and Knowledge Organisation\n\n\n\nSlide 23\n\n\nLarge Language Models offer diverse applications for information management and knowledge organisation, encompassing text clean-up, summarisation, and information extraction. Crucially, they facilitate the creation of knowledge graphs and enhance access and retrieval mechanisms through effective categorisation.\nAlves and Bagdasarov specifically tasked the LLM with assuming the role of a librarian. This involved reading and analysing article content and its historical context. The model then suggested alternative, more reflective titles for the articles. Furthermore, it generated concise three-to-four-sentence TL;DR summaries, capturing the essence and main findings in simple language suitable for a high school student. The LLM also identified five main topics, conceptualised as Wikipedia Keywords, for thematic grouping. A hierarchical classification system required the model to assign a primary scientific discipline from a predefined list—including Physics, Chemistry, Environmental & Earth Sciences, Astronomy, Biology & Life Sciences, Medicine & Health Sciences, Mathematics & Statistics, Engineering & Technology, and Social Sciences & Humanities—and a suitable second-level sub-discipline, which could not be one of the primary disciplines.\nFor this undertaking, the team employed Llama 3, specifically the Hermes-2-Pro-Llama-3-8B variant, which possesses 8 billion parameters. This model had undergone instruction-tuning and further fine-tuning to excel at producing structured output, particularly in JSON and YAML formats. The system prompt meticulously defined the LLM’s role: “Act as a librarian and organize a collection of historical scientific articles from the Royal Society of London, published between 1665 and 1996.” Its objective was to “read, analyze, and organize a large corpus of historical scientific articles… The goal is to create a comprehensive and structured database that facilitates search, retrieval, and analysis…” The input description clarified that the model would receive “OCR-extracted text of the original articles, along with some of their corresponding metadata, including title, author(s), publication date, journal, and a short text snippet.” An example input, featuring Isaac Newton’s “A Letter of Mr. Isaac Newton…” from 1672, demonstrated the expected text snippet. The prompt then provided an example of the desired YAML output, showcasing a revised title (“A New Theory of Light and Colours”), relevant topics (e.g., “Optics,” “Refraction”), a TL;DR summary, and the hierarchical scientific classification (“Physics” as primary, “Optics & Light” as sub-discipline). To ensure data integrity, the prompt explicitly mandated that the output must be a valid YAML file, containing no additional text.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "href": "chapter_ai-nepi_018.html#outcomes-and-interpretations-of-llm-driven-metadata-generation",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation",
    "text": "16.5 Outcomes and Interpretations of LLM-driven Metadata Generation\n\n\n\nSlide 46\n\n\nThe LLM-driven metadata generation process yielded highly valid outputs. A remarkable 99.81% of the generated files conformed to the specified YAML format, with only a negligible 0.19% exhibiting invalid structures. Furthermore, the model demonstrated strong accuracy in discipline prediction; 94% of the assigned scientific disciplines fell within the predefined set of nine categories.\nNevertheless, the system did exhibit some minor anomalies or “hallucinations.” For instance, the LLM occasionally rendered “Earth Sciences” instead of the full “Environmental & Earth Sciences” and, in some rare cases, invented entirely novel categories, such as “Music.” Moreover, the model sometimes inadvertently included the numerical index as part of the discipline string, for example, “3. Earth Sciences.” Despite these minor issues, the majority of papers received correct assignments.\nAlves and Bagdasarov’s analysis of the distribution of files per discipline revealed that Biology and Life Sciences accounted for the highest number of articles, closely followed by Physics and Chemistry. Examining the Royal Society articles over time provided compelling insights into disciplinary evolution. Prior to the late 18th century, a more homogeneous distribution of disciplines characterised the publications. However, the late 18th century witnessed a distinct peak in chemical articles, a phenomenon directly correlating with the chemical revolution. Subsequently, chemistry solidified its position as a main pillar of the Royal Society. From the 19th century onwards, Biology, Physics, and Chemistry collectively emerged as the three dominant fields within the journal’s publications.\nA preliminary visualisation of the TL;DR summaries, employing t-SNE projection, illustrated how different disciplines distribute within the semantic space. This projection revealed significant overlap between Chemistry, Physics, and Biology, with chemistry often situated centrally. Conversely, Humanities, Astronomy, and Mathematics formed more isolated clusters, indicating less semantic proximity. This initial analysis underscores the potential for future diachronic studies to precisely trace the shifts and overlaps between these disciplines over extended periods.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "href": "chapter_ai-nepi_018.html#diachronic-analysis-of-chemical-space-methods-and-tools",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools",
    "text": "16.6 Diachronic Analysis of Chemical Space: Methods and Tools\n\n\n\nSlide 56\n\n\nFor the diachronic analysis of the chemical space, Alves and Bagdasarov concentrated solely on three disciplines most frequently encountered within the corpus: chemistry, biology, and physics. To extract chemical terms, they employed ChemDataExtractor, a Python module specifically designed for the automatic identification of chemical substances. The application of this tool involved a two-stage process: an initial pass across the entire text generated considerable noise, necessitating a subsequent refinement. Consequently, a second application of ChemDataExtractor, this time targeting only the list of previously extracted substances, significantly reduced the extraneous output.\nKullback-Leibler Divergence (KLD) served as the core analytical method. KLD, a measure of relative entropy, enables language models to detect changes across situational contexts. It quantifies the additional bits required to encode a given dataset (A) when utilising a sub-optimal model derived from another dataset (B). The authors applied KLD in two distinct ways. Firstly, they conducted a diachronic analysis within each discipline independently, tracing the evolution of the chemical space along the timeline for chemistry, physics, and biology. This involved comparing a 20-year period preceding a specific date with a 20-year period following it, then iteratively sliding the comparison window by five years along the timeline. Secondly, they performed pairwise interdisciplinary comparisons, specifically between chemistry and physics, and chemistry and biology. This latter analysis relied on 50-year periods of the Royal Society Corpus.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "href": "chapter_ai-nepi_018.html#findings-from-diachronic-analysis-of-chemical-space",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.7 Findings from Diachronic Analysis of Chemical Space",
    "text": "16.7 Findings from Diachronic Analysis of Chemical Space\n\n\n\nSlide 61\n\n\nThe Kullback-Leibler Divergence (KLD) analysis yielded compelling results regarding the evolution of chemical space within each discipline. A striking similarity in trends emerged across chemistry, biology, and physics, with peaks and troughs occurring in roughly the same periods. Towards the end of the timeline, the KLD plots flattened considerably, and the overall KLD decreased, indicating reduced variation between future and past periods.\nAlves and Bagdasarov’s further investigation focused on the pronounced KLD peak observed in the late 18th century, specifically between 1740 and 1816. KLD proved instrumental in pinpointing the specific chemical substances driving this period of significant change. In both biology and physics, one or two elements exhibited exceptionally high KLD values, effectively propelling the observed shifts. Interestingly, the same core elements appeared across chemistry, biology, and physics during this early period.\nA distinct pattern emerged when examining the second half of the 19th century, from 1851 to 1896. Here, the graphs for biology and physics became considerably more populated, and the individual contributions of elements appeared far more uniform. Notably, biology began evolving distinctly towards biochemistry. Conversely, chemistry and physics increasingly focused on noble gases and radioactive elements, substances whose discoveries largely characterised the close of the 19th century.\nPairwise interdisciplinary comparisons, visualised through word clouds, further corroborated these findings. When contrasting chemistry and biology in the 20th century, the biology word cloud prominently featured substances associated with biochemical processes in living organisms. In contrast, the chemistry word cloud highlighted substances pertinent to organic chemistry, such as hydrocarbons and benzene. Comparing chemistry with physics revealed a greater emphasis on metals, noble gases, and various types of metals, including rare earth, semi-metals, and radioactive metals. These comparisons effectively elucidated the thematic divergences between disciplines.\nCrucially, this pairwise analysis facilitated the detection of “knowledge transfer” instances. This phenomenon describes an element initially distinctive of one discipline in an earlier period subsequently becoming more distinctive of another. For example, tin, initially a hallmark of chemistry in the early 18th century, clearly shifted to become distinctive of physics by the late 18th century. Similar shifts were observed for other elements in the early 20th century. In the 20th century, elements becoming distinctive of biology consistently related to biochemical processes, underscoring the evolving interconnections between fields.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "href": "chapter_ai-nepi_018.html#concluding-remarks-and-future-research-directions",
    "title": "16  Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts",
    "section": "16.8 Concluding Remarks and Future Research Directions",
    "text": "16.8 Concluding Remarks and Future Research Directions\n\n\n\nSlide 74\n\n\nIn conclusion, Alves and Bagdasarov successfully employed a Large Language Model to enhance article categorisation and topic modelling within the corpus. Building upon the metadata generated by the LLM, they conducted a comprehensive diachronic analysis of the chemical space across three key disciplines: chemistry, biology, and physics. This work also encompassed an interdisciplinary comparison of the chemical space, revealing dynamic relationships between fields.\nNevertheless, considerable scope for future work remains. For the LLM-driven metadata generation, the authors plan to test other LLMs and conduct a more rigorous evaluation of the current results. Regarding the diachronic analysis, future efforts will focus on more fine-grained interdisciplinary analysis, experimenting with different diachronic sliding windows. Furthermore, the team intends to incorporate additional disciplines, such as comparing chemistry with medicine, and explore tracing the evolution of chemical space using surprisal as an analytical metric.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html",
    "href": "chapter_ai-nepi_019.html",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "",
    "text": "Overview\nResearchers within the Cascade project, a Marie Curie doctoral network, meticulously explore the computational analysis of semantic change. PhD student Sophia Aguilar leads this investigation, focusing on modelling diverse contextual factors and their intricate interplay. Building upon previous work that modelled distinct context types in isolation, the current objective is to integrate these approaches, thereby illuminating their complex interactions.\nThe chemical revolution, specifically the profound shift from the century-old phlogiston theory to Lavoisier’s oxygen theory within the Royal Society Corpus (RSC), serves as a pivotal pilot study. Linguists engaged in this endeavour examine how language adapts to real-world transformations, drawing upon register theory and principles of rational communication. The study aims to detect periods of significant linguistic change, analyse lexical and grammatical shifts, identify influential figures, and ultimately comprehend the linguistic mechanisms and communicative drivers underpinning these transformations. To this end, the authors propose a novel framework employing Graph Convolutional Networks (GCNs) to model language dynamics, positioning context as a central signal and seeking to overcome limitations of existing methods in capturing the interaction between contextual signals.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "href": "chapter_ai-nepi_019.html#foundations-context-theoretical-frameworks-and-the-chemical-revolution-pilot",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot",
    "text": "17.1 Foundations: Context, Theoretical Frameworks, and the Chemical Revolution Pilot\nWithin the Cascade project, a Marie Curie doctoral network, researchers meticulously investigate the computational analysis of semantic change. PhD student Sophia Aguilar spearheads efforts to model context comprehensively, examining the interplay between its various dimensions. This work builds upon previous studies that modelled distinct types of context in isolation, now seeking to integrate these approaches for a more complete understanding of their interactions.\nThe chemical revolution provides a compelling pilot study for these methodological explorations, drawing upon the Royal Society Corpus (RSC). This historical period witnessed the significant conceptual shift from the long-standing phlogiston theory to Lavoisier’s oxygen theory—a transformation documented at resources such as chemistryworld.com and vividly represented by contemporary art, including the painting of Lavoisier and his wife. The investigation aims to model a spectrum of contextual factors:\n\nSituational (where)\nTemporal (when)\nExperiential (what)\nInterpersonal (who)\nTextual (how)\nCausal (why)\n\nFrom a linguistic standpoint, the core interest lies in how language adapts to such profound worldly changes. Two primary theoretical frameworks guide this inquiry. Firstly, language variation and register theory, as articulated by Halliday (1985) and Biber (1988), posits that situational context directly influences language use. Concurrently, the linguistic system itself offers variation, allowing concepts to be expressed in multiple ways—for instance, the evolution from “…air which was dephlogisticated…” to “…dephlogisticated air…” and ultimately to “…oxygen…”. Secondly, principles of rational communication and information theory, associated with the IDeaL SFB 1102 research centre and drawing on work by Jaeger and Levy (2007) and Plantadosi et al. (2011), suggest that this linguistic variation serves to modulate information content. Such modulation optimises communication for efficiency whilst maintaining cognitive effort at a reasonable level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "href": "chapter_ai-nepi_019.html#detecting-temporal-shifts-in-language-use-via-kullback-leibler-divergence",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence",
    "text": "17.2 Detecting Temporal Shifts in Language Use via Kullback-Leibler Divergence\nTo pinpoint precisely when linguistic transformations occur, investigators employ Kullback-Leibler Divergence (KLD), moving beyond comparisons of arbitrarily predefined periods. This information-theoretic measure, represented as p(unit|context), quantifies the difference in language use—specifically, the probability distributions of linguistic units within a given context—between distinct timeframes. For instance, language from the 1740s exhibits low divergence when compared to the 1780s, indicating relative similarity, whereas a comparison with the 1980s would reveal higher divergence due to substantial linguistic evolution.\nDegaetano-Ortlieb and Teich (2018, 2019) refined this into a continuous comparison method. Their technique systematically contrasts a “PAST” temporal window (e.g., the 20 years preceding a specific year) with a “FUTURE” window (e.g., the 20 years following it) across a corpus. Plotting KLD values over time—for example, from 1725 to 1845—reveals periods of accelerated linguistic change. Analysis of lexical units (lemmas) using this method highlights the shifting prominence of terms such as “electricity,” “dephlogisticated,” “experiment,” “nitrous,” “air,” “acid,” “oxide,” “hydrogen,” and “oxygen,” alongside others like “glacier,” “corpuscule,” “urine,” and “current.” Such patterns often signal the emergence or redefinition of concepts. Indeed, a notable period of divergence between 1760 and 1810 coincides with crucial scientific discoveries, including Henry Cavendish’s identification of hydrogen (then “inflammable air”) in 1766 and Joseph Priestley’s discovery of oxygen (as “dephlogisticated air”) in 1774.\nFurthermore, this KLD-based approach extends beyond vocabulary to encompass grammatical shifts. By defining the linguistic unit as a Part-of-Speech (POS) trigram, analysts can track changes in grammatical patterns. Comparisons between KLD analyses of lexis and grammar reveal that periods of significant lexical innovation often correspond with alterations in syntactic structures, suggesting a coupled evolution of vocabulary and grammar.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "href": "chapter_ai-nepi_019.html#analysing-paradigmatic-change-and-identifying-scientific-influence-with-cascade-models",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models",
    "text": "17.3 Analysing Paradigmatic Change and Identifying Scientific Influence with Cascade Models\nBeyond temporal detection, the investigation delves into paradigmatic context and the dynamics of conceptual change, referencing work by Fankhauser et al. (2017) and Bizzoni et al. (2019). One analytical technique involves plotting logarithmic growth curves of the relative frequency of specific chemical terms over extended periods, such as 1780 to 1840. In these visualisations, the size of bubbles corresponds to the square root of a term’s relative frequency, clearly depicting which terms are increasing or decreasing in usage. Accompanying scatter plots for distinct years—for example, 1780, 1800, and 1840—reveal evolving clusters of related chemical terms, with data often sourced from repositories like corpora.ids-mannheim.de.\nTo understand who spearheads and propagates these linguistic and conceptual shifts, Yuri Bizzoni, Katrin Menzel, and Elke Teich (associated with IDeaL SFB 1102) employ Cascade models, specifically Hawkes processes (Bizzoni et al. 2021). These models generate heatmaps that illustrate networks of influence, plotting “influencers” against “influencees”—typically scientists active during the period. The intensity of colour, often shades of blue, signifies the strength of influence. Through such analysis, distinct roles in the dissemination of new theories and terminologies emerge. For instance, in the context of the chemical revolution, Priestley appears as an “Innovator,” Pearson as an “Early Adopter,” and figures like Davy contribute to the “Early Majority” uptake.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "href": "chapter_ai-nepi_019.html#investigating-linguistic-realisation-and-communicative-drivers-of-change-through-surprisal",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal",
    "text": "17.4 Investigating Linguistic Realisation and Communicative Drivers of Change through Surprisal\nThe inquiry extends to how linguistic change manifests and the communicative pressures that might drive it, drawing on research by Viktoria Lima-Vaz (MA-Thesis, 2025) and Degaetano-Ortlieb et al. (under review), with contributions from Elke Teich. A key concept in this strand of analysis is “surprisal,” originating from Shannon’s (1949) information theory and further developed by Hale (2001), Levy (2008), and Crocker et al. (2016). Surprisal posits that the cognitive effort required to process a linguistic unit is proportional to its unexpectedness or improbability in a given context; for example, the word completing “Jane bought a ____” might have a different surprisal value than one completing “Jane read a ____.”\nApplying this to linguistic change, the research team examines shifts in how concepts are encoded. A single concept might transition through various linguistic forms, such as a clausal construction like “…the oxygen (which) was consumed,” a prepositional phrase like “…the consumption of oxygen…,” and eventually a more concise compound form like “…the oxygen consumption…”. The underlying hypothesis suggests that shorter, more communicatively efficient encodings tend to emerge and gain currency within a speech community. Longitudinal analysis, visualised through graphs plotting surprisal against year, supports this. For instance, comparing the surprisal trajectories of “consumption of oxygen” (prepositional phrase) and “oxygen consumption” (compound) often reveals that as the shorter compound form becomes more established, its average surprisal value decreases, indicating a reduction in cognitive processing effort for the community using that form.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "href": "chapter_ai-nepi_019.html#a-proposed-framework-graph-convolutional-networks-for-modelling-contextual-dynamics",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics",
    "text": "17.5 A Proposed Framework: Graph Convolutional Networks for Modelling Contextual Dynamics\nECR Sofía Aguilar, funded by the European Union through the CASCADE project, proposes a novel framework for modelling context in the analysis of language variation and change. This work stems from the understanding that language change is intrinsically linked to shifts in social context, including evolving goals, social structures, and domain-specific conventions. Current methodologies, such as semantic change studies, KLD applications, and static network approaches, effectively track shifts but often fall short in modelling the intricate interactions between various contextual signals. Aguilar’s proposed framework positions context as a central signal for modelling language dynamics, identifying Graph Convolutional Networks (GCNs) as a promising technological direction due to their capacity for powerfully modelling complex relational data.\nA pilot application of this framework targets the chemical revolution period (1760s-1820s) and unfolds in four stages:\n\nData Sampling: This stage involves using KLD to identify words distinctive for specific sub-periods within this era, thereby pinpointing relevant lexical items whose KLD contributions are high.\nNetwork Construction: The process begins by creating word- and time-aware feature vectors. BERT generates word vectors, whilst one-hot encoding captures temporal and other features. These combine to form a node feature matrix for successive 20-year intervals. KLD then measures the dissimilarity in these node feature vectors across periods, yielding a diachronic series of graphs. To manage complexity, the authors refine network size using community detection algorithms, such as that proposed by Riolo Newman (2020).\nLink Prediction: This stage aims to model how, when, and by whom specific words are used. Word profiles are augmented with semantic embeddings (e.g., from BERT), contextual metadata (such as author, journal, and period), and grammatical information (including part-of-speech tags and syntactic roles). A Transformer-GCN model then learns patterns from these rich profiles to predict new links within the network. The graph convolution component captures structural relationships, while the transformer’s attention mechanism highlights the most influential contextual features.\nEntity Alignment: This final stage facilitates the inspection and interpretation of change. It employs network motifs—small, overrepresented subgraphs that signify meaningful interaction structures. The Kavosh algorithm assists by grouping isomorphic graphs to identify these recurring motifs within the networks, offering insights into the patterns of linguistic and conceptual evolution.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "href": "chapter_ai-nepi_019.html#reflections-limitations-and-future-research-directions",
    "title": "17  Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution",
    "section": "17.6 Reflections: Limitations and Future Research Directions",
    "text": "17.6 Reflections: Limitations and Future Research Directions\nThe research acknowledges several profound questions that delineate its current limitations and chart future directions. A primary concern involves the nature of computationally tracing conceptual change: can current and future models move beyond capturing mere linguistic drift to apprehend deeper epistemic shifts? Another critical area of inquiry centres on how context integrates into the meaning-making processes of language models—specifically, whether metadata functions as external noise or as a core, internalised signal.\nFurther consideration must be given to defining the fundamental ‘unit’ of language change. Investigators question whether shifts are most effectively observed at the level of individual words, broader concepts, grammatical structures, or larger discourse patterns. The possibility of identifying recurring linguistic pathways for the emergence of new concepts also presents an intriguing avenue: do novel ideas tend to follow predictable linguistic trajectories as they gain acceptance? Finally, the challenge of interpretability in increasingly complex models remains paramount. Ensuring that the explanations generated by these models are genuinely meaningful, rather than merely plausible, is crucial for advancing the field.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html",
    "href": "chapter_ai-nepi_020.html",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "",
    "text": "Overview\nThe research team investigates the complexities of science funding, moving beyond traditional analyses of publications and grants to explore the internal processes of funding agencies. The National Human Genome Research Institute (NHGRI) serves as a pivotal case study, owing to its central role in the Human Genome Project and its recognised innovative capacity within the National Institutes of Health (NIH). An interdisciplinary team, comprising historians, physicists, ethicists, computer scientists, and former NHGRI leadership, meticulously analyses the institute’s extensive born-physical archive. This collection contains over two million pages of internal documents, including meeting notes, handwritten correspondence, presentations, and spreadsheets.\nTo manage and interpret this vast dataset, the investigators developed advanced computational tools. These include a bespoke handwriting removal model, leveraging U-Net architectures and synthetic data, to improve Optical Character Recognition (OCR) and enable separate handwriting analysis. Multimodal models combine vision, text, and layout modalities for tasks such as entity extraction and synthetic document generation. This capability proves crucial for training classifiers and ensuring Personally Identifiable Information (PII) redaction.\nCase studies powerfully demonstrate the efficacy of these methods. One reconstructs email correspondence networks within NHGRI during the International HapMap Project, revealing informal leadership structures like the “Kitchen Cabinet” and analysing their brokerage roles. Another models funding decisions for organism sequencing, incorporating biological, project-specific, reputational, and linguistic features to understand factors influencing success, thereby highlighting phenomena such as the Matthew Effect. The overarching aim involves transforming born-physical archives into accessible, interoperable digital knowledge to inform policy, enhance data accessibility, and address significant scientific questions about how science operates and innovation emerges. The consortium extends this approach to other archives, including federal court records and seismological data, actively seeking partners to engage with their newly funded initiative: “Born Physical, Studied Digitally.”",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "href": "chapter_ai-nepi_020.html#limitations-in-understanding-science-funding-through-public-data",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.1 Limitations in Understanding Science Funding through Public Data",
    "text": "18.1 Limitations in Understanding Science Funding through Public Data\nState-sponsored research has profoundly shaped the scientific landscape since the Second World War, operating under a social contract where public funds aim to yield societal benefits through policy, clinical applications, and technological advancements. Scholars in the science of science traditionally analyse this enterprise by examining publicly accessible data, primarily scientific publications and grant awards. Such analyses have indeed offered valuable insights into phenomena like the long-term impact of research, optimal team sizes, the genesis of interdisciplinary fields, and patterns of career mobility amongst scientists.\nNevertheless, relying solely on scientific articles presents a skewed and incomplete understanding of the scientific endeavour. Equating bibliometrics with the entirety of scientific activity constitutes an oversimplification of its inherent complexity. The authors contend that researchers can achieve a more profound comprehension by investigating the underlying processes, rather than focusing exclusively on the often-flawed representation provided by published outputs.\nDelving into these processes allows for the exploration of critical questions. For instance, does scientific inquiry shape funding priorities, or do funding mechanisms dictate the direction of science? Within the innovation pipeline, stretching from initial ideation to long-term impact, where do innovative ideas flourish, where do they spill over into other domains, and, crucially, where do they falter? Published articles seldom document failed projects, obscuring a significant portion of the scientific journey. Furthermore, understanding how funding agencies offer support beyond monetary grants and identifying potential biases in funding allocation remain central to a comprehensive view. The interplay between federal agencies, grants, scholars, knowledge creation, and eventual public benefit involves intricate pathways, including cooperative agreements, technology development feedback loops, and community engagement.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "href": "chapter_ai-nepi_020.html#the-human-genome-project-a-paradigm-of-big-science-in-biology",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology",
    "text": "18.2 The Human Genome Project: A Paradigm of “Big Science” in Biology\nThe Human Genome Project (HGP) stands as a seminal example of “big science” in biology, drawing parallels with Netpi-funded investigations into large-scale particle physics research. Launched with the ambitious goal of sequencing the entire human genome, the HGP mobilised tens of countries and thousands of researchers, marking a new era for biological inquiry. This colossal undertaking garnered public attention far exceeding that of previous biological research, which often focused on model organisms like Drosophila and C. elegans in laboratory settings.\nIts legacy endures profoundly; a vast majority of contemporary biological research, especially omics methodologies, depends critically on the reference genome established by the HGP. Indeed, the project catalysed the very emergence of genomics as a distinct scientific discipline. Furthermore, the HGP pioneered data-sharing practices that are now standard in the scientific community and successfully integrated computational methods with biological investigation.\nTwo principal organisations spearheaded this international effort: the Wellcome Trust in the United Kingdom and, in the United States, the National Human Genome Research Institute (NHGRI), which functioned as the HGP division of the National Institutes of Health (NIH). Francis Collins, then director of NHGRI and later NIH, played a prominent leadership role. Subsequent analyses reveal NHGRI as one of the NIH’s most innovative funding bodies. This distinction is evidenced by multiple metrics: a significant proportion of NHGRI-funded publications rank amongst the top 5% most cited; its research demonstrates high citation impact within a decade; it generates numerous patents leading to clinical applications; and its funded projects often exhibit high “disruption” scores. Despite this recognised innovativeness, the specific processes and strategies underpinning NHGRI’s success warrant deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "href": "chapter_ai-nepi_020.html#the-nhgri-archive-a-rich-resource-for-studying-science-in-action",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action",
    "text": "18.3 The NHGRI Archive: A Rich Resource for Studying Science in Action\nAn interdisciplinary team, uniting engineers with historians, physicists, ethicists, computer scientists, and even key figures like Francis Collins, former director of both NHGRI and NIH, spearheads an effort to understand the dynamics of scientific progress. Their research seeks to unravel the ascent of genomics, identify common failure modes in large scientific endeavours, trace innovation spillovers, and examine how funding agencies and academic researchers collaborate to advance scientific practice.\nCentral to this investigation is the NHGRI Archive, a unique repository preserved owing to the HGP’s historical significance. This archive houses a wealth of internal documentation spanning from the 1980s onwards, encompassing daily meeting notes from scientists coordinating the project, handwritten correspondence, conference agendas, formal presentations, spreadsheets, newspaper clippings marking pivotal moments, research proposals, and internal emails. Amounting to over two million pages and expanding by 5% each year through ongoing digitisation efforts, this born-physical collection presents a considerable challenge for large-scale analysis.\nThe content of these internal documents differs markedly from publicly accessible materials like Requests for Applications (RFAs) or peer-reviewed publications found in databases such as PubMed. Visualisations of the archive’s content reveal that internal documents, pertaining to major genomic initiatives like ENCODE, the International HapMap Project, and H3Africa—projects often commanding budgets in the tens or hundreds of millions of dollars and involving thousands of researchers—form distinct clusters, separate from the more homogenous categories of RFAs and publications. These internal records offer a granular view of the efforts to build foundational resources for the genomics community.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "href": "chapter_ai-nepi_020.html#computational-methodologies-for-archive-analysis-handwriting-and-multimodal-models",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models",
    "text": "18.4 Computational Methodologies for Archive Analysis: Handwriting and Multimodal Models\nThe analysis of the born-physical NHGRI archive necessitates sophisticated computational approaches, particularly for handling the extensive handwritten material it contains. The research team acknowledges the ethical complexities associated with applying AI to handwriting, given the potential for uncovering sensitive or private information. To navigate this, they developed a bespoke handwriting model, likely based on a U-Net architecture, specifically to remove handwritten portions from documents. This crucial step not only enhances the accuracy of Optical Character Recognition (OCR) on the remaining printed text but also allows for the creation of a distinct processing pipeline dedicated to handwriting recognition itself.\nBeyond handwriting, the team employs advanced multimodal models, drawing upon innovations from the burgeoning field of document intelligence. These models ingeniously integrate visual information (the document image), textual content, and layout structures. Such integration facilitates a range of tasks, prominently including precise entity extraction from complex documents. Moreover, these models enable the generation of synthetic documents. This capability proves invaluable for creating tailored training datasets, which in turn accelerates the development and refinement of new classification algorithms for various document analysis tasks. The process involves joint embedding of text and image inputs, supervised by layout information, and trained using a masked autoencoder objective, leading to separate text and vision decoders for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "href": "chapter_ai-nepi_020.html#entity-recognition-pii-redaction-and-email-network-reconstruction",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction",
    "text": "18.5 Entity Recognition, PII Redaction, and Email Network Reconstruction\nA critical aspect of processing the NHGRI archive involves the meticulous identification and handling of entities, particularly Personally Identifiable Information (PII). The archive contains sensitive details such as names of real individuals, credit card numbers, and social security numbers, pertaining to people some of whom remain active in governmental and academic roles. Consequently, robust methods for masking, removing, or disambiguating such information are paramount. The developed entity recognition models demonstrate strong performance, achieving F1 scores exceeding 0.9 for categories like ‘PERSON’ and ‘ORGANIZATION’ even with a modest number of fine-tuning samples (up to 500). These models identify various entities, including locations, email addresses, and identification numbers, as illustrated by examples of processed documents.\nTo showcase the analytical power derived from these processed documents, the investigators reconstructed an email correspondence network from the HGP era. By extracting entities from 5,414 scanned email documents, they mapped 62,511 email conversations. This network, visualised as a complex graph, allows for the association of individuals with their respective affiliations—be it NIH, an external academic institution, or a private company. Such mapping provides a structural basis for analysing communication patterns and collaborations during this pivotal period in genomics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "href": "chapter_ai-nepi_020.html#analysing-leadership-structures-the-international-hapmap-project-case-study",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study",
    "text": "18.6 Analysing Leadership Structures: The International HapMap Project Case Study\nNetwork analysis techniques offer powerful means to scrutinise the intricate coordination mechanisms within large scientific collaborations. The investigators applied these methods to the International HapMap Project, a significant genomics initiative that followed the HGP. Unlike the HGP’s focus on sequencing, the HapMap Project concentrated on cataloguing human genetic variation, thereby laying the groundwork for subsequent Genome-Wide Association Studies (GWAS). This multi-institutional, multi-agency endeavour raised questions about how funding bodies effectively manage such complex undertakings.\nEmploying community detection algorithms like stochastic block models, the research team identified distinct interacting groups within the HapMap Project’s communication network, such as those affiliated with academia versus the NIH. A particularly insightful discovery emerged from analysing leadership structures. Beyond the formally constituted Steering Committee, which comprised representatives from all participating institutions, their analysis computationally uncovered a previously undocumented informal leadership group, termed the “Kitchen Cabinet.” This select circle apparently convened prior to official steering committee meetings, likely to address potential issues proactively and ensure smoother formal proceedings.\nFurther analysis of brokerage roles within these communication networks revealed distinct operational styles. The “Kitchen Cabinet,” for instance, predominantly exhibited a “consultant” brokerage pattern—receiving and disseminating information primarily within its own group—a characteristic that distinguished it from the formal Steering Committee and the broader network. Notably, figures like Francis Collins were identified as playing significant consultant roles within this informal leadership body.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "href": "chapter_ai-nepi_020.html#modelling-funding-decisions-for-organism-sequencing",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.7 Modelling Funding Decisions for Organism Sequencing",
    "text": "18.7 Modelling Funding Decisions for Organism Sequencing\nThe rich dataset allows for portfolio analysis, offering insights into the decision-making processes of funding agencies. One compelling case study involved modelling the NHGRI’s decisions regarding which non-human organismal genomes to prioritise for sequencing following the completion of the Human Genome Project. Funders faced the complex task of allocating resources amongst numerous proposals, each advocating for a different organism, such as various primates.\nTo understand these decisions, the research team developed a machine learning model designed to recapitulate the actual funding outcomes. This model incorporated a diverse array of features. Biological characteristics, such as an organism’s genome size and its evolutionary distance to already sequenced model organisms, demonstrated predictive value (ROC AUC: 0.76 ± 0.05). Project-specific attributes—including the size of the proposing team, the time elapsed since their initial submission, gender equity within the team, and whether the proposal was standalone or internally generated—also proved influential (ROC AUC: 0.83 ± 0.04). Furthermore, reputational factors, such as the H-indices of the authors, the size of the scientific community supporting the proposal, and the proposers’ centrality within the NHGRI network, significantly impacted predictions (ROC AUC: 0.87 ± 0.04). Linguistic elements of the proposals themselves, like the strength of argumentation and the degree of repetitiveness, were also informative (ROC AUC: 0.85 ± 0.04).\nWhen all these feature categories were combined, the model achieved a high predictive accuracy (ROC AUC: 0.94 ± 0.03), indicating that each type of information contributes to understanding funding decisions. Subsequent feature interpretability analysis shed light on the relative importance of these factors. Notably, the findings suggested a “Matthew Effect” at play: proposals from authors with higher H-indices and those advocating for organisms supported by larger, more established scientific communities were more likely to secure funding. This aligns with the strategic objective of funding agencies to maximise downstream scientific impact and potential clinical applications.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "href": "chapter_ai-nepi_020.html#broader-applications-and-the-born-physical-studied-digitally-consortium",
    "title": "18  Our current understanding of funders of science is a limited view.",
    "section": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium",
    "text": "18.8 Broader Applications and the “Born Physical, Studied Digitally” Consortium\nThe methodologies developed for analysing the NHGRI archive represent a broader strategy for unlocking knowledge from born-physical historical records using advanced computational tools. The NHGRI project itself forms part of a larger consortium that extends this approach to diverse data sources, including United States federal court records and seismological data from initiatives like the EarthScope Consortium. This comprehensive workflow encompasses several stages: initial data and metadata ingestion, followed by sophisticated knowledge creation processes such as page stream segmentation, handwriting extraction, entity disambiguation, layout modelling, document categorisation, entity recognition, personal information redaction, and decision modelling. The ultimate aim is to apply these generated insights to answer pressing scientific questions, inform policy-making, and significantly improve data accessibility.\nA strong emphasis is placed on the critical need to preserve born-physical data. Vast quantities of such materials currently reside in vulnerable conditions, such as shipping containers, susceptible to damage and neglect. Ensuring their preservation and digitisation is vital for future scholarly and scientific inquiry. To this end, the researchers have established a newly funded consortium named “Born Physical, Studied Digitally,” supported by organisations including the NHGRI, NVIDIA, and the National Science Foundation (NSF). They actively seek testers, partners, and users to engage with their tools and methodologies.\nThis work gains particular urgency in light of recent discussions in the United States regarding the potential dissolution of agencies like NHGRI. Given NHGRI’s history as a highly innovative funding body, its archives contain invaluable data that can illuminate numerous unanswered scientific questions, underscoring the importance of its continued existence and the study of its legacy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Our current understanding of funders of science is a limited view.</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html",
    "href": "chapter_ai-nepi_021.html",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "",
    "text": "Overview\nMalte, Raphael, and Alex K. (attending via Zoom) explore innovative methods for transforming unstructured biographical sources into structured knowledge graphs. Their work thereby enables sophisticated querying and analysis. The team addresses the persistent challenge of computationally accessing the rich information contained within traditional formats, such as printed books and archives, which often lack inherent digital structure. Their core objective involves leveraging Large Language Models (LLMs) not as standalone solutions, but as integral components within a multi-stage pipeline designed for specific tasks. This pipeline aims to impose structure on unstructured data in a controllable manner.\nThe process commences with sources such as Polish biographical materials and German biographical handbooks, including Wer war wer in der DDR?. It then proceeds to extract entities—persons, places, countries, works—and their relationships, representing them as nodes and edges in a knowledge graph. Visualisation occurs through tools like Neo4j. This structured representation facilitates complex queries, such as investigating network formations amongst professionals in specific periods or tracing the evolution of ideas. The methodology emphasises a research-driven, data-oriented approach, incorporating human-in-the-loop validation, transparency, task decomposition, and modularity. Key outputs include validated triples (subject-predicate-object statements), ontologies tailored to research questions, and disambiguated entities linked to resources like Wikidata. The ultimate goal is to construct multilayered networks for deeper structural analysis and potentially enable natural language querying through technologies like GraphRAG.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "href": "chapter_ai-nepi_021.html#introduction-accessing-unstructured-biographical-knowledge",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.1 Introduction: Accessing Unstructured Biographical Knowledge",
    "text": "19.1 Introduction: Accessing Unstructured Biographical Knowledge\nInvestigators confront the significant challenge of unlocking knowledge embedded within unstructured biographical sources. Historically, valuable information, particularly within printed books and archives, has remained largely inaccessible to computational analysis due to its lack of inherent digital structure. Whilst earlier tools like Get Grasso aimed to digitise and process printed materials, the current investigation by Malte, Raphael, and Alex K. centres on biographical sources replete with detailed personal data. Such data proves crucial for understanding, for instance, specific phases in the evolution of physics. These sources, however, typically do not permit direct structured questioning.\nTo address this limitation, the authors propose employing Large Language Models (LLMs). Their core idea involves harnessing LLMs not as all-encompassing solutions, but as specialised tools within a controllable pipeline to construct knowledge graphs from this unstructured data. Such a knowledge graph comprises entities—individuals, locations, organisations, or publications—represented as nodes, and the relationships between them, depicted as edges. Polish biographical collections and German-language resources, such as the handbook Wer war wer in der DDR?, serve as primary examples of the source material. Visualisation and management of these graphs can occur through platforms like Neo4j. Crucially, the project seeks the most useful LLM for specific tasks within this chain, rather than pursuing a universally perfect model. This approach allows for complex queries, such as mapping an individual’s birth and work locations or analysing how professional networks formed and evolved during particular historical periods.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "href": "chapter_ai-nepi_021.html#conceptual-framework-from-text-to-knowledge-graph",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.2 Conceptual Framework: From Text to Knowledge Graph",
    "text": "19.2 Conceptual Framework: From Text to Knowledge Graph\nThe transformation of raw biographical text into a structured knowledge graph follows a carefully designed conceptual framework. Initially, textual information, such as entries from biographical dictionaries, undergoes processing into manageable segments or “chunks”. From these chunks, an extraction pipeline works to identify key entities and their interrelations, which the authors then assemble into a knowledge graph. For instance, a biographical entry for “Bartsch Henryk” might yield entities like his name (PERSON), role (“ks. ewang.” - evangelical priest, ROLE), birth date (DATE), and birthplace (“Władysławowo”, LOCATION), alongside relationships such as “born in” or “travelled to” various locations like Italy (Włochy) or Egypt (Egipt). These relationships manifest as structured triples, for example, “(Bartsch Henryk, is a, ks. ewang.)”.\nThis entire process unfolds within a two-stage pipeline. The first stage, “Ontology agnostic Open Information Extraction (OIE)”, focuses on loading and chunking data, performing initial information extraction, validating these extractions, and standardising them, with a quality check to determine sufficiency. Subsequently, the second stage, “Ontology driven Knowledge Graph (KG) building”, commences with the formulation of Competency Questions (CQs) that guide ontology creation. This stage further involves creating SHACL (Shapes Constraint Language) shapes for validation, mapping extracted information to the ontology, performing entity disambiguation (including linking to Wiki IDs), and finally validating the resultant graph using RDF Star and SHACL. Both stages integrate LLM interaction and maintain a crucial “Human-in-the-Loop” layer for oversight and refinement.\nFive core principles underpin this methodology:\n\nA research-driven and data-oriented approach ensures relevance.\nHuman-in-the-loop mechanisms guarantee quality and address ambiguities.\nTransparency allows for process verification.\nTask decomposition simplifies complexity.\nModularity facilitates iterative development and improvement of individual components.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-1-ontology-agnostic-open-information-extraction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction",
    "text": "19.3 Pipeline Stage 1: Ontology-Agnostic Open Information Extraction\nThe initial stage of the pipeline, ontology-agnostic Open Information Extraction (OIE), systematically processes raw biographical texts into preliminary structured data. It commences with loading and chunking pre-processed data files. This step transforms biographical information into a semi-structured format, typically a table where each row represents a text chunk associated with a name, role, the chunk content itself (“Biografie”), and a unique chunk identifier.\nFollowing data preparation, the OIE Extraction step performs an “Extraction Call”. Using the person’s name and the relevant biographical text chunk as input (for example, ‘Havemann, Robert’ and text like ‘… 1935 Prom. mit … an der Univ. Berlin…’), this process generates raw Subject-Predicate-Object (SPO) triples. These triples also include associated metadata, such as the timeframe of the event. Subsequently, an OIE Validation step scrutinises these raw triples. Taking the original text chunk and the extracted triples, a “Validation Call” produces validated SPO triples, now augmented with a confidence score for each assertion.\nTo ensure the reliability of this extraction phase, the OIE Standard step compares the validated triples against a “Gold Standard”—a manually created or verified sample of triples. This comparison yields crucial quality metrics, including F1 score, precision, and recall. These metrics inform a decision point regarding whether the OIE quality is sufficient to proceed to the next stage of the pipeline or if further refinement of the OIE steps proves necessary.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "href": "chapter_ai-nepi_021.html#pipeline-stage-2-ontology-driven-knowledge-graph-construction",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction",
    "text": "19.4 Pipeline Stage 2: Ontology-Driven Knowledge Graph Construction\nOnce high-quality triples emerge from Stage 1, the second stage, ontology-driven Knowledge Graph (KG) construction, commences. This phase begins with the formulation of Competency Questions (CQs), which the authors manually refine based on a sample of the validated triples. These CQs articulate the specific research queries the final knowledge graph should address; for instance, “Which GDR scientists conducted research or studied in the Soviet Union between 1950 and 1989? What are their later Party affiliations?”.\nGuided by these CQs and the sample triples, an Ontology Creation step, via a “Generation Call”, produces an ontology definition. This definition specifies classes (e.g., “Person” as an owl:Class, “doctorate” as a class of “academicEvent”) and properties (e.g., :eventYear, :eventInstitution, :eventTopic). Concurrently, the team creates SHACL (Shapes Constraint Language) shapes to define constraints for validating the graph’s structure and content.\nThe subsequent Ontology Mapping step, through a “Mapping Call”, aligns the validated triples with this newly defined ontology and SHACL shapes, incorporating relevant metadata. This results in mapped triples expressed as Conceptual RDF. An example might show an instance for Havemann achieving a doctorate event, with properties specifying the year (1935), topic (Colloid Chemistry), and institution (University of Berlin). Following this, the Disambiguation Wiki ID step processes these mapped triples. This crucial step resolves ambiguities and links entities to external identifiers, such as Wikidata IDs (e.g., mapping “Robert Havemann” to wd:Q77116), producing disambiguated triples and RDF-star statements that allow metadata to be attached to individual statements. Finally, the entire constructed knowledge graph undergoes RDF Star and SHACL Validation to ensure its integrity and conformance to the defined ontology and constraints.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "href": "chapter_ai-nepi_021.html#illustrative-applications-zieliński-compilations-and-gdr-biographies",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies",
    "text": "19.5 Illustrative Applications: Zieliński Compilations and GDR Biographies\nMalte, Raphael, and Alex K. illustrate the utility of their knowledge graph extraction pipeline through two distinct case studies. The first involves an analysis of Zieliński’s compilations, which encompass three complementary works: a biographical dictionary, a dictionary of publications, and a dictionary of achievements. By applying their knowledge-graph approach to this corpus, the investigators can explore complex questions concerning the roles of individuals in idea development, migration patterns of scholars, the function of journals as boundary objects, and the identification of central journals within specific academic disciplines. An example network derived from these compilations reveals a graph of 3598 nodes and 5443 edges, where nodes representing editors are coloured green and others pink, clearly showing clusters of interconnected individuals.\n\n\n\nSlide 20\n\n\nThe second case study focuses on the German biographical lexicon Wer war wer in der DDR? Ein Lexikon ostdeutscher Biographien. Published in the 1990s, this resource contains approximately 4000 entries on key figures from East German history and is frequently consulted by researchers and journalists. The presentation displays sample entries for Gustav Hertz and Robert Havemann.\n\n\n\nSlide 21\n\n\nAn analytical example derived from this dataset examines correlations between awards received, attainment of high positions, and SED (Socialist Unity Party of Germany) affiliation. A scatter plot visualises these relationships for various awards, including the Karl-Marx-Orden and the Nationalpreis der DDR.\n\n\n\nSlide 22\n\n\nFurther comparative data highlights differences between recipients of the Karl-Marx-Orden and non-recipients regarding SED membership share, average birth year, and holding of specific high-ranking positions within structures like the Politbüro or Ministerrat.\n\n\n\nSlide 23",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  },
  {
    "objectID": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "href": "chapter_ai-nepi_021.html#conclusion-and-future-trajectories",
    "title": "19  From Source to Structure: Extracting Knowledge Graphs with LLMs",
    "section": "19.6 Conclusion and Future Trajectories",
    "text": "19.6 Conclusion and Future Trajectories\nThe project successfully demonstrates a method to progress from isolated biographical entries to a system capable of supporting structural queries across large datasets. Nevertheless, Malte, Raphael, and Alex K. identify ongoing challenges, primarily in refining entity disambiguation techniques and improving benchmarking procedures to assess performance rigorously.\nImmediate next steps involve completing and further enhancing the extraction pipeline. The team also plans a systematic comparison of its results against those produced by other established pipelines and software packages. Subsequently, the authors intend to scale their methodology to analyse full datasets comprehensively.\nLooking further ahead, future goals include fine-tuning the pipeline to cater to more specific use cases. The investigators will also explore the potential of GraphRAG (Graph Retrieval Augmented Generation) to enable natural language querying of the generated knowledge graphs, thereby making complex data more accessible. Furthermore, the team plans to construct multilayered networks, potentially using frameworks like ModelSEN, to facilitate deeper and more nuanced structural analyses of the biographical information.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>From Source to Structure: Extracting Knowledge Graphs with LLMs</span>"
    ]
  }
]