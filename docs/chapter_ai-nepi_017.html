<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jochen Büttner">
<meta name="dcterms.date" content="2025-06-21">

<title>15&nbsp; Making Transformer-Based LLMs Time-Aware: A Proof of Concept – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_018.html" rel="next">
<link href="./chapter_ai-nepi_016.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-c2d8198b7f72dec16de60f0cb3fab69f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a0afd4a9b901cc50d8ed64d4ec5e2aec.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_017.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Large Language Models for History, Philosophy and Sociology of Science - Workshop Proceedings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI-assisted Methods for History and Philosophy of Science: Workshop Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Investigating the transdiciplinary application of model templates through projective methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Towards Computational HPSS: Tracing the Influence of the Ancient Wisdom Tradition on Early Modern Science using the LLM-powered Semantic Matching Tool of the VERITRACE project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interpretability for LLMs: Scientific Insights, Transparency, and Applications in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Scientific Reasoning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers - the mixed use of LLM’s and semantic artifacts to support the understanding of science dynamics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Applying Retrieval-Augmented Generation to Philosophical Research: A Case Study and Methodological Insights</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural Pursuit: The Case of Quantum Gravity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Towards Interpretable Models: Bridging Traditional and Deep Learning Methods for Tracing Linguistic Change in English Scientific Writing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">From Source to Structure – Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#the-problem-of-implicit-time" id="toc-the-problem-of-implicit-time" class="nav-link" data-scroll-target="#the-problem-of-implicit-time"><span class="header-section-number">15.1</span> The Problem of Implicit Time</a></li>
  <li><a href="#formalising-temporal-dependence" id="toc-formalising-temporal-dependence" class="nav-link" data-scroll-target="#formalising-temporal-dependence"><span class="header-section-number">15.2</span> Formalising Temporal Dependence</a></li>
  <li><a href="#the-time-transformer-architecture" id="toc-the-time-transformer-architecture" class="nav-link" data-scroll-target="#the-time-transformer-architecture"><span class="header-section-number">15.3</span> The <em>Time Transformer</em> Architecture</a></li>
  <li><a href="#dataset-curation-and-pre-processing" id="toc-dataset-curation-and-pre-processing" class="nav-link" data-scroll-target="#dataset-curation-and-pre-processing"><span class="header-section-number">15.4</span> Dataset Curation and Pre-processing</a></li>
  <li><a href="#baseline-model-and-training" id="toc-baseline-model-and-training" class="nav-link" data-scroll-target="#baseline-model-and-training"><span class="header-section-number">15.5</span> Baseline Model and Training</a></li>
  <li><a href="#implementing-the-time-transformer" id="toc-implementing-the-time-transformer" class="nav-link" data-scroll-target="#implementing-the-time-transformer"><span class="header-section-number">15.6</span> Implementing the <em>Time Transformer</em></a></li>
  <li><a href="#experiment-one-synonymic-succession" id="toc-experiment-one-synonymic-succession" class="nav-link" data-scroll-target="#experiment-one-synonymic-succession"><span class="header-section-number">15.7</span> Experiment One: Synonymic Succession</a></li>
  <li><a href="#experiment-two-collocation-fixation" id="toc-experiment-two-collocation-fixation" class="nav-link" data-scroll-target="#experiment-two-collocation-fixation"><span class="header-section-number">15.8</span> Experiment Two: Collocation Fixation</a></li>
  <li><a href="#conclusions-and-future-challenges" id="toc-conclusions-and-future-challenges" class="nav-link" data-scroll-target="#conclusions-and-future-challenges"><span class="header-section-number">15.9</span> Conclusions and Future Challenges</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Making Transformer-Based LLMs Time-Aware: A Proof of Concept</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jochen Büttner <a href="mailto:buettner@gea.mpg.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Max Planck Institute of Geoanthropology
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>This chapter details a novel approach for integrating explicit time awareness into <em>Transformer</em>-based Large Language Models (LLMs). The authors identify a core limitation in current models: they possess only an implicit, statistical understanding of time derived from their training data. This deficiency leads to an inability to resolve time-dependent contradictory information and contributes to a pronounced recency bias.</p>
<p>To address this, the team proposes the <em>Time Transformer</em>, an architecture that incorporates an explicit temporal dimension directly into the token embedding space. This minimal adjustment allows the model to learn the influence of time on language patterns without altering the fundamental training objective of maximising log likelihood.</p>
<p>A proof of concept was developed using a small, decoder-only <em>Transformer</em> model built from scratch, featuring 39 million parameters. For training, the authors curated a specialised dataset of UK Met Office daily weather reports from 2018 to 2024, selected for its restricted vocabulary and repetitive linguistic structures.</p>
<p>Two experiments involving synthetically injected temporal drift demonstrated the model’s efficacy. The first involved a ‘synonymic succession’ where one word was progressively replaced by another over a year, a pattern the model successfully learned and reproduced. The second, a more complex ‘collocation fixation’, altered word co-occurrence probabilities over time, which the model also learned, as verified by analysing its internal attention mechanisms.</p>
<p>Whilst the proof of concept is successful, the authors acknowledge significant challenges for broader application. These include the necessity of training new models from scratch and the extensive data curation required to assign accurate timestamps to all training sequences.</p>
</section>
<section id="the-problem-of-implicit-time" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="the-problem-of-implicit-time"><span class="header-section-number">15.1</span> The Problem of Implicit Time</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Current Large Language Models derive their understanding of time statistically, extracting implicit cues from vast training corpora. Whilst remarkably capable, this approach has inherent limitations. Models cannot easily resolve information that is contradictory without temporal context; for example, two statements identifying different dominant neural network architectures are both valid, but at different points in time. During training, these sentences compete directly for attention, forcing the model into a state where it cannot perfectly fulfil its objective because validating one statement necessitates penalising the other.</p>
<p>Consequently, during inference, these models often exhibit a recency bias, favouring the most recently prevalent information. An input sequence about neural architectures will likely elicit the completion ‘Transformers’, even though ‘long short-term memories’ (<em>LSTMs</em>) is also present within its learned knowledge. To retrieve this older information, users must resort to prompt engineering—adjusting the input by adding a year or changing a verb’s tense. This process is imprecise and unreliable, as it depends on exploiting how the model has happened to learn temporal cues.</p>
<p>A more robust solution requires models that are explicitly time-aware, capable of learning and reproducing evolving patterns as a direct function of time. To this end, the authors have developed a proof of concept that achieves this for generative language models.</p>
</section>
<section id="formalising-temporal-dependence" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="formalising-temporal-dependence"><span class="header-section-number">15.2</span> Formalising Temporal Dependence</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_05.png" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>To formalise the problem, it is necessary to consider the core function of a language model. Based on its training data, an LLM learns to estimate the probability distribution over its entire vocabulary for the next token, given a preceding sequence of tokens. It then outputs the most probable continuation.</p>
<p>A critical factor, however, is that these probabilities are not static in the real world; they are inherently a function of time. For instance, the probability of the token ‘Transformers’ completing a specific sentence about neural architectures was effectively zero in 2017.</p>
<p>Despite this reality, the training process typically treats the probability distributions for token sequences as static. This simplification means that when the model is later used for inference, it can only reflect the temporal drift in language patterns via in-context learning. The model’s ability to generate time-appropriate text is therefore contingent on the specific cues provided in the immediate prompt, rather than on a fundamental, built-in understanding of temporal dynamics.</p>
</section>
<section id="the-time-transformer-architecture" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="the-time-transformer-architecture"><span class="header-section-number">15.3</span> The <em>Time Transformer</em> Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>To model time-dependent probability distributions effectively, the authors propose a new architecture, the <em>Time Transformer</em>, as a more efficient alternative to data-intensive methods like time-slicing. The core idea is elegant in its simplicity. Every natural language processing task begins by converting tokens into a vectorial representation, or embedding, within a latent space that is learned during training. The <em>Time Transformer</em> augments this process by adding a single, additional dimension to the embedding that explicitly encodes the token’s time of utterance.</p>
<p>In this model, every token in an input sequence is assigned a specific time value, meaning its vector representation will differ slightly based on when it was recorded. When these time-aware embeddings are processed by the <em>Transformer</em>, the resulting output probability distributions for subsequent tokens are inherently time-dependent.</p>
<p>Crucially, the training objective remains the standard maximisation of log likelihood. The beauty of this approach lies in the <em>Transformer’s</em> innate ability to process statistical relationships; it learns precisely how much influence the temporal dimension should have on each token, allowing some words to remain stable over time whilst others change significantly.</p>
</section>
<section id="dataset-curation-and-pre-processing" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="dataset-curation-and-pre-processing"><span class="header-section-number">15.4</span> Dataset Curation and Pre-processing</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>To test the <em>Time Transformer</em> concept, the authors required a dataset with a restricted language and a small vocabulary, which would allow a modest model to learn its linguistic patterns. They identified UK Met Office daily weather reports as an ideal candidate. These reports, available online as monthly PDFs from the UK’s national meteorological service, feature highly repetitive language. As a potential alternative, the <em>TinyStories</em> dataset was also noted.</p>
<p>The team scraped all daily reports from 2018 to 2024, creating a corpus of roughly 2,500 documents. The text was then chunked and tokenised using <em>Keras TextVectorization</em>. A simple pre-processing scheme was employed, which ignored case and punctuation and did not use subword tokenisation. This straightforward approach was sufficient for the simple language of the reports, which, across seven years of data, comprised a vocabulary of only 3,400 words.</p>
</section>
<section id="baseline-model-and-training" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="baseline-model-and-training"><span class="header-section-number">15.5</span> Baseline Model and Training</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>Before implementing the temporal component, the authors constructed a baseline ‘vanilla’ <em>Transformer</em> from scratch to confirm that a small model could learn the language patterns of the weather report dataset. This modest, decoder-only model features four decoder layers. Each layer is composed of a multi-head attention block with eight heads, followed by a normalisation layer, a non-linear feed-forward layer, and a final normalisation layer. A concluding dense layer performs the classification task of assigning probabilities across the vocabulary.</p>
<p>The resulting model is very small by modern standards, with only 39 million parameters and a file size of 150 MB. It was trained on an HPC cluster in Munich using two A100 GPUs, with each training epoch completing in just 11 seconds. After training, the model demonstrated its ability to autoregressively generate coherent and realistic weather reports from a simple seed phrase, proving it had successfully captured the linguistic patterns of the source data. The code for this model and the subsequent experiments is publicly available.</p>
</section>
<section id="implementing-the-time-transformer" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="implementing-the-time-transformer"><span class="header-section-number">15.6</span> Implementing the <em>Time Transformer</em></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>Adapting the baseline model into the <em>Time Transformer</em> requires a remarkably minimal architectural change. The implementation reserves just one dimension within the model’s 512-dimensional latent semantic space to carry temporal information. For this proof of concept, every token is assigned a non-trainable, min-max normalised value corresponding to the day of the year on which its source report was published.</p>
<p>This specific encoding—the day of the year—was deliberately chosen to exploit the seasonal variations naturally present in the weather data, such as the higher frequency of ‘snow’ in winter and ‘hot’ in summer. This design choice enables the model to directly associate linguistic patterns with cyclical time. Nevertheless, the framework is flexible, and any other form of time embedding could be integrated as needed.</p>
</section>
<section id="experiment-one-synonymic-succession" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="experiment-one-synonymic-succession"><span class="header-section-number">15.7</span> Experiment One: Synonymic Succession</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>The first experiment sought to determine if the new architecture could efficiently learn a temporal drift injected into its training data. The authors designed a ‘synonymic succession’ by systematically replacing the word ‘rain’ with ‘liquid sunshine’. This replacement was governed by a sigmoid function tied to the day of the year, where the probability of replacement was zero at the beginning of the year and gradually increased to one by the end.</p>
<p>After training on this modified dataset, the model was tasked with generating one weather prediction for every day of the year. By counting the monthly frequencies of ‘rain’ and ‘liquid sunshine’ in the output, the team confirmed the experiment’s success. The generated text, with some expected statistical variation, faithfully reproduced the engineered pattern: ‘rain’ appeared almost exclusively in the early months, ‘liquid sunshine’ dominated the later months, and the transition occurred precisely in the middle of the year.</p>
</section>
<section id="experiment-two-collocation-fixation" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="experiment-two-collocation-fixation"><span class="header-section-number">15.8</span> Experiment Two: Collocation Fixation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>Seeking a more complex challenge than simple word replacement, a second experiment was designed to test if the model could learn a change in word co-occurrence. This ‘collocation fixation’ synthetically altered the weather language over the course of the year. Specifically, instances of the word ‘rain’ not already followed by ‘and’ were replaced with the phrase ‘rain and snow’, with the probability of this replacement increasing throughout the year. From the model’s perspective, this changes the language such that by the year’s end, the appearance of ‘snow’ becomes almost entirely conditioned on the preceding token being ‘rain’.</p>
<p>Once again, the model successfully learned the injected pattern. Generated forecasts for the end of the year almost always featured ‘rain and snow’ together, whereas forecasts for the beginning of the year showed more varied patterns. Deeper analysis into the model’s internals provided further proof. The authors found that specific attention heads had specialised to detect this relationship, paying significantly more attention to the token ‘rain’ late in the year when deciding whether to generate ‘snow’.</p>
</section>
<section id="conclusions-and-future-challenges" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="conclusions-and-future-challenges"><span class="header-section-number">15.9</span> Conclusions and Future Challenges</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_017_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>The research successfully demonstrates as a proof of concept that <em>Transformer</em>-based LLMs can be made explicitly time-aware. This is achieved simply by adding a temporal dimension to the initial token embeddings. Whilst this opens up fascinating application possibilities, several potential next steps and significant challenges remain. One promising avenue for future work is to investigate whether the explicit time signal could actually make training more efficient, as it provides a clear signal for patterns the model would otherwise have to decipher from implicit clues.</p>
<p>However, major hurdles exist for widespread application. Firstly, because this is a novel architecture, one cannot simply fine-tune an existing pre-trained model; it requires training from scratch, a computationally prohibitive task for large-scale models. Secondly, the approach sacrifices the simplicity of metadata-free learning. It necessitates a rigorous data curation process to assign an accurate date to every text sequence, a complex and often ambiguous task, especially for historians.</p>
<p>As a final thought, a more practical application might be to use this principle not for a full generative model, but to build a targeted, <em>BERT</em>-like embedder for specific, time-sensitive analytical tasks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_016.html" class="pagination-link" aria-label="Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance across Text Levels</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_018.html" class="pagination-link" aria-label="Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>