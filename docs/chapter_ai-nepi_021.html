<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Schlattmann, Aleksandra Kaye &amp; Malte Vogl">
<meta name="dcterms.date" content="2025-01-01">

<title>19&nbsp; From Source to Structure: Extracting Knowledge Graphs with LLMs – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./chapter_ai-nepi_020.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-fe5eeb5af71a333b155c360431d06b9a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e463572c889c87c7eefd27e1777fa793.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="19&nbsp; From Source to Structure: Extracting Knowledge Graphs with LLMs – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta property="og:description" content="The project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using Large Language Models (LLMs) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries. The problem addressed is the lack of inherent structure in many valuable historical sources, preventing complex analytical queries. The proposed solutio…">
<meta property="og:image" content="images/ai-nepi_021_slide_01.jpg">
<meta property="og:site_name" content="AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:title" content="19&nbsp; From Source to Structure: Extracting Knowledge Graphs with LLMs – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:description" content="The project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using Large Language Models (LLMs) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries. The problem addressed is the lack of inherent structure in many valuable historical sources, preventing complex analytical queries. The proposed solutio…">
<meta name="twitter:image" content="images/ai-nepi_021_slide_01.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_021.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper: Transdisciplinary Investigations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computational HPSS: Tracing Ancient Wisdom’s Influence with VERITRACE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and Scientific Insights in Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems in Philosophy and HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural pursuit across scales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">LLMs for Chemical Knowledge Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Interpretable Models for Linguistic Change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">LLM for HPS Studies: Analyzing the NHGRI Archive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">19.1</span> Overview</a></li>
  <li><a href="#introduction-extracting-structure-from-unstructured-sources" id="toc-introduction-extracting-structure-from-unstructured-sources" class="nav-link" data-scroll-target="#introduction-extracting-structure-from-unstructured-sources"><span class="header-section-number">19.2</span> Introduction: Extracting Structure from Unstructured Sources</a></li>
  <li><a href="#two-stage-pipeline-stage-1---open-information-extraction" id="toc-two-stage-pipeline-stage-1---open-information-extraction" class="nav-link" data-scroll-target="#two-stage-pipeline-stage-1---open-information-extraction"><span class="header-section-number">19.3</span> Two-Stage Pipeline: Stage 1 - Open Information Extraction</a></li>
  <li><a href="#two-stage-pipeline-stage-2---knowledge-graph-structuring" id="toc-two-stage-pipeline-stage-2---knowledge-graph-structuring" class="nav-link" data-scroll-target="#two-stage-pipeline-stage-2---knowledge-graph-structuring"><span class="header-section-number">19.4</span> Two-Stage Pipeline: Stage 2 - Knowledge Graph Structuring</a></li>
  <li><a href="#use-cases-and-applications" id="toc-use-cases-and-applications" class="nav-link" data-scroll-target="#use-cases-and-applications"><span class="header-section-number">19.5</span> Use Cases and Applications</a></li>
  <li><a href="#conclusion-challenges-and-future-work" id="toc-conclusion-challenges-and-future-work" class="nav-link" data-scroll-target="#conclusion-challenges-and-future-work"><span class="header-section-number">19.6</span> Conclusion, Challenges, and Future Work</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Show code</button></div></div>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Raphael Schlattmann, Aleksandra Kaye &amp; Malte Vogl <a href="mailto:raphael.schlattmann@tu-berlin.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Technische Universität Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    The project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using Large Language Models (LLMs) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries. The problem addressed is the lack of inherent structure in many valuable historical sources, preventing complex analytical queries. The proposed solutio…
  </div>
</div>


</header>


<section id="overview" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">19.1</span> Overview</h2>
<p>The project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using <em>Large Language Models</em> (<em>LLMs</em>) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries.</p>
<p>The problem addressed is the lack of inherent structure in many valuable historical sources, which prevents complex analytical queries. The proposed solution is a two-stage pipeline. Stage 1 involves <em>Open Information Extraction</em> (<em>OIE</em>) using an <em>LLM</em> to extract subject-predicate-object triples from the text. This is followed by validation and refinement using an <em>LLM</em> ensemble acting as an adversary.</p>
<p>This stage includes a human-in-the-loop evaluation against domain expert extractions to assess quality using classical performance metrics. Stage 2 focuses on structuring the extracted triples into a knowledge graph. This stage is driven by research questions, defined as competency questions, which guide the creation of a domain-specific ontology.</p>
<p><em>LLMs</em> are used to draft both competency questions and the ontology, with human experts providing final refinement. Entity disambiguation is performed, including resolution to <em>Wikidata</em> instances. The structured data, along with metadata (source, scoring, chunk), is encoded into an <em>RDF star graph</em>. The pipeline includes a final validation step before export.</p>
<p>Case studies include Zielinski’s Polish biographical dictionary (1930) and the “Who was who in the GDR” reference work. Initial experiments demonstrate the ability to build networks (e.g., editors and authors) and conduct structured analyses (e.g., correlation between state awards and political affiliation/roles).</p>
<p>Key challenges identified are improving entity disambiguation and establishing proper benchmarking. Future work involves refining and completing the pipeline, systematic comparison with other graph extraction tools (<em>Neo4j graph builder</em>, <em>Microsoft graph rack</em>), developing graph <em>RAG</em> systems for natural language querying, and building multi-layered networks for deeper analysis. The approach emphasizes task decomposition, data/research question-driven structuring, and verifiability through human intervention at key points.</p>
</section>
<section id="introduction-extracting-structure-from-unstructured-sources" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="introduction-extracting-structure-from-unstructured-sources"><span class="header-section-number">19.2</span> Introduction: Extracting Structure from Unstructured Sources</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>The project aims to extract knowledge graphs from source material using <em>Large Language Models</em> (<em>LLMs</em>). The primary focus is on accessing and utilizing new types of sources for research that are currently computationally inaccessible due to their unstructured nature. While historical, philosophical, and social science (<em>HPSS</em>) research often utilizes structured data sources such as publication databases or email archives, a significant amount of valuable information resides in unstructured formats, particularly printed books and biographical dictionaries. The core problem addressed is the inability to perform structured queries on these unstructured sources.</p>
<p><em>LLMs</em> offer the potential to impose structure on this unstructured data. Historically, efforts like the <em>Get Grass</em> project attempted to address the computational accessibility of printed books. The current project specifically targets biographical sources, which are rich in detailed information about individuals but lack inherent structure. This absence of structure prevents researchers from asking complex, structured questions beyond simple facts like birth dates or work locations.</p>
<p>The goal is to enable queries about how professions formed networks over specific periods, how individuals migrating between locations contributed to the spread of ideas, or the specific roles of editors within a corpus in disseminating knowledge. The proposed solution involves using <em>LLMs</em> to construct knowledge graphs from this unstructured data in a controllable manner. A knowledge graph represents information as entities (such as persons, places, countries, or works) which become nodes in the graph. Relationships identified between these entities in the source material are represented as edges connecting the nodes. This structure allows for sophisticated structured querying.</p>
<p>The <em>Neo4j</em> graph database is used for representation and querying of the resulting knowledge graphs. The approach positions the <em>LLM</em> as one component within a larger processing pipeline, emphasizing its utility for specific tasks rather than seeking a universally perfect model. An example source snippet, an entry about the evangelical priest Henrik Bartsch born in 1832 who traveled and wrote books, illustrates the type of material processed. Traditional <em>Natural Language Processing</em> (<em>NLP</em>) approaches, such as <em>NLTK</em>, are often insufficient to extract the full contextual richness from such entries. The desired output is structured data in the form of statements or triples, capturing details like profession, birth date, birth place, and travel destinations from the text.</p>
</section>
<section id="two-stage-pipeline-stage-1---open-information-extraction" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="two-stage-pipeline-stage-1---open-information-extraction"><span class="header-section-number">19.3</span> Two-Stage Pipeline: Stage 1 - Open Information Extraction</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_08.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>The process of transforming unstructured text into a structured knowledge graph is implemented as a two-stage pipeline. The first stage is responsible for extracting statements from the input text, while the second stage constructs the knowledge graph from these extracted statements, adhering to defined rules to ensure consistency and clear categorization.</p>
<p>The pipeline is built upon several core principles:</p>
<ul>
<li><p>It employs task decomposition, breaking down the complex process into smaller, controllable, and verifiable steps.</p></li>
<li><p>It is data and research question driven, meaning the final structure of the knowledge graph is explicitly guided by the specific research aims of the project, rather than being solely dictated by a predefined ontology.</p></li>
<li><p>Verifiability is integrated through human-in-the-loop steps at critical points in the process.</p></li>
</ul>
<p>The input data for the pipeline often originates from messy, unstructured sources that typically require <em>Optical Character Recognition</em> (<em>OCR</em>) or scraping. Following these initial steps, the data undergoes preprocessing to achieve a semi-structured format before entering the pipeline.</p>
<p>The first central step of Stage 1 is <em>Open Information Extraction</em> (<em>OIE</em>). This step utilizes a <em>Large Language Model</em> (<em>LLM</em>) to extract all subject-predicate-object triples it can identify within the text, without relying on any preset categories or schemas. <em>OIE</em> is a rapidly evolving research area, with a significant shift towards using <em>LLMs</em> for this task.</p>
<p>The second step in Stage 1 involves <em>LLM</em> ensembling. A second <em>LLM</em> model is employed to validate and refine the initial statements extracted by the first model. This second model is specifically prompted to act as an adversary to the first, critically evaluating its output, correcting errors, identifying any missed triples, and assigning a confidence score to the extraction. This ensembling approach significantly enhances the quality of the extracted statements.</p>
<p>The final part of Stage 1 is evaluation. A sample of the validated output is evaluated against a corresponding sample created independently by domain experts. Classical performance metrics are calculated to quantify the quality of the extraction. This evaluation step represents the first instance of human-in-the-loop intervention, where domain experts judge the quality. A decision point is established: if the quality score meets a predefined threshold, the process moves to Stage 2; otherwise, Stage 1 is refined. The determination of what constitutes a “good enough” score is dependent on the specific use case and the characteristics of the dataset being processed.</p>
</section>
<section id="two-stage-pipeline-stage-2---knowledge-graph-structuring" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="two-stage-pipeline-stage-2---knowledge-graph-structuring"><span class="header-section-number">19.4</span> Two-Stage Pipeline: Stage 2 - Knowledge Graph Structuring</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_11.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>Following the extraction of statements in Stage 1, Stage 2 of the pipeline focuses on imposing further structure on this knowledge to form a knowledge graph.</p>
<p>The first step in Stage 2 involves defining competency questions. These are specific questions that the final knowledge graph is designed to answer. The purpose of starting with competency questions is to ensure that the structure of the knowledge graph is tailored to the specific research questions driving the project, making it research-driven rather than solely dependent on a predefined, potentially overly broad, ontology. A reasoning model is utilized to draft an initial set of 20 to 30 competency questions. This draft is then refined and finalized by a human domain expert, incorporating a crucial human-in-the-loop step.</p>
<p>Building upon the competency questions and the extracted triples, the next step is to construct the ontology for the knowledge graph. A reasoning model drafts this ontology, which is subsequently finalized and corrected by a domain expert, again involving human oversight.</p>
<p>The final step in Stage 2 encompasses several tasks: entity disambiguation, data encoding, and metadata inclusion. Entity disambiguation involves resolving variations in names or references to the same entity, such as mapping “Humboldt Uni Berlin” to its standardized form “Humboldt Universität zu Berlin”. Entities are also resolved to external, standardized instances, such as those found in <em>Wikidata</em>. The data is then encoded, and relevant metadata is included. This metadata comprises the original source data, the scoring data generated during Stage 1’s evaluation, and the initial text chunk from which the triples were extracted. The output format for the structured data is an <em>RDF star graph</em>.</p>
<p>A final validation step is performed on the constructed knowledge graph. The resulting graph can then be exported in various formats depending on the intended use. Options include exporting it for network analysis into graph databases like <em>Neo4j</em> or storing it as a triple store to facilitate subsequent reasoning tasks. The pipeline is designed to handle the time dimension inherent in many biographical sources; if a time stamp is available for a statement in the source text, it is extracted along with the triple and modeled within the <em>RDF star graph</em>, which is crucial for analyzing developments and changes over time.</p>
</section>
<section id="use-cases-and-applications" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="use-cases-and-applications"><span class="header-section-number">19.5</span> Use Cases and Applications</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_021_slide_15.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>The primary motivation for this pipeline is to provide a controlled method for converting unstructured data into structured data, thereby enabling researchers to ask overarching questions that were previously impossible.</p>
<p>One key use case involves Zielinski’s Polish biographical dictionary, a source compiled in 1930 as part of nation-building efforts during the formation of the Polish nation. This dictionary lists Poles who traveled abroad for exploration and other purposes. Although a PDF version is available, the printed format prevents structured querying. The pipeline enables researchers to ask specific questions about this corpus, such as how the migration of individuals facilitated the introduction of new ideas and fostered innovation in different locations, or the specific role played by editors listed in the dictionary in the spread of knowledge. An example result is a network graph, compiled by Alex Kay, visualizing the relationships between editors (represented as green nodes) and authors (represented as pink nodes). This type of network analysis, including the calculation of centrality measures and analysis across time, is not feasible manually from the printed PDF and provides novel information about the corpus.</p>
<p>Another use case is the biographical reference work “Who was who in the GDR”. This source documents approximately 4,000 prominent figures from East German history, including politicians, dissidents, scientists, and artists. First published in the early 1990s, it was digitized and made available online in the 2000s, allowing for text searches but not structured queries. The source is noted to have a bias towards including individuals based on their fame. An example of a structured question that can be explored after processing this source with the pipeline is identifying the differences between individuals who received state awards and those who did not, specifically concerning their political affiliations and roles within the state apparatus.</p>
<p>An initial experiment using an early version of the pipeline, applied to 1,000 randomly sampled biographies from this source (not the final validated set), yielded preliminary findings. It indicated strong correlations between state awards such as the <em>Karl Marx Orden</em> and <em>Held der DDR</em> with affiliation to the <em>Socialist Unity Party of Germany</em> (<em>SED</em>) and holding political power. In contrast, the <em>Nationalpreis</em> showed a weaker link to high positions compared to individuals who did not receive any award. This type of analysis is highly relevant for <em>HPSS</em> research as it allows for a structural investigation into the interconnection between science and politics. It enables researchers to examine how factors like education, political affiliation, or positions of power varied across different generations or cohorts of scientists, providing insights into the dynamics of this relationship over time.</p>
</section>
<section id="conclusion-challenges-and-future-work" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="conclusion-challenges-and-future-work"><span class="header-section-number">19.6</span> Conclusion, Challenges, and Future Work</h2>
<p>In summary, the project facilitates a transition from viewing biographical sources as collections of isolated entities to enabling complex structural queries across the data.</p>
<p>The main challenges currently faced include improving the accuracy and robustness of entity disambiguation and establishing proper benchmarking methodologies to systematically evaluate the pipeline’s performance.</p>
<p>Immediate future work involves refining and completing the current pipeline, which is presently considered a proof of concept. A key next step is to systematically compare its performance against other existing graph extraction pipelines, such as the <em>Neo4j graph builder</em> and the <em>Microsoft graph rack</em>.</p>
<p>Looking further ahead, future perspectives include utilizing the constructed knowledge graph in conjunction with the initially extracted text chunks to develop a graph <em>Retrieval Augmented Generation</em> (<em>RAG</em>) system. The objective of this graph <em>RAG</em> system is to allow users to query the entire dataset using natural language. Additionally, the project aims to develop methods for building multi-layered networks from the knowledge graph to support deeper structural analysis of the relationships within the data.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Create burger menu button
  const toggleButton = document.createElement('button');
  toggleButton.className = 'sidebar-toggle';
  toggleButton.setAttribute('aria-label', 'Toggle sidebar');
  toggleButton.innerHTML = `
    <div class="burger-icon">
      <span></span>
      <span></span>
      <span></span>
    </div>
  `;
  
  // Create backdrop for mobile
  const backdrop = document.createElement('div');
  backdrop.className = 'sidebar-backdrop';
  
  // Add elements to page
  document.body.appendChild(toggleButton);
  document.body.appendChild(backdrop);
  
  // Get sidebar and main content elements
  const sidebar = document.querySelector('.sidebar') || 
                 document.querySelector('.quarto-sidebar') || 
                 document.querySelector('.sidebar-navigation');
  const mainContent = document.querySelector('main') || 
                     document.querySelector('.main-content') || 
                     document.querySelector('.quarto-container') || 
                     document.body;
  
  // State management
  let sidebarOpen = window.innerWidth > 768; // Start open on desktop, closed on mobile
  
  // Initialize sidebar state
  function initializeSidebar() {
    if (window.innerWidth <= 768) {
      sidebarOpen = false;
    }
    updateSidebarState();
  }
  
  // Update sidebar state and classes
  function updateSidebarState() {
    if (sidebar) {
      if (sidebarOpen) {
        sidebar.classList.remove('collapsed');
        toggleButton.classList.add('sidebar-open');
        mainContent.classList.add('sidebar-open');
        mainContent.classList.remove('sidebar-closed');
        if (window.innerWidth <= 768) {
          backdrop.classList.add('active');
        }
      } else {
        sidebar.classList.add('collapsed');
        toggleButton.classList.remove('sidebar-open');
        mainContent.classList.remove('sidebar-open');
        mainContent.classList.add('sidebar-closed');
        backdrop.classList.remove('active');
      }
    }
    
    // Store state in localStorage
    localStorage.setItem('sidebarOpen', sidebarOpen);
  }
  
  // Toggle sidebar
  function toggleSidebar() {
    sidebarOpen = !sidebarOpen;
    updateSidebarState();
  }
  
  // Close sidebar (for chapter links)
  function closeSidebar() {
    if (window.innerWidth <= 768) { // Only auto-close on mobile
      sidebarOpen = false;
      updateSidebarState();
    }
  }
  
  // Event listeners
  toggleButton.addEventListener('click', toggleSidebar);
  backdrop.addEventListener('click', toggleSidebar);
  
  // Auto-close sidebar when clicking chapter links
  if (sidebar) {
    const chapterLinks = sidebar.querySelectorAll('a[href]');
    chapterLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        // Small delay to allow navigation to start
        setTimeout(closeSidebar, 100);
      });
    });
  }
  
  // Handle window resize
  window.addEventListener('resize', function() {
    if (window.innerWidth > 768 && !sidebarOpen) {
      sidebarOpen = true;
      updateSidebarState();
    } else if (window.innerWidth <= 768 && sidebarOpen) {
      sidebarOpen = false;
      updateSidebarState();
    }
  });
  
  // Handle escape key
  document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape' && sidebarOpen && window.innerWidth <= 768) {
      closeSidebar();
    }
  });
  
  // Restore saved state from localStorage
  const savedState = localStorage.getItem('sidebarOpen');
  if (savedState !== null) {
    sidebarOpen = savedState === 'true';
  }
  
  // Initialize
  initializeSidebar();
  
  // Add keyboard navigation support
  toggleButton.addEventListener('keydown', function(e) {
    if (e.key === 'Enter' || e.key === ' ') {
      e.preventDefault();
      toggleSidebar();
    }
  });
  
  // Improve accessibility
  toggleButton.setAttribute('role', 'button');
  toggleButton.setAttribute('tabindex', '0');
  
  // Update aria-expanded attribute
  function updateAriaExpanded() {
    toggleButton.setAttribute('aria-expanded', sidebarOpen);
  }
  
  // Call updateAriaExpanded whenever sidebar state changes
  const originalUpdateSidebarState = updateSidebarState;
  updateSidebarState = function() {
    originalUpdateSidebarState();
    updateAriaExpanded();
  };
  
  updateAriaExpanded();
  
  // Ensure TOC sticky positioning works properly
  function ensureTOCSticky() {
    // Find all possible TOC elements
    const tocSelectors = [
      '#TOC',
      '.table-of-contents',
      '.quarto-sidebar-toc',
      '.toc',
      '.quarto-toc',
      'nav[role="doc-toc"]',
      '.margin-sidebar',
      '.sidebar-right',
      '.quarto-margin-sidebar',
      '.column-margin'
    ];
    
    let toc = null;
    for (const selector of tocSelectors) {
      toc = document.querySelector(selector);
      if (toc) break;
    }
    
    if (toc) {
      console.log('Found TOC element:', toc.className || toc.id);
      
      // Force sticky positioning with important styles
      toc.style.setProperty('position', 'sticky', 'important');
      toc.style.setProperty('top', '1rem', 'important');
      toc.style.setProperty('max-height', 'calc(100vh - 2rem)', 'important');
      toc.style.setProperty('overflow-y', 'auto', 'important');
      toc.style.setProperty('z-index', '100', 'important');
      
      // Ensure parent containers support sticky
      let parent = toc.parentElement;
      while (parent && parent !== document.body) {
        parent.style.setProperty('position', 'relative', 'important');
        parent.style.setProperty('height', 'auto', 'important');
        parent = parent.parentElement;
      }
      
      // Add scroll event listener to maintain visibility
      let lastScrollTop = 0;
      const scrollHandler = function() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        
        // Ensure TOC remains visible and properly positioned
        if (toc && window.innerWidth > 768) {
          toc.style.setProperty('position', 'sticky', 'important');
          toc.style.setProperty('top', '1rem', 'important');
        }
        
        lastScrollTop = scrollTop;
      };
      
      // Remove existing scroll listeners to avoid duplicates
      window.removeEventListener('scroll', scrollHandler);
      window.addEventListener('scroll', scrollHandler, { passive: true });
      
      // Also apply to any nested TOC elements
      const nestedTocs = toc.querySelectorAll('#TOC, .toc, .table-of-contents');
      nestedTocs.forEach(nestedToc => {
        nestedToc.style.setProperty('position', 'sticky', 'important');
        nestedToc.style.setProperty('top', '0', 'important');
      });
    } else {
      console.log('No TOC element found');
    }
  }
  
  // Initialize TOC sticky behavior
  ensureTOCSticky();
  
  // Re-initialize periodically to ensure it stays sticky
  setInterval(ensureTOCSticky, 2000);
  
  // Re-initialize on window resize
  window.addEventListener('resize', function() {
    setTimeout(ensureTOCSticky, 100);
  });
  
  // Re-initialize if content changes
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.type === 'childList') {
        setTimeout(ensureTOCSticky, 100);
      }
    });
  });
  
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });
  
  // Force re-initialization after page load
  window.addEventListener('load', function() {
    setTimeout(ensureTOCSticky, 500);
  });
});
</script>

<style>
/* Additional styles for better integration */
body {
  overflow-x: hidden;
}

.sidebar-toggle {
  -webkit-tap-highlight-color: transparent;
}

/* Ensure smooth transitions on all relevant elements */
.sidebar,
.sidebar-toggle,
.main-content,
.sidebar-backdrop {
  will-change: transform, opacity, margin;
}

/* Focus styles for accessibility */
.sidebar-toggle:focus {
  outline: 2px solid white;
  outline-offset: 2px;
}

/* Prevent text selection on burger icon */
.burger-icon {
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
</style> 
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_020.html" class="pagination-link" aria-label="LLM for HPS Studies: Analyzing the NHGRI Archive">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">LLM for HPS Studies: Analyzing the NHGRI Archive</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">abstract:</span><span class="co"> "\n      The project focuses on extracting structured knowledge graphs from\</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">  \ unstructured historical and biographical sources using Large Language Models (LLMs)\</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">  \ as part of a processing pipeline. The primary objective is to enable structured\</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">  \ querying of these previously computationally inaccessible sources, such as printed\</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  \ biographical dictionaries. The problem addressed is the lack of inherent structure\</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  \ in many valuable historical sources, preventing complex analytical queries. The\</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">  \ proposed solutio..."</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="an">author:</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">- affiliation: Technische Universität Berlin</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">  email: raphael.schlattmann@tu-berlin.de</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">  name: Raphael Schlattmann, Aleksandra Kaye &amp; Malte Vogl</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="an">bibliography:</span><span class="co"> bibliography.bib</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="an">date:</span><span class="co"> '2025'</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">---</span></span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="fu"># From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="fu">## Overview</span></span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>The project focuses on extracting structured knowledge graphs from unstructured historical and biographical sources using *Large Language Models* (*LLMs*) as part of a processing pipeline. The primary objective is to enable structured querying of these previously computationally inaccessible sources, such as printed biographical dictionaries.</span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>The problem addressed is the lack of inherent structure in many valuable historical sources, which prevents complex analytical queries. The proposed solution is a two-stage pipeline. Stage 1 involves *Open Information Extraction* (*OIE*) using an *LLM* to extract subject-predicate-object triples from the text. This is followed by validation and refinement using an *LLM* ensemble acting as an adversary.</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a>This stage includes a human-in-the-loop evaluation against domain expert extractions to assess quality using classical performance metrics. Stage 2 focuses on structuring the extracted triples into a knowledge graph. This stage is driven by research questions, defined as competency questions, which guide the creation of a domain-specific ontology.</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>*LLMs* are used to draft both competency questions and the ontology, with human experts providing final refinement. Entity disambiguation is performed, including resolution to *Wikidata* instances. The structured data, along with metadata (source, scoring, chunk), is encoded into an *RDF star graph*. The pipeline includes a final validation step before export.</span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a>Case studies include Zielinski's Polish biographical dictionary (1930) and the "Who was who in the GDR" reference work. Initial experiments demonstrate the ability to build networks (e.g., editors and authors) and conduct structured analyses (e.g., correlation between state awards and political affiliation/roles).</span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>Key challenges identified are improving entity disambiguation and establishing proper benchmarking. Future work involves refining and completing the pipeline, systematic comparison with other graph extraction tools (*Neo4j graph builder*, *Microsoft graph rack*), developing graph *RAG* systems for natural language querying, and building multi-layered networks for deeper analysis. The approach emphasizes task decomposition, data/research question-driven structuring, and verifiability through human intervention at key points.</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="fu">## Introduction: Extracting Structure from Unstructured Sources</span></span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="al">![Slide 01](images/ai-nepi_021_slide_01.jpg)</span></span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>The project aims to extract knowledge graphs from source material using *Large Language Models* (*LLMs*). The primary focus is on accessing and utilizing new types of sources for research that are currently computationally inaccessible due to their unstructured nature. While historical, philosophical, and social science (*HPSS*) research often utilizes structured data sources such as publication databases or email archives, a significant amount of valuable information resides in unstructured formats, particularly printed books and biographical dictionaries. The core problem addressed is the inability to perform structured queries on these unstructured sources.</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a>*LLMs* offer the potential to impose structure on this unstructured data. Historically, efforts like the *Get Grass* project attempted to address the computational accessibility of printed books. The current project specifically targets biographical sources, which are rich in detailed information about individuals but lack inherent structure. This absence of structure prevents researchers from asking complex, structured questions beyond simple facts like birth dates or work locations.</span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a>The goal is to enable queries about how professions formed networks over specific periods, how individuals migrating between locations contributed to the spread of ideas, or the specific roles of editors within a corpus in disseminating knowledge. The proposed solution involves using *LLMs* to construct knowledge graphs from this unstructured data in a controllable manner. A knowledge graph represents information as entities (such as persons, places, countries, or works) which become nodes in the graph. Relationships identified between these entities in the source material are represented as edges connecting the nodes. This structure allows for sophisticated structured querying.</span>
<span id="cb1-43"><a href="#cb1-43"></a></span>
<span id="cb1-44"><a href="#cb1-44"></a>The *Neo4j* graph database is used for representation and querying of the resulting knowledge graphs. The approach positions the *LLM* as one component within a larger processing pipeline, emphasizing its utility for specific tasks rather than seeking a universally perfect model. An example source snippet, an entry about the evangelical priest Henrik Bartsch born in 1832 who traveled and wrote books, illustrates the type of material processed. Traditional *Natural Language Processing* (*NLP*) approaches, such as *NLTK*, are often insufficient to extract the full contextual richness from such entries. The desired output is structured data in the form of statements or triples, capturing details like profession, birth date, birth place, and travel destinations from the text.</span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="fu">## Two-Stage Pipeline: Stage 1 - Open Information Extraction</span></span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="al">![Slide 08](images/ai-nepi_021_slide_08.jpg)</span></span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a>The process of transforming unstructured text into a structured knowledge graph is implemented as a two-stage pipeline. The first stage is responsible for extracting statements from the input text, while the second stage constructs the knowledge graph from these extracted statements, adhering to defined rules to ensure consistency and clear categorization.</span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>The pipeline is built upon several core principles:</span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a><span class="ss">-   </span>It employs task decomposition, breaking down the complex process into smaller, controllable, and verifiable steps.</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a><span class="ss">-   </span>It is data and research question driven, meaning the final structure of the knowledge graph is explicitly guided by the specific research aims of the project, rather than being solely dictated by a predefined ontology.</span>
<span id="cb1-57"><a href="#cb1-57"></a></span>
<span id="cb1-58"><a href="#cb1-58"></a><span class="ss">-   </span>Verifiability is integrated through human-in-the-loop steps at critical points in the process.</span>
<span id="cb1-59"><a href="#cb1-59"></a></span>
<span id="cb1-60"><a href="#cb1-60"></a>The input data for the pipeline often originates from messy, unstructured sources that typically require *Optical Character Recognition* (*OCR*) or scraping. Following these initial steps, the data undergoes preprocessing to achieve a semi-structured format before entering the pipeline.</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a>The first central step of Stage 1 is *Open Information Extraction* (*OIE*). This step utilizes a *Large Language Model* (*LLM*) to extract all subject-predicate-object triples it can identify within the text, without relying on any preset categories or schemas. *OIE* is a rapidly evolving research area, with a significant shift towards using *LLMs* for this task.</span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a>The second step in Stage 1 involves *LLM* ensembling. A second *LLM* model is employed to validate and refine the initial statements extracted by the first model. This second model is specifically prompted to act as an adversary to the first, critically evaluating its output, correcting errors, identifying any missed triples, and assigning a confidence score to the extraction. This ensembling approach significantly enhances the quality of the extracted statements.</span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a>The final part of Stage 1 is evaluation. A sample of the validated output is evaluated against a corresponding sample created independently by domain experts. Classical performance metrics are calculated to quantify the quality of the extraction. This evaluation step represents the first instance of human-in-the-loop intervention, where domain experts judge the quality. A decision point is established: if the quality score meets a predefined threshold, the process moves to Stage 2; otherwise, Stage 1 is refined. The determination of what constitutes a "good enough" score is dependent on the specific use case and the characteristics of the dataset being processed.</span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a><span class="fu">## Two-Stage Pipeline: Stage 2 - Knowledge Graph Structuring</span></span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a><span class="al">![Slide 11](images/ai-nepi_021_slide_11.jpg)</span></span>
<span id="cb1-71"><a href="#cb1-71"></a></span>
<span id="cb1-72"><a href="#cb1-72"></a>Following the extraction of statements in Stage 1, Stage 2 of the pipeline focuses on imposing further structure on this knowledge to form a knowledge graph.</span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a>The first step in Stage 2 involves defining competency questions. These are specific questions that the final knowledge graph is designed to answer. The purpose of starting with competency questions is to ensure that the structure of the knowledge graph is tailored to the specific research questions driving the project, making it research-driven rather than solely dependent on a predefined, potentially overly broad, ontology. A reasoning model is utilized to draft an initial set of 20 to 30 competency questions. This draft is then refined and finalized by a human domain expert, incorporating a crucial human-in-the-loop step.</span>
<span id="cb1-75"><a href="#cb1-75"></a></span>
<span id="cb1-76"><a href="#cb1-76"></a>Building upon the competency questions and the extracted triples, the next step is to construct the ontology for the knowledge graph. A reasoning model drafts this ontology, which is subsequently finalized and corrected by a domain expert, again involving human oversight.</span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a>The final step in Stage 2 encompasses several tasks: entity disambiguation, data encoding, and metadata inclusion. Entity disambiguation involves resolving variations in names or references to the same entity, such as mapping "Humboldt Uni Berlin" to its standardized form "Humboldt Universität zu Berlin". Entities are also resolved to external, standardized instances, such as those found in *Wikidata*. The data is then encoded, and relevant metadata is included. This metadata comprises the original source data, the scoring data generated during Stage 1's evaluation, and the initial text chunk from which the triples were extracted. The output format for the structured data is an *RDF star graph*.</span>
<span id="cb1-79"><a href="#cb1-79"></a></span>
<span id="cb1-80"><a href="#cb1-80"></a>A final validation step is performed on the constructed knowledge graph. The resulting graph can then be exported in various formats depending on the intended use. Options include exporting it for network analysis into graph databases like *Neo4j* or storing it as a triple store to facilitate subsequent reasoning tasks. The pipeline is designed to handle the time dimension inherent in many biographical sources; if a time stamp is available for a statement in the source text, it is extracted along with the triple and modeled within the *RDF star graph*, which is crucial for analyzing developments and changes over time.</span>
<span id="cb1-81"><a href="#cb1-81"></a></span>
<span id="cb1-82"><a href="#cb1-82"></a><span class="fu">## Use Cases and Applications</span></span>
<span id="cb1-83"><a href="#cb1-83"></a></span>
<span id="cb1-84"><a href="#cb1-84"></a><span class="al">![Slide 15](images/ai-nepi_021_slide_15.jpg)</span></span>
<span id="cb1-85"><a href="#cb1-85"></a></span>
<span id="cb1-86"><a href="#cb1-86"></a>The primary motivation for this pipeline is to provide a controlled method for converting unstructured data into structured data, thereby enabling researchers to ask overarching questions that were previously impossible.</span>
<span id="cb1-87"><a href="#cb1-87"></a></span>
<span id="cb1-88"><a href="#cb1-88"></a>One key use case involves Zielinski's Polish biographical dictionary, a source compiled in 1930 as part of nation-building efforts during the formation of the Polish nation. This dictionary lists Poles who traveled abroad for exploration and other purposes. Although a PDF version is available, the printed format prevents structured querying. The pipeline enables researchers to ask specific questions about this corpus, such as how the migration of individuals facilitated the introduction of new ideas and fostered innovation in different locations, or the specific role played by editors listed in the dictionary in the spread of knowledge. An example result is a network graph, compiled by Alex Kay, visualizing the relationships between editors (represented as green nodes) and authors (represented as pink nodes). This type of network analysis, including the calculation of centrality measures and analysis across time, is not feasible manually from the printed PDF and provides novel information about the corpus.</span>
<span id="cb1-89"><a href="#cb1-89"></a></span>
<span id="cb1-90"><a href="#cb1-90"></a>Another use case is the biographical reference work "Who was who in the GDR". This source documents approximately 4,000 prominent figures from East German history, including politicians, dissidents, scientists, and artists. First published in the early 1990s, it was digitized and made available online in the 2000s, allowing for text searches but not structured queries. The source is noted to have a bias towards including individuals based on their fame. An example of a structured question that can be explored after processing this source with the pipeline is identifying the differences between individuals who received state awards and those who did not, specifically concerning their political affiliations and roles within the state apparatus.</span>
<span id="cb1-91"><a href="#cb1-91"></a></span>
<span id="cb1-92"><a href="#cb1-92"></a>An initial experiment using an early version of the pipeline, applied to 1,000 randomly sampled biographies from this source (not the final validated set), yielded preliminary findings. It indicated strong correlations between state awards such as the *Karl Marx Orden* and *Held der DDR* with affiliation to the *Socialist Unity Party of Germany* (*SED*) and holding political power. In contrast, the *Nationalpreis* showed a weaker link to high positions compared to individuals who did not receive any award. This type of analysis is highly relevant for *HPSS* research as it allows for a structural investigation into the interconnection between science and politics. It enables researchers to examine how factors like education, political affiliation, or positions of power varied across different generations or cohorts of scientists, providing insights into the dynamics of this relationship over time.</span>
<span id="cb1-93"><a href="#cb1-93"></a></span>
<span id="cb1-94"><a href="#cb1-94"></a><span class="fu">## Conclusion, Challenges, and Future Work</span></span>
<span id="cb1-95"><a href="#cb1-95"></a></span>
<span id="cb1-96"><a href="#cb1-96"></a>In summary, the project facilitates a transition from viewing biographical sources as collections of isolated entities to enabling complex structural queries across the data.</span>
<span id="cb1-97"><a href="#cb1-97"></a></span>
<span id="cb1-98"><a href="#cb1-98"></a>The main challenges currently faced include improving the accuracy and robustness of entity disambiguation and establishing proper benchmarking methodologies to systematically evaluate the pipeline's performance.</span>
<span id="cb1-99"><a href="#cb1-99"></a></span>
<span id="cb1-100"><a href="#cb1-100"></a>Immediate future work involves refining and completing the current pipeline, which is presently considered a proof of concept. A key next step is to systematically compare its performance against other existing graph extraction pipelines, such as the *Neo4j graph builder* and the *Microsoft graph rack*.</span>
<span id="cb1-101"><a href="#cb1-101"></a></span>
<span id="cb1-102"><a href="#cb1-102"></a>Looking further ahead, future perspectives include utilizing the constructed knowledge graph in conjunction with the initially extracted text chunks to develop a graph *Retrieval Augmented Generation* (*RAG*) system. The objective of this graph *RAG* system is to allow users to query the entire dataset using natural language. Additionally, the project aims to develop methods for building multi-layered networks from the knowledge graph to support deeper structural analysis of the relationships within the data.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>