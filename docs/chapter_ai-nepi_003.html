<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arno Simons">
<meta name="dcterms.date" content="2025-01-01">

<title>3&nbsp; A Primer on Large Language Models – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_004.html" rel="next">
<link href="./chapter_ai-nepi_001.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-fe5eeb5af71a333b155c360431d06b9a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e463572c889c87c7eefd27e1777fa793.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="3&nbsp; A Primer on Large Language Models – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta property="og:description" content="The presentation provides a primer on Large Language Models (LLMs), beginning with the Transformer architecture, its encoder-decoder structure, and the concept of contextualized word embeddings. It details the development of pre-trained language models, distinguishing between encoder-based models like BERT (Bidirectional Encoder Representations from Transformers) for full context understanding and decoder-based models like GPT (Generative Pre-trained Transformers) for text generation. The…">
<meta property="og:image" content="images/ai-nepi_003_slide_01.jpg">
<meta property="og:site_name" content="AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:title" content="3&nbsp; A Primer on Large Language Models – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:description" content="The presentation provides a primer on Large Language Models (LLMs), beginning with the Transformer architecture, its encoder-decoder structure, and the concept of contextualized word embeddings. It details the development of pre-trained language models, distinguishing between encoder-based models like BERT (Bidirectional Encoder Representations from Transformers) for full context understanding and decoder-based models like GPT (Generative Pre-trained Transformers) for text generation. The…">
<meta name="twitter:image" content="images/ai-nepi_003_slide_01.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_003.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper: Transdisciplinary Investigations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computational HPSS: Tracing Ancient Wisdom’s Influence with VERITRACE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and Scientific Insights in Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems in Philosophy and HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural pursuit across scales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">LLMs for Chemical Knowledge Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Interpretable Models for Linguistic Change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">LLM for HPS Studies: Analyzing the NHGRI Archive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">3.1</span> Overview</a></li>
  <li><a href="#the-transformer-architecture-foundation-of-llms" id="toc-the-transformer-architecture-foundation-of-llms" class="nav-link" data-scroll-target="#the-transformer-architecture-foundation-of-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundation of LLMs</a></li>
  <li><a href="#pre-trained-language-models-bert-and-gpt" id="toc-pre-trained-language-models-bert-and-gpt" class="nav-link" data-scroll-target="#pre-trained-language-models-bert-and-gpt"><span class="header-section-number">3.3</span> Pre-trained Language Models: BERT and GPT</a></li>
  <li><a href="#evolution-and-adaptation-of-llms-for-scientific-domains" id="toc-evolution-and-adaptation-of-llms-for-scientific-domains" class="nav-link" data-scroll-target="#evolution-and-adaptation-of-llms-for-scientific-domains"><span class="header-section-number">3.4</span> Evolution and Adaptation of LLMs for Scientific Domains</a></li>
  <li><a href="#key-llm-concepts-and-application-categories-in-hpss-research" id="toc-key-llm-concepts-and-application-categories-in-hpss-research" class="nav-link" data-scroll-target="#key-llm-concepts-and-application-categories-in-hpss-research"><span class="header-section-number">3.5</span> Key LLM Concepts and Application Categories in HPSS Research</a></li>
  <li><a href="#trends-concerns-and-accessibility-in-hpss-llm-usage" id="toc-trends-concerns-and-accessibility-in-hpss-llm-usage" class="nav-link" data-scroll-target="#trends-concerns-and-accessibility-in-hpss-llm-usage"><span class="header-section-number">3.6</span> Trends, Concerns, and Accessibility in HPSS LLM Usage</a></li>
  <li><a href="#hpss-specific-challenges-and-methodological-considerations-for-llm-adoption" id="toc-hpss-specific-challenges-and-methodological-considerations-for-llm-adoption" class="nav-link" data-scroll-target="#hpss-specific-challenges-and-methodological-considerations-for-llm-adoption"><span class="header-section-number">3.7</span> HPSS-Specific Challenges and Methodological Considerations for LLM Adoption</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Show code</button></div></div>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arno Simons </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            AI-NEPI Conference Participant
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    The presentation provides a primer on Large Language Models (LLMs), beginning with the Transformer architecture, its encoder-decoder structure, and the concept of contextualized word embeddings. It details the development of pre-trained language models, distinguishing between encoder-based models like BERT (Bidirectional Encoder Representations from Transformers) for full context understanding and decoder-based models like GPT (Generative Pre-trained Transformers) for text generation. The…
  </div>
</div>


</header>


<section id="overview" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">3.1</span> Overview</h2>
<p>This presentation provides a primer on Large Language Models (LLMs), beginning with the <em>Transformer</em> architecture, its encoder-decoder structure, and the concept of contextualized word embeddings. It details the development of pre-trained language models, distinguishing between encoder-based models like <em>BERT</em> (Bidirectional Encoder Representations from Transformers) for full context understanding and decoder-based models like <em>GPT</em> (Generative Pre-trained Transformers) for text generation.</p>
<p>The evolution of LLMs in science domains is discussed, highlighting models such as <em>BioBERT</em>, <em>Specter</em>, and <em>SciBERT</em>, and various adaptation methods including continued pre-training, fine-tuning, contrastive learning (e.g., <em>SentenceBERT</em>), and Retrieval Augmented Generation (<em>RAG</em>). Key distinctions in LLM concepts cover architectures, fine-tuning strategies, and word versus sentence embeddings.</p>
<p>Applications of LLMs in History and Philosophy of Science and Technology Studies (HPSS) research are categorized into: dealing with data/sources, analyzing knowledge structures (e.g., entity extraction, mapping discourses), studying dynamics (e.g., conceptual histories), and examining knowledge practices (e.g., citation context analysis). Trends in HPSS LLM usage include accelerating interest, publication in diverse journals, varied customization levels, and increased accessibility (e.g., <em>BERTopic</em>).</p>
<p>Significant concerns are computational resources, model opaqueness, and lack of training data/benchmarks. HPSS-specific challenges involve the historical evolution of language, the need for critical reconstructive perspectives, and issues with sparse, multilingual, or old script data. Recommendations emphasize building LLM literacy, developing shared resources, and maintaining HPSS methodological integrity while leveraging LLMs to bridge qualitative/quantitative approaches and reflect on the field’s intellectual history (e.g., co-word analysis).</p>
</section>
<section id="the-transformer-architecture-foundation-of-llms" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-transformer-architecture-foundation-of-llms"><span class="header-section-number">3.2</span> The Transformer Architecture: Foundation of LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>The <em>Transformer</em> architecture, introduced in 2017, serves as the fundamental framework for virtually all contemporary Large Language Models (LLMs). Originally conceived for machine translation tasks, such as converting German text to English, the <em>Transformer</em> model features a distinctive dual-stream structure. These two streams, an encoder on the left and a decoder on the right, are interconnected.</p>
<p>The encoder processes the input source language sentence. For instance, words from a German sentence are fed into the encoder, where they are transformed into numerical representations. These numerical data undergo processing through multiple layers, within which contextualized word embeddings are progressively refined. A key characteristic of the encoder is its ability to process the entire input sentence simultaneously. Each word in the source sentence can interact with, or “attend to,” every other word, enabling the model to construct a comprehensive representation of the sentence’s overall meaning.</p>
<p>The numerical representation generated by the encoder is then passed to the decoder stream. The decoder’s function is to generate the output sentence in the target language, for example, English words. As each English word is produced, it is fed back into the decoder as input for generating the subsequent word. This iterative process continues until the complete target sentence is formed.</p>
<p>Unlike the encoder, the decoder operates with a unidirectional attention mechanism, meaning that generated words can only consider preceding words in the sequence. They cannot access future words, a constraint inherent to the next-word prediction task. However, they can look back at the words already generated in the sequence. Both streams utilize layers to increasingly contextualize word embeddings.</p>
</section>
<section id="pre-trained-language-models-bert-and-gpt" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="pre-trained-language-models-bert-and-gpt"><span class="header-section-number">3.3</span> Pre-trained Language Models: BERT and GPT</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_03.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Following the introduction of the <em>Transformer</em> architecture, researchers began re-engineering its encoder and decoder streams independently to develop pre-trained language models. These models are designed to achieve a strong general understanding or generation capability in a language, which can then be adapted through further, often minor, training for specific Natural Language Processing (NLP) tasks.</p>
<p>On the encoder side, this re-engineering led to the development of models like <em>BERT</em>, which stands for Bidirectional Encoder Representations from Transformers. The <em>BERT</em> family of models remains highly influential. <em>BERT</em>’s operational principle allows every word in an input sequence to attend to every other word, facilitating a comprehensive, full-context understanding of the entire input simultaneously. The term “bidirectional” in its name refers to this ability of words to consider context from both preceding and succeeding words, while “encoder-based” indicates its derivation from the <em>Transformer</em>’s original encoder stream.</p>
<p>Conversely, the decoder stream gave rise to models like <em>GPT</em> (Generative Pre-trained Transformers), which form the basis of well-known applications such as <em>ChatGPT</em>. Due to their architectural constraint of only looking at predecessor words, <em>GPT</em> models excel at generating new words and, consequently, new text. This generative capability is a primary differentiator from <em>BERT</em> models, which are not inherently designed for extensive text generation.</p>
<p>The core difference lies in their primary functions: <em>GPT</em> models are generative, designed to produce language, whereas <em>BERT</em>-like models are geared towards a coherent, full-context understanding of sentences. Beyond these two primary types, other model architectures exist, including those that combine encoder and decoder components. Furthermore, there are advanced techniques for utilizing decoders in ways that enable them to perform more like encoders, achieving bidirectional context understanding. An example of such an approach is <em>XLM</em>, which is based on <em>XLNet</em>. Understanding the distinction between generative models and full-context understanding models is crucial.</p>
</section>
<section id="evolution-and-adaptation-of-llms-for-scientific-domains" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="evolution-and-adaptation-of-llms-for-scientific-domains"><span class="header-section-number">3.4</span> Evolution and Adaptation of LLMs for Scientific Domains</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_003_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The evolution of Large Language Models (LLMs) has seen a significant focus on applications within various scientific domains and tasks. An overview of this development reveals a diverse landscape of models. Notably, encoder-type models, similar to <em>BERT</em>, are more commonly developed and applied in scientific contexts than decoder-type models. Early influential models in this space include <em>BioBERT</em>, <em>Specter</em>, and <em>SciBERT</em>. These, along with newer models, cater to a wide array of scientific fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and the social sciences.</p>
<p>Adapting LLMs to the specific language of scientific domains involves several methods. The foundational method is pre-training, where a model initially learns language patterns. This occurs either by predicting the next token, typical for <em>GPT</em>-style models, or by predicting randomly masked words, characteristic of <em>BERT</em>-style models. However, full pre-training demands extensive computational resources and vast datasets, making it impractical for many research groups. A more feasible approach is continued pre-training, where an already pre-trained model (e.g., a general <em>BERT</em> model) is further trained on a corpus of domain-specific text, such as physics literature.</p>
<p>Another common adaptation strategy is fine-tuning for downstream tasks. This involves adding new layers on top of a pre-trained model and training these specific layers to perform tasks like classification, for example, to determine sentiment or identify named entities. Prompt-based methods were mentioned as another adaptation technique, though not elaborated upon.</p>
<p>Contrastive learning is a key method for generating sentence or document embeddings that reside in the same vector space as word embeddings. <em>SentenceBERT</em> is a prominent and widely adopted technique for achieving this. This is particularly relevant as Iryna Gurevych, a keynote speaker, is known for her work in this area and might discuss <em>SentenceBERT</em>.</p>
<p>Retrieval Augmented Generation (<em>RAG</em>) represents a significant adaptation approach. <em>RAG</em> is not a single model but rather a pipeline or system where at least two, and often more, models work together. This technique allows for the adaptation of an LLM to a specific domain, such as a scientific field, without the need for extensive retraining of the core generative model. The <em>RAG</em> process typically involves a user submitting a query (e.g., “What are LLMs?”). A model, often <em>BERT</em>-like, then encodes this query into a sentence embedding. This embedding is used to search a database of relevant documents, retrieving the most similar passages. These retrieved passages are then integrated into the prompt provided to a generative LLM, which uses this augmented context to produce an answer. This mechanism is commonly used by systems like <em>ChatGPT</em> when they access external information, such as searching the internet.</p>
<p>Finally, reasoning models or agents are emerging as complex systems. These are not monolithic LLMs but rather integrated ensembles of multiple LLMs combined with a variety of other software tools.</p>
</section>
<section id="key-llm-concepts-and-application-categories-in-hpss-research" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="key-llm-concepts-and-application-categories-in-hpss-research"><span class="header-section-number">3.5</span> Key LLM Concepts and Application Categories in HPSS Research</h2>
<p>Several key distinctions are important to retain as a primer on Large Language Models. These include the existence of different model architectures (such as encoder-only, decoder-only, and encoder-decoder structures), a variety of fine-tuning strategies for adapting models to specific tasks, the crucial difference between word embeddings and sentence embeddings (which represent fundamentally different levels of semantic representation), and the varying levels of abstraction at which these models can be applied.</p>
<p>An ongoing survey is being conducted on the use of LLMs as tools in History and Philosophy of Science and Technology Studies (HPSS) research. Preliminary findings from this survey have led to the identification of four main categories, or bins, for classifying these applications:</p>
<ul>
<li><p>Dealing with Data and Sources: This category encompasses uses of LLMs for interacting with research data and primary/secondary sources. Specific tasks include facilitating data discovery, parsing complex or large-scale textual data, and improving the overall management of source materials.</p></li>
<li><p>Knowledge Structures Analysis: LLMs are employed to analyze and extract knowledge structures embedded in texts. This includes the identification and extraction of specific entities relevant to HPSS, such as scientific instruments, celestial bodies, or chemical substances. It also involves mapping complex conceptual landscapes, a traditional area of HPSS inquiry, like analyzing science policy discourses or tracing the formation and interaction of interdisciplinary fields.</p></li>
<li><p>Dynamics: This category focuses on using LLMs to study the evolution and change of concepts and language over time. A prime example is the analysis of conceptual histories of words, tracking how their meanings and usages shift within scientific communities, an approach similar to that demonstrated by researchers like Nina Janich and Pelin Doğan.</p></li>
<li><p>Knowledge Practices: LLMs can be applied to investigate various knowledge practices. One specific example is citation context analysis. This method has an established tradition within HPSS, though in recent times it has often been predominantly used for evaluative purposes (e.g., research assessment). However, it holds potential for a broader range of HPSS-specific analytical tasks.</p></li>
</ul>
</section>
<section id="trends-concerns-and-accessibility-in-hpss-llm-usage" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="trends-concerns-and-accessibility-in-hpss-llm-usage"><span class="header-section-number">3.6</span> Trends, Concerns, and Accessibility in HPSS LLM Usage</h2>
<p>Several trends and concerns are evident regarding the use of Large Language Models (LLMs) in History and Philosophy of Science and Technology Studies (HPSS). There is an accelerating interest in LLMs within the HPSS community. Publications detailing LLM applications are found predominantly in information science-oriented journals such as <em>Scientometrics</em> and <em>JASIST</em> (Journal of the Association for Information Science and Technology). However, an increasing number of papers are also appearing in journals traditionally less focused on computational methods. This suggests that the enhanced semantic capabilities of modern LLMs are making them more attractive and relevant to qualitative researchers, philosophers, and other scholars within HPSS.</p>
<p>The degree of customization in LLM use varies widely across the field. Some researchers utilize readily available, off-the-shelf tools like <em>ChatGPT</em>, while others at the other end of the spectrum are engaged in developing entirely new LLM architectures tailored to specific HPSS needs.</p>
<p>Despite the growing adoption, several repeating concerns are frequently voiced. The substantial computational resources required to train or even fine-tune large models pose a significant barrier. The opaqueness of these models, often referred to as their “black box” nature, makes it difficult to understand their internal decision-making processes, which is a concern for interpretability and trustworthiness. There is also a perceived lack of sufficient training data, particularly for specialized historical contexts or languages relevant to HPSS research. Furthermore, the absence of established benchmarks specifically designed for evaluating LLM performance on HPSS-relevant tasks makes it challenging to compare different models or approaches systematically. Finally, researchers face a trade-off between different types of LLMs, as no single model is universally optimal; the most adequate model must be chosen based on the specific research question and purpose.</p>
<p>On a positive note, there is a discernible trend towards increased accessibility of LLM-related tools. For example, <em>BERTopic</em>, a popular library for topic modeling, is noted for its ease of use, largely due to robust maintenance and active development, making advanced techniques more approachable for a wider range of researchers.</p>
</section>
<section id="hpss-specific-challenges-and-methodological-considerations-for-llm-adoption" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="hpss-specific-challenges-and-methodological-considerations-for-llm-adoption"><span class="header-section-number">3.7</span> HPSS-Specific Challenges and Methodological Considerations for LLM Adoption</h2>
<p>The adoption of Large Language Models (LLMs) in History and Philosophy of Science and Technology Studies (HPSS) necessitates acknowledging several challenges specific to the discipline. A primary challenge is the historical evolution of concepts and language. Most LLMs are trained on contemporary language, which may not align with the historical texts and linguistic conventions central to much HPSS research. This requires strategies such as training custom models on historical corpora or using existing models with a keen awareness of their inherent biases and limitations when applied to historical material.</p>
<p>Another significant challenge stems from the reconstructive and critically reflective perspective characteristic of HPSS. Scholars in this field typically do not take scientific texts at face value but instead engage in critical interpretation, reading “between the lines” to understand the authors’ situated contexts, motivations, and subtle discursive strategies, such as boundary work. Current LLMs are generally not trained to detect or analyze these nuanced aspects of texts. Therefore, methods need to be developed to enable models to approximate this type of critical reading. Furthermore, HPSS research often contends with practical data issues like sparse datasets, the presence of multiple languages (often historical variants), and archaic scripts, all of which pose difficulties for standard LLM application.</p>
<p>To address these challenges and effectively leverage LLMs, several recommendations are proposed for the HPSS community. Firstly, there is a need to build LLM literacy. This involves understanding the underlying theory of these models, their capabilities, limitations, and the broader implications of their use. While natural language interfaces for coding may become more common, acquiring some coding skills can be beneficial. A crucial aspect of literacy is moving beyond the superficial use of tools that might produce appealing visualizations or graphs without a deep comprehension of the underlying processes or the significance of the results.</p>
<p>Secondly, the development of shared datasets and benchmarks specifically tailored to HPSS research questions and materials is essential for robust and comparable evaluations. Thirdly, it is vital to stay true to core HPSS methodologies. While HPSS research problems need to be translated into tractable NLP tasks (such as classification, generation, or summarization), care must be taken to ensure that these technical tasks do not overshadow or “hijack” the fundamental research purpose and critical inquiries of HPSS.</p>
<p>Despite these challenges, LLMs also present new opportunities. They offer promising avenues for bridging qualitative and quantitative research approaches, potentially fostering more integrated and multifaceted analyses. Moreover, the rise of LLMs provides an occasion for HPSS to reflect on its own intellectual history and pre-existing analytical frameworks. For instance, current LLM techniques resonate with earlier methods developed within HPSS, such as co-word analysis, pioneered by scholars like <em>Michel Callon</em> and <em>Arie Rip</em> in the 1980s, often in conjunction with <em>Actor-Network Theory</em> (<em>ANT</em>).</p>


<!-- -->

</section>

</main> <!-- /main -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Create burger menu button
  const toggleButton = document.createElement('button');
  toggleButton.className = 'sidebar-toggle';
  toggleButton.setAttribute('aria-label', 'Toggle sidebar');
  toggleButton.innerHTML = `
    <div class="burger-icon">
      <span></span>
      <span></span>
      <span></span>
    </div>
  `;
  
  // Create backdrop for mobile
  const backdrop = document.createElement('div');
  backdrop.className = 'sidebar-backdrop';
  
  // Add elements to page
  document.body.appendChild(toggleButton);
  document.body.appendChild(backdrop);
  
  // Get sidebar and main content elements
  const sidebar = document.querySelector('.sidebar') || 
                 document.querySelector('.quarto-sidebar') || 
                 document.querySelector('.sidebar-navigation');
  const mainContent = document.querySelector('main') || 
                     document.querySelector('.main-content') || 
                     document.querySelector('.quarto-container') || 
                     document.body;
  
  // State management
  let sidebarOpen = window.innerWidth > 768; // Start open on desktop, closed on mobile
  
  // Initialize sidebar state
  function initializeSidebar() {
    if (window.innerWidth <= 768) {
      sidebarOpen = false;
    }
    updateSidebarState();
  }
  
  // Update sidebar state and classes
  function updateSidebarState() {
    if (sidebar) {
      if (sidebarOpen) {
        sidebar.classList.remove('collapsed');
        toggleButton.classList.add('sidebar-open');
        mainContent.classList.add('sidebar-open');
        mainContent.classList.remove('sidebar-closed');
        if (window.innerWidth <= 768) {
          backdrop.classList.add('active');
        }
      } else {
        sidebar.classList.add('collapsed');
        toggleButton.classList.remove('sidebar-open');
        mainContent.classList.remove('sidebar-open');
        mainContent.classList.add('sidebar-closed');
        backdrop.classList.remove('active');
      }
    }
    
    // Store state in localStorage
    localStorage.setItem('sidebarOpen', sidebarOpen);
  }
  
  // Toggle sidebar
  function toggleSidebar() {
    sidebarOpen = !sidebarOpen;
    updateSidebarState();
  }
  
  // Close sidebar (for chapter links)
  function closeSidebar() {
    if (window.innerWidth <= 768) { // Only auto-close on mobile
      sidebarOpen = false;
      updateSidebarState();
    }
  }
  
  // Event listeners
  toggleButton.addEventListener('click', toggleSidebar);
  backdrop.addEventListener('click', toggleSidebar);
  
  // Auto-close sidebar when clicking chapter links
  if (sidebar) {
    const chapterLinks = sidebar.querySelectorAll('a[href]');
    chapterLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        // Small delay to allow navigation to start
        setTimeout(closeSidebar, 100);
      });
    });
  }
  
  // Handle window resize
  window.addEventListener('resize', function() {
    if (window.innerWidth > 768 && !sidebarOpen) {
      sidebarOpen = true;
      updateSidebarState();
    } else if (window.innerWidth <= 768 && sidebarOpen) {
      sidebarOpen = false;
      updateSidebarState();
    }
  });
  
  // Handle escape key
  document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape' && sidebarOpen && window.innerWidth <= 768) {
      closeSidebar();
    }
  });
  
  // Restore saved state from localStorage
  const savedState = localStorage.getItem('sidebarOpen');
  if (savedState !== null) {
    sidebarOpen = savedState === 'true';
  }
  
  // Initialize
  initializeSidebar();
  
  // Add keyboard navigation support
  toggleButton.addEventListener('keydown', function(e) {
    if (e.key === 'Enter' || e.key === ' ') {
      e.preventDefault();
      toggleSidebar();
    }
  });
  
  // Improve accessibility
  toggleButton.setAttribute('role', 'button');
  toggleButton.setAttribute('tabindex', '0');
  
  // Update aria-expanded attribute
  function updateAriaExpanded() {
    toggleButton.setAttribute('aria-expanded', sidebarOpen);
  }
  
  // Call updateAriaExpanded whenever sidebar state changes
  const originalUpdateSidebarState = updateSidebarState;
  updateSidebarState = function() {
    originalUpdateSidebarState();
    updateAriaExpanded();
  };
  
  updateAriaExpanded();
  
  // Ensure TOC sticky positioning works properly
  function ensureTOCSticky() {
    // Find all possible TOC elements
    const tocSelectors = [
      '#TOC',
      '.table-of-contents',
      '.quarto-sidebar-toc',
      '.toc',
      '.quarto-toc',
      'nav[role="doc-toc"]',
      '.margin-sidebar',
      '.sidebar-right',
      '.quarto-margin-sidebar',
      '.column-margin'
    ];
    
    let toc = null;
    for (const selector of tocSelectors) {
      toc = document.querySelector(selector);
      if (toc) break;
    }
    
    if (toc) {
      console.log('Found TOC element:', toc.className || toc.id);
      
      // Force sticky positioning with important styles
      toc.style.setProperty('position', 'sticky', 'important');
      toc.style.setProperty('top', '1rem', 'important');
      toc.style.setProperty('max-height', 'calc(100vh - 2rem)', 'important');
      toc.style.setProperty('overflow-y', 'auto', 'important');
      toc.style.setProperty('z-index', '100', 'important');
      
      // Ensure parent containers support sticky
      let parent = toc.parentElement;
      while (parent && parent !== document.body) {
        parent.style.setProperty('position', 'relative', 'important');
        parent.style.setProperty('height', 'auto', 'important');
        parent = parent.parentElement;
      }
      
      // Add scroll event listener to maintain visibility
      let lastScrollTop = 0;
      const scrollHandler = function() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        
        // Ensure TOC remains visible and properly positioned
        if (toc && window.innerWidth > 768) {
          toc.style.setProperty('position', 'sticky', 'important');
          toc.style.setProperty('top', '1rem', 'important');
        }
        
        lastScrollTop = scrollTop;
      };
      
      // Remove existing scroll listeners to avoid duplicates
      window.removeEventListener('scroll', scrollHandler);
      window.addEventListener('scroll', scrollHandler, { passive: true });
      
      // Also apply to any nested TOC elements
      const nestedTocs = toc.querySelectorAll('#TOC, .toc, .table-of-contents');
      nestedTocs.forEach(nestedToc => {
        nestedToc.style.setProperty('position', 'sticky', 'important');
        nestedToc.style.setProperty('top', '0', 'important');
      });
    } else {
      console.log('No TOC element found');
    }
  }
  
  // Initialize TOC sticky behavior
  ensureTOCSticky();
  
  // Re-initialize periodically to ensure it stays sticky
  setInterval(ensureTOCSticky, 2000);
  
  // Re-initialize on window resize
  window.addEventListener('resize', function() {
    setTimeout(ensureTOCSticky, 100);
  });
  
  // Re-initialize if content changes
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.type === 'childList') {
        setTimeout(ensureTOCSticky, 100);
      }
    });
  });
  
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });
  
  // Force re-initialization after page load
  window.addEventListener('load', function() {
    setTimeout(ensureTOCSticky, 500);
  });
});
</script>

<style>
/* Additional styles for better integration */
body {
  overflow-x: hidden;
}

.sidebar-toggle {
  -webkit-tap-highlight-color: transparent;
}

/* Ensure smooth transitions on all relevant elements */
.sidebar,
.sidebar-toggle,
.main-content,
.sidebar-backdrop {
  will-change: transform, opacity, margin;
}

/* Focus styles for accessibility */
.sidebar-toggle:focus {
  outline: 2px solid white;
  outline-offset: 2px;
}

/* Prevent text selection on burger icon */
.burger-icon {
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
</style> 
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_001.html" class="pagination-link" aria-label="Large Language Models for the History, Philosophy and Sociology of Science (Workshop)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_004.html" class="pagination-link" aria-label="OpenAlex Mapper: Transdisciplinary Investigations">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper: Transdisciplinary Investigations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">abstract:</span><span class="co"> "\n      The presentation provides a primer on Large Language Models (LLMs),\</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">  \ beginning with the Transformer architecture, its encoder-decoder structure, and\</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">  \ the concept of contextualized word embeddings. It details the development of pre-trained\</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">  \ language models, distinguishing between encoder-based models like BERT (Bidirectional\</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  \ Encoder Representations from Transformers) for full context understanding and\</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  \ decoder-based models like GPT (Generative Pre-trained Transformers) for text generation.\</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">  \ The..."</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="an">author:</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">- affiliation: AI-NEPI Conference Participant</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">  name: Arno Simons</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="an">bibliography:</span><span class="co"> bibliography.bib</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="an">date:</span><span class="co"> '2025'</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">---</span></span>
<span id="cb1-16"><a href="#cb1-16"></a></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="fu"># A Primer on Large Language Models</span></span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="fu">## Overview</span></span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>This presentation provides a primer on Large Language Models (LLMs), beginning with the *Transformer* architecture, its encoder-decoder structure, and the concept of contextualized word embeddings. It details the development of pre-trained language models, distinguishing between encoder-based models like *BERT* (Bidirectional Encoder Representations from Transformers) for full context understanding and decoder-based models like *GPT* (Generative Pre-trained Transformers) for text generation.</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a>The evolution of LLMs in science domains is discussed, highlighting models such as *BioBERT*, *Specter*, and *SciBERT*, and various adaptation methods including continued pre-training, fine-tuning, contrastive learning (e.g., *SentenceBERT*), and Retrieval Augmented Generation (*RAG*). Key distinctions in LLM concepts cover architectures, fine-tuning strategies, and word versus sentence embeddings.</span>
<span id="cb1-24"><a href="#cb1-24"></a></span>
<span id="cb1-25"><a href="#cb1-25"></a>Applications of LLMs in History and Philosophy of Science and Technology Studies (HPSS) research are categorized into: dealing with data/sources, analyzing knowledge structures (e.g., entity extraction, mapping discourses), studying dynamics (e.g., conceptual histories), and examining knowledge practices (e.g., citation context analysis). Trends in HPSS LLM usage include accelerating interest, publication in diverse journals, varied customization levels, and increased accessibility (e.g., *BERTopic*).</span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a>Significant concerns are computational resources, model opaqueness, and lack of training data/benchmarks. HPSS-specific challenges involve the historical evolution of language, the need for critical reconstructive perspectives, and issues with sparse, multilingual, or old script data. Recommendations emphasize building LLM literacy, developing shared resources, and maintaining HPSS methodological integrity while leveraging LLMs to bridge qualitative/quantitative approaches and reflect on the field's intellectual history (e.g., co-word analysis).</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="fu">## The Transformer Architecture: Foundation of LLMs</span></span>
<span id="cb1-30"><a href="#cb1-30"></a></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="al">![Slide 01](images/ai-nepi_003_slide_01.jpg)</span></span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a>The *Transformer* architecture, introduced in 2017, serves as the fundamental framework for virtually all contemporary Large Language Models (LLMs). Originally conceived for machine translation tasks, such as converting German text to English, the *Transformer* model features a distinctive dual-stream structure. These two streams, an encoder on the left and a decoder on the right, are interconnected.</span>
<span id="cb1-34"><a href="#cb1-34"></a></span>
<span id="cb1-35"><a href="#cb1-35"></a>The encoder processes the input source language sentence. For instance, words from a German sentence are fed into the encoder, where they are transformed into numerical representations. These numerical data undergo processing through multiple layers, within which contextualized word embeddings are progressively refined. A key characteristic of the encoder is its ability to process the entire input sentence simultaneously. Each word in the source sentence can interact with, or "attend to," every other word, enabling the model to construct a comprehensive representation of the sentence's overall meaning.</span>
<span id="cb1-36"><a href="#cb1-36"></a></span>
<span id="cb1-37"><a href="#cb1-37"></a>The numerical representation generated by the encoder is then passed to the decoder stream. The decoder's function is to generate the output sentence in the target language, for example, English words. As each English word is produced, it is fed back into the decoder as input for generating the subsequent word. This iterative process continues until the complete target sentence is formed.</span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>Unlike the encoder, the decoder operates with a unidirectional attention mechanism, meaning that generated words can only consider preceding words in the sequence. They cannot access future words, a constraint inherent to the next-word prediction task. However, they can look back at the words already generated in the sequence. Both streams utilize layers to increasingly contextualize word embeddings.</span>
<span id="cb1-40"><a href="#cb1-40"></a></span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="fu">## Pre-trained Language Models: BERT and GPT</span></span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a><span class="al">![Slide 03](images/ai-nepi_003_slide_03.jpg)</span></span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a>Following the introduction of the *Transformer* architecture, researchers began re-engineering its encoder and decoder streams independently to develop pre-trained language models. These models are designed to achieve a strong general understanding or generation capability in a language, which can then be adapted through further, often minor, training for specific Natural Language Processing (NLP) tasks.</span>
<span id="cb1-46"><a href="#cb1-46"></a></span>
<span id="cb1-47"><a href="#cb1-47"></a>On the encoder side, this re-engineering led to the development of models like *BERT*, which stands for Bidirectional Encoder Representations from Transformers. The *BERT* family of models remains highly influential. *BERT*'s operational principle allows every word in an input sequence to attend to every other word, facilitating a comprehensive, full-context understanding of the entire input simultaneously. The term "bidirectional" in its name refers to this ability of words to consider context from both preceding and succeeding words, while "encoder-based" indicates its derivation from the *Transformer*'s original encoder stream.</span>
<span id="cb1-48"><a href="#cb1-48"></a></span>
<span id="cb1-49"><a href="#cb1-49"></a>Conversely, the decoder stream gave rise to models like *GPT* (Generative Pre-trained Transformers), which form the basis of well-known applications such as *ChatGPT*. Due to their architectural constraint of only looking at predecessor words, *GPT* models excel at generating new words and, consequently, new text. This generative capability is a primary differentiator from *BERT* models, which are not inherently designed for extensive text generation.</span>
<span id="cb1-50"><a href="#cb1-50"></a></span>
<span id="cb1-51"><a href="#cb1-51"></a>The core difference lies in their primary functions: *GPT* models are generative, designed to produce language, whereas *BERT*-like models are geared towards a coherent, full-context understanding of sentences. Beyond these two primary types, other model architectures exist, including those that combine encoder and decoder components. Furthermore, there are advanced techniques for utilizing decoders in ways that enable them to perform more like encoders, achieving bidirectional context understanding. An example of such an approach is *XLM*, which is based on *XLNet*. Understanding the distinction between generative models and full-context understanding models is crucial.</span>
<span id="cb1-52"><a href="#cb1-52"></a></span>
<span id="cb1-53"><a href="#cb1-53"></a><span class="fu">## Evolution and Adaptation of LLMs for Scientific Domains</span></span>
<span id="cb1-54"><a href="#cb1-54"></a></span>
<span id="cb1-55"><a href="#cb1-55"></a><span class="al">![Slide 06](images/ai-nepi_003_slide_06.jpg)</span></span>
<span id="cb1-56"><a href="#cb1-56"></a></span>
<span id="cb1-57"><a href="#cb1-57"></a>The evolution of Large Language Models (LLMs) has seen a significant focus on applications within various scientific domains and tasks. An overview of this development reveals a diverse landscape of models. Notably, encoder-type models, similar to *BERT*, are more commonly developed and applied in scientific contexts than decoder-type models. Early influential models in this space include *BioBERT*, *Specter*, and *SciBERT*. These, along with newer models, cater to a wide array of scientific fields such as biomedicine, chemistry, material science, climate science, mathematics, physics, and the social sciences.</span>
<span id="cb1-58"><a href="#cb1-58"></a></span>
<span id="cb1-59"><a href="#cb1-59"></a>Adapting LLMs to the specific language of scientific domains involves several methods. The foundational method is pre-training, where a model initially learns language patterns. This occurs either by predicting the next token, typical for *GPT*-style models, or by predicting randomly masked words, characteristic of *BERT*-style models. However, full pre-training demands extensive computational resources and vast datasets, making it impractical for many research groups. A more feasible approach is continued pre-training, where an already pre-trained model (e.g., a general *BERT* model) is further trained on a corpus of domain-specific text, such as physics literature.</span>
<span id="cb1-60"><a href="#cb1-60"></a></span>
<span id="cb1-61"><a href="#cb1-61"></a>Another common adaptation strategy is fine-tuning for downstream tasks. This involves adding new layers on top of a pre-trained model and training these specific layers to perform tasks like classification, for example, to determine sentiment or identify named entities. Prompt-based methods were mentioned as another adaptation technique, though not elaborated upon.</span>
<span id="cb1-62"><a href="#cb1-62"></a></span>
<span id="cb1-63"><a href="#cb1-63"></a>Contrastive learning is a key method for generating sentence or document embeddings that reside in the same vector space as word embeddings. *SentenceBERT* is a prominent and widely adopted technique for achieving this. This is particularly relevant as Iryna Gurevych, a keynote speaker, is known for her work in this area and might discuss *SentenceBERT*.</span>
<span id="cb1-64"><a href="#cb1-64"></a></span>
<span id="cb1-65"><a href="#cb1-65"></a>Retrieval Augmented Generation (*RAG*) represents a significant adaptation approach. *RAG* is not a single model but rather a pipeline or system where at least two, and often more, models work together. This technique allows for the adaptation of an LLM to a specific domain, such as a scientific field, without the need for extensive retraining of the core generative model. The *RAG* process typically involves a user submitting a query (e.g., "What are LLMs?"). A model, often *BERT*-like, then encodes this query into a sentence embedding. This embedding is used to search a database of relevant documents, retrieving the most similar passages. These retrieved passages are then integrated into the prompt provided to a generative LLM, which uses this augmented context to produce an answer. This mechanism is commonly used by systems like *ChatGPT* when they access external information, such as searching the internet.</span>
<span id="cb1-66"><a href="#cb1-66"></a></span>
<span id="cb1-67"><a href="#cb1-67"></a>Finally, reasoning models or agents are emerging as complex systems. These are not monolithic LLMs but rather integrated ensembles of multiple LLMs combined with a variety of other software tools.</span>
<span id="cb1-68"><a href="#cb1-68"></a></span>
<span id="cb1-69"><a href="#cb1-69"></a><span class="fu">## Key LLM Concepts and Application Categories in HPSS Research</span></span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a>Several key distinctions are important to retain as a primer on Large Language Models. These include the existence of different model architectures (such as encoder-only, decoder-only, and encoder-decoder structures), a variety of fine-tuning strategies for adapting models to specific tasks, the crucial difference between word embeddings and sentence embeddings (which represent fundamentally different levels of semantic representation), and the varying levels of abstraction at which these models can be applied.</span>
<span id="cb1-72"><a href="#cb1-72"></a></span>
<span id="cb1-73"><a href="#cb1-73"></a>An ongoing survey is being conducted on the use of LLMs as tools in History and Philosophy of Science and Technology Studies (HPSS) research. Preliminary findings from this survey have led to the identification of four main categories, or bins, for classifying these applications:</span>
<span id="cb1-74"><a href="#cb1-74"></a></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="ss">- </span>Dealing with Data and Sources: This category encompasses uses of LLMs for interacting with research data and primary/secondary sources. Specific tasks include facilitating data discovery, parsing complex or large-scale textual data, and improving the overall management of source materials.</span>
<span id="cb1-76"><a href="#cb1-76"></a></span>
<span id="cb1-77"><a href="#cb1-77"></a><span class="ss">- </span>Knowledge Structures Analysis: LLMs are employed to analyze and extract knowledge structures embedded in texts. This includes the identification and extraction of specific entities relevant to HPSS, such as scientific instruments, celestial bodies, or chemical substances. It also involves mapping complex conceptual landscapes, a traditional area of HPSS inquiry, like analyzing science policy discourses or tracing the formation and interaction of interdisciplinary fields.</span>
<span id="cb1-78"><a href="#cb1-78"></a></span>
<span id="cb1-79"><a href="#cb1-79"></a><span class="ss">- </span>Dynamics: This category focuses on using LLMs to study the evolution and change of concepts and language over time. A prime example is the analysis of conceptual histories of words, tracking how their meanings and usages shift within scientific communities, an approach similar to that demonstrated by researchers like Nina Janich and Pelin Doğan.</span>
<span id="cb1-80"><a href="#cb1-80"></a></span>
<span id="cb1-81"><a href="#cb1-81"></a><span class="ss">- </span>Knowledge Practices: LLMs can be applied to investigate various knowledge practices. One specific example is citation context analysis. This method has an established tradition within HPSS, though in recent times it has often been predominantly used for evaluative purposes (e.g., research assessment). However, it holds potential for a broader range of HPSS-specific analytical tasks.</span>
<span id="cb1-82"><a href="#cb1-82"></a></span>
<span id="cb1-83"><a href="#cb1-83"></a><span class="fu">## Trends, Concerns, and Accessibility in HPSS LLM Usage</span></span>
<span id="cb1-84"><a href="#cb1-84"></a></span>
<span id="cb1-85"><a href="#cb1-85"></a>Several trends and concerns are evident regarding the use of Large Language Models (LLMs) in History and Philosophy of Science and Technology Studies (HPSS). There is an accelerating interest in LLMs within the HPSS community. Publications detailing LLM applications are found predominantly in information science-oriented journals such as *Scientometrics* and *JASIST* (Journal of the Association for Information Science and Technology). However, an increasing number of papers are also appearing in journals traditionally less focused on computational methods. This suggests that the enhanced semantic capabilities of modern LLMs are making them more attractive and relevant to qualitative researchers, philosophers, and other scholars within HPSS.</span>
<span id="cb1-86"><a href="#cb1-86"></a></span>
<span id="cb1-87"><a href="#cb1-87"></a>The degree of customization in LLM use varies widely across the field. Some researchers utilize readily available, off-the-shelf tools like *ChatGPT*, while others at the other end of the spectrum are engaged in developing entirely new LLM architectures tailored to specific HPSS needs.</span>
<span id="cb1-88"><a href="#cb1-88"></a></span>
<span id="cb1-89"><a href="#cb1-89"></a>Despite the growing adoption, several repeating concerns are frequently voiced. The substantial computational resources required to train or even fine-tune large models pose a significant barrier. The opaqueness of these models, often referred to as their "black box" nature, makes it difficult to understand their internal decision-making processes, which is a concern for interpretability and trustworthiness. There is also a perceived lack of sufficient training data, particularly for specialized historical contexts or languages relevant to HPSS research. Furthermore, the absence of established benchmarks specifically designed for evaluating LLM performance on HPSS-relevant tasks makes it challenging to compare different models or approaches systematically. Finally, researchers face a trade-off between different types of LLMs, as no single model is universally optimal; the most adequate model must be chosen based on the specific research question and purpose.</span>
<span id="cb1-90"><a href="#cb1-90"></a></span>
<span id="cb1-91"><a href="#cb1-91"></a>On a positive note, there is a discernible trend towards increased accessibility of LLM-related tools. For example, *BERTopic*, a popular library for topic modeling, is noted for its ease of use, largely due to robust maintenance and active development, making advanced techniques more approachable for a wider range of researchers.</span>
<span id="cb1-92"><a href="#cb1-92"></a></span>
<span id="cb1-93"><a href="#cb1-93"></a><span class="fu">## HPSS-Specific Challenges and Methodological Considerations for LLM Adoption</span></span>
<span id="cb1-94"><a href="#cb1-94"></a></span>
<span id="cb1-95"><a href="#cb1-95"></a>The adoption of Large Language Models (LLMs) in History and Philosophy of Science and Technology Studies (HPSS) necessitates acknowledging several challenges specific to the discipline. A primary challenge is the historical evolution of concepts and language. Most LLMs are trained on contemporary language, which may not align with the historical texts and linguistic conventions central to much HPSS research. This requires strategies such as training custom models on historical corpora or using existing models with a keen awareness of their inherent biases and limitations when applied to historical material.</span>
<span id="cb1-96"><a href="#cb1-96"></a></span>
<span id="cb1-97"><a href="#cb1-97"></a>Another significant challenge stems from the reconstructive and critically reflective perspective characteristic of HPSS. Scholars in this field typically do not take scientific texts at face value but instead engage in critical interpretation, reading "between the lines" to understand the authors' situated contexts, motivations, and subtle discursive strategies, such as boundary work. Current LLMs are generally not trained to detect or analyze these nuanced aspects of texts. Therefore, methods need to be developed to enable models to approximate this type of critical reading. Furthermore, HPSS research often contends with practical data issues like sparse datasets, the presence of multiple languages (often historical variants), and archaic scripts, all of which pose difficulties for standard LLM application.</span>
<span id="cb1-98"><a href="#cb1-98"></a></span>
<span id="cb1-99"><a href="#cb1-99"></a>To address these challenges and effectively leverage LLMs, several recommendations are proposed for the HPSS community. Firstly, there is a need to build LLM literacy. This involves understanding the underlying theory of these models, their capabilities, limitations, and the broader implications of their use. While natural language interfaces for coding may become more common, acquiring some coding skills can be beneficial. A crucial aspect of literacy is moving beyond the superficial use of tools that might produce appealing visualizations or graphs without a deep comprehension of the underlying processes or the significance of the results.</span>
<span id="cb1-100"><a href="#cb1-100"></a></span>
<span id="cb1-101"><a href="#cb1-101"></a>Secondly, the development of shared datasets and benchmarks specifically tailored to HPSS research questions and materials is essential for robust and comparable evaluations. Thirdly, it is vital to stay true to core HPSS methodologies. While HPSS research problems need to be translated into tractable NLP tasks (such as classification, generation, or summarization), care must be taken to ensure that these technical tasks do not overshadow or "hijack" the fundamental research purpose and critical inquiries of HPSS.</span>
<span id="cb1-102"><a href="#cb1-102"></a></span>
<span id="cb1-103"><a href="#cb1-103"></a>Despite these challenges, LLMs also present new opportunities. They offer promising avenues for bridging qualitative and quantitative research approaches, potentially fostering more integrated and multifaceted analyses. Moreover, the rise of LLMs provides an occasion for HPSS to reflect on its own intellectual history and pre-existing analytical frameworks. For instance, current LLM techniques resonate with earlier methods developed within HPSS, such as co-word analysis, pioneered by scholars like *Michel Callon* and *Arie Rip* in the 1980s, often in conjunction with *Actor-Network Theory* (*ANT*).</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>