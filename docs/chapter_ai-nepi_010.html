<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Boulanger, David Carreto Fidalgo &amp; Andreas Wagner">
<meta name="dcterms.date" content="2025-01-01">

<title>10&nbsp; Extracting Citation Data from Law and Humanities Scholarship – AI-NEPI Conference Proceedings - Enhanced Edition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_011.html" rel="next">
<link href="./chapter_ai-nepi_009.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-fe5eeb5af71a333b155c360431d06b9a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e463572c889c87c7eefd27e1777fa793.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="10&nbsp; Extracting Citation Data from Law and Humanities Scholarship – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta property="og:description" content="The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (Web of Science, Scopus, OpenAlex) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses. Traditional machine learning tools like ExCite perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., ExCite …">
<meta property="og:image" content="images/ai-nepi_010_slide_01.jpg">
<meta property="og:site_name" content="AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:title" content="10&nbsp; Extracting Citation Data from Law and Humanities Scholarship – AI-NEPI Conference Proceedings - Enhanced Edition">
<meta name="twitter:description" content="The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (Web of Science, Scopus, OpenAlex) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses. Traditional machine learning tools like ExCite perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., ExCite …">
<meta name="twitter:image" content="images/ai-nepi_010_slide_01.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_010.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings - Enhanced Edition</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy and Sociology of Science (Workshop)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Primer on Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OpenAlex Mapper: Transdisciplinary Investigations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computational HPSS: Tracing Ancient Wisdom’s Influence with VERITRACE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and Scientific Insights in Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Science: LLM for the History, Philosophy and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG Systems in Philosophy and HPSS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Plural pursuit across scales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Text Granularity and Topic Model Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-Aware Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">LLMs for Chemical Knowledge Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Interpretable Models for Linguistic Change</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">LLM for HPS Studies: Analyzing the NHGRI Archive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">10.1</span> Overview</a></li>
  <li><a href="#project-scope-and-problem" id="toc-project-scope-and-problem" class="nav-link" data-scroll-target="#project-scope-and-problem"><span class="header-section-number">10.2</span> Project Scope and Problem</a></li>
  <li><a href="#problem-bibliometric-database-coverage" id="toc-problem-bibliometric-database-coverage" class="nav-link" data-scroll-target="#problem-bibliometric-database-coverage"><span class="header-section-number">10.3</span> Problem: Bibliometric Database Coverage</a></li>
  <li><a href="#complex-footnotes-and-training-data" id="toc-complex-footnotes-and-training-data" class="nav-link" data-scroll-target="#complex-footnotes-and-training-data"><span class="header-section-number">10.4</span> Complex Footnotes and Training Data</a></li>
  <li><a href="#limitations-of-existing-tools" id="toc-limitations-of-existing-tools" class="nav-link" data-scroll-target="#limitations-of-existing-tools"><span class="header-section-number">10.5</span> Limitations of Existing Tools</a></li>
  <li><a href="#llms-as-solution-the-trust-problem" id="toc-llms-as-solution-the-trust-problem" class="nav-link" data-scroll-target="#llms-as-solution-the-trust-problem"><span class="header-section-number">10.6</span> LLMs as Solution: The Trust Problem</a></li>
  <li><a href="#solution-requirements" id="toc-solution-requirements" class="nav-link" data-scroll-target="#solution-requirements"><span class="header-section-number">10.7</span> Solution Requirements</a></li>
  <li><a href="#gold-standard-dataset" id="toc-gold-standard-dataset" class="nav-link" data-scroll-target="#gold-standard-dataset"><span class="header-section-number">10.8</span> Gold Standard Dataset</a></li>
  <li><a href="#llamore-extraction-and-evaluation-tool" id="toc-llamore-extraction-and-evaluation-tool" class="nav-link" data-scroll-target="#llamore-extraction-and-evaluation-tool"><span class="header-section-number">10.9</span> Llamore: Extraction and Evaluation Tool</a></li>
  <li><a href="#evaluation-methodology" id="toc-evaluation-methodology" class="nav-link" data-scroll-target="#evaluation-methodology"><span class="header-section-number">10.10</span> Evaluation Methodology</a></li>
  <li><a href="#evaluation-results" id="toc-evaluation-results" class="nav-link" data-scroll-target="#evaluation-results"><span class="header-section-number">10.11</span> Evaluation Results</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Show code</button></div></div>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Christian Boulanger, David Carreto Fidalgo &amp; Andreas Wagner <a href="mailto:boulanger@lhlt.mpg.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Max Planck Institute for Legal History and Legal Theory
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (<em>Web of Science</em>, <em>Scopus</em>, <em>OpenAlex</em>) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses. Traditional machine learning tools like <em>ExCite</em> perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., <em>ExCite</em> …
  </div>
</div>


</header>


<section id="overview" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">10.1</span> Overview</h2>
<p>The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (<em>Web of Science</em>, <em>Scopus</em>, <em>OpenAlex</em>) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses.</p>
<p>Traditional machine learning tools like <em>ExCite</em> perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., <em>ExCite</em> accuracy around 0.22-0.26 for extraction). Footnotes in SSH are often complex, containing commentary, abbreviations, and multiple references (“footnotes from hell”). Creating training data for traditional methods is laborious using annotation tools.</p>
<p><em>Large Language Models</em> (<em>LLMs</em>) and <em>Vision Language Models</em> (<em>VLMs</em>) show promise for handling messy textual data and PDFs, but the primary challenge is trusting the results due to potential hallucinations (e.g., inventing non-existent citations, as seen in legal cases).</p>
<p>A robust testing and evaluation solution is required, necessitating a high-quality gold standard dataset, a flexible evaluation framework adaptable to fast-moving technology, and solid testing algorithms for comparable metrics.</p>
<p>The project develops a specialized gold standard dataset encoded in <em>TEI XML</em>, chosen for its well-established standard, detailed specification covering phenomena beyond mere reference management (including context for citation intention), and compatibility with existing digital humanities corpora and tools like <em>Grobid</em>. The dataset creation involves stages: screenshot of the PDF, segmentation of reference strings from non-reference text within footnotes, and parsed structured data.</p>
<p>The dataset currently includes over 1,500 references from open access journals to enable full publication from PDF to parsed data structures.</p>
<p>A <em>Python</em> package named <em>Llamore</em> (<em>Large Language Models for Reference Extraction</em>) is developed. <em>Llamore</em> is lightweight, acting as an interface to various <em>LLMs</em>/<em>VLMs</em> (compatible with <em>OpenAI API</em>, covering <em>Ollama</em>, <em>VLLM</em>, etc.). It takes text or PDFs as input and outputs references in <em>TEI XML</em> format.</p>
<p>It also provides an evaluation function using the <em>F1 score</em> metric to compare extracted references against gold standard references. The <em>F1 score</em> calculation involves counting exact matches of bibliographic elements (analytic title, monographic title, surname, publication date, etc.) and dividing by the number of predicted and gold elements.</p>
<p>The problem of aligning extracted references to gold references is solved using an unbalanced assignment problem solver (from <em>SciPy</em>), maximizing the total <em>F1 score</em> with unique assignments and penalizing missing or hallucinated references with an <em>F1 score</em> of zero.</p>
<p>Evaluation results show that <em>Llamore</em> performs comparably to <em>Grobid</em> on biomedical datasets (<em>PLOS 1000</em>) but significantly outperforms <em>Grobid</em> on the specialized humanities dataset, where <em>Grobid</em> struggles due to being out of distribution. While <em>LLMs</em> require significantly more compute than <em>Grobid</em>, their performance on complex SSH footnotes is superior.</p>
<p>Error analysis suggests issues include difficulty distinguishing volume/page numbers, misclassifying names in titles as authors, and misinterpreting terminology like “idem” as authors. The required <em>F1 score</em> for “good enough” data depends on the analysis goal (tendencies vs.&nbsp;accurate data). The current focus is on achieving reliable results before scaling up to avoid interpreting hallucinated data.</p>
<p>Future work could involve <em>retrieval augmented generation</em> (<em>RAG</em>) using disciplinary databases for validation and potentially human-in-the-loop workflows for error correction and iterative model improvement. The project aims for both specific research results (e.g., citation trends in a specific journal) and generic applicability to other projects.</p>
</section>
<section id="project-scope-and-problem" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="project-scope-and-problem"><span class="header-section-number">10.2</span> Project Scope and Problem</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>The project focuses on extracting citation data specifically from Law and Humanities scholarship, a domain characterized by extensive and complex footnotes. The primary challenge involves parsing these footnotes using <em>Large Language Models</em> (<em>LLMs</em>) or other algorithmic approaches.</p>
<p>The extracted data is intended for generating citation graphs, which are valuable tools in intellectual history and the history of science. Citation graphs enable the discovery of patterns and relationships within knowledge production, facilitate the reconstruction of intellectual influences, and allow for the measurement of the reception of specific ideas. An example application involves tracking the change in most-cited authors over time, such as an analysis conducted for the <em>Journal of Law and Society</em> between 1994 and 2003.</p>
</section>
<section id="problem-bibliometric-database-coverage" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="problem-bibliometric-database-coverage"><span class="header-section-number">10.3</span> Problem: Bibliometric Database Coverage</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_01.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 01</figcaption>
</figure>
</div>
<p>A significant problem is the extremely poor coverage of historical <em>Social Sciences and Humanities</em> (<em>SSH</em>) scholarship in existing bibliometric databases. The primary databases in this space include <em>Web of Science</em>, <em>Scopus</em>, and <em>OpenAlex</em>. <em>Web of Science</em> and <em>Scopus</em> are characterized by high costs and very restrictive licenses, making dependence on them undesirable.</p>
<p>While <em>OpenAlex</em> is preferable due to its open access nature, it also lacks sufficient coverage for the specific content required for this research.</p>
<p>Common coverage gaps across these databases for <em>SSH</em> literature include a lack of inclusion for journals not classified as “A-journals,” insufficient data for publications from the pre-digital age, and a general lack of coverage for non-English language content.</p>
<p>An illustrative example is the <em>Zeitschrift für Rechtssoziologie</em>, a German journal established in 1980. Analysis of available citation data by decade shows very low coverage across <em>Dimensions</em>, <em>OpenAlex</em>, and <em>Web of Science</em> for the decades before the 2000s. Although coverage improves somewhat after 2000, it remains incomplete, particularly for <em>Web of Science</em>.</p>
<p>Several factors contribute to this poor coverage in <em>SSH</em>. Firstly, there is a perceived lack of financial incentive compared to <em>STEM</em>, medicine, and economics, which are typically well-represented.</p>
<p>Secondly, these databases often focus on metrics like the “impact factor,” which aligns with science evaluation goals but not necessarily with the objectives of intellectual history research. Finally, the literature itself presents a technical challenge due to the extensive use of complex footnotes, which are difficult for automated systems to process accurately.</p>
</section>
<section id="complex-footnotes-and-training-data" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="complex-footnotes-and-training-data"><span class="header-section-number">10.4</span> Complex Footnotes and Training Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_04.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>Problem 2 concerns the nature of the footnotes themselves, often referred to as “footnotes from hell.” These are typical of humanities scholarship and contain complex structures, including commentary, messy formatting, abbreviations, parenthetical information, and multiple distinct references embedded within surrounding non-citation text. An example image displays a scanned page with a footnote exhibiting these characteristics, featuring content in both German and English, various abbreviations, and multiple citations within a single numbered entry.</p>
<p>Problem 3 highlights the difficulty in creating training data for citation extraction. The traditional approach involves a laborious manual annotation process. A web-based annotation tool is utilized for this purpose, allowing users to highlight segments of text within footnotes and assign specific labels corresponding to bibliographic elements such as:</p>
<ul>
<li><p>Author</p></li>
<li><p>Title</p></li>
<li><p>Date</p></li>
<li><p>Journal</p></li>
<li><p>Volume</p></li>
<li><p>Pages</p></li>
<li><p>DOI</p></li>
<li><p>ISBN</p></li>
<li><p>URL</p></li>
<li><p>BackRef</p></li>
<li><p>Signal</p></li>
<li><p>Ignore</p></li>
</ul>
<p>and indicators for whether a segment is a reference or not. This manual annotation requires a significant investment of time and effort.</p>
</section>
<section id="limitations-of-existing-tools" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="limitations-of-existing-tools"><span class="header-section-number">10.5</span> Limitations of Existing Tools</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_05.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>Problem 4 identifies a critical limitation: existing tools designed for citation extraction are unable to handle the complexity of humanities footnotes effectively. These traditional tools typically rely on machine learning methods such as <em>Conditional Random Forests</em>. However, their performance on the specific type of complex footnote data encountered in this domain is poor.</p>
<p>An example illustrating this limitation is the performance of the tool <em>ExCite</em>. Its performance is evaluated using metrics including Extraction Accuracy and Segmentation Accuracy across different training data configurations: Default, Footnoted, and Combined. The results show consistently low accuracy scores.</p>
<p>With Default training data, Extraction Accuracy is 0.24 and Segmentation Accuracy is 0.37. Using Footnoted training data yields Extraction Accuracy of 0.26 and Segmentation Accuracy of 0.37. The Combined training data results in Extraction Accuracy of 0.22 and Segmentation Accuracy of 0.47. These figures, sourced from Boulanger/Iurshina (2022), Table 1, demonstrate that <em>ExCite</em>, and by extension other similar traditional tools, struggle significantly with this task, regardless of the training data used.</p>
</section>
<section id="llms-as-solution-the-trust-problem" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="llms-as-solution-the-trust-problem"><span class="header-section-number">10.6</span> LLMs as Solution: The Trust Problem</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_05.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 05</figcaption>
</figure>
</div>
<p>The question arises whether <em>Large Language Models</em> (<em>LLMs</em>) can offer a solution to the challenges of extracting citation data from complex footnotes. Early experiments conducted in 2022 using models such as <em>text-davinci-003</em> demonstrated the potential power of <em>LLMs</em> to extract references even from messy textual data. Newer models promise even better performance.</p>
<p>Furthermore, <em>Vision Language Models</em> (<em>VLMs</em>) possess the capability to process PDFs directly, which is advantageous given that source materials are frequently available in this format.</p>
<p>Potential methods for leveraging <em>LLMs</em> include <em>prompt engineering</em>, <em>Retrieval Augmented Generation</em> (<em>RAG</em>), and <em>finetuning</em>. However, a primary concern is the trustworthiness of the results produced by these models. A significant problem is the potential for hallucination, where <em>LLMs</em> invent information that does not exist, including fabricating citations.</p>
<p>A notable example illustrating this issue involved a lawyer who used <em>ChatGPT</em> for a federal court filing and cited non-existent cases invented by the model, resulting in severe consequences. This highlights the critical principle that analysis should not be undertaken unless the results are known to be correct and sufficient validation data is available to verify accuracy.</p>
</section>
<section id="solution-requirements" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="solution-requirements"><span class="header-section-number">10.7</span> Solution Requirements</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_06.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>To address the trust issue and enable reliable citation extraction, a robust testing and evaluation solution is required. This solution must meet several key requirements.</p>
<ul>
<li><p>Firstly, it necessitates a high-quality Gold Standard dataset against which extracted results can be compared.</p></li>
<li><p>Secondly, it requires a flexible framework capable of adapting easily to the rapidly evolving technology landscape of <em>LLMs</em> and <em>VLMs</em>.</p></li>
<li><p>Finally, the solution must incorporate solid testing and evaluation algorithms designed to produce comparable metrics, allowing for objective assessment of different approaches and models.</p></li>
</ul>
</section>
<section id="gold-standard-dataset" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="gold-standard-dataset"><span class="header-section-number">10.8</span> Gold Standard Dataset</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_07.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p>The project involves compiling a high-quality gold standard dataset intended for both training and evaluation purposes. The chosen encoding standard for this dataset is <em>TEI XML</em>. This standard was selected because it is well-established and precisely specified within the humanities and digital editorics fields.</p>
<p><em>TEI XML</em> is more comprehensive than simpler bibliographical standards like <em>CSL</em> or <em>BibTeX</em>, covering a wider range of phenomena and extending beyond basic reference management. It allows for encoding contextual information, which can be valuable for tasks such as classifying citation intention.</p>
<p>Furthermore, adopting <em>TEI XML</em> enables the project to potentially utilize existing text collections and corpora from other digital editorics projects that publish their source data, including detailed reference encodings, in this format. Such existing corpora can also serve as resources for testing the generalization and robustness of the developed mechanisms.</p>
<p>The dataset establishment process is currently underway. The encoding involves several stages, illustrated by an example of footnote number four. These stages include capturing a screenshot of the PDF source, segmenting the text to distinguish the actual reference string from surrounding non-reference text within the footnote (such as introductory phrases), and finally creating a parsed structured data representation of the reference.</p>
<p>Midway through the project, the strategy for building the dataset was adjusted. Initially, the focus was on data directly relevant to the primary research question. More recently, the decision was made to include the source PDFs and structure the dataset to enable the use of <em>Vision Language Models</em> (<em>VLMs</em>). The goal is to be able to publish the entire dataset pipeline, from the source PDFs through to the parsed data structures.</p>
<p>To facilitate this, the selection of source journals was changed to focus on open access publications. The dataset currently includes the encoding of over 1,500 references. It is noted that this count refers to occurrences, meaning the same work referenced multiple times is encoded separately to capture the specific context of each mention.</p>
<p>A significant benefit of using the interoperable <em>TEI XML</em> standard is the availability of numerous tools. A particularly relevant tool for this project is <em>Grobid</em>, a popular system for reference and information extraction. <em>Grobid</em> utilizes <em>TEI XML</em> for its own training and evaluation processes. By using the same data format, the project can directly compare its performance against <em>Grobid</em>, potentially use <em>Grobid</em>’s existing training data, and contribute the newly created dataset to the <em>Grobid</em> development team.</p>
</section>
<section id="llamore-extraction-and-evaluation-tool" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="llamore-extraction-and-evaluation-tool"><span class="header-section-number">10.9</span> Llamore: Extraction and Evaluation Tool</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_14.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>The project developed a tool named <em>Llamore</em>, which stands for <em>Large Language Models for Reference Extraction</em>. <em>Llamore</em> is implemented as a small <em>Python</em> package. Its core capabilities include taking text or PDF documents as input, extracting references from them, and exporting these extracted references as <em>TEI</em> formatted <em>XML</em> files.</p>
<p>Additionally, if gold standard references are provided, <em>Llamore</em> can evaluate the performance of the extraction process.</p>
<p>The design of <em>Llamore</em> prioritized two main objectives: being lightweight and ensuring broad compatibility. It is lightweight because it does not contain any language models internally; instead, it functions as an interface to a model selected by the user. This design also ensures compatibility with a wide range of both open and closed <em>LLMs</em> and <em>VLMs</em>.</p>
<p>Implementation details include its availability on <em>PyPI</em>, allowing simple installation via the <code>pip</code> package manager. The extraction workflow involves defining an extractor object specific to the chosen model, such as using the <em>OpenAIExtractor</em>. This particular extractor provides compatibility with many open models (like those served by <em>Ollama</em> or <em>VLLM</em>) by interacting with their <em>API</em> endpoints that are designed to be compatible with the <em>OpenAI API</em> specification.</p>
<p>The user provides a PDF or text input to the extractor, receives the extracted references, and can then export these references to an <em>XML</em> file. For evaluation, the user imports the <em>F1</em> class from the package and provides both the gold standard references and the extracted references to this class to compute performance metrics.</p>
</section>
<section id="evaluation-methodology" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="evaluation-methodology"><span class="header-section-number">10.10</span> Evaluation Methodology</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_16.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>The evaluation methodology employed utilizes the <em>F1 score</em>, a well-established metric for comparing structured data. The <em>F1 score</em> is calculated based on Precision and Recall, which in turn depend on the number of matches between elements in an extracted reference and a corresponding gold standard reference. A match is defined based on the exact correspondence of specific bibliographic elements, such as analytic title, monographic title, surname, and publication date.</p>
<p>Partial matches, like a forename with an extra dot, might not count as an exact match depending on configuration. Precision is calculated as the number of matches divided by the total number of elements predicted in the extracted reference, while Recall is the number of matches divided by the total number of elements in the gold reference. The <em>F1 score</em> is the harmonic mean of Precision and Recall. An <em>F1 score</em> of 1 indicates perfect extraction, where the reference is perfectly captured, while an <em>F1 score</em> of 0 signifies no matches were found.</p>
<p>A key challenge in evaluating performance across a set of references is aligning the extracted references with their corresponding gold standard references. This is tackled by formulating the problem as an unbalanced assignment problem. <em>Llamore</em> uses a solver from the <em>SciPy</em> library internally to address this. The process involves computing the <em>F1 score</em> for every possible pairing between each extracted reference and each gold reference, constructing a matrix of scores.</p>
<p>The solver then finds the assignment of extracted references to gold references that maximizes the total sum of <em>F1 scores</em>, ensuring that each reference is assigned uniquely. The final overall score is the macro average of the <em>F1 scores</em> for the assigned pairs. The method penalizes missing gold references (those not matched by an extracted reference) and hallucinated extracted references (those not matched to a gold reference) by assigning them an <em>F1 score</em> of zero in the averaging process. This approach is noted as being similar to a method recently published by <em>Packet et al.</em></p>
</section>
<section id="evaluation-results" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="evaluation-results"><span class="header-section-number">10.11</span> Evaluation Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_010_slide_20.jpg" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>Evaluation was conducted to assess the effectiveness of the developed approach. Using the <em>PLOS 1000</em> dataset, which comprises 1,000 PDFs from the biomedical field, <em>Llamore</em>’s performance was found to be comparable to that of <em>Grobid</em>. It is noted that <em>Grobid</em> was specifically trained on data from similar journal articles.</p>
<p>However, in terms of efficiency and compute resources, <em>Grobid</em> is significantly superior, requiring orders of magnitude less computational power than <em>LLMs</em> like <em>Gemini</em>.</p>
<p>When evaluated on the specialized humanities dataset created by the project, <em>Grobid</em> struggled significantly to extract references, indicating that this data is out of its training distribution. In contrast, the <em>LLM</em>-based approach implemented in <em>Llamore</em> performed significantly better on this complex dataset.</p>
<p>The conclusion drawn is that while less computationally efficient on standard datasets, the <em>LLM</em>-based approach implemented in <em>Llamore</em> is more effective for extracting references from the challenging, out-of-distribution data characteristic of humanities footnotes compared to <em>Grobid</em>.</p>
<p>It is acknowledged that the <em>F1 scores</em> achieved are not considered high overall, suggesting substantial room for improvement. The current implementation utilizes a very basic approach, essentially sending the raw PDF text to a pre-trained model. Future work could explore potential improvements through methods such as <em>fine-tuning</em> models or incorporating more contextual information to enhance performance.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Create burger menu button
  const toggleButton = document.createElement('button');
  toggleButton.className = 'sidebar-toggle';
  toggleButton.setAttribute('aria-label', 'Toggle sidebar');
  toggleButton.innerHTML = `
    <div class="burger-icon">
      <span></span>
      <span></span>
      <span></span>
    </div>
  `;
  
  // Create backdrop for mobile
  const backdrop = document.createElement('div');
  backdrop.className = 'sidebar-backdrop';
  
  // Add elements to page
  document.body.appendChild(toggleButton);
  document.body.appendChild(backdrop);
  
  // Get sidebar and main content elements
  const sidebar = document.querySelector('.sidebar') || 
                 document.querySelector('.quarto-sidebar') || 
                 document.querySelector('.sidebar-navigation');
  const mainContent = document.querySelector('main') || 
                     document.querySelector('.main-content') || 
                     document.querySelector('.quarto-container') || 
                     document.body;
  
  // State management
  let sidebarOpen = window.innerWidth > 768; // Start open on desktop, closed on mobile
  
  // Initialize sidebar state
  function initializeSidebar() {
    if (window.innerWidth <= 768) {
      sidebarOpen = false;
    }
    updateSidebarState();
  }
  
  // Update sidebar state and classes
  function updateSidebarState() {
    if (sidebar) {
      if (sidebarOpen) {
        sidebar.classList.remove('collapsed');
        toggleButton.classList.add('sidebar-open');
        mainContent.classList.add('sidebar-open');
        mainContent.classList.remove('sidebar-closed');
        if (window.innerWidth <= 768) {
          backdrop.classList.add('active');
        }
      } else {
        sidebar.classList.add('collapsed');
        toggleButton.classList.remove('sidebar-open');
        mainContent.classList.remove('sidebar-open');
        mainContent.classList.add('sidebar-closed');
        backdrop.classList.remove('active');
      }
    }
    
    // Store state in localStorage
    localStorage.setItem('sidebarOpen', sidebarOpen);
  }
  
  // Toggle sidebar
  function toggleSidebar() {
    sidebarOpen = !sidebarOpen;
    updateSidebarState();
  }
  
  // Close sidebar (for chapter links)
  function closeSidebar() {
    if (window.innerWidth <= 768) { // Only auto-close on mobile
      sidebarOpen = false;
      updateSidebarState();
    }
  }
  
  // Event listeners
  toggleButton.addEventListener('click', toggleSidebar);
  backdrop.addEventListener('click', toggleSidebar);
  
  // Auto-close sidebar when clicking chapter links
  if (sidebar) {
    const chapterLinks = sidebar.querySelectorAll('a[href]');
    chapterLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        // Small delay to allow navigation to start
        setTimeout(closeSidebar, 100);
      });
    });
  }
  
  // Handle window resize
  window.addEventListener('resize', function() {
    if (window.innerWidth > 768 && !sidebarOpen) {
      sidebarOpen = true;
      updateSidebarState();
    } else if (window.innerWidth <= 768 && sidebarOpen) {
      sidebarOpen = false;
      updateSidebarState();
    }
  });
  
  // Handle escape key
  document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape' && sidebarOpen && window.innerWidth <= 768) {
      closeSidebar();
    }
  });
  
  // Restore saved state from localStorage
  const savedState = localStorage.getItem('sidebarOpen');
  if (savedState !== null) {
    sidebarOpen = savedState === 'true';
  }
  
  // Initialize
  initializeSidebar();
  
  // Add keyboard navigation support
  toggleButton.addEventListener('keydown', function(e) {
    if (e.key === 'Enter' || e.key === ' ') {
      e.preventDefault();
      toggleSidebar();
    }
  });
  
  // Improve accessibility
  toggleButton.setAttribute('role', 'button');
  toggleButton.setAttribute('tabindex', '0');
  
  // Update aria-expanded attribute
  function updateAriaExpanded() {
    toggleButton.setAttribute('aria-expanded', sidebarOpen);
  }
  
  // Call updateAriaExpanded whenever sidebar state changes
  const originalUpdateSidebarState = updateSidebarState;
  updateSidebarState = function() {
    originalUpdateSidebarState();
    updateAriaExpanded();
  };
  
  updateAriaExpanded();
  
  // Ensure TOC sticky positioning works properly
  function ensureTOCSticky() {
    // Find all possible TOC elements
    const tocSelectors = [
      '#TOC',
      '.table-of-contents',
      '.quarto-sidebar-toc',
      '.toc',
      '.quarto-toc',
      'nav[role="doc-toc"]',
      '.margin-sidebar',
      '.sidebar-right',
      '.quarto-margin-sidebar',
      '.column-margin'
    ];
    
    let toc = null;
    for (const selector of tocSelectors) {
      toc = document.querySelector(selector);
      if (toc) break;
    }
    
    if (toc) {
      console.log('Found TOC element:', toc.className || toc.id);
      
      // Force sticky positioning with important styles
      toc.style.setProperty('position', 'sticky', 'important');
      toc.style.setProperty('top', '1rem', 'important');
      toc.style.setProperty('max-height', 'calc(100vh - 2rem)', 'important');
      toc.style.setProperty('overflow-y', 'auto', 'important');
      toc.style.setProperty('z-index', '100', 'important');
      
      // Ensure parent containers support sticky
      let parent = toc.parentElement;
      while (parent && parent !== document.body) {
        parent.style.setProperty('position', 'relative', 'important');
        parent.style.setProperty('height', 'auto', 'important');
        parent = parent.parentElement;
      }
      
      // Add scroll event listener to maintain visibility
      let lastScrollTop = 0;
      const scrollHandler = function() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        
        // Ensure TOC remains visible and properly positioned
        if (toc && window.innerWidth > 768) {
          toc.style.setProperty('position', 'sticky', 'important');
          toc.style.setProperty('top', '1rem', 'important');
        }
        
        lastScrollTop = scrollTop;
      };
      
      // Remove existing scroll listeners to avoid duplicates
      window.removeEventListener('scroll', scrollHandler);
      window.addEventListener('scroll', scrollHandler, { passive: true });
      
      // Also apply to any nested TOC elements
      const nestedTocs = toc.querySelectorAll('#TOC, .toc, .table-of-contents');
      nestedTocs.forEach(nestedToc => {
        nestedToc.style.setProperty('position', 'sticky', 'important');
        nestedToc.style.setProperty('top', '0', 'important');
      });
    } else {
      console.log('No TOC element found');
    }
  }
  
  // Initialize TOC sticky behavior
  ensureTOCSticky();
  
  // Re-initialize periodically to ensure it stays sticky
  setInterval(ensureTOCSticky, 2000);
  
  // Re-initialize on window resize
  window.addEventListener('resize', function() {
    setTimeout(ensureTOCSticky, 100);
  });
  
  // Re-initialize if content changes
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.type === 'childList') {
        setTimeout(ensureTOCSticky, 100);
      }
    });
  });
  
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });
  
  // Force re-initialization after page load
  window.addEventListener('load', function() {
    setTimeout(ensureTOCSticky, 500);
  });
});
</script>

<style>
/* Additional styles for better integration */
body {
  overflow-x: hidden;
}

.sidebar-toggle {
  -webkit-tap-highlight-color: transparent;
}

/* Ensure smooth transitions on all relevant elements */
.sidebar,
.sidebar-toggle,
.main-content,
.sidebar-backdrop {
  will-change: transform, opacity, margin;
}

/* Focus styles for accessibility */
.sidebar-toggle:focus {
  outline: 2px solid white;
  outline-offset: 2px;
}

/* Prevent text selection on burger icon */
.burger-icon {
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
</style> 
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_009.html" class="pagination-link" aria-label="The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_011.html" class="pagination-link" aria-label="Chatting with Papers">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chatting with Papers</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">abstract:</span><span class="co"> "The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (*Web of Science*, *Scopus*, *OpenAlex*) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses. Traditional machine learning tools like *ExCite* perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., *ExCite* ..."</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="an">author:</span></span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">- affiliation: Max Planck Institute for Legal History and Legal Theory</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  email: boulanger@lhlt.mpg.de</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  name: Christian Boulanger, David Carreto Fidalgo &amp; Andreas Wagner</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="an">bibliography:</span><span class="co"> bibliography.bib</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="an">date:</span><span class="co"> '2025'</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">---</span></span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="fu"># Extracting Citation Data from Law and Humanities Scholarship</span></span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="fu">## Overview</span></span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a>The project addresses the problem of extracting citation data from Law and Humanities scholarship, which heavily relies on complex footnotes. Existing bibliometric databases (*Web of Science*, *Scopus*, *OpenAlex*) have extremely poor coverage for historical and non-English SSH publications, are expensive, and have restrictive licenses.</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a>Traditional machine learning tools like *ExCite* perform poorly on complex footnote structures, exhibiting low extraction and segmentation accuracy (e.g., *ExCite* accuracy around 0.22-0.26 for extraction). Footnotes in SSH are often complex, containing commentary, abbreviations, and multiple references ("footnotes from hell"). Creating training data for traditional methods is laborious using annotation tools.</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a>*Large Language Models* (*LLMs*) and *Vision Language Models* (*VLMs*) show promise for handling messy textual data and PDFs, but the primary challenge is trusting the results due to potential hallucinations (e.g., inventing non-existent citations, as seen in legal cases).</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>A robust testing and evaluation solution is required, necessitating a high-quality gold standard dataset, a flexible evaluation framework adaptable to fast-moving technology, and solid testing algorithms for comparable metrics.</span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>The project develops a specialized gold standard dataset encoded in *TEI XML*, chosen for its well-established standard, detailed specification covering phenomena beyond mere reference management (including context for citation intention), and compatibility with existing digital humanities corpora and tools like *Grobid*. The dataset creation involves stages: screenshot of the PDF, segmentation of reference strings from non-reference text within footnotes, and parsed structured data.</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a>The dataset currently includes over 1,500 references from open access journals to enable full publication from PDF to parsed data structures.</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>A *Python* package named *Llamore* (*Large Language Models for Reference Extraction*) is developed. *Llamore* is lightweight, acting as an interface to various *LLMs*/*VLMs* (compatible with *OpenAI API*, covering *Ollama*, *VLLM*, etc.). It takes text or PDFs as input and outputs references in *TEI XML* format.</span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a>It also provides an evaluation function using the *F1 score* metric to compare extracted references against gold standard references. The *F1 score* calculation involves counting exact matches of bibliographic elements (analytic title, monographic title, surname, publication date, etc.) and dividing by the number of predicted and gold elements.</span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>The problem of aligning extracted references to gold references is solved using an unbalanced assignment problem solver (from *SciPy*), maximizing the total *F1 score* with unique assignments and penalizing missing or hallucinated references with an *F1 score* of zero.</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a>Evaluation results show that *Llamore* performs comparably to *Grobid* on biomedical datasets (*PLOS 1000*) but significantly outperforms *Grobid* on the specialized humanities dataset, where *Grobid* struggles due to being out of distribution. While *LLMs* require significantly more compute than *Grobid*, their performance on complex SSH footnotes is superior.</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>Error analysis suggests issues include difficulty distinguishing volume/page numbers, misclassifying names in titles as authors, and misinterpreting terminology like "idem" as authors. The required *F1 score* for "good enough" data depends on the analysis goal (tendencies vs. accurate data). The current focus is on achieving reliable results before scaling up to avoid interpreting hallucinated data.</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>Future work could involve *retrieval augmented generation* (*RAG*) using disciplinary databases for validation and potentially human-in-the-loop workflows for error correction and iterative model improvement. The project aims for both specific research results (e.g., citation trends in a specific journal) and generic applicability to other projects.</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="fu">## Project Scope and Problem</span></span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a><span class="al">![Slide 01](images/ai-nepi_010_slide_01.jpg)</span></span>
<span id="cb1-43"><a href="#cb1-43"></a></span>
<span id="cb1-44"><a href="#cb1-44"></a>The project focuses on extracting citation data specifically from Law and Humanities scholarship, a domain characterized by extensive and complex footnotes. The primary challenge involves parsing these footnotes using *Large Language Models* (*LLMs*) or other algorithmic approaches.</span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a>The extracted data is intended for generating citation graphs, which are valuable tools in intellectual history and the history of science. Citation graphs enable the discovery of patterns and relationships within knowledge production, facilitate the reconstruction of intellectual influences, and allow for the measurement of the reception of specific ideas. An example application involves tracking the change in most-cited authors over time, such as an analysis conducted for the *Journal of Law and Society* between 1994 and 2003.</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="fu">## Problem: Bibliometric Database Coverage</span></span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a><span class="al">![Slide 01](images/ai-nepi_010_slide_01.jpg)</span></span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>A significant problem is the extremely poor coverage of historical *Social Sciences and Humanities* (*SSH*) scholarship in existing bibliometric databases. The primary databases in this space include *Web of Science*, *Scopus*, and *OpenAlex*. *Web of Science* and *Scopus* are characterized by high costs and very restrictive licenses, making dependence on them undesirable.</span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a>While *OpenAlex* is preferable due to its open access nature, it also lacks sufficient coverage for the specific content required for this research.</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>Common coverage gaps across these databases for *SSH* literature include a lack of inclusion for journals not classified as "A-journals," insufficient data for publications from the pre-digital age, and a general lack of coverage for non-English language content.</span>
<span id="cb1-57"><a href="#cb1-57"></a></span>
<span id="cb1-58"><a href="#cb1-58"></a>An illustrative example is the *Zeitschrift für Rechtssoziologie*, a German journal established in 1980. Analysis of available citation data by decade shows very low coverage across *Dimensions*, *OpenAlex*, and *Web of Science* for the decades before the 2000s. Although coverage improves somewhat after 2000, it remains incomplete, particularly for *Web of Science*.</span>
<span id="cb1-59"><a href="#cb1-59"></a></span>
<span id="cb1-60"><a href="#cb1-60"></a>Several factors contribute to this poor coverage in *SSH*. Firstly, there is a perceived lack of financial incentive compared to *STEM*, medicine, and economics, which are typically well-represented.</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a>Secondly, these databases often focus on metrics like the "impact factor," which aligns with science evaluation goals but not necessarily with the objectives of intellectual history research. Finally, the literature itself presents a technical challenge due to the extensive use of complex footnotes, which are difficult for automated systems to process accurately.</span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a><span class="fu">## Complex Footnotes and Training Data</span></span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a><span class="al">![Slide 04](images/ai-nepi_010_slide_04.jpg)</span></span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>Problem 2 concerns the nature of the footnotes themselves, often referred to as "footnotes from hell." These are typical of humanities scholarship and contain complex structures, including commentary, messy formatting, abbreviations, parenthetical information, and multiple distinct references embedded within surrounding non-citation text. An example image displays a scanned page with a footnote exhibiting these characteristics, featuring content in both German and English, various abbreviations, and multiple citations within a single numbered entry.</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a>Problem 3 highlights the difficulty in creating training data for citation extraction. The traditional approach involves a laborious manual annotation process. A web-based annotation tool is utilized for this purpose, allowing users to highlight segments of text within footnotes and assign specific labels corresponding to bibliographic elements such as:</span>
<span id="cb1-71"><a href="#cb1-71"></a></span>
<span id="cb1-72"><a href="#cb1-72"></a><span class="ss">- </span>Author</span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="ss">- </span>Title</span>
<span id="cb1-75"><a href="#cb1-75"></a></span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="ss">- </span>Date</span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="ss">- </span>Journal</span>
<span id="cb1-79"><a href="#cb1-79"></a></span>
<span id="cb1-80"><a href="#cb1-80"></a><span class="ss">- </span>Volume</span>
<span id="cb1-81"><a href="#cb1-81"></a></span>
<span id="cb1-82"><a href="#cb1-82"></a><span class="ss">- </span>Pages</span>
<span id="cb1-83"><a href="#cb1-83"></a></span>
<span id="cb1-84"><a href="#cb1-84"></a><span class="ss">- </span>DOI</span>
<span id="cb1-85"><a href="#cb1-85"></a></span>
<span id="cb1-86"><a href="#cb1-86"></a><span class="ss">- </span>ISBN</span>
<span id="cb1-87"><a href="#cb1-87"></a></span>
<span id="cb1-88"><a href="#cb1-88"></a><span class="ss">- </span>URL</span>
<span id="cb1-89"><a href="#cb1-89"></a></span>
<span id="cb1-90"><a href="#cb1-90"></a><span class="ss">- </span>BackRef</span>
<span id="cb1-91"><a href="#cb1-91"></a></span>
<span id="cb1-92"><a href="#cb1-92"></a><span class="ss">- </span>Signal</span>
<span id="cb1-93"><a href="#cb1-93"></a></span>
<span id="cb1-94"><a href="#cb1-94"></a><span class="ss">- </span>Ignore</span>
<span id="cb1-95"><a href="#cb1-95"></a></span>
<span id="cb1-96"><a href="#cb1-96"></a>and indicators for whether a segment is a reference or not. This manual annotation requires a significant investment of time and effort.</span>
<span id="cb1-97"><a href="#cb1-97"></a></span>
<span id="cb1-98"><a href="#cb1-98"></a><span class="fu">## Limitations of Existing Tools</span></span>
<span id="cb1-99"><a href="#cb1-99"></a></span>
<span id="cb1-100"><a href="#cb1-100"></a><span class="al">![Slide 05](images/ai-nepi_010_slide_05.jpg)</span></span>
<span id="cb1-101"><a href="#cb1-101"></a></span>
<span id="cb1-102"><a href="#cb1-102"></a>Problem 4 identifies a critical limitation: existing tools designed for citation extraction are unable to handle the complexity of humanities footnotes effectively. These traditional tools typically rely on machine learning methods such as *Conditional Random Forests*. However, their performance on the specific type of complex footnote data encountered in this domain is poor.</span>
<span id="cb1-103"><a href="#cb1-103"></a></span>
<span id="cb1-104"><a href="#cb1-104"></a>An example illustrating this limitation is the performance of the tool *ExCite*. Its performance is evaluated using metrics including Extraction Accuracy and Segmentation Accuracy across different training data configurations: Default, Footnoted, and Combined. The results show consistently low accuracy scores.</span>
<span id="cb1-105"><a href="#cb1-105"></a></span>
<span id="cb1-106"><a href="#cb1-106"></a>With Default training data, Extraction Accuracy is 0.24 and Segmentation Accuracy is 0.37. Using Footnoted training data yields Extraction Accuracy of 0.26 and Segmentation Accuracy of 0.37. The Combined training data results in Extraction Accuracy of 0.22 and Segmentation Accuracy of 0.47. These figures, sourced from Boulanger/Iurshina (2022), Table 1, demonstrate that *ExCite*, and by extension other similar traditional tools, struggle significantly with this task, regardless of the training data used.</span>
<span id="cb1-107"><a href="#cb1-107"></a></span>
<span id="cb1-108"><a href="#cb1-108"></a><span class="fu">## LLMs as Solution: The Trust Problem</span></span>
<span id="cb1-109"><a href="#cb1-109"></a></span>
<span id="cb1-110"><a href="#cb1-110"></a><span class="al">![Slide 05](images/ai-nepi_010_slide_05.jpg)</span></span>
<span id="cb1-111"><a href="#cb1-111"></a></span>
<span id="cb1-112"><a href="#cb1-112"></a>The question arises whether *Large Language Models* (*LLMs*) can offer a solution to the challenges of extracting citation data from complex footnotes. Early experiments conducted in 2022 using models such as *text-davinci-003* demonstrated the potential power of *LLMs* to extract references even from messy textual data. Newer models promise even better performance.</span>
<span id="cb1-113"><a href="#cb1-113"></a></span>
<span id="cb1-114"><a href="#cb1-114"></a>Furthermore, *Vision Language Models* (*VLMs*) possess the capability to process PDFs directly, which is advantageous given that source materials are frequently available in this format.</span>
<span id="cb1-115"><a href="#cb1-115"></a></span>
<span id="cb1-116"><a href="#cb1-116"></a>Potential methods for leveraging *LLMs* include *prompt engineering*, *Retrieval Augmented Generation* (*RAG*), and *finetuning*. However, a primary concern is the trustworthiness of the results produced by these models. A significant problem is the potential for hallucination, where *LLMs* invent information that does not exist, including fabricating citations.</span>
<span id="cb1-117"><a href="#cb1-117"></a></span>
<span id="cb1-118"><a href="#cb1-118"></a>A notable example illustrating this issue involved a lawyer who used *ChatGPT* for a federal court filing and cited non-existent cases invented by the model, resulting in severe consequences. This highlights the critical principle that analysis should not be undertaken unless the results are known to be correct and sufficient validation data is available to verify accuracy.</span>
<span id="cb1-119"><a href="#cb1-119"></a></span>
<span id="cb1-120"><a href="#cb1-120"></a><span class="fu">## Solution Requirements</span></span>
<span id="cb1-121"><a href="#cb1-121"></a></span>
<span id="cb1-122"><a href="#cb1-122"></a><span class="al">![Slide 06](images/ai-nepi_010_slide_06.jpg)</span></span>
<span id="cb1-123"><a href="#cb1-123"></a></span>
<span id="cb1-124"><a href="#cb1-124"></a>To address the trust issue and enable reliable citation extraction, a robust testing and evaluation solution is required. This solution must meet several key requirements.</span>
<span id="cb1-125"><a href="#cb1-125"></a></span>
<span id="cb1-126"><a href="#cb1-126"></a><span class="ss">- </span>Firstly, it necessitates a high-quality Gold Standard dataset against which extracted results can be compared.</span>
<span id="cb1-127"><a href="#cb1-127"></a></span>
<span id="cb1-128"><a href="#cb1-128"></a><span class="ss">- </span>Secondly, it requires a flexible framework capable of adapting easily to the rapidly evolving technology landscape of *LLMs* and *VLMs*.</span>
<span id="cb1-129"><a href="#cb1-129"></a></span>
<span id="cb1-130"><a href="#cb1-130"></a><span class="ss">- </span>Finally, the solution must incorporate solid testing and evaluation algorithms designed to produce comparable metrics, allowing for objective assessment of different approaches and models.</span>
<span id="cb1-131"><a href="#cb1-131"></a></span>
<span id="cb1-132"><a href="#cb1-132"></a><span class="fu">## Gold Standard Dataset</span></span>
<span id="cb1-133"><a href="#cb1-133"></a></span>
<span id="cb1-134"><a href="#cb1-134"></a><span class="al">![Slide 07](images/ai-nepi_010_slide_07.jpg)</span></span>
<span id="cb1-135"><a href="#cb1-135"></a></span>
<span id="cb1-136"><a href="#cb1-136"></a>The project involves compiling a high-quality gold standard dataset intended for both training and evaluation purposes. The chosen encoding standard for this dataset is *TEI XML*. This standard was selected because it is well-established and precisely specified within the humanities and digital editorics fields.</span>
<span id="cb1-137"><a href="#cb1-137"></a></span>
<span id="cb1-138"><a href="#cb1-138"></a>*TEI XML* is more comprehensive than simpler bibliographical standards like *CSL* or *BibTeX*, covering a wider range of phenomena and extending beyond basic reference management. It allows for encoding contextual information, which can be valuable for tasks such as classifying citation intention.</span>
<span id="cb1-139"><a href="#cb1-139"></a></span>
<span id="cb1-140"><a href="#cb1-140"></a>Furthermore, adopting *TEI XML* enables the project to potentially utilize existing text collections and corpora from other digital editorics projects that publish their source data, including detailed reference encodings, in this format. Such existing corpora can also serve as resources for testing the generalization and robustness of the developed mechanisms.</span>
<span id="cb1-141"><a href="#cb1-141"></a></span>
<span id="cb1-142"><a href="#cb1-142"></a>The dataset establishment process is currently underway. The encoding involves several stages, illustrated by an example of footnote number four. These stages include capturing a screenshot of the PDF source, segmenting the text to distinguish the actual reference string from surrounding non-reference text within the footnote (such as introductory phrases), and finally creating a parsed structured data representation of the reference.</span>
<span id="cb1-143"><a href="#cb1-143"></a></span>
<span id="cb1-144"><a href="#cb1-144"></a>Midway through the project, the strategy for building the dataset was adjusted. Initially, the focus was on data directly relevant to the primary research question. More recently, the decision was made to include the source PDFs and structure the dataset to enable the use of *Vision Language Models* (*VLMs*). The goal is to be able to publish the entire dataset pipeline, from the source PDFs through to the parsed data structures.</span>
<span id="cb1-145"><a href="#cb1-145"></a></span>
<span id="cb1-146"><a href="#cb1-146"></a>To facilitate this, the selection of source journals was changed to focus on open access publications. The dataset currently includes the encoding of over 1,500 references. It is noted that this count refers to occurrences, meaning the same work referenced multiple times is encoded separately to capture the specific context of each mention.</span>
<span id="cb1-147"><a href="#cb1-147"></a></span>
<span id="cb1-148"><a href="#cb1-148"></a>A significant benefit of using the interoperable *TEI XML* standard is the availability of numerous tools. A particularly relevant tool for this project is *Grobid*, a popular system for reference and information extraction. *Grobid* utilizes *TEI XML* for its own training and evaluation processes. By using the same data format, the project can directly compare its performance against *Grobid*, potentially use *Grobid*'s existing training data, and contribute the newly created dataset to the *Grobid* development team.</span>
<span id="cb1-149"><a href="#cb1-149"></a></span>
<span id="cb1-150"><a href="#cb1-150"></a><span class="fu">## Llamore: Extraction and Evaluation Tool</span></span>
<span id="cb1-151"><a href="#cb1-151"></a></span>
<span id="cb1-152"><a href="#cb1-152"></a><span class="al">![Slide 14](images/ai-nepi_010_slide_14.jpg)</span></span>
<span id="cb1-153"><a href="#cb1-153"></a></span>
<span id="cb1-154"><a href="#cb1-154"></a>The project developed a tool named *Llamore*, which stands for *Large Language Models for Reference Extraction*. *Llamore* is implemented as a small *Python* package. Its core capabilities include taking text or PDF documents as input, extracting references from them, and exporting these extracted references as *TEI* formatted *XML* files.</span>
<span id="cb1-155"><a href="#cb1-155"></a></span>
<span id="cb1-156"><a href="#cb1-156"></a>Additionally, if gold standard references are provided, *Llamore* can evaluate the performance of the extraction process.</span>
<span id="cb1-157"><a href="#cb1-157"></a></span>
<span id="cb1-158"><a href="#cb1-158"></a>The design of *Llamore* prioritized two main objectives: being lightweight and ensuring broad compatibility. It is lightweight because it does not contain any language models internally; instead, it functions as an interface to a model selected by the user. This design also ensures compatibility with a wide range of both open and closed *LLMs* and *VLMs*.</span>
<span id="cb1-159"><a href="#cb1-159"></a></span>
<span id="cb1-160"><a href="#cb1-160"></a>Implementation details include its availability on *PyPI*, allowing simple installation via the `pip` package manager. The extraction workflow involves defining an extractor object specific to the chosen model, such as using the *OpenAIExtractor*. This particular extractor provides compatibility with many open models (like those served by *Ollama* or *VLLM*) by interacting with their *API* endpoints that are designed to be compatible with the *OpenAI API* specification.</span>
<span id="cb1-161"><a href="#cb1-161"></a></span>
<span id="cb1-162"><a href="#cb1-162"></a>The user provides a PDF or text input to the extractor, receives the extracted references, and can then export these references to an *XML* file. For evaluation, the user imports the *F1* class from the package and provides both the gold standard references and the extracted references to this class to compute performance metrics.</span>
<span id="cb1-163"><a href="#cb1-163"></a></span>
<span id="cb1-164"><a href="#cb1-164"></a><span class="fu">## Evaluation Methodology</span></span>
<span id="cb1-165"><a href="#cb1-165"></a></span>
<span id="cb1-166"><a href="#cb1-166"></a><span class="al">![Slide 16](images/ai-nepi_010_slide_16.jpg)</span></span>
<span id="cb1-167"><a href="#cb1-167"></a></span>
<span id="cb1-168"><a href="#cb1-168"></a>The evaluation methodology employed utilizes the *F1 score*, a well-established metric for comparing structured data. The *F1 score* is calculated based on Precision and Recall, which in turn depend on the number of matches between elements in an extracted reference and a corresponding gold standard reference. A match is defined based on the exact correspondence of specific bibliographic elements, such as analytic title, monographic title, surname, and publication date.</span>
<span id="cb1-169"><a href="#cb1-169"></a></span>
<span id="cb1-170"><a href="#cb1-170"></a>Partial matches, like a forename with an extra dot, might not count as an exact match depending on configuration. Precision is calculated as the number of matches divided by the total number of elements predicted in the extracted reference, while Recall is the number of matches divided by the total number of elements in the gold reference. The *F1 score* is the harmonic mean of Precision and Recall. An *F1 score* of 1 indicates perfect extraction, where the reference is perfectly captured, while an *F1 score* of 0 signifies no matches were found.</span>
<span id="cb1-171"><a href="#cb1-171"></a></span>
<span id="cb1-172"><a href="#cb1-172"></a>A key challenge in evaluating performance across a set of references is aligning the extracted references with their corresponding gold standard references. This is tackled by formulating the problem as an unbalanced assignment problem. *Llamore* uses a solver from the *SciPy* library internally to address this. The process involves computing the *F1 score* for every possible pairing between each extracted reference and each gold reference, constructing a matrix of scores.</span>
<span id="cb1-173"><a href="#cb1-173"></a></span>
<span id="cb1-174"><a href="#cb1-174"></a>The solver then finds the assignment of extracted references to gold references that maximizes the total sum of *F1 scores*, ensuring that each reference is assigned uniquely. The final overall score is the macro average of the *F1 scores* for the assigned pairs. The method penalizes missing gold references (those not matched by an extracted reference) and hallucinated extracted references (those not matched to a gold reference) by assigning them an *F1 score* of zero in the averaging process. This approach is noted as being similar to a method recently published by *Packet et al.*</span>
<span id="cb1-175"><a href="#cb1-175"></a></span>
<span id="cb1-176"><a href="#cb1-176"></a><span class="fu">## Evaluation Results</span></span>
<span id="cb1-177"><a href="#cb1-177"></a></span>
<span id="cb1-178"><a href="#cb1-178"></a><span class="al">![Slide 20](images/ai-nepi_010_slide_20.jpg)</span></span>
<span id="cb1-179"><a href="#cb1-179"></a></span>
<span id="cb1-180"><a href="#cb1-180"></a>Evaluation was conducted to assess the effectiveness of the developed approach. Using the *PLOS 1000* dataset, which comprises 1,000 PDFs from the biomedical field, *Llamore*'s performance was found to be comparable to that of *Grobid*. It is noted that *Grobid* was specifically trained on data from similar journal articles.</span>
<span id="cb1-181"><a href="#cb1-181"></a></span>
<span id="cb1-182"><a href="#cb1-182"></a>However, in terms of efficiency and compute resources, *Grobid* is significantly superior, requiring orders of magnitude less computational power than *LLMs* like *Gemini*.</span>
<span id="cb1-183"><a href="#cb1-183"></a></span>
<span id="cb1-184"><a href="#cb1-184"></a>When evaluated on the specialized humanities dataset created by the project, *Grobid* struggled significantly to extract references, indicating that this data is out of its training distribution. In contrast, the *LLM*-based approach implemented in *Llamore* performed significantly better on this complex dataset.</span>
<span id="cb1-185"><a href="#cb1-185"></a></span>
<span id="cb1-186"><a href="#cb1-186"></a>The conclusion drawn is that while less computationally efficient on standard datasets, the *LLM*-based approach implemented in *Llamore* is more effective for extracting references from the challenging, out-of-distribution data characteristic of humanities footnotes compared to *Grobid*.</span>
<span id="cb1-187"><a href="#cb1-187"></a></span>
<span id="cb1-188"><a href="#cb1-188"></a>It is acknowledged that the *F1 scores* achieved are not considered high overall, suggesting substantial room for improvement. The current implementation utilizes a very basic approach, essentially sending the raw PDF text to a pre-trained model. Future work could explore potential improvements through methods such as *fine-tuning* models or incorporating more contextual information to enhance performance.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>