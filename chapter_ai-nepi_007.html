<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.14">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oliver Eberle">
<meta name="dcterms.date" content="2025-06-21">

<title>7&nbsp; Explainable AI and AI-based Scientific Insights in the Humanities – AI-NEPI Conference Proceedings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter_ai-nepi_008.html" rel="next">
<link href="./chapter_ai-nepi_006.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-8f1af79587c2686b78fe4e1fbadd71ab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d85b766abd26745604bb74d2576c60a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter_ai-nepi_007.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI-NEPI Conference Proceedings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">AI-NEPI Conference Proceedings - Enhanced Edition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_001.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models for the History, Philosophy, and Sociology of Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_003.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Large Language Models in History, Philosophy, and Sociology of Science: Architectures, Adaptation, and Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_004.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing OpenAlex Mapper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_005.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Genre Classification for Historical Medical Periodicals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_006.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_007.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_008.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">LLM: Evolution of competence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_009.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Representation of SDG-Related Research in Bibliometric Databases: A Conceptual Inquiry via LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_010.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Extracting Citation Data from Law and Humanities Scholarship Using LLMs and a Specialized Gold Standard Dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_011.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Science dynamics and AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_012.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RAG systems solve central problems of LLMs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_015.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantum gravity and plural pursuit in science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_016.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Titles, Abstracts, or Full-Texts? A Comparative Study of LDA and BERTopic Performance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_017.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Time-aware large language models towards a novel architecture for historical analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_018.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Leveraging Large Language Models for Metadata Enrichment and Diachronic Analysis of Chemical Knowledge in Historical Scientific Texts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_019.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Modelling Context and its Interplay in Language Variation and Change: A Pilot Study on the Chemical Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_020.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Our current understanding of funders of science is a limited view.</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter_ai-nepi_021.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">From Source to Structure: Extracting Knowledge Graphs with LLMs</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction-to-explainable-ai-and-humanities-applications" id="toc-introduction-to-explainable-ai-and-humanities-applications" class="nav-link" data-scroll-target="#introduction-to-explainable-ai-and-humanities-applications"><span class="header-section-number">7.1</span> Introduction to Explainable AI and Humanities Applications</a></li>
  <li><a href="#explainable-ai-1.0-feature-attributions" id="toc-explainable-ai-1.0-feature-attributions" class="nav-link" data-scroll-target="#explainable-ai-1.0-feature-attributions"><span class="header-section-number">7.2</span> Explainable AI 1.0: Feature Attributions</a></li>
  <li><a href="#understanding-black-box-ai-systems-and-the-need-for-explainability" id="toc-understanding-black-box-ai-systems-and-the-need-for-explainability" class="nav-link" data-scroll-target="#understanding-black-box-ai-systems-and-the-need-for-explainability"><span class="header-section-number">7.3</span> Understanding Black Box AI Systems and the Need for Explainability</a></li>
  <li><a href="#transition-to-generative-ai-and-multi-task-foundation-models" id="toc-transition-to-generative-ai-and-multi-task-foundation-models" class="nav-link" data-scroll-target="#transition-to-generative-ai-and-multi-task-foundation-models"><span class="header-section-number">7.4</span> Transition to Generative AI and Multi-Task Foundation Models</a></li>
  <li><a href="#common-mistakes-in-ai-models-object-detection-and-multi-step-planning" id="toc-common-mistakes-in-ai-models-object-detection-and-multi-step-planning" class="nav-link" data-scroll-target="#common-mistakes-in-ai-models-object-detection-and-multi-step-planning"><span class="header-section-number">7.5</span> Common Mistakes in AI Models: Object Detection and Multi-Step Planning</a></li>
  <li><a href="#xai-2.0-structured-interpretability" id="toc-xai-2.0-structured-interpretability" class="nav-link" data-scroll-target="#xai-2.0-structured-interpretability"><span class="header-section-number">7.6</span> XAI 2.0: Structured Interpretability</a></li>
  <li><a href="#first-order-explanations-for-classifier-predictions" id="toc-first-order-explanations-for-classifier-predictions" class="nav-link" data-scroll-target="#first-order-explanations-for-classifier-predictions"><span class="header-section-number">7.7</span> First-Order Explanations for Classifier Predictions</a></li>
  <li><a href="#second-order-explanations-pairwise-relationships-and-similarity" id="toc-second-order-explanations-pairwise-relationships-and-similarity" class="nav-link" data-scroll-target="#second-order-explanations-pairwise-relationships-and-similarity"><span class="header-section-number">7.8</span> Second-Order Explanations: Pairwise Relationships and Similarity</a></li>
  <li><a href="#higher-order-explanations-graph-structures-and-mechanistic-understanding" id="toc-higher-order-explanations-graph-structures-and-mechanistic-understanding" class="nav-link" data-scroll-target="#higher-order-explanations-graph-structures-and-mechanistic-understanding"><span class="header-section-number">7.9</span> Higher-Order Explanations: Graph Structures and Mechanistic Understanding</a></li>
  <li><a href="#first-order-attributions-in-llms-biased-sentiment-predictions" id="toc-first-order-attributions-in-llms-biased-sentiment-predictions" class="nav-link" data-scroll-target="#first-order-attributions-in-llms-biased-sentiment-predictions"><span class="header-section-number">7.10</span> First-Order Attributions in LLMs: Biased Sentiment Predictions</a></li>
  <li><a href="#long-range-dependencies-in-llms-context-prioritisation" id="toc-long-range-dependencies-in-llms-context-prioritisation" class="nav-link" data-scroll-target="#long-range-dependencies-in-llms-context-prioritisation"><span class="header-section-number">7.11</span> Long-Range Dependencies in LLMs: Context Prioritisation</a></li>
  <li><a href="#second-higher-order-interactions-in-text-explaining-sentence-similarities" id="toc-second-higher-order-interactions-in-text-explaining-sentence-similarities" class="nav-link" data-scroll-target="#second-higher-order-interactions-in-text-explaining-sentence-similarities"><span class="header-section-number">7.12</span> Second &amp; Higher-Order Interactions in Text: Explaining Sentence Similarities</a></li>
  <li><a href="#graph-neural-networks-for-structured-predictions-and-llm-interpretability" id="toc-graph-neural-networks-for-structured-predictions-and-llm-interpretability" class="nav-link" data-scroll-target="#graph-neural-networks-for-structured-predictions-and-llm-interpretability"><span class="header-section-number">7.13</span> Graph Neural Networks for Structured Predictions and LLM Interpretability</a></li>
  <li><a href="#higher-order-interactions-for-complex-language-structure" id="toc-higher-order-interactions-for-complex-language-structure" class="nav-link" data-scroll-target="#higher-order-interactions-for-complex-language-structure"><span class="header-section-number">7.14</span> Higher-Order Interactions for Complex Language Structure</a></li>
  <li><a href="#ai-based-scientific-insights-in-the-humanities-visual-definitions" id="toc-ai-based-scientific-insights-in-the-humanities-visual-definitions" class="nav-link" data-scroll-target="#ai-based-scientific-insights-in-the-humanities-visual-definitions"><span class="header-section-number">7.15</span> AI-based Scientific Insights in the Humanities: Visual Definitions</a></li>
  <li><a href="#corpus-level-analysis-of-early-modern-astronomical-tables" id="toc-corpus-level-analysis-of-early-modern-astronomical-tables" class="nav-link" data-scroll-target="#corpus-level-analysis-of-early-modern-astronomical-tables"><span class="header-section-number">7.16</span> Corpus-Level Analysis of Early Modern Astronomical Tables</a></li>
  <li><a href="#historical-insights-at-scale-the-xai-historian-workflow" id="toc-historical-insights-at-scale-the-xai-historian-workflow" class="nav-link" data-scroll-target="#historical-insights-at-scale-the-xai-historian-workflow"><span class="header-section-number">7.17</span> Historical Insights at Scale: The XAI-Historian Workflow</a></li>
  <li><a href="#cluster-entropy-analysis-for-investigating-innovation-spread" id="toc-cluster-entropy-analysis-for-investigating-innovation-spread" class="nav-link" data-scroll-target="#cluster-entropy-analysis-for-investigating-innovation-spread"><span class="header-section-number">7.18</span> Cluster Entropy Analysis for Investigating Innovation Spread</a></li>
  <li><a href="#identifying-historical-anomalies-through-cluster-entropy" id="toc-identifying-historical-anomalies-through-cluster-entropy" class="nav-link" data-scroll-target="#identifying-historical-anomalies-through-cluster-entropy"><span class="header-section-number">7.19</span> Identifying Historical Anomalies Through Cluster Entropy</a></li>
  <li><a href="#conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities" id="toc-conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities" class="nav-link" data-scroll-target="#conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities"><span class="header-section-number">7.20</span> Conclusion: AI-based Methods for the Humanities – Challenges and Opportunities</a></li>
  <li><a href="#additional-visual-materials" id="toc-additional-visual-materials" class="nav-link" data-scroll-target="#additional-visual-materials"><span class="header-section-number">7.21</span> Additional Visual Materials</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Explainable AI and AI-based Scientific Insights in the Humanities</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Oliver Eberle <a href="mailto:oliver.eberle@tu-berlin.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            BIFOLD / TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Oliver Eberle, a postdoctoral senior researcher at the Berlin Institute for Learning and Data, presented a comprehensive exploration of Explainable AI (<em>XAI</em>) and its profound applications within the digital humanities. He structured his compelling presentation into two principal sections: firstly, a meticulous examination of <em>XAI</em> and the interpretability of <em>Large Language Models</em> (<em>LLMs</em>); secondly, a compelling demonstration of <em>AI</em>-based scientific insights within humanities research.</p>
<p>Eberle’s initial discourse focused on <em>XAI</em> 1.0, which primarily involved feature attributions for classification models. Researchers often visualised these attributions through heatmaps to identify pixel responsibility in image data. This approach aimed to verify predictions, identify model flaws and biases, facilitate learning about underlying problems, and ensure compliance with emerging legislation such as the European <em>AI</em> Act. Subsequently, Eberle shifted his focus to the complexities introduced by generative <em>AI</em> and multi-task foundation models, necessitating more advanced interpretability methods beyond simple heatmaps, such as feature interactions and mechanistic perspectives (<em>XAI</em> 2.0).</p>
<p>His empirical examples highlighted biases in <em>Transformer LLMs’</em> sentiment predictions and their tendency to prioritise later context in long-range dependencies. The discussion then progressed to second and higher-order interactions, explaining sentence similarities through dot products of embeddings and employing <em>Graph Neural Networks</em> (<em>GNNs</em>) for structured predictions. Eberle and his team utilised ‘walk-based’ explanations to uncover complex language structures.</p>
<p>The latter part of the presentation showcased practical applications in the humanities, including extracting visual definitions from corpora of mathematical instruments and conducting corpus-level analyses of early modern astronomical tables, specifically the Sacrobosco Corpus. A key development Eberle presented was the <em>XAI-Historian</em> workflow, which he designed to aid historians in generating data-driven hypotheses at scale. This involved developing statistical models for bigram representations and utilising cluster entropy analysis to investigate the spread of innovation across historical European print locations, revealing insights into publishing programmes and political controls. Eberle concluded his presentation by acknowledging the significant challenges in applying <em>AI</em> to humanities data, particularly concerning heterogeneity, limited annotations, and the limitations of current foundation models for complex research questions involving low-resource or out-of-domain historical data.</p>
</section>
<section id="introduction-to-explainable-ai-and-humanities-applications" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="introduction-to-explainable-ai-and-humanities-applications"><span class="header-section-number">7.1</span> Introduction to Explainable AI and Humanities Applications</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_02.png" class="img-fluid figure-img"></p>
<figcaption>Slide 02</figcaption>
</figure>
</div>
<p>Oliver Eberle, a postdoctoral senior researcher at the Berlin Institute for Learning and Data, commenced his presentation by highlighting his background in machine learning and his recent engagement with digital humanities through collaborations with historians. He outlined the talk’s dual focus: firstly, on <em>Explainable AI</em> (<em>XAI</em>) and the interpretability of <em>Large Language Models</em> (<em>LLMs</em>); and secondly, on the application of <em>AI</em> to generate scientific insights within the humanities.</p>
</section>
<section id="explainable-ai-1.0-feature-attributions" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="explainable-ai-1.0-feature-attributions"><span class="header-section-number">7.2</span> Explainable AI 1.0: Feature Attributions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_03.png" class="img-fluid figure-img"></p>
<figcaption>Slide 03</figcaption>
</figure>
</div>
<p>Eberle introduced <em>Explainable AI</em> (<em>XAI</em>) 1.0, focusing specifically on ‘Feature attributions’, a concept central to the machine learning community’s understanding of model explanations. Historically, this field predominantly applied to visual data, particularly images. Over the past decade, however, a significant shift has occurred, with a burgeoning interest in language-based applications.</p>
</section>
<section id="understanding-black-box-ai-systems-and-the-need-for-explainability" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="understanding-black-box-ai-systems-and-the-need-for-explainability"><span class="header-section-number">7.3</span> Understanding Black Box AI Systems and the Need for Explainability</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_04.png" class="img-fluid figure-img"></p>
<figcaption>Slide 04</figcaption>
</figure>
</div>
<p>Understanding the internal workings of ‘Black Box <em>AI</em> Systems’ presents a significant challenge, particularly in classification tasks. For instance, when an input image of a rooster yields a ‘Rooster’ prediction, the user typically lacks insight into the basis of this classification. Consequently, the field of <em>Explainable AI</em> (<em>XAI</em>) has dedicated substantial research to tracing the origins of these predictions. <em>XAI</em> 1.0 primarily employed post-hoc explainability methods, such as heatmaps, which visually highlight the specific pixels responsible for a model’s prediction, thereby clarifying, for example, why a rooster was recognised. Beyond this specific application, the broader imperative for explainability encompasses several critical objectives: verifying predictions to ensure the model’s logical operation; identifying flaws and biases to facilitate error correction; learning about the underlying problem space, as models can uncover unexpected solutions; and, increasingly, ensuring compliance with legislative frameworks, such as the European <em>AI</em> Act.</p>
</section>
<section id="transition-to-generative-ai-and-multi-task-foundation-models" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="transition-to-generative-ai-and-multi-task-foundation-models"><span class="header-section-number">7.4</span> Transition to Generative AI and Multi-Task Foundation Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_06.png" class="img-fluid figure-img"></p>
<figcaption>Slide 06</figcaption>
</figure>
</div>
<p>The landscape of <em>AI</em> has significantly evolved from traditional classification models to the era of Generative <em>AI</em> (<em>Gen AI</em>), where models exhibit a diverse array of capabilities. These advanced systems can now perform classification, retrieve similar images, generate novel images, and answer complex questions across various topics. This expansion of functionality, however, introduces a substantial challenge: grounding the predictions or answers from <em>Large Language Models</em> (<em>LLMs</em>) back to their specific inputs becomes considerably more difficult. Consequently, the future of <em>Explainable AI</em> (<em>XAI</em>) necessitates moving beyond simple heatmap representations towards understanding intricate ‘feature interactions’ and adopting more ‘mechanistic views’ of model behaviour. Crucially, contemporary foundation models operate as both ‘multi-task’ and ‘world models’, offering profound insights into societal dynamics, the evolution of textual data, and inherent textual features.</p>
</section>
<section id="common-mistakes-in-ai-models-object-detection-and-multi-step-planning" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="common-mistakes-in-ai-models-object-detection-and-multi-step-planning"><span class="header-section-number">7.5</span> Common Mistakes in AI Models: Object Detection and Multi-Step Planning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_07.png" class="img-fluid figure-img"></p>
<figcaption>Slide 07</figcaption>
</figure>
</div>
<p><em>AI</em> models, despite their sophistication, frequently exhibit surprising errors. Two prominent examples illustrate these limitations. Firstly, in object detection, a standard classifier might erroneously base its ‘boat’ prediction on the surrounding water, a correlated textural feature, rather than the boat itself, indicating a reliance on superficial cues. Secondly, in multi-step planning, <em>Large Language Models</em> (<em>LLMs</em>) demonstrate failures in complex tasks such as the Tower of Hanoi puzzle. Here, an <em>LLM</em> might attempt to move the largest, inaccessible disk directly to the final peg, thereby violating the inherent physical constraints of the problem. This highlights a fundamental lack of understanding regarding the physical rules governing such planning scenarios.</p>
</section>
<section id="xai-2.0-structured-interpretability" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="xai-2.0-structured-interpretability"><span class="header-section-number">7.6</span> XAI 2.0: Structured Interpretability</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_08.png" class="img-fluid figure-img"></p>
<figcaption>Slide 08</figcaption>
</figure>
</div>
<p>Eberle then shifted his focus to ‘Structured Interpretability’, representing <em>XAI</em> 2.0. This advanced approach aims to transcend the limitations of traditional heatmap representations, seeking deeper insights into model decisions. Whilst acknowledging the potential for improved reasoning in more recent models, such as <em>Llama 3.something</em>, the core objective remains to develop more sophisticated methods for understanding <em>AI</em> behaviour.</p>
</section>
<section id="first-order-explanations-for-classifier-predictions" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="first-order-explanations-for-classifier-predictions"><span class="header-section-number">7.7</span> First-Order Explanations for Classifier Predictions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_09.png" class="img-fluid figure-img"></p>
<figcaption>Slide 09</figcaption>
</figure>
</div>
<p>First-order explanations prove particularly useful for understanding classifier behaviour, typically by generating heatmaps over classifications. For instance, in an application involving historical data tables, Eberle and his team aimed to distinguish specific subgroups. By employing heatmaps, they verified that the classifier’s predictions were based on meaningful features. Crucially, the model correctly focused on the numerical content within the tables, which served as an effective proxy for identifying numerical tables, thereby validating its intended operation.</p>
</section>
<section id="second-order-explanations-pairwise-relationships-and-similarity" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="second-order-explanations-pairwise-relationships-and-similarity"><span class="header-section-number">7.8</span> Second-Order Explanations: Pairwise Relationships and Similarity</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>Beyond individual features, the authors extended their analysis to second-order explanations, concentrating on pairwise relationships. They investigated similarity by computing the dot product of embeddings derived from two inputs, such as images. This process yielded a similarity score, which required further explanation. Eberle and his team discovered that interaction scores provided an appropriate method for representing these similarity predictions. For example, in the context of historical tables, interactions between specific digits indicated that two tables were identical. This approach successfully verified that the model functioned as intended, accurately identifying meaningful similarities.</p>
</section>
<section id="higher-order-explanations-graph-structures-and-mechanistic-understanding" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="higher-order-explanations-graph-structures-and-mechanistic-understanding"><span class="header-section-number">7.9</span> Higher-Order Explanations: Graph Structures and Mechanistic Understanding</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>In more recent work, Eberle and his colleagues explored higher-order interactions within graph structures, finding that these provide more meaningful insights. This approach applies to various network types, such as citation networks or networks of books and entities, where models are trained on classification tasks. Their explanation method involves identifying ‘feature subgraphs’ or ‘feature walks’, which represent sets of features that become relevant collectively. This technique aims to yield more complex insights into model behaviour, ultimately progressing towards a ‘circuit level understanding’ of their internal mechanisms.</p>
</section>
<section id="first-order-attributions-in-llms-biased-sentiment-predictions" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="first-order-attributions-in-llms-biased-sentiment-predictions"><span class="header-section-number">7.10</span> First-Order Attributions in LLMs: Biased Sentiment Predictions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>Applying interpretability techniques to language and humanities contexts, Eberle and his team investigated first-order attributions for sentiment predictions within <em>Transformer Large Language Models</em> (<em>LLMs</em>). Utilising a common dataset of movie reviews, they ranked sentences and generated heatmaps with a newly proposed method tailored for <em>Transformers</em>. Their findings revealed significant biases in sentiment predictions: male Western names, such as Lee, Barry, Raphael, or the Cohen Brothers, correlated more strongly with positive sentiment, whilst foreign-sounding names like Saddam, Castro, or Chan were more likely to elicit negative scores. This research underscores the utility of <em>Explainable AI</em> in detecting such fine-grained biases within models.</p>
</section>
<section id="long-range-dependencies-in-llms-context-prioritisation" class="level2" data-number="7.11">
<h2 data-number="7.11" class="anchored" data-anchor-id="long-range-dependencies-in-llms-context-prioritisation"><span class="header-section-number">7.11</span> Long-Range Dependencies in LLMs: Context Prioritisation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>Eberle and his team investigated <em>Large Language Models</em> (<em>LLMs</em>) in a standard scenario involving long context windows, such as 8,000-token Wikipedia articles, where the task involved generating a summary from provided text. Their primary research question concerned the models’ capacity to utilise long-range information. By analysing the origin of information within the generated summaries, they discovered that <em>LLMs</em> predominantly focus on the later parts of the context, exhibiting a strong tendency to prioritise information presented closer to the prompt. Whilst models can retrieve information from the very beginning of the context, this occurs significantly less frequently, as indicated by a log scale of counts. Consequently, summaries produced by <em>LLMs</em> may not offer a balanced representation of the entire input text, instead favouring more recently presented information.</p>
</section>
<section id="second-higher-order-interactions-in-text-explaining-sentence-similarities" class="level2" data-number="7.12">
<h2 data-number="7.12" class="anchored" data-anchor-id="second-higher-order-interactions-in-text-explaining-sentence-similarities"><span class="header-section-number">7.12</span> Second &amp; Higher-Order Interactions in Text: Explaining Sentence Similarities</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>The investigation extended to second and higher-order interactions within textual data, specifically addressing the explanation of sentence similarities and embeddings. Eberle and his colleagues employed an unsupervised analysis of representations extracted from a pre-trained foundation model, such as a <em>Sentence-BERT</em> model. This involved computing similarity scores between sentence pairs using dot products of their embeddings, denoted as ⟨ϕ(x), ϕ(x’)⟩. The core challenge lay in understanding the underlying reasons for a given similarity score. Second-order explanations provided a solution by yielding interaction scores between individual tokens. For instance, the high similarity between ‘A cat I really like.’ and ‘It is a great cat!’ was attributed to specific token interactions. Findings often revealed reliance on straightforward noun matching strategies, including synonyms or identical noun tokens, alongside some noun-verb and separator token interactions. This suggested that models, whilst compressing vast amounts of information, frequently resort to surprisingly simplistic strategies. This observation, though not immediately intuitive, holds significant relevance for those embedding data and computing rankings, as the features contributing to high similarity scores can be remarkably simple.</p>
</section>
<section id="graph-neural-networks-for-structured-predictions-and-llm-interpretability" class="level2" data-number="7.13">
<h2 data-number="7.13" class="anchored" data-anchor-id="graph-neural-networks-for-structured-predictions-and-llm-interpretability"><span class="header-section-number">7.13</span> Graph Neural Networks for Structured Predictions and LLM Interpretability</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>The discussion then turned to <em>Graph Neural Networks</em> (<em>GNNs</em>) and their utility for structured predictions. <em>GNNs</em> offer attributions in terms of ‘walks’, which represent interactions between features within a graph. Crucially, the structural information encoded by <em>GNNs</em> can be conceptually framed as <em>Large Language Models</em> (<em>LLMs</em>), given that <em>LLM</em> attention networks effectively determine which tokens can ‘message pass’, a process analogous to <em>GNN</em> interactions. This conceptual alignment allows for the application of <em>GNN</em>-based interpretability methods to analyse complex language structures.</p>
</section>
<section id="higher-order-interactions-for-complex-language-structure" class="level2" data-number="7.14">
<h2 data-number="7.14" class="anchored" data-anchor-id="higher-order-interactions-for-complex-language-structure"><span class="header-section-number">7.14</span> Higher-Order Interactions for Complex Language Structure</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>Standard explanation methods, such as <em>Bag of Words</em> (<em>BoW</em>), often fail to capture the inherent complexity of natural language, particularly nuances like negation. For instance, a <em>BoW</em> model might incorrectly assign a positive score to the phrase ‘First I didn’t like the boring pictures’ due to the presence of ‘like’, overlooking the crucial negation. In contrast, higher-order explanation methods significantly improve this capability, accurately interpreting complex language structures. These methods can correctly assign a negative score to the entire negated clause and precisely interpret the hierarchical structure of subsequent positive clauses, such as ‘but it is certainly one of the best movies I have ever seen.’ This enhanced understanding stems from the inherent suitability of hierarchical natural language structures for graph representations. The methodology involves training a <em>Graph Neural Network</em> (<em>GNN</em>) or <em>Large Language Model</em> (<em>LLM</em>) on sentiment analysis tasks, such as movie reviews, and subsequently extracting ‘walks’ to explain their predictions.</p>
</section>
<section id="ai-based-scientific-insights-in-the-humanities-visual-definitions" class="level2" data-number="7.15">
<h2 data-number="7.15" class="anchored" data-anchor-id="ai-based-scientific-insights-in-the-humanities-visual-definitions"><span class="header-section-number">7.15</span> AI-based Scientific Insights in the Humanities: Visual Definitions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p>The presentation transitioned to <em>AI</em>-based scientific insights within the humanities, commencing with heatmap-based approaches. Eberle and his team utilised a corpus of mathematical instruments, specifically the <em>Sphaera Cor.</em> corpus, to classify images into categories such as ‘machine’ or ‘mathematical instrument’. In close collaboration with historians, including Matteo Valleriani and Jochen Büttner, they sought to establish more objective criteria for visual definitions. A key finding revealed that the fine-grained scales present on mathematical instruments were highly relevant features for the model’s classification decisions. This work underscored the critical necessity of continuous engagement with domain experts to ensure the meaningfulness and accuracy of these <em>AI</em>-derived definitions.</p>
</section>
<section id="corpus-level-analysis-of-early-modern-astronomical-tables" class="level2" data-number="7.16">
<h2 data-number="7.16" class="anchored" data-anchor-id="corpus-level-analysis-of-early-modern-astronomical-tables"><span class="header-section-number">7.16</span> Corpus-Level Analysis of Early Modern Astronomical Tables</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>In their most extensive digital humanities collaboration to date, Eberle and his colleagues undertook a corpus-level analysis of numerical tables from the <em>Sphaera Corpus</em>, an early modern text spanning from 1472 to 1650. Historians Matteo Valleriani and Jochen Büttner presented this challenging dataset, which initially appeared intractable for automated analysis. The primary objective was to develop an automated method for matching and identifying semantic similarities between these historical tables, a task previously unfeasible at scale. This initiative aimed to unlock new avenues for historical research by enabling comprehensive analysis of this rich, yet previously inaccessible, data.</p>
</section>
<section id="historical-insights-at-scale-the-xai-historian-workflow" class="level2" data-number="7.17">
<h2 data-number="7.17" class="anchored" data-anchor-id="historical-insights-at-scale-the-xai-historian-workflow"><span class="header-section-number">7.17</span> Historical Insights at Scale: The XAI-Historian Workflow</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>Eberle and his team developed the <em>XAI-Historian</em> workflow, a collaborative project designed to empower historians with scalable insights and facilitate data-driven hypothesis generation. This initiative centred on the Sacrobosco Corpus, an extensive collection of 76,000 pages of university textbooks from 1472 to 1650. The project faced significant machine learning challenges, including highly heterogeneous data, severely limited annotations, and the inadequacy of standard <em>OCR</em> and foundation models for this out-of-domain historical material. Consequently, the authors implemented a bespoke three-stage methodology. Firstly, data collection involved curating the Sacrobosco Corpus. Secondly, an atomisation-recomposition process transformed input tables into bigram maps, subsequently generating histograms (Φ(x)). Thirdly, corpus-level analysis involved embedding these historical tables into a spatial representation, where data similarity was computed as the inner product of their feature representations, y(x, x’) = ⟨Φ(x), Φ(x’)⟩. Crucially, the team developed a specific statistical model for bigram representations, as generic foundation models proved unsuitable. The model’s efficacy was verified by its consistent detection of identical bigrams across different inputs, such as ‘38’, thereby establishing its reliability for historical analysis.</p>
</section>
<section id="cluster-entropy-analysis-for-investigating-innovation-spread" class="level2" data-number="7.18">
<h2 data-number="7.18" class="anchored" data-anchor-id="cluster-entropy-analysis-for-investigating-innovation-spread"><span class="header-section-number">7.18</span> Cluster Entropy Analysis for Investigating Innovation Spread</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>Eberle and his team applied cluster entropy analysis in case studies to investigate the historical spread of innovation across Europe. This involved examining the publication output of specific historical print locations, or cities, each of which produced a distinct ‘programme’ of publication types. Previously, analysing the diversity or focus of these print programmes at scale proved impossible. To overcome this, the authors devised a novel clustering approach, leveraging representations derived from their model. Entropy (H(p)) served as the key metric to quantify diversity: a low entropy score indicated a city’s tendency to reproduce the same content, signifying a less diverse print programme, whilst a higher entropy score denoted a more varied and diverse output. The primary objective was to ascertain the number of distinct clusters a given city produced.</p>
</section>
<section id="identifying-historical-anomalies-through-cluster-entropy" class="level2" data-number="7.19">
<h2 data-number="7.19" class="anchored" data-anchor-id="identifying-historical-anomalies-through-cluster-entropy"><span class="header-section-number">7.19</span> Identifying Historical Anomalies Through Cluster Entropy</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>Employing a distance-based clustering method, Eberle and his team quantified the diversity of print programmes produced by various historical cities using entropy as a metric. A low entropy score indicated a city’s propensity to reproduce identical content, signifying a less diverse output, whereas a higher score denoted a more varied print programme. This analysis led to the identification of two particularly interesting cases exhibiting the lowest entropy scores. Frankfurt/Main emerged as a known historical centre for the repeated reprinting of editions. More notably, Wittenberg presented a compelling case where the political control exerted by Protestant reformers actively restricted the print programme, dictating the curriculum content. This finding represented a significant historical anomaly, which, crucially, aligned with and supported existing historical intuition.</p>
</section>
<section id="conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities" class="level2" data-number="7.20">
<h2 data-number="7.20" class="anchored" data-anchor-id="conclusion-ai-based-methods-for-the-humanities-challenges-and-opportunities"><span class="header-section-number">7.20</span> Conclusion: AI-based Methods for the Humanities – Challenges and Opportunities</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ai-nepi_007_slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>In conclusion, whilst humanities and Digital Humanities (<em>DH</em>) researchers have historically concentrated on digitising source material, the automated analysis of these corpora presents significant challenges due to their inherent heterogeneity and scarcity of labels. Nevertheless, the integration of <em>Machine Learning</em> (<em>ML</em>) with <em>Explainable AI</em> (<em>XAI</em>) offers a promising pathway to scale humanities research and cultivate novel research directions. Foundation Models and <em>Large Language Models</em> (<em>LLMs</em>) can automate various intermediate tasks, including labelling, data curation, and error correction; however, their utility remains limited for more complex research questions. A significant roadblock persists in the challenges posed by low-resource data for <em>ML</em>, particularly concerning scaling laws. Consequently, the out-of-domain transfer of models to historical and small-scale data necessitates rigorous evaluation. Crucially, current <em>LLMs</em> are primarily trained and aligned for natural language tasks and code generation, indicating a fundamental mismatch with the specific data requirements of humanities research.</p>
</section>
<section id="additional-visual-materials" class="level2" data-number="7.21">
<h2 data-number="7.21" class="anchored" data-anchor-id="additional-visual-materials"><span class="header-section-number">7.21</span> Additional Visual Materials</h2>
<p>The following slides provide supplementary visual information relevant to the presentation:</p>
<p><img src="images/ai-nepi_007_slide_05.png" class="img-fluid" alt="Slide 05"> This slide, titled ‘Post-Hoc Explainability’, presents a conceptual diagram illustrating how <em>AI</em> predictions can be interpreted. The top section depicts the standard classification process: an ‘input x’ image of a rooster is fed into a ‘Black Box <em>AI</em> System’ to ‘classify image’, which then outputs a ‘Rooster’ ‘prediction f(x)’. Below this, the diagram introduces the concept of explainability. An arrow labelled ‘Explanation methods’ extends from the ‘prediction f(x)’ box, leading to a stylised neural network diagram. This network diagram, labelled ‘explain prediction’, shows interconnected nodes with some paths highlighted in red, suggesting the internal workings or important pathways within the explanation process. An arrow from this network points to a ‘heatmap’, which is a blank square with a small, irregular red area circled on its top right. An annotation next to the heatmap clarifies that the ‘<em>AI</em> system’s decision is based on these pixels’, indicating that the heatmap visually highlights the most relevant parts of the input image for the <em>AI</em>’s prediction. A curved arrow from the heatmap explanation leads to a dashed-line box titled ‘Why explainability ?’. This box lists four key reasons: ‘Verify predictions’, ‘Identify flaws and biases’, ‘Learn about the problem’, and ‘Ensure compliance to legislation’. The slide number ‘5’ is in the bottom centre, and ‘Samek et al.&nbsp;(2017)’ is cited in the bottom right, indicating the source or a key reference for the presented concepts.</p>
<p><img src="images/ai-nepi_007_slide_13.png" class="img-fluid" alt="Slide 13"> This slide, titled ‘Ex 1a: Biased Sentiment Predictions in <em>Transformer LLMs</em>’, details an experiment setup and its results. The ‘Setup’ section outlines two objectives: first, ‘Understanding feature importance in <em>LLMs</em> using <em>Explainable AI</em>’, and second, to ‘Analyze what names are most / least relevant to change the prediction towards a positive / negative review’. A diagram in the top right corner visually represents feature importance, showing a dashed circle containing four smaller circles labelled x1, x2, x3, x4, with x1 highlighted in a light red colour, indicating it as the most important feature amongst others. The ‘Results’ section displays a vertical axis labelled ‘name relevance’, with a ‘+’ sign at the top indicating higher relevance and a ‘-’ sign at the bottom indicating lower relevance. Several example sentences, each enclosed by <code>[CLS]</code> and <code>[SEP]</code> tokens, are listed along this axis. Words within these sentences are highlighted in either red/pink or blue/purple to denote their relevance. Names highlighted in red/pink are positioned towards the top of the ‘name relevance’ axis, indicating high relevance. These include ‘malcolm d.&nbsp;lee’, ‘dave barry’, ‘raphael atmosphere’ (from ‘the sally jesse raphael atmosphere’), ‘coe n brothers’ (from ‘the coe n brothers and’), ‘jackie chan’ (from ‘a jackie chan movie’), and ‘cuban leader fide l castro’ (from ‘of cuban leader fide l castro’). Names highlighted in blue/purple are positioned towards the bottom, indicating lower relevance. These include ‘martha stewart’ and ‘program run am ok’ (from ‘some sort of martha stewart decor ating program run am ok’), and ‘comedy equivalent’ and ‘saddam hussein’ (from ‘the comedy equivalent of saddam hussein’). The ellipsis ‘…’ indicates more examples are omitted. This visualisation demonstrates how specific names disproportionately influence the sentiment predictions of <em>LLMs</em>, revealing potential biases. The slide is numbered ‘13’ and attributes the work to ‘Ali et al., <em>XAI for Transformers</em>. (ICML ‘22)’.</p>
<p><img src="images/ai-nepi_007_slide_15.png" class="img-fluid" alt="Slide 15"> This slide, titled ‘Ex 1b: Long-range dependencies in <em>LLMs</em>’, illustrates the experimental setup and findings regarding how <em>Large Language Models</em> (<em>LLMs</em>) process information across long contexts. The ‘Setup’ section describes the task: ‘Given a long context (e.g.&nbsp;8k tokens Wikipedia article), generate a summary from the provided text’. The core research question posed is: ‘Can <em>LLMs</em> utilise long-range information?’ The slide visually represents a long context window, showing a sequence of tokens from ‘Token 1’ to ‘Token 8000’. A red arrow labelled ‘Attention’ points from the later tokens (e.g., ‘Token 7999’, ‘Token 8000’) towards the ‘Summary’, indicating that the model’s attention is concentrated on the end of the input. The ‘Results’ section presents a bar chart with a logarithmic y-axis labelled ‘Counts’ (ranging from 10^0 to 10^5) and an x-axis labelled ‘Position in context’ (ranging from 0 to 8000). The bars show that information from the end of the context (e.g., positions 7000-8000) is retrieved significantly more often than information from the beginning (e.g., positions 0-1000). A red dashed line highlights the sharp decline in counts for earlier positions. This demonstrates that <em>LLMs</em> exhibit a strong ‘recency bias’, predominantly focusing on the later parts of the context when generating summaries. The slide number ‘15’ is in the bottom centre, and the work is attributed to ‘Eberle et al., <em>Explaining Long-Range Dependencies in Transformers</em>. (NeurIPS ’23)’.</p>
<p><img src="images/ai-nepi_007_slide_17.png" class="img-fluid" alt="Slide 17"> This slide, titled ‘Ex 2: Explaining sentence similarities and embeddings’, focuses on understanding how sentence embeddings capture semantic relationships. The ‘Setup’ section states the core methodology: ‘Unsupervised analysis of the representations extracted from a pretrained foundation model using dot-products ⟨ϕ(x), ϕ(x’)⟩‘. This implies that the similarity is computed as the dot product of the embeddings of two sentences, x and x’. Section ‘I. Forward prediction / encoder’ visually demonstrates this process. Two input sentences, ‘A cat I really like.’ and ‘It is a great cat!’, are shown entering separate encoder models, represented as trapezoidal shapes. Each encoder is depicted with internal layers labelled ϕ1, ϕ2, …, ϕL, signifying the transformation of the input sentence into a high-dimensional representation or embedding. The outputs of these encoders are then fed into a dot product operation, indicated by ‘⟨.,⟩’, which yields a ‘similarity score’. An example score of ‘0.92’ is given, followed by the question ‘but why?’, highlighting the need for interpretability. Section ‘II. Token interactions’ addresses this ‘why’ question by visualising the contributions of individual tokens to the overall similarity. This diagram shows the two sentences, ‘A cat I really like.’ and ‘It is a great cat!’, with lines connecting words between them. The thickness and colour of these lines indicate the strength of interaction or contribution. For instance, a thick red line connects ‘cat’ from the first sentence to ‘cat’ from the second, and another thick red line connects ‘like’ from the first sentence to ‘great’ from the second, suggesting these word pairs are highly influential in the high similarity score. Thinner, lighter lines connect other words, indicating weaker interactions. Two mathematical equations are presented. The first, <code>y(x, x') = ⟨ϕ(x), ϕ(x')⟩</code>, formally defines the similarity score <code>y</code> between sentences <code>x</code> and <code>x'</code> as the dot product of their embeddings <code>ϕ(x)</code> and <code>ϕ(x')</code>. The second, <code>BiLRP(y, x, x') = Σ_{m=1}^h LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x) ⊗ LRP([ϕ_L ◦ ... ◦ ϕ_1]_m, x')</code>, introduces ‘<em>BiLRP</em>’ (Bilinear Layer-wise Relevance Propagation), a method for explaining the similarity score by decomposing it into contributions from individual tokens. This equation involves a summation over <code>h</code> components and a tensor product (⊗) of <em>LRP</em> scores for each sentence’s tokens, indicating how relevance is propagated back through the model layers (ϕ1 to ϕL) to the input tokens. The method is attributed to ‘Eberle et al.&nbsp;(<em>TPAMI</em> ’22)’. In the top right corner, a small circular diagram shows four nodes (x1, x2, x3, x4) within a dashed circle. Nodes x1 and x4 are highlighted in red and connected by a thick red line, visually reinforcing the concept of identifying strong relationships or similarities between specific elements within a set. The slide number ‘17’ is visible at the bottom centre.</p>
<p><img src="images/ai-nepi_007_slide_18.png" class="img-fluid" alt="Slide 18"> This slide, titled ‘Ex 2: Explaining sentence similarities and embeddings’, continues the discussion on understanding how sentence embeddings capture semantic relationships. The ‘Results’ section presents key findings. The first point states: ‘Models compress vast amounts of information, but often resort to surprisingly simple strategies’. This suggests that despite their complexity, models may rely on straightforward mechanisms for certain tasks. The second point elaborates on these strategies: ‘Often simple noun matching strategies (synonyms, identical noun tokens)’. This indicates that direct word matches, or their synonyms, frequently drive similarity predictions. The third point adds: ‘Some noun-verb and separator token interactions’. This suggests that beyond simple noun matching, models also consider relationships between nouns and verbs, and the role of punctuation or other separator tokens. The slide includes two example sentences: ‘A cat I really like.’ and ‘It is a great cat!’. Below these, a diagram visually represents the ‘token interactions’ contributing to their similarity. Words are connected by lines, with thickness and colour indicating interaction strength. For instance, a thick red line connects ‘cat’ to ‘cat’, and ‘like’ to ‘great’, highlighting their strong contribution to similarity. Thinner lines connect other words, showing weaker interactions. The slide concludes with a crucial implication: ‘This is important for those embedding data and computing rankings, as the features contributing to high similarity scores can be remarkably simple’. This advises practitioners to be aware that seemingly complex model behaviours might stem from surprisingly basic underlying features. The slide number ‘18’ is in the bottom centre, and the work is attributed to ‘Eberle et al.&nbsp;(<em>TPAMI</em> ’22)’.</p>
<p><img src="images/ai-nepi_007_slide_20.png" class="img-fluid" alt="Slide 20"> This slide, titled “Using walks to explain <em>GNN</em> (and <em>LLM</em>) predictions,” presents a framework for model interpretability. It is divided into two main sections: “Model” at the top and “Walk-based relevance” at the bottom. The “Model” section begins with an “input graph,” depicted as a complex, multi-layered graph structure resembling two interconnected hexagonal prisms. This input graph feeds into a series of processing blocks labelled <code>H₀</code>, <code>H₁</code>, and <code>H_T</code>, which represent interaction layers within the model. These layers culminate in a “prediction” output, indicated by a triangular symbol. The entire sequence from input graph to prediction is labelled “interaction.” A red vertical line extends from the “prediction” output, turning left to connect to the “explaining the prediction” process. In the top right, a smaller illustrative graph shows four nodes (<code>x₁</code>, <code>x₂</code>, <code>x₃</code>, <code>x₄</code>) within a dashed circle. Nodes <code>x₁</code>, <code>x₂</code>, and <code>x₄</code> are highlighted in red, along with edges connecting <code>x₁</code> to <code>x₄</code> and <code>x₂</code> to <code>x₄</code>, whilst <code>x₃</code> remains unhighlighted, likely demonstrating a simple example of relevant nodes and edges. The “Walk-based relevance” section visually explains how the prediction is interpreted. It starts with a large, detailed version of the input graph, where a prominent, thick red path, explicitly labelled “walk,” traces a specific sequence of nodes and edges through the graph. Many other edges and nodes in this graph are also highlighted with varying shades of red and purple, indicating different degrees of relevance. An arrow points from this detailed graph to a simplified version, where only a subset of the graph’s structure and edges are highlighted in red, suggesting a relevant subgraph. Another arrow then points to a further simplified representation where only the nodes are shown, with relevant nodes encircled in red, indicating the most critical elements for the prediction. The red line from the “prediction” output connects to these visual explanations, signifying that these walk-based visualisations are the result of “explaining the prediction.” The bottom right of the slide includes a citation: “Schnake et al., <em>Higher-Order Explanations of Graph Neural Networks via Relevant Walks</em>. (<em>TPAMI</em> ’22)”, and the page number “20” is in the bottom left.</p>
<p><img src="images/ai-nepi_007_slide_22.png" class="img-fluid" alt="Slide 22"> This slide, titled “Ex 3: Interaction of nodes learns complex language structure,” begins with a “Setup” section outlining two key points: first, that “Hierarchical structure in natural language is well suited to graph structures,” implying that language’s inherent organisation aligns well with graph representations. Second, it states the methodology: “Train a <em>GNN</em> on movie review sentiment task and extract walks,” indicating that a <em>GNN</em> is trained for sentiment classification, and then paths (walks) through the learned graph are used for explanation. The slide then presents an example sentence: “First I didn’t like the boring pictures, but it is certainly one of the best movies I have ever seen.” This sentence contains both negative and positive sentiment. Under the heading “high-order interactions” on the left, a graph diagram illustrates the <em>GNN</em>’s learned structure. Words are represented as nodes (ovals), and connections (edges) between them are coloured and weighted. Blue edges indicate a contribution to negative sentiment, whilst red edges indicate a contribution to positive sentiment, with thicker lines denoting stronger connections. The diagram clearly shows the sentence’s two clauses: the first clause (“First I didn’t like the boring pictures,”) is predominantly connected by blue edges, highlighting the negative sentiment associated with “didn’t like” and “boring pictures.” The second clause (“but it is certainly one of the best movies I have ever seen.”) is predominantly connected by red edges, emphasising the positive sentiment from phrases like “certainly one of the best movies” and “have ever seen.” This visualisation demonstrates how the <em>GNN</em> captures the structural interplay of words to determine sentiment. In contrast, the bottom section, labelled “standard explanations” and “<em>BoW</em>” (<em>Bag of Words</em>), shows the same sentence with individual words highlighted. Blue highlights indicate negative sentiment, and red highlights indicate positive sentiment. Words like “didn’t,” “like,” “boring,” and “pictures” are highlighted in blue, whilst words such as “certainly,” “best,” “movies,” and “seen” are highlighted in red. This <em>Bag-of-Words</em> approach attributes sentiment to individual words without explicitly showing their interdependencies. To the right of the <em>BoW</em> text, another graph diagram is presented, focusing on the positive part of the sentence (“of movies the best seen I have ever”). This diagram uses only red edges, reinforcing the positive sentiment of this phrase and possibly representing a more traditional parse tree. The overall comparison highlights that <em>GNNs</em>, by learning high-order interactions and graph structures, can provide more nuanced and context-aware explanations of sentiment compared to simpler <em>Bag-of-Words</em> models. The slide number “22” is in the bottom centre, and the citation “Schnake et al.&nbsp;(<em>TPAMI</em> ’22)” is in the bottom right corner.</p>
<p><img src="images/ai-nepi_007_slide_24.png" class="img-fluid" alt="Slide 24"> This slide, titled ‘Ex 4: Extracting visual definitions from corpora,’ with a subtitle ‘Math. instruments; <em>Sphaera Cor.</em> (Valleriani+’19)’, illustrates the application of <em>AI</em> in visual humanities. The left side of the slide displays three historical engravings, labelled A, B, and C. Image A depicts a ‘time measurement instrument (Cortés, 1556),’ which appears to be an ornate sundial or astrolabe. Image B shows a ‘device to measure ang. height of celestial bodies (Finé, 1587),’ illustrated as a geometric quadrant. Image C presents a ‘Machine to strike gold medals (Branca,1629),’ a detailed engraving of a large, gear-driven machine operated by a person in a workshop setting. The right side of the slide, under the heading ‘Class-specific Heatmap explanations,’ demonstrates how machine learning models interpret these images. Three smaller heatmaps at the top show abstract representations for ‘math. instrument,’ ‘machine,’ and ‘scientific illustration,’ indicating salient features for each category. Below these, a larger heatmap, specifically for the ‘machine’ class, is overlaid on a faded version of Image C. This heatmap uses red and blue gradients to highlight regions of the image that are most influential in its classification as a ‘machine.’ Three light blue circles on this heatmap point to specific components of the machine, labelled with their identified functions: ‘scaffolding’ for the upper structure, ‘support pole’ for a vertical element, and ‘footstand’ for its base. This visual analysis provides an explanation of the model’s reasoning. The footer indicates the source as ‘El-Hajj &amp; Eberle+, <em>Explainability and transparency in the realm of DH</em>. (Int J Digit Humanities ’23)’ and the page number ‘24’.</p>
<p><img src="images/ai-nepi_007_slide_27.png" class="img-fluid" alt="Slide 27"> This slide, titled ‘Verifying modelling and features using <em>XAI</em> and Historians’, is structured into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section outlines three key points: First, ‘Historical tables are carriers of scientific knowledge processes’, emphasising their importance as data sources. Second, the approach is to ‘Represent tables using bag of bigrams (e.g.&nbsp;’01’ or ‘21’)‘, indicating a method of feature extraction based on sequences of two characters or elements. Third, the system uses ’Limited annotations: learned feature extractor + hard-coded structure’, suggesting a hybrid approach combining machine learning with predefined rules. Visually, in the top right, a diagram illustrates a concept of feature interaction or relationship. It shows a dashed circle enclosing four nodes labelled ‘x₁’, ‘x₂’, ‘x₃’, and ‘x₄’. Nodes ‘x₁’ and ‘x₄’ are highlighted in red and connected by a red line, implying a detected relationship or a significant feature pair. The ‘Results’ section presents findings through various visual aids. On the bottom left, a scatter plot or network diagram shows a dense cluster of points, with arrows pointing from it to a small, old-looking table image. An arrow labelled ‘x’ points from the cluster to the table, and another labelled ‘x’’ points from the cluster to a specific region within the table. An ‘s’ label points to the overall cluster, suggesting the source data or a spatial representation. The small table image is a snippet from a historical document, with text ‘384 Comment. in III. Cap.’, ‘TABVLA ASCENS’, and ‘Obliquation.’. To the right of this, under the heading ‘Bigram Model’, two larger images of historical tables are displayed side-by-side. These tables contain columns with labels like ‘V’, ‘M’, ‘G. M.’, and ‘G.’, and rows of numbers including ‘0’, ‘II’, ‘38’, ‘22’, ‘12’, ‘4’, ‘44’, ‘30’, ‘61’. Red lines connect specific numerical entries across the two tables, visually demonstrating the bigram model’s ability to identify correspondences or relationships between elements in different tables or different parts of the same table. On the far right, two data visualisations present quantitative results. The first is a table titled ‘Histogram correlation’, with columns ‘Density’, ‘ρ’ (correlation coefficient), ‘Nbig’ (number of bigrams), and ‘Nuni’ (number of unigrams). It shows results for different data densities: ‘Low (≤150)’ with ρ=0.84, Nbig=493, Nuni=916; ‘Dense (150–300)’ with ρ=0.88, Nbig=786, Nuni=1501; and ‘Very dense (&gt;300)’ with ρ=0.93, Nbig=982, Nuni=1764. This indicates that correlation and the number of extracted features increase with data density. The second visualisation is a horizontal bar chart labelled ‘Cluster classification’ on the y-axis and ‘Cluster purity’ on the x-axis, ranging from 0.3 to 0.9. It compares the purity of clusters obtained using different methods: ‘Bigram’ (blue bar, highest purity, close to 0.9), ‘Pooled’ (grey bar), ‘Unigram’ (grey bar), and ‘<em>VGG-16</em>’ (grey bar, lowest purity, around 0.6). Error bars are shown for each method. This chart highlights the superior performance of the ‘Bigram’ model in achieving higher cluster purity. Two citations are listed at the bottom right: ‘Eberle et al.&nbsp;(<em>TPAMI</em> ’22)’ and ‘Eberle et al.&nbsp;(<em>Sci Adv</em> ’24)’, crediting the research to the authors. The slide number ‘27’ is in the bottom centre.</p>
<p><img src="images/ai-nepi_007_slide_30.png" class="img-fluid" alt="Slide 30"> This slide, titled ‘Cluster entropy analysis to investigate innovation’, is divided into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section describes the methodology: ‘- Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This indicates that the analysis quantifies the deviation of actual innovation clusters from a theoretical maximum dispersion, likely to understand the concentration or diffusion of innovation. The ‘Results’ section presents visual findings. On the left, a diagram illustrates the concept of ‘clustering’. It shows three distinct, layered clusters, each representing a geographic region associated with historical printing or publication. The top layer, depicted with black dots, is labelled ‘Lisbon’. The middle layer, shown with magenta dots, is labelled ‘Venice’. The bottom layer, with blue/purple dots, is labelled ‘Wittenberg’. An arrow points from the ‘clustering’ process to a bar chart labelled ‘table clusters’, which displays a series of vertical bars of varying heights, suggesting a quantitative representation of the characteristics or distribution of these clusters. On the right, a detailed geographical map of Europe displays numerous ‘<em>Sphaera</em> publication cities’ marked with red dots. The map spans from approximately 5.000°W to 15.000°E longitude and 40.000°N to 50.000°N latitude. Key cities labelled include: London, Leiden, Antwerp, Leuven, Cologne, Mainz, Frankfurt (Main), Neustadt an der Weinstraße, Paris, Strasbourg, Dijon, Basel, Lyon, Geneva, Saint Gervais, Avignon, Milan, Bologna, Ferrara, Florence, Perugia, Rome, Seville, Madrid, Alcalá de Henares, Salamanca, Valladolid, Coimbra, Lisbon, Wittenberg, Leipzig, Kraków, Vienna, Ingolstadt, Dillingen an der Donau, Heidelberg, Nuremberg, and Lemgo. A legend in the bottom right corner clarifies that the dots represent ‘<em>Sphaera</em> publication cities’ and notes the coordinate system ‘<em>EPSG:4326</em>’, along with a North arrow. This map provides the spatial context for the ‘print locations’ analysed for cluster entropy, visually connecting the abstract clusters to their real-world geographic distribution. The slide also includes a page number ‘30’ and a citation ‘Eberle et al.&nbsp;(<em>Sci Adv</em> ’24)’ at the bottom right, attributing the research to a publication by Eberle and colleagues in <em>Science Advances</em> from 2024.</p>
<p><img src="images/ai-nepi_007_slide_31.png" class="img-fluid" alt="Slide 31"> This slide, titled ‘Cluster entropy analysis to investigate innovation’, is divided into two main sections: ‘Setup’ and ‘Results’. The ‘Setup’ section defines the core metric used in the analysis: ‘Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This indicates that the study quantifies how far a given cluster’s entropy deviates from its theoretical maximum, likely as a measure of its structure or predictability. The ‘Results’ section presents the findings through two interconnected visual elements. On the left, a small bar chart labelled ‘table clusters’ is shown, depicting a distribution of cluster sizes or counts. An arrow labelled ‘clustering’ points from this bar chart towards a series of three layered 2D scatter plots, each representing a different city. The top layer is labelled ‘Lisbon’ and shows a cluster of black dots. The middle layer is labelled ‘Venice’ and displays a cluster of magenta dots. The bottom layer is labelled ‘Wittenberg’ and shows a cluster of purple dots. These scatter plots likely visualise the spatial distribution or characteristics of the ‘clusters’ being analysed. On the right, a bar chart displays the calculated entropy difference for numerous cities. The vertical y-axis is labelled ‘H(p) - H(p_max)’ and ranges from 0 down to -3. The horizontal x-axis lists various historical print locations vertically, including ‘Alcalá Hen.’, ‘Antwerp’, ‘Avignon’, ‘Basel’, ‘Coimbra’, ‘Cologne’, ‘Florence’, ‘Frankfurt/Main’, ‘Geneva’, ‘Heidelberg’, ‘Kraków’, ‘Leiden’, ‘Leipzig’, ‘Lemgo’, ‘Lisbon’, ‘London’, ‘Lyon’, ‘Madrid’, ‘Mainz’, ‘Mexico City’, ‘Neustadt/Wstr.’, ‘Paris’, ‘Rome’, ‘Seville’, ‘Siena’, ‘Sine loco’, ‘St.&nbsp;Gervais’, ‘Strasbourg’, ‘Valladolid’, ‘Venice’, ‘Vienna’, and ‘Wittenberg’. The bars are coloured either black or light grey, suggesting a categorisation or distinction between the cities based on their entropy difference values. For example, Lisbon, Florence, London, and Rome show large negative differences (black bars), whilst Venice and Wittenberg show smaller negative differences (grey bars). The footer indicates the source of the work as ‘Eberle et al.&nbsp;(<em>Sci Adv</em> ’24)’ and the slide number ‘31’.</p>
<p><img src="images/ai-nepi_007_slide_32.png" class="img-fluid" alt="Slide 32"> This slide, titled ‘Cluster entropy analysis to investigate innovation’, presents the methodology and key findings. The ‘Setup’ section defines the metric used: ‘Difference between the observed cluster entropy H(p) to maximum attainable entropy at this print location’. This implies a measure of how much the actual clustering deviates from a state of maximum possible disorder or randomness. The ‘Results’ section begins with a visual explanation of ‘clustering’. On the left, a small bar chart labelled ‘table clusters’ shows varying bar heights, likely representing the distribution or size of different clusters. Below this, three stacked 2D scatter plots illustrate different clustering patterns for specific cities: the top layer shows black points labelled ‘Lisbon’, the middle layer shows magenta points labelled ‘Venice’, and the bottom layer shows purple points labelled ‘Wittenberg’. An arrow points from the ‘clustering’ text to the top scatter plot, indicating these are examples of the clustering being analysed. The main visual element is a bar chart on the right, displaying ‘H(p) - H(p_max)’ on the y-axis, ranging from 0 down to -3. The x-axis lists numerous cities and locations, including ‘Alcalá Hen.’, ‘Antwerp’, ‘Avignon’, ‘Basel’, ‘Coimbra’, ‘Cologne’, ‘Florence’, ‘Frankfurt/Main’, ‘Geneva’, ‘Heidelberg’, ‘Kraków’, ‘Leiden’, ‘Leipzig’, ‘Lemgo’, ‘Lisbon’, ‘London’, ‘Lyon’, ‘Madrid’, ‘Mainz’, ‘Mexico City’, ‘Neustadt/Wstr.’, ‘Paris’, ‘Rome’, ‘Seville’, ‘Siena’, ‘Sine loco’, ‘St.&nbsp;Gervais’, ‘Strasbourg’, ‘Valladolid’, ‘Venice’, ‘Vienna’, and ‘Wittenberg’. Most bars are light grey, but two cities, ‘Frankfurt/Main’ and ‘Wittenberg’, are highlighted with black bars and blue outlines, indicating they are points of interest. These two cities exhibit the most negative values on the y-axis, suggesting their observed cluster entropy is significantly lower than the maximum attainable entropy, implying a high degree of structure or distinct clustering. Corresponding text below the chart provides context for these highlighted cities: ‘Frankfurt/Main: Centre for reprinting editions’ and ‘Wittenberg: Political control of the reformers’. This suggests a correlation between the entropy measure and the historical roles or characteristics of these locations. The slide includes a citation ‘Eberle et al.&nbsp;(<em>Sci Adv</em> ’24)’ and the page number ‘32’ in the bottom right.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter_ai-nepi_006.html" class="pagination-link" aria-label="The VERITRACE Project">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The VERITRACE Project</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter_ai-nepi_008.html" class="pagination-link" aria-label="LLM: Evolution of competence">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">LLM: Evolution of competence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>